\section{Introduction}
A number of different knowledge graphs have been constructed in recent years such as DBpedia \cite{dbpedia-swj}, Wikidata \cite{Vrandecic:2014:WFC:2661061.2629489}, Freebase \cite{Bollacker08freebase:a}, YAGO \cite{Suchanek:2007:YCS:1242572.1242667} and NELL \cite{Carlson:2010:TAN:2898607.2898816}, and these have provided significant advantages in a number of different application areas, such as search, semantic parsing, named entity disambiguation, information extraction, question answering and even image classification (see \cite{journals/tkde/WangMWG17} for instance, for a comprehensive review of the use of knowledge graph embeddings in applications).  Inspired by the value of these knowledge graphs for a variety of applications, we asked how one might build a knowledge graph in the domain of programs, rather than everyday facts.  There are a number of applications around code that could potentially benefit from such knowledge graphs, such as code search, code automation, refactoring, bug detection, and code optimization (\cite{Allamanis:2018:SML:3236632.3212695} for a review of a number of such applications).  Yet, there is no comprehensive resource such as a knowledge graph that can be leveraged in these applications as yet.  In this paper, we describe the specific challenges and solutions we came up with for building a knowledge graph for computer programs.  Specifically, we addressed three key questions outlined below.

\textit{What is a suitable representation for code in a knowledge graph?}
Our goal here is similar to what is typical of existing knowledge graphs - which is to extract the \textit{semantics} of code arifacts.  The first challenge is to determine what constitutes the `entities` of a code knowledge graph.  The semantics of code is often given by the use of library code simply because the use of libraries \textit{generalize} across user programs.  Capturing code semantics in terms of library usage has immediate benefits to many applications of code such as code search, code automation, refactoring etc. The 'entities' in our knowledge graph are therefore functions and classes of libraries that are used heavily in user code in the wild.  

How do we define the edges of a knowledge graph for code to capture program semantics?  For real world knowledge graphs, this amounts to associating textual elements that describe the `entities', along with properties defined often from semi-structured, tabular data.  For code, we extract textual elements that map natural language to code entities such as classes or functions to help define its semantics (e.g., documentation about specific functions or classes, Stack Overflow posts on web forums that describe its usage, etc).  In addition, we extract from code usage in the wild, program data and control flow, which specifies how libraries tend to be used in the wild.  We note that this method of representing programs is very powerful, and gets more at the semantics of code than relying on surface representations of code such as treating code as natural language tokens or using ASTs.  To compute this representation, we deploy program analysis techniques that are generalizable across programming languages, and we show how one might scale these techniques to millions of programs.  Of course, nothing precludes applications from using the surface representations of programs by themselves for a specific task.  However, we note that in many applications such as finding security bugs in code (\cite{DBLP:journals/corr/abs-1807-06756}), predicting variable names, method names, or types (\cite{DBLP:journals/corr/abs-1803-09544}), code summarization (\cite{DBLP:journals/corr/abs-1708-01837}), clone detection (\cite{White:2016:DLC:2970276.2970326}), predicting variable misuse, variable naming, deobfuscation, and method naming (\cite{DBLP:journals/corr/abs-1711-00740}, \cite{Bichsel:2016:SDA:2976749.2978422}, \cite{DBLP:journals/corr/abs-1808-01400}), data flow and control flow have been selectively added to surface representations of programs to gain a performance advantage.  Our approach provides this capability in a more general way, using state of the art techniques to follow object use across procedure calls, and to model heap usage. 

\textit{What are the concepts and properties in the knowledge graph?}
To formalize the semantics of types and properties in the knowledge graph, we re-used the PROV-O ontology \cite{733f89c65e4844f9aabcae1c276a5602} and schema.org \cite{Guha:2015:SES:2857274.2857276} to develop an ontology for representing code.  This step is key to understand for instance that a particular object for instance is used by another object in a function call as a specific named argument, or a particular question on stack overflow had as part of its answer a reference to a method from a specific library.  We developed an ontology to describe code and its associated artifacts such as stack overflow posts, with XXX concepts and YYY proproperties.

\textit{What is a useful interface for this knowledge graph?}
Inspired by Wikidata and Freebase, we wanted the knowledge graph to be \textit{extensible}, such that developers who mine more useful artifacts about code can feel free to publish those artifacts into the knowledge graph.  We adopted WhyIs \cite{} as our knowledge management and publishing framework because it provides three key valuable aspects to interfacing with the knowledge graph: (a) It provides views over the knowledge graph which can quickly help users of the graph get access to knowledge as designed for a specific application (e.g., say code search can extract a view over code and documentation with minimal knowledge of how the data is actually modeled across versions of code, etc)., (b) It provides inferencing capabilities over the knowledge graph, to allow a developer to plugin code to expand the graph (e.g., a recent paper analyzed code snippets in stack overflow posts to annotate a specific code snippet as a question; these could be 'added' to the entities existing in the graph), (c) It is designed to maintain provenance for each addition to the graph, which is critical if the graph can be extended by the community, and (d) It provides some basic entity resolution capabilities which allows entities mentioned in text to be linked directly.

We test these ideas by developing a comprehensive knowledge graph for 1.2 million Python programs on GitHub.  Our key contributions are as follows: (a) we describe a language independent approach to building the knowledge graph for code, although we apply it only to Python in this paper, (b) we open source a knowledge graph for Python programs, with XXX entities and YYYY edges, (b) for the popular libraries (defined by more than 1K imports of the library), we extracted documentation associated with functions and calls of the libraries, for a total of XXX functions and XXX classes across 404 libraries, (c) we extracted all stack overflow posts, and used search indexes to efficiently associate posts with specific functions, (d) we try to rigorously validate the different aspects of the knowledge graph that we built. 

