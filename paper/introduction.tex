\section{Introduction}
A number of different knowledge graphs have been constructed in recent years such as DBpedia \cite{dbpedia-swj}, Wikidata, Freebase \cite{Bollacker08freebase:a}, YAGO \cite{Suchanek:2007:YCS:1242572.1242667} and NELL \cite{Carlson:2010:TAN:2898607.2898816}, and these have been used successfully in a number of different applications, such as search, semantic parsing, named entity disambiguation, information extraction, question answering and even image classification (see \cite{journals/tkde/WangMWG17} for a comprehensive review of uses of knowledge graph embeddings in applications).  To our knowledge, no such knowledge graph exists for code, although it would be equally useful in driving applications around code such as code search, code automation, refactoring, bug detection, code optimization, etc.  In this paper, we describe a set of generic techniques to construct such a knowledge graph, and we apply it to mining knowledge graphs about Python code on the web.

We note that such a knowledge graph would be as useful in a variety of applications that center around code, such as code search, code refactoring, code automation, etc.  While some simpler applications can likely be performed with treating code as tokens of natural language (see \cite{Allamanis:2018:SML:3236632.3212695} for a comprehensive review), more complex tasks can benefit from using more structured representations of code.  Machine learning based approaches to finding security bugs in code for instance have focused on specialized subsets of the abstract syntax tree of the program to look for security vulnerabilities (\cite{DBLP:journals/corr/abs-1807-06756}).  Abstract syntax trees are also common for a whole host of tasks such as predicting variable names, method names, or types (\cite{DBLP:journals/corr/abs-1803-09544}), code summarization (\cite{DBLP:journals/corr/abs-1708-01837}) and clone detection (\cite{White:2016:DLC:2970276.2970326}).  Attempts have been made to enhance these ASTs with limited data flow or program dependence information to perform tasks such as predicting variable misuse, variable naming, deobfuscation, and method naming (\cite{DBLP:journals/corr/abs-1711-00740}, \cite{Bichsel:2016:SDA:2976749.2978422}, \cite{DBLP:journals/corr/abs-1808-01400}).   The focus of all this work has been abstractions of code applicable to each specific problem, thus there are numerous differing abstractions derived from ASTs, each evaluated for a specific problem.  

Our work focuses building a generic knowledge graph that can support multiple use cases.   We show how to construct a generic knowledge graph from code, and we validate that the graph captures semantics that generalizes well to unseen code, based on cross validation.  Further, we demonstrate that it can support multiple use cases using a standard declarative query language (SPARQL) over the graph.  As examples, we give concrete SPARQL queries for three disparate use cases: (a) detecting bad coding practices in code, (b) performing code completion, (c) helping automate hyper-parameter tuning for machine learning models.  

% - graphs for code require analysis  
%  - data and control flow crucial  
%  - complex flows require real analysis 
To support the widest variety of use cases, the knowledge graph needs to be constructed with whole program analysis, so another contribution of our work is that we use traditional program analysis techniques to build our knowledge graph.  As pointed out by Allamanis et al. (\cite{Allamanis:2018:SML:3236632.3212695}), building models of code has not generally been done with traditional program analysis frameworks, and the use of more sophisticated techniques such as interprocedural analysis has previously been limited.  Even when interprocedural analysis has been employed, there has been no work that has exploited traditional analysis frameworks to make aggressive use of advanced techniques like context-sensitive analysis.  This is despite the fact that in many cases, the amount of user code involved tends to be relatively modest and well within the range of known techniques.  

However, the application of traditional analysis techniques on dynamic languages such as Python is not trivial.  Simple interprocedural issues like determining the target of a method call become involved: in Python, there are no static types on variables to reveal method targets and, in any case, methods as well as fields can be assigned freely, thus making types themselves of limited value.  However, there is a long history of addressing such challenges in dynamic languges such as Self\cite{Agesen1995TypeIO,chambers1990iterative}, Scheme\cite{taminglambda}, and JavaScript\cite{DBLP:conf/icse/FeldthausSSDT13}, and we re-use the techniques developed there, as embodied in WALA, to create precise abstractions of code.  Similarly, Python code often makes heavy use of heap data structures, such a dictionaries, which greatly complicates precise tracking of data flow.  Once again, we re-use a range of powerful abstractions for modeling the heap that have been employed in a variety of analysis problems.  These are often based on potentially context-sensitive notions of allocation sites, which is especially applicable in languages with little notion of static type.  

% - programming makes heavy use of large apis
%  - JavaScript frameworks on the Web: jquery, angular, ...
%  - Python frameworks for data science: sklearn, numpy, ...
Perhaps the most challenging aspect of applying these techniques is that modern programs tend to be heavily based on a wide selection of rich APIs.  Data science libraries in Python for instance encapsulate a rich collection of APIs around commonly-needed mathematics and statistics functionality, as well as higher level functions needed to build complex neural networks.  At one level, these APIs simplify the code that a user needs to write by providing higher-level abstractions; on the other hand, these APIs can complicate analysis by providing a wide range of semantically complex functionality that grows and changes rapidly.

% - large apis challenge analysis
%  - too many, too large and too rapidly-changing to model accurately
%  - many applications have api calls woven throughout
So far, prior work has applied one of two basic options for handling these frameworks.  The simplest is to treat APIs just like user code and analyze all the code together.  In the data science domain, this is challenging to scale for three reasons:
\begin{itemize}
\item the APIs are much larger than the user program, and hence is challenging to precisely analyze
\item the library code includes many low level details that are irrelevant to user code
\item many of the API implementations are not even in the same language, with portions in C or Fortran.
\end{itemize}

The other common approach is to model APIs, writing formal descriptions that capture the semantics intended by the API.  This is challenging due to sheer scale: there are many relevant APIs, they are each large, and they grow and change rapidly.  As an example, in data science libraries, this would mean modeling over 11K calls.  This makes creating correct models a daunting task.  And modeling data flow through the APIs is crucial because user code often has API calls throughout; the code is mostly calling APIs and then using the returned values for further API calls.  Hence, data flow through API calls is needed to understand the flow through user code.

% - many applications focus on api usage
%  - key is to analyze existing api usage
%  - approximate modeling of apis is sufficient
Our contribution to this analysis problem is to build an abstraction of user code which captures its behavior but does not depend on the precise meaning of API code.  We observe how portions of the APIs fit together by analyzing their usage across a large corpus of code examples.  For instance, in data science, it is often the case that classifiers must be first created, trained and then used for classification.  We can observe patterns like that repeatedly in a large body of user code and learn to recognize it, all without any detailed semantics of what the API code actually does.  All we need is to coarsely capture the fact that something flows from the creation to the training and classification calls, and the control flow constraints between them.  All of that is present in the user code, as long as we capture that the creation call returns something.  We call our approach \textit{turtles}\footnote{from the famous quote ``turtles all the way down''}: any top level call to an API returns an object, a turtle, and subsequent calls on that turtle return another turtle.  Field accesses on a turtle return the same turtle.  This captures the fact there is dataflow through the API.
 
% - results show approximate models work
% - graph captures real code well
We perform this \textit{turtle} based analysis of approximate 9K data science Python files, extracted from GitHub.  We abstract a knowledge graph of 16M edges from this analysis, and test its coverage and its generalizability.  We then illustrate how this knowledge graph can be used with a standard query language to support disparate use cases.  The paper is organized as follows.  Section 2 describes related work, Section 3 gives a running example used to illustrate the analysis techniques in the paper, Section 4 describes how we use the \textit{turtle} analysis techniques to construct the knowledge graph, Section 5 evaluates the quality of the knowledge graph, and Section 6 shows how the knowledge graph may be used using a standard query language.

\begin{figure*}[htb]
\centering 
{\renewcommand\thelstnumber{%
\ifnum\value{lstnumber}>29\relax \the\numexpr 
278+\value{lstnumber}\relax\else 
\ifnum\value{lstnumber}>25\relax \the\numexpr 
139+\value{lstnumber}\relax\else 
\ifnum\value{lstnumber}>22\relax \the\numexpr 
132+\value{lstnumber}\relax\else 
\ifnum\value{lstnumber}>20\relax \the\numexpr 
130+\value{lstnumber}\relax\else 
\ifnum\value{lstnumber}>7\relax \the\numexpr 
92+\value{lstnumber}\relax\else 
\the\numexpr 
6+\value{lstnumber}\fi\fi\fi\fi\fi}
\lstinputlisting[language=Python,escapechar=|]{./example.py}}
\caption{Code example from GitHub}
\label{running_example}
\end{figure*}
