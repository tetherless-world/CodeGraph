#!/usr/bin/env python
# coding: utf-8

# **[Deep Learning Course Home Page](https://www.kaggle.com/learn/deep-learning)**
# 
# ---
# 

# # Intro
# 
# At the end of this lesson, you will understand how stochastic gradient descent and back-propagation are used to set the weights in a deep learning model. These topics are complex, but many experts view them as the most important ideas in deep learning.
# 
# # Lesson
# 

# In[ ]:


from IPython.display import YouTubeVideo
YouTubeVideo('kQmHaI5Jw1c', width=800, height=450)

# **Links Mentioned**
# 
# **[ReLU activation function](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)**
# 
# # Keep Going
# Now you are ready to **[train your own models from scratch](https://www.kaggle.com/dansbecker/deep-learning-from-scratch).**
# 
# ---
# 

# ---
# **[Deep Learning Course Home Page](https://www.kaggle.com/learn/deep-learning)**
# 
# 
