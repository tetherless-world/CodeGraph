#!/usr/bin/env python
# coding: utf-8

# The public kernals are filled with Blends of Blends of Blends and maybe someone can blend to 1.00000 too. 
# 
# Ok, Nothing wrong in that. I just thought good solid kernals should not be lost within the chaos.  
# 
# So, I've compiled and categorized a list of resources that have taught something to me. Please add on to this list in the comments below, if I've missed any good kernals.
# 
# ### Simple Naive-Bayes:
# * https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline
#     * https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline#261316          # Custom SK learn class for Naive Bayes
# 
# ### Tf-IDF and simple Logistic Regression:
# 
# * https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams            # ngram range=(2,6)   -->    can capture a lot of information!
# * https://www.kaggle.com/yekenot/toxic-regression
# 
# ### Wordbatch:
# * https://www.kaggle.com/anttip/wordbatch-1-3-3-fm-ftrl-lb-0-9812                          # FM_FTRL
# 
# ### H20, Word2Vec in R:
# * https://www.kaggle.com/brandenkmurray/h2o-word2vec-starter-toxic-comments
# 
# # Deep learning:
# ![](https://pbs.twimg.com/media/DWmKGuAXkAE9IXf.jpg)
# ### GRU(Gated Recurrent Units):
# * https://www.kaggle.com/yekenot/pooled-gru-fasttext/code
# 
# ### Capsule Net(with GRU):
# * https://www.kaggle.com/chongjiujjin/capsule-net-with-gru
# * https://github.com/Godricly/comment_toxic
#     * https://github.com/bojone/Capsule/blob/master/Capsule_Keras.py                   # Base implementations of CapsNet in Keras
#     * https://github.com/XifengGuo/CapsNet-Keras
#     
# 
# ### LSTM:
# * https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras                             # Explains basic of LSTM well
# * https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-069                         # Simple baseline
# * https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout                              # Glove pretrained features + LSTM
# 
# ### Attention:
# * https://www.kaggle.com/sermakarevich/hierarchical-attention-network
# * https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043                      # LSTM + Attention layer
# 
# ### Convolution(CNNs):
# 
# * https://www.kaggle.com/yekenot/textcnn-2d-convolution
# * https://www.kaggle.com/sbongo/for-beginners-go-even-deeper-with-char-gram-cnn
# 
# ### Bi Directional GRU CNNs:
# * https://www.kaggle.com/konohayui/bi-gru-cnn-poolings
# * https://www.kaggle.com/eashish/bidirectional-gru-with-convolution
# 
# 
# ## Blending Helpers:
#  What is blending?
#  * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51058
#  
#  What all do I need to check before blending?
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50827            ## Model correlations
# * https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/49964          ## Class leakage in level 0 models
# 
# ### OOF stackering:
# * https://www.kaggle.com/hhstrand/oof-stacking-regime
# 
# ### Stacker scrpits:
# * https://www.kaggle.com/reppic/lazy-ensembling-algorithm
# 
# ### Covariance shift / Adversarial Validation: ( By how much test is different than train?)
# * https://www.kaggle.com/ogrellier/check-unicode-script-distribution
# * https://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup
# * https://www.kaggle.com/konradb/adversarial-validation
# 
# 
# 
# # Others:
# 
# ### Creating more data:
# Using translations
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038
# 
# 
# ### Study materials:
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46073#latest-277688
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46038
# 
# ### Spell checker:
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51426
# 
# ### Tuning:
# Tuning DL models :
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/50602
# 
# And a company good at Marketing.
# * https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/47172
# 
# Good pipeline though.
# https://github.com/neptune-ml/kaggle-toxic-starter
# 
# 
# # And finally here's my blender: 

# ![](https://s.financesonline.com/uploads/blender-1024x823.jpg)
# 
# [Ahem](https://financesonline.com/10-most-expensive-kitchen-appliances-super-expensive-corkscrews-coffee-machines/)

# PS: Not to hurt the other blender's feelings but this one is the best! 

# ![](http://i0.kym-cdn.com/photos/images/facebook/001/240/075/90f.png)

# 
