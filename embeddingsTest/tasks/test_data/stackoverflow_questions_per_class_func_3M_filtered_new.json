{"head": {"vars": ["class_func", "class_func_label", "class_func_type", "docstr", "q", "title", "qVoteCount", "content", "answer"]}, "results": {"bindings": [{"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47316917"}, "title": {"type": "literal", "value": "How to properly get table-row indexes and named values from trapped var-binds in pysnmp"}, "content": {"type": "literal", "value": "<p>I'm trying to keep my code as clean as possible but I'm not completely satisfied with what I achieved so far.</p>\n\n<p>I built a SNMP manager which receive traps from another device using a custom MIB, which I will refer to as MY-MIB.</p>\n\n<p>I am not sure this is the cleanest way, but essentially I have:</p>\n\n<pre><code>from pysnmp.entity import engine, config\nfrom pysnmp.carrier.asynsock.dgram import udp\nfrom pysnmp.entity.rfc3413 import ntfrcv, context\nfrom pysnmp.smi import builder, rfc1902\nfrom pysnmp.smi.view import MibViewController\nfrom pysnmp.entity.rfc3413 import mibvar\n\n_snmp_engine = engine.SnmpEngine()\n_snmpContext = context.SnmpContext(_snmpEngine)\n_mibBuilder = _snmpContext.getMibInstrum().getMibBuilder()\n#Add local path where MY-MIB is located\n_mibSources = _mibBuilder.getMibSources() + (builder.DirMibSource('.'),)\n_mibBuilder.setMibSources(*mibSources)\n_mibBuilder.loadModules('MY-MIB')\n_view_controller = MibViewController(_mibBuilder)    \n\n\ndef my_callback_trap_processor(snmp_engine, state_reference,\n                                   context_id, context_name, var_binds, ctx):\n\n    #...CALLBACK CODE...\n\n\nconfig.addV1System(snmp_engine, 'my-area', 'MYCOMMUNITY')\nconfig.addTargetParams(snmp_engine, 'my-creds', 'my-area',\n                       'noAuthNoPriv', 1)\n\nconfig.addSocketTransport(snmp_engine,\n                          udp.domainName + (1,),\n                          udp.UdpTransport().openServerMode((IP_ADDRESS,\n                                                             PORT)))\n\nntfrcv.NotificationReceiver(snmp_engine, my_callback_trap_processor)\n\nsnmp_engine.transportDispatcher.jobStarted(1)\n\ntry:\n    snmp_engine.transportDispatcher.runDispatcher()\nexcept:\n    snmp_engine.transportDispatcher.closeDispatcher()\n    raise\n</code></pre>\n\n<p>In the callback function above I can get a pretty intelligible print by just using the following code:</p>\n\n<pre><code>    varBinds = [rfc1902.ObjectType(rfc1902.ObjectIdentity(x[0]), x[1]).resolveWithMib(_view_controller) for x in var_binds]\n    for varBind in varBinds:\n        print(varBind.prettyPrint())\n</code></pre>\n\n<p>which, from a given trap that I receive, gives me:</p>\n\n<pre><code>SNMPv2-MIB::sysUpTime.0 = 0\nSNMPv2-MIB::snmpTrapOID.0 = MY-MIB::myNotificationType\nMY-MIB::myReplyKey.47746.\"ABC\" = 0x00000000000000000000000000000000000\nMY-MIB::myTime.0 = 20171115131544Z\nMY-MIB::myOperationMode.0 = 'standalone'\n</code></pre>\n\n<p>Nice. But I want to manipulate/dissect each bit of information from the given var-binds, especially in a higher level way.</p>\n\n<p>Looking at the innards of the library I was able to gather this code up:</p>\n\n<pre><code>for varBind in var_binds:   \n    objct = rfc1902.ObjectIdentity(varBind[0]).resolveWithMib(self._view_controller)\n    (symName, modName), indices = mibvar.oidToMibName(\n                self._view_controller, objct.getOid()\n                )\n    print(symName, modName, indices, varBind[1])\n</code></pre>\n\n<p>that gives me:</p>\n\n<pre><code>sysUpTime SNMPv2-MIB (Integer(0),) 0\nsnmpTrapOID SNMPv2-MIB (Integer(0),) 1.3.6.1.X.Y.Z.A.B.C.D\nmyReplyKey MY-MIB (myTimeStamp(47746), myName(b'X00080')) 0x00000000000000000000000000000000000\nmyTime MY-MIB (Integer(0),) 20171115131544Z\nmyOperationMode MY-MIB (Integer(0),) 1\n</code></pre>\n\n<p>and in the case of myReplyKey indexes I can just do a:</p>\n\n<pre><code>    for idx in indices:\n        try:\n            print(idx.getValue())\n        except AttributeError:\n            print(int(idx))\n</code></pre>\n\n<p>But in the case of the <code>myOperationMode</code> var-bind, how do I get the named-value <code>'standalone'</code> instead of <code>1</code>? And how to get the names of the indexes (<code>myTimeStamp</code> and <code>myName</code>)?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>After Ilya's suggestions I researched the library a little bit more for getting the namedValues and, also, I used some Python hacking to get what I was looking for on the indices.</p>\n\n<pre><code>varBinds = [rfc1902.ObjectType(rfc1902.ObjectIdentity(x[0]), x[1]).resolveWithMib(_view_controller) for x in var_binds]\nprocessed_var_binds = []\nfor var_bind in resolved_var_binds:   \n\n    object_identity, object_value = var_bind\n    mod_name, var_name, indices = object_identity.getMibSymbol()\n\n    var_bind_dict = {'mib': mod_name, 'name': var_name, 'indices': {}}\n\n    for idx in indices:\n        try:\n            value = idx.getValue()\n        except AttributeError:\n            var_bind_dict['indices'] = int(idx.prettyPrint())\n        else:\n            var_bind_dict['indices'][type(value).__name__] = str(value)\n\n    try:\n        var_bind_dict['value'] = object_value.namedValues[object_value]\n    except (AttributeError, KeyError):\n        try:\n            var_bind_dict['value'] = int(object_value.prettyPrint())\n        except ValueError:\n            var_bind_dict['value'] = object_value.prettyPrint()\n\n    processed_var_binds.append(var_bind_dict)\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre><code>from pysnmp.smi.rfc1902 import *\n\nvar_binds = [ObjectType(ObjectIdentity(x[0]), x[1]).resolveWithMib(mibViewController)\n            for x in var_binds]\n</code></pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The <code>ObjectType</code> instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre><code>var_bind = var_binds[0]\nobject_identity, object_value = var_bind\n</code></pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as <code>object_value</code>:</p>\n\n<pre><code>&gt;&gt;&gt; object_identity.getMibSymbol()\n('SNMPv2-MIB', 'sysDescr', (0,))\n</code></pre>\n\n<p>The enumeration, should it present, is reported by <code>.prettyPrint()</code>:</p>\n\n<pre><code>&gt;&gt;&gt; from pysnmp.proto.rfc1902 import *\n&gt;&gt;&gt; Error = Integer.withNamedValues(**{'disk-full': 1, 'no-disk': -1})\n&gt;&gt;&gt; error = Error(1)\n&gt;&gt;&gt; error.prettyPrint()\n'disk-full'\n&gt;&gt;&gt; int(error)\n1\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre><code>from pysnmp.smi.rfc1902 import *\n\nvar_binds = [ObjectType(ObjectIdentity(x[0]), x[1]).resolveWithMib(mibViewController)\n            for x in var_binds]\n</code></pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The <code>ObjectType</code> instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre><code>var_bind = var_binds[0]\nobject_identity, object_value = var_bind\n</code></pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as <code>object_value</code>:</p>\n\n<pre><code>&gt;&gt;&gt; object_identity.getMibSymbol()\n('SNMPv2-MIB', 'sysDescr', (0,))\n</code></pre>\n\n<p>The enumeration, should it present, is reported by <code>.prettyPrint()</code>:</p>\n\n<pre><code>&gt;&gt;&gt; from pysnmp.proto.rfc1902 import *\n&gt;&gt;&gt; Error = Integer.withNamedValues(**{'disk-full': 1, 'no-disk': -1})\n&gt;&gt;&gt; error = Error(1)\n&gt;&gt;&gt; error.prettyPrint()\n'disk-full'\n&gt;&gt;&gt; int(error)\n1\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I'm trying to keep my code as clean as possible but I'm not completely satisfied with what I achieved so far.</p>\n\n<p>I built a SNMP manager which receive traps from another device using a custom MIB, which I will refer to as MY-MIB.</p>\n\n<p>I am not sure this is the cleanest way, but essentially I have:</p>\n\n<pre> </pre>\n\n<p>In the callback function above I can get a pretty intelligible print by just using the following code:</p>\n\n<pre> </pre>\n\n<p>which, from a given trap that I receive, gives me:</p>\n\n<pre> </pre>\n\n<p>Nice. But I want to manipulate/dissect each bit of information from the given var-binds, especially in a higher level way.</p>\n\n<p>Looking at the innards of the library I was able to gather this code up:</p>\n\n<pre> </pre>\n\n<p>that gives me:</p>\n\n<pre> </pre>\n\n<p>and in the case of myReplyKey indexes I can just do a:</p>\n\n<pre> </pre>\n\n<p>But in the case of the   var-bind, how do I get the named-value   instead of  ? And how to get the names of the indexes (  and  )?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>After Ilya's suggestions I researched the library a little bit more for getting the namedValues and, also, I used some Python hacking to get what I was looking for on the indices.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre> </pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The   instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre> </pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as  :</p>\n\n<pre> </pre>\n\n<p>The enumeration, should it present, is reported by  :</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47316917"}, "title": {"type": "literal", "value": "How to properly get table-row indexes and named values from trapped var-binds in pysnmp"}, "content": {"type": "literal", "value": "<p>I'm trying to keep my code as clean as possible but I'm not completely satisfied with what I achieved so far.</p>\n\n<p>I built a SNMP manager which receive traps from another device using a custom MIB, which I will refer to as MY-MIB.</p>\n\n<p>I am not sure this is the cleanest way, but essentially I have:</p>\n\n<pre><code>from pysnmp.entity import engine, config\nfrom pysnmp.carrier.asynsock.dgram import udp\nfrom pysnmp.entity.rfc3413 import ntfrcv, context\nfrom pysnmp.smi import builder, rfc1902\nfrom pysnmp.smi.view import MibViewController\nfrom pysnmp.entity.rfc3413 import mibvar\n\n_snmp_engine = engine.SnmpEngine()\n_snmpContext = context.SnmpContext(_snmpEngine)\n_mibBuilder = _snmpContext.getMibInstrum().getMibBuilder()\n#Add local path where MY-MIB is located\n_mibSources = _mibBuilder.getMibSources() + (builder.DirMibSource('.'),)\n_mibBuilder.setMibSources(*mibSources)\n_mibBuilder.loadModules('MY-MIB')\n_view_controller = MibViewController(_mibBuilder)    \n\n\ndef my_callback_trap_processor(snmp_engine, state_reference,\n                                   context_id, context_name, var_binds, ctx):\n\n    #...CALLBACK CODE...\n\n\nconfig.addV1System(snmp_engine, 'my-area', 'MYCOMMUNITY')\nconfig.addTargetParams(snmp_engine, 'my-creds', 'my-area',\n                       'noAuthNoPriv', 1)\n\nconfig.addSocketTransport(snmp_engine,\n                          udp.domainName + (1,),\n                          udp.UdpTransport().openServerMode((IP_ADDRESS,\n                                                             PORT)))\n\nntfrcv.NotificationReceiver(snmp_engine, my_callback_trap_processor)\n\nsnmp_engine.transportDispatcher.jobStarted(1)\n\ntry:\n    snmp_engine.transportDispatcher.runDispatcher()\nexcept:\n    snmp_engine.transportDispatcher.closeDispatcher()\n    raise\n</code></pre>\n\n<p>In the callback function above I can get a pretty intelligible print by just using the following code:</p>\n\n<pre><code>    varBinds = [rfc1902.ObjectType(rfc1902.ObjectIdentity(x[0]), x[1]).resolveWithMib(_view_controller) for x in var_binds]\n    for varBind in varBinds:\n        print(varBind.prettyPrint())\n</code></pre>\n\n<p>which, from a given trap that I receive, gives me:</p>\n\n<pre><code>SNMPv2-MIB::sysUpTime.0 = 0\nSNMPv2-MIB::snmpTrapOID.0 = MY-MIB::myNotificationType\nMY-MIB::myReplyKey.47746.\"ABC\" = 0x00000000000000000000000000000000000\nMY-MIB::myTime.0 = 20171115131544Z\nMY-MIB::myOperationMode.0 = 'standalone'\n</code></pre>\n\n<p>Nice. But I want to manipulate/dissect each bit of information from the given var-binds, especially in a higher level way.</p>\n\n<p>Looking at the innards of the library I was able to gather this code up:</p>\n\n<pre><code>for varBind in var_binds:   \n    objct = rfc1902.ObjectIdentity(varBind[0]).resolveWithMib(self._view_controller)\n    (symName, modName), indices = mibvar.oidToMibName(\n                self._view_controller, objct.getOid()\n                )\n    print(symName, modName, indices, varBind[1])\n</code></pre>\n\n<p>that gives me:</p>\n\n<pre><code>sysUpTime SNMPv2-MIB (Integer(0),) 0\nsnmpTrapOID SNMPv2-MIB (Integer(0),) 1.3.6.1.X.Y.Z.A.B.C.D\nmyReplyKey MY-MIB (myTimeStamp(47746), myName(b'X00080')) 0x00000000000000000000000000000000000\nmyTime MY-MIB (Integer(0),) 20171115131544Z\nmyOperationMode MY-MIB (Integer(0),) 1\n</code></pre>\n\n<p>and in the case of myReplyKey indexes I can just do a:</p>\n\n<pre><code>    for idx in indices:\n        try:\n            print(idx.getValue())\n        except AttributeError:\n            print(int(idx))\n</code></pre>\n\n<p>But in the case of the <code>myOperationMode</code> var-bind, how do I get the named-value <code>'standalone'</code> instead of <code>1</code>? And how to get the names of the indexes (<code>myTimeStamp</code> and <code>myName</code>)?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>After Ilya's suggestions I researched the library a little bit more for getting the namedValues and, also, I used some Python hacking to get what I was looking for on the indices.</p>\n\n<pre><code>varBinds = [rfc1902.ObjectType(rfc1902.ObjectIdentity(x[0]), x[1]).resolveWithMib(_view_controller) for x in var_binds]\nprocessed_var_binds = []\nfor var_bind in resolved_var_binds:   \n\n    object_identity, object_value = var_bind\n    mod_name, var_name, indices = object_identity.getMibSymbol()\n\n    var_bind_dict = {'mib': mod_name, 'name': var_name, 'indices': {}}\n\n    for idx in indices:\n        try:\n            value = idx.getValue()\n        except AttributeError:\n            var_bind_dict['indices'] = int(idx.prettyPrint())\n        else:\n            var_bind_dict['indices'][type(value).__name__] = str(value)\n\n    try:\n        var_bind_dict['value'] = object_value.namedValues[object_value]\n    except (AttributeError, KeyError):\n        try:\n            var_bind_dict['value'] = int(object_value.prettyPrint())\n        except ValueError:\n            var_bind_dict['value'] = object_value.prettyPrint()\n\n    processed_var_binds.append(var_bind_dict)\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre><code>from pysnmp.smi.rfc1902 import *\n\nvar_binds = [ObjectType(ObjectIdentity(x[0]), x[1]).resolveWithMib(mibViewController)\n            for x in var_binds]\n</code></pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The <code>ObjectType</code> instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre><code>var_bind = var_binds[0]\nobject_identity, object_value = var_bind\n</code></pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as <code>object_value</code>:</p>\n\n<pre><code>&gt;&gt;&gt; object_identity.getMibSymbol()\n('SNMPv2-MIB', 'sysDescr', (0,))\n</code></pre>\n\n<p>The enumeration, should it present, is reported by <code>.prettyPrint()</code>:</p>\n\n<pre><code>&gt;&gt;&gt; from pysnmp.proto.rfc1902 import *\n&gt;&gt;&gt; Error = Integer.withNamedValues(**{'disk-full': 1, 'no-disk': -1})\n&gt;&gt;&gt; error = Error(1)\n&gt;&gt;&gt; error.prettyPrint()\n'disk-full'\n&gt;&gt;&gt; int(error)\n1\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre><code>from pysnmp.smi.rfc1902 import *\n\nvar_binds = [ObjectType(ObjectIdentity(x[0]), x[1]).resolveWithMib(mibViewController)\n            for x in var_binds]\n</code></pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The <code>ObjectType</code> instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre><code>var_bind = var_binds[0]\nobject_identity, object_value = var_bind\n</code></pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as <code>object_value</code>:</p>\n\n<pre><code>&gt;&gt;&gt; object_identity.getMibSymbol()\n('SNMPv2-MIB', 'sysDescr', (0,))\n</code></pre>\n\n<p>The enumeration, should it present, is reported by <code>.prettyPrint()</code>:</p>\n\n<pre><code>&gt;&gt;&gt; from pysnmp.proto.rfc1902 import *\n&gt;&gt;&gt; Error = Integer.withNamedValues(**{'disk-full': 1, 'no-disk': -1})\n&gt;&gt;&gt; error = Error(1)\n&gt;&gt;&gt; error.prettyPrint()\n'disk-full'\n&gt;&gt;&gt; int(error)\n1\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I'm trying to keep my code as clean as possible but I'm not completely satisfied with what I achieved so far.</p>\n\n<p>I built a SNMP manager which receive traps from another device using a custom MIB, which I will refer to as MY-MIB.</p>\n\n<p>I am not sure this is the cleanest way, but essentially I have:</p>\n\n<pre> </pre>\n\n<p>In the callback function above I can get a pretty intelligible print by just using the following code:</p>\n\n<pre> </pre>\n\n<p>which, from a given trap that I receive, gives me:</p>\n\n<pre> </pre>\n\n<p>Nice. But I want to manipulate/dissect each bit of information from the given var-binds, especially in a higher level way.</p>\n\n<p>Looking at the innards of the library I was able to gather this code up:</p>\n\n<pre> </pre>\n\n<p>that gives me:</p>\n\n<pre> </pre>\n\n<p>and in the case of myReplyKey indexes I can just do a:</p>\n\n<pre> </pre>\n\n<p>But in the case of the   var-bind, how do I get the named-value   instead of  ? And how to get the names of the indexes (  and  )?</p>\n\n<p><strong>Update:</strong></p>\n\n<p>After Ilya's suggestions I researched the library a little bit more for getting the namedValues and, also, I used some Python hacking to get what I was looking for on the indices.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>To resolve SNMP PDU var-bindings against a MIB you can use <a href=\"http://snmplabs.com/pysnmp/examples/smi/manager/browsing-mib-tree.html#pdu-var-binds-to-mib-objects\" rel=\"nofollow noreferrer\">this snippet</a> what I think you have done already:</p>\n\n<pre> </pre>\n\n<p>By this point you have a list of <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">rfc1902.ObjectType</a> objects. The   instance mimics a two-element tuple: <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> and <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value object</a>.</p>\n\n<pre> </pre>\n\n<p>Now, <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getMibSymbol\" rel=\"nofollow noreferrer\">getMibSymbol()</a> will give you MIB name, MIB object name and the tuple of indices made up from the trailing part of the OID. Index elements are <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#snmp-base-types\" rel=\"nofollow noreferrer\">SNMP value objects</a> just as  :</p>\n\n<pre> </pre>\n\n<p>The enumeration, should it present, is reported by  :</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/getaddrinfo"}, "class_func_label": {"type": "literal", "value": "getaddrinfo"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nResolve host and port into list of address info entries.\n\nTranslate the host/port argument into a sequence of 5-tuples that contain\nall the necessary arguments for creating a socket connected to that service.\nhost is a domain name, a string representation of an IPv4/v6 address or\nNone. port is a string service name such as 'http', a numeric port number or\nNone. By passing None as the value of host and port, you can pass NULL to\nthe underlying C API.\n\nThe family, type and proto arguments can be optionally specified in order to\nnarrow the list of addresses returned. Passing zero as a value for each of\nthese arguments selects the full range of results."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42398808"}, "title": {"type": "literal", "value": "How do I generate a Set Command to my proprietary MIB"}, "content": {"type": "literal", "value": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdgen.CommandGenerator().getCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360)\n )\n\nprint (varBinds)\n\nprint (varBinds[0])\n\nprint (varBinds[0][0])\n\nprint (varBinds[0][1])\n</code></pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, rspVarBinds = cmdgen.CommandGenerator().setCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360),\n        (1),\n )\n</code></pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre><code>from pysnmp.hlapi import SnmpEngine, setCmd, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity\n\nfrom pysnmp.proto.api.v2c import Integer32\nsetCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget('10.0.1.134', 161),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.4.1.4515.1.8.1.1.1.8.1295360.1295360'), Integer32(1)))\n</code></pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n\n\n<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre> </pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre> </pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre> </pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n", "answer_wo_code": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre> </pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre> </pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n\n\n<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre> </pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre> </pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42398808"}, "title": {"type": "literal", "value": "How do I generate a Set Command to my proprietary MIB"}, "content": {"type": "literal", "value": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdgen.CommandGenerator().getCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360)\n )\n\nprint (varBinds)\n\nprint (varBinds[0])\n\nprint (varBinds[0][0])\n\nprint (varBinds[0][1])\n</code></pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, rspVarBinds = cmdgen.CommandGenerator().setCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360),\n        (1),\n )\n</code></pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre><code>from pysnmp.hlapi import SnmpEngine, setCmd, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity\n\nfrom pysnmp.proto.api.v2c import Integer32\nsetCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget('10.0.1.134', 161),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.4.1.4515.1.8.1.1.1.8.1295360.1295360'), Integer32(1)))\n</code></pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre> </pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre> </pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre> </pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n", "answer_wo_code": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre> </pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre> </pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sys.exc_info"}, "class_func_label": {"type": "literal", "value": "sys.exc_info"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "exc_info() -> (type, value, traceback)\n\nReturn information about the most recent exception caught by an except\nclause in the current stack frame or in an older stack frame."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42398808"}, "title": {"type": "literal", "value": "How do I generate a Set Command to my proprietary MIB"}, "content": {"type": "literal", "value": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdgen.CommandGenerator().getCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360)\n )\n\nprint (varBinds)\n\nprint (varBinds[0])\n\nprint (varBinds[0][0])\n\nprint (varBinds[0][1])\n</code></pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, rspVarBinds = cmdgen.CommandGenerator().setCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360),\n        (1),\n )\n</code></pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre><code>from pysnmp.hlapi import SnmpEngine, setCmd, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity\n\nfrom pysnmp.proto.api.v2c import Integer32\nsetCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget('10.0.1.134', 161),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.4.1.4515.1.8.1.1.1.8.1295360.1295360'), Integer32(1)))\n</code></pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre> </pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre> </pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre> </pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n", "answer_wo_code": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre> </pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre> </pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/socket.getaddrinfo"}, "class_func_label": {"type": "literal", "value": "socket.getaddrinfo"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "Resolve host and port into list of address info entries.\n\nTranslate the host/port argument into a sequence of 5-tuples that contain\nall the necessary arguments for creating a socket connected to that service.\nhost is a domain name, a string representation of an IPv4/v6 address or\nNone. port is a string service name such as 'http', a numeric port number or\nNone. By passing None as the value of host and port, you can pass NULL to\nthe underlying C API.\n\nThe family, type and proto arguments can be optionally specified in order to\nnarrow the list of addresses returned. Passing zero as a value for each of\nthese arguments selects the full range of results."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42398808"}, "title": {"type": "literal", "value": "How do I generate a Set Command to my proprietary MIB"}, "content": {"type": "literal", "value": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdgen.CommandGenerator().getCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360)\n )\n\nprint (varBinds)\n\nprint (varBinds[0])\n\nprint (varBinds[0][0])\n\nprint (varBinds[0][1])\n</code></pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\nerrorIndication, errorStatus, errorIndex, rspVarBinds = cmdgen.CommandGenerator().setCmd(\n        cmdgen.CommunityData('10.0.1.134', 'admin', 0),\n        cmdgen.UdpTransportTarget(('10.0.1.134', 161)),\n        (1,3,6,1,4,1,4515,1,8,1,1,1,8,1295360,1295360),\n        (1),\n )\n</code></pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre><code>from pysnmp.hlapi import SnmpEngine, setCmd, CommunityData, UdpTransportTarget, ContextData, ObjectType, ObjectIdentity\n\nfrom pysnmp.proto.api.v2c import Integer32\nsetCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget('10.0.1.134', 161),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.4.1.4515.1.8.1.1.1.8.1295360.1295360'), Integer32(1)))\n</code></pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('1.3.6.1.2.1.1.9.0'), Integer32(1)))\n</code></pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre><code>setCmd(SnmpEngine(),\n       CommunityData('public', mpModel=0),\n       UdpTransportTarget(('demo.snmplabs.com', 161)),\n       ContextData(),\n       ObjectType(ObjectIdentity('MY-CUSTOM-MIB', 'myCustomVariable', 0), 'up'))\n</code></pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I finally managed to generate a Get Command to my proprietary MIB with the following python script:</p>\n\n<pre> </pre>\n\n<p>The varBinds print in 4 different ways was just for the learning process.\nNow I have tried to imitate the same script while turning it to a Set Command (without any success). There are lots of examples to get but non fitted. So, I have tried the following:</p>\n\n<pre> </pre>\n\n<p>I didn't succeed in generating the Set Command as you can see. And i don't see how can I get it to work. By the way, the OID that I am setting is a read-write, Integer (32 bit) with the following possible values: up(1), down(2) and standby(3).</p>\n\n<p>What do i need to change in order to succeed in Setting the OID with any of the possible values ?</p>\n\n<p>I have tried your suggested script and failed:</p>\n\n<pre> </pre>\n\n<p>This is the error messages that I got:    </p>\n\n<p>**Traceback (most recent call last): \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 56, in _resolveAddr socket.IPPROTO_UDP)[0][4][:2] \nFile \"C:\\Program Files\\Python35\\lib\\socket.py\", line 732, in getaddrinfo for res in _socket.getaddrinfo(host, port, family, type, proto, flags): socket.gaierror: [Errno 11004] getaddrinfo failed</p>\n\n<p>During handling of the above exception, another exception occurred:</p>\n\n<p>Traceback (most recent call last):\nFile \"\", line 3, in  \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\transport.py\", line 17, in <strong>init</strong> self.transportAddr = self._resolveAddr(transportAddr) \nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\pysnmp-4.3.2-py3.5.egg\\pysnmp\\hlapi\\asyncore\\transport.py\", line 58, in _resolveAddr raise error.PySnmpError('Bad IPv4/UDP transport address %s: %s' % ('@'.join([str(x) for x in transportAddr]), sys.exc_info()[1])) pysnmp.error.PySnmpError: Bad IPv4/UDP transport address 1@0@.@0@.@1@.@1@3@4: [Errno 11004] getaddrinfo failed**</p>\n\n<p>What do I need to do in order to fix this ?</p>\n", "answer_wo_code": "<p>You have to wrap your OID-value pair(s) into <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object(s):</p>\n\n<pre> </pre>\n\n<p>Code above does not really use any MIBs. If you want to operate on a more human friendly terms, you can replace OID with MIB module+symbol and integer with label (as described in the MIB):</p>\n\n<pre> </pre>\n\n<p>Here's a <a href=\"http://pysnmp.sourceforge.net/examples/hlapi/asyncore/sync/manager/cmdgen/modifying-variables.html#set-scalars-values\" rel=\"nofollow noreferrer\">working example</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.NotificationType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.NotificationType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing SNMP Notification.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.NotificationType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying particular notification) and a collection\n    of MIB variables IDs that\n    :py:class:`~pysnmp.entity.rfc3413.oneliner.cmdgen.NotificationOriginator`\n    should gather and put into notification message.\n\n    Typical notification is defined like this (from *IF-MIB.txt*):\n\n    .. code-block:: bash\n\n       linkDown NOTIFICATION-TYPE\n           OBJECTS { ifIndex, ifAdminStatus, ifOperStatus }\n           STATUS  current\n           DESCRIPTION\n                  \"A linkDown trap signifies that the SNMP entity...\"\n           ::= { snmpTraps 3 }\n\n    Corresponding NotificationType instantiation would look like this:\n\n    .. code-block:: python\n\n        NotificationType(ObjectIdentity('IF-MIB', 'linkDown'))\n\n    To retain similarity with SNMP variable-bindings,\n    :py:class:`~pysnmp.smi.rfc1902.NotificationType` objects behave like\n    a sequence of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class\n    instances.\n\n    See :RFC:`1902#section-2` for more information on NOTIFICATION-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB notification type identification.\n    instanceIndex : :py:class:`~pysnmp.proto.rfc1902.ObjectName`\n        Trailing part of MIB variables OID identification that represents\n        concrete instance of a MIB variable. When notification is prepared,\n        `instanceIndex` is appended to each MIB variable identification\n        listed in NOTIFICATION-TYPE->OBJECTS clause.\n    objects : dict\n        Dictionary-like object that may return values by OID key. The\n        `objects` dictionary is consulted when notification is being\n        prepared. OIDs are taken from MIB variables listed in\n        NOTIFICATION-TYPE->OBJECTS with `instanceIndex` part appended.\n\n    Notes\n    -----\n        Actual notification type and MIB variables look up occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.NotificationType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> NotificationType(ObjectIdentity('1.3.6.1.6.3.1.1.5.3'))\n    NotificationType(ObjectIdentity('1.3.6.1.6.3.1.1.5.3'), (), {})\n    >>> NotificationType(ObjectIdentity('IP-MIB', 'linkDown'), ObjectName('3.5'))\n    NotificationType(ObjectIdentity('1.3.6.1.6.3.1.1.5.3'), ObjectName('3.5'), {})\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42998290"}, "title": {"type": "literal", "value": "PYSNMP custom trap sender , wich are those extra oids?"}, "content": {"type": "literal", "value": "<p>First i got to say i'm not anyway a snmp expert, just started some weeks ago in my leisure time to implement a new function in a system of the company.</p>\n\n<p>I copy pasted a answer i found here</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysnmp import debug\n\ndebug.setLogger(debug.Debug('msgproc'))\n\nnext(sendNotification(SnmpEngine(),\n                      CommunityData('public'),\n                      UdpTransportTarget(('192.168.1.92',162)),\n                      ContextData(),\n                      'trap',\n                      [ObjectType(ObjectIdentity('1.3.6.1.2.7.8'), Integer32(5)),\n                       ObjectType(ObjectIdentity('1.3.6.6.7'),Integer32(45))]\n                      )\n     )\n</code></pre>\n\n<p>My receiver catch 4 varbinds not just 2 like i specified and the debug shows the next </p>\n\n<pre><code>2017-03-24 09:07:53,015 pysnmp: running pysnmp version 4.3.4\n2017-03-24 09:07:53,016 pysnmp: debug category 'msgproc' enabled\n2017-03-24 09:07:54,115 pysnmp: StatusInformation: {'errorIndication': &lt;pysnmp.proto.errind.AccessAllowed object at 0x762eb170&gt;}\n2017-03-24 09:07:54,116 pysnmp: StatusInformation: {'errorIndication': &lt;pysnmp.proto.errind.AccessAllowed object at 0x762eb170&gt;}\n2017-03-24 09:07:54,120 pysnmp: prepareOutgoingMessage: using contextEngineId SnmpEngineID() contextName b''\n2017-03-24 09:07:54,123 pysnmp: generateRequestMsg: Message:\n version=1\n community=public\n data=PDUs:\n  snmpV2-trap=SNMPv2TrapPDU:\n   request-id=10292983\n   error-status='noError'\n   error-index=0\n   variable-bindings=VarBindList:\n    VarBind:\n     name=1.3.6.1.2.1.1.3.0\n     =_BindValue:\n      value=ObjectSyntax:\n       application-wide=ApplicationSyntax:\n        timeticks-value=0\n\n\n\n    VarBind:\n     name=1.3.6.1.6.3.1.1.4.1.0\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        objectID-value=1.3.6.1.6.3.1.1.5.1\n\n\n\n    VarBind:\n     name=1.3.6.1.2.7.8\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        integer-value=5\n\n\n\n    VarBind:\n     name=1.3.6.6.7\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        integer-value=45\n</code></pre>\n\n<p>The problem i got is that i don't really know what are the meaning of the first two OIDS.</p>\n\n<pre><code>**1.3.6.1.2.1.1.3.0 = 0\n1.3.6.1.6.3.1.1.4.1.0 = 1.3.6.1.6.3.1.1.5.1**\n1.3.6.1.2.7.8 = 5\n1.3.6.6.7 = 45\n</code></pre>\n\n<p>looking out seems like they are a OID from the snmpv2-mib but i'm not sure.</p>\n\n<p>Thanks in advance ,</p>\n\n<p>Carlos</p>\n"}, "answerContent": {"type": "literal", "value": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, "answer_1": {"type": "literal", "value": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>First i got to say i'm not anyway a snmp expert, just started some weeks ago in my leisure time to implement a new function in a system of the company.</p>\n\n<p>I copy pasted a answer i found here</p>\n\n<pre> </pre>\n\n<p>My receiver catch 4 varbinds not just 2 like i specified and the debug shows the next </p>\n\n<pre> </pre>\n\n<p>The problem i got is that i don't really know what are the meaning of the first two OIDS.</p>\n\n<pre> </pre>\n\n<p>looking out seems like they are a OID from the snmpv2-mib but i'm not sure.</p>\n\n<p>Thanks in advance ,</p>\n\n<p>Carlos</p>\n", "answer_wo_code": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.hlapi.CommunityData"}, "class_func_label": {"type": "literal", "value": "pysnmp.hlapi.CommunityData"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Creates SNMP v1/v2c configuration entry.\n\n    This object can be used by\n    :py:class:`~pysnmp.hlapi.asyncore.AsyncCommandGenerator` or\n    :py:class:`~pysnmp.hlapi.asyncore.AsyncNotificationOriginator`\n    and their derivatives for adding new entries to Local Configuration\n    Datastore (LCD) managed by :py:class:`~pysnmp.hlapi.SnmpEngine`\n    class instance.\n\n    See :RFC:`2576#section-5.3` for more information on the\n    *SNMP-COMMUNITY-MIB::snmpCommunityTable*.\n\n    Parameters\n    ----------\n    communityIndex: :py:class:`str`, :py:class:`~pysnmp.proto.rfc1902.OctetString`\n        Unique index value of a row in snmpCommunityTable. If it is the\n        only positional parameter, it is treated as a *communityName*.\n\n    communityName: :py:class:`str`, :py:class:`~pysnmp.proto.rfc1902.OctetString`\n        SNMP v1/v2c community string.\n\n    mpModel: :py:class:`int`\n        SNMP message processing model AKA SNMP version. Known SNMP versions are:\n\n        * `0` - for SNMP v1\n        * `1` - for SNMP v2c (default)\n\n\n    contextEngineId: :py:class:`str`, :py:class:`~pysnmp.proto.rfc1902.OctetString`\n        Indicates the location of the context in which management\n        information is accessed when using the community string\n        specified by the above communityName.\n\n    contextName: :py:class:`str`, :py:class:`~pysnmp.proto.rfc1902.OctetString`\n        The context in which management information is accessed when\n        using the above communityName.\n\n    tag: :py:class:`str`\n        Arbitrary string that specifies a set of transport endpoints\n        from which a command responder application will accept\n        management requests with given *communityName* or to which\n        notification originator application will send notifications\n        when targets are specified by a tag value(s).\n\n        The other way to look at the *tag* feature is that it can make\n        specific *communityName* only valid for certain targets.\n\n        The other use-case is when multiple distinct SNMP peers share\n        the same *communityName* -- binding each instance of\n        *communityName* to transport endpoint lets you distinguish\n        SNMP peers from each other (e.g. resolving *communityName* into\n        proper *securityName*).\n\n        For more technical information on SNMP configuration tags please\n        refer to :RFC:`3413#section-4.1.1` and :RFC:`2576#section-5.3`\n        (e.g. the *snmpCommunityTransportTag* object).\n\n        See also: :py:class:`~pysnmp.hlapi.UdpTransportTarget`\n\n    Warnings\n    --------\n    If the same *communityIndex* value is supplied repeatedly with\n    different *communityName* (or other parameters), the later call\n    supersedes all previous calls.\n\n    Make sure not to configure duplicate *communityName* values unless\n    they have distinct *mpModel* and/or *tag* fields. This will make\n    *communityName* based database lookup ambiguous.\n\n    Examples\n    --------\n    >>> from pysnmp.hlapi import CommunityData\n    >>> CommunityData('public')\n    CommunityData(communityIndex='s1410706889', communityName=<COMMUNITY>, mpModel=1, contextEngineId=None, contextName='', tag='')\n    >>> CommunityData('public', 'public')\n    CommunityData(communityIndex='public', communityName=<COMMUNITY>, mpModel=1, contextEngineId=None, contextName='', tag='')\n    >>>\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42998290"}, "title": {"type": "literal", "value": "PYSNMP custom trap sender , wich are those extra oids?"}, "content": {"type": "literal", "value": "<p>First i got to say i'm not anyway a snmp expert, just started some weeks ago in my leisure time to implement a new function in a system of the company.</p>\n\n<p>I copy pasted a answer i found here</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysnmp import debug\n\ndebug.setLogger(debug.Debug('msgproc'))\n\nnext(sendNotification(SnmpEngine(),\n                      CommunityData('public'),\n                      UdpTransportTarget(('192.168.1.92',162)),\n                      ContextData(),\n                      'trap',\n                      [ObjectType(ObjectIdentity('1.3.6.1.2.7.8'), Integer32(5)),\n                       ObjectType(ObjectIdentity('1.3.6.6.7'),Integer32(45))]\n                      )\n     )\n</code></pre>\n\n<p>My receiver catch 4 varbinds not just 2 like i specified and the debug shows the next </p>\n\n<pre><code>2017-03-24 09:07:53,015 pysnmp: running pysnmp version 4.3.4\n2017-03-24 09:07:53,016 pysnmp: debug category 'msgproc' enabled\n2017-03-24 09:07:54,115 pysnmp: StatusInformation: {'errorIndication': &lt;pysnmp.proto.errind.AccessAllowed object at 0x762eb170&gt;}\n2017-03-24 09:07:54,116 pysnmp: StatusInformation: {'errorIndication': &lt;pysnmp.proto.errind.AccessAllowed object at 0x762eb170&gt;}\n2017-03-24 09:07:54,120 pysnmp: prepareOutgoingMessage: using contextEngineId SnmpEngineID() contextName b''\n2017-03-24 09:07:54,123 pysnmp: generateRequestMsg: Message:\n version=1\n community=public\n data=PDUs:\n  snmpV2-trap=SNMPv2TrapPDU:\n   request-id=10292983\n   error-status='noError'\n   error-index=0\n   variable-bindings=VarBindList:\n    VarBind:\n     name=1.3.6.1.2.1.1.3.0\n     =_BindValue:\n      value=ObjectSyntax:\n       application-wide=ApplicationSyntax:\n        timeticks-value=0\n\n\n\n    VarBind:\n     name=1.3.6.1.6.3.1.1.4.1.0\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        objectID-value=1.3.6.1.6.3.1.1.5.1\n\n\n\n    VarBind:\n     name=1.3.6.1.2.7.8\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        integer-value=5\n\n\n\n    VarBind:\n     name=1.3.6.6.7\n     =_BindValue:\n      value=ObjectSyntax:\n       simple=SimpleSyntax:\n        integer-value=45\n</code></pre>\n\n<p>The problem i got is that i don't really know what are the meaning of the first two OIDS.</p>\n\n<pre><code>**1.3.6.1.2.1.1.3.0 = 0\n1.3.6.1.6.3.1.1.4.1.0 = 1.3.6.1.6.3.1.1.5.1**\n1.3.6.1.2.7.8 = 5\n1.3.6.6.7 = 45\n</code></pre>\n\n<p>looking out seems like they are a OID from the snmpv2-mib but i'm not sure.</p>\n\n<p>Thanks in advance ,</p>\n\n<p>Carlos</p>\n"}, "answerContent": {"type": "literal", "value": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, "answer_1": {"type": "literal", "value": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>First i got to say i'm not anyway a snmp expert, just started some weeks ago in my leisure time to implement a new function in a system of the company.</p>\n\n<p>I copy pasted a answer i found here</p>\n\n<pre> </pre>\n\n<p>My receiver catch 4 varbinds not just 2 like i specified and the debug shows the next </p>\n\n<pre> </pre>\n\n<p>The problem i got is that i don't really know what are the meaning of the first two OIDS.</p>\n\n<pre> </pre>\n\n<p>looking out seems like they are a OID from the snmpv2-mib but i'm not sure.</p>\n\n<p>Thanks in advance ,</p>\n\n<p>Carlos</p>\n", "answer_wo_code": "<p>So, you are sending a SNMPv2 TRAP (CommunityData(<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.hlapi.CommunityData\" rel=\"nofollow noreferrer\">mpModel=1</a>) implies). According to the chapter 4.2.6 of <a href=\"https://tools.ietf.org/html/rfc1905\" rel=\"nofollow noreferrer\">RFC1905</a>:</p>\n\n<blockquote>\n  <p>The first two variable bindings in the variable binding list of an\n  SNMPv2-Trap-PDU are sysUpTime.0 and snmpTrapOID.0\n  respectively.  </p>\n</blockquote>\n\n<p>Since you have not supplied those yourself, pysnmp adds them automatically to produce a well-formed PDU.</p>\n\n<p>Note that, depending on the ID of TRAP you are sending, pysnmp may attempt to find and attach more OID-value pairs to the var-binds as RFC mandates:</p>\n\n<blockquote>\n  <p>If the OBJECTS clause is present in the invocation of\n  the corresponding NOTIFICATION-TYPE macro, then each corresponding\n  variable, as instantiated by this notification, is copied, in order,\n  to the variable-bindings field. </p>\n</blockquote>\n\n<p>You can pass a lookup map (<a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.NotificationType\" rel=\"nofollow noreferrer\">\"objects\" parameter</a>) to initialize those OBJECTS OIDs. Otherwise pysnmp will search them in its local MIB.</p>\n\n<p>Finally, the OIDs you are explicitly passing belong to this part of the RFC:</p>\n\n<blockquote>\n  <p>If any additional variables are being included (at the option of the \n  generating SNMPv2 entity), then each is copied to the variable-bindings\n  field.</p>\n</blockquote>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/51663129"}, "title": {"type": "literal", "value": "pysnmp not handling large string returns"}, "content": {"type": "literal", "value": "<p>I am trying to walk the sysORTable using the bulkget commandgenerator using the following code based from the samples:</p>\n\n<pre><code>cmdGen = cmdgen.CommandGenerator()\nerrorIndication, errorStatus, errorIndex, varBinds = cmdGen.bulkCmd(\n    cmdgen.UsmUserData(user, \n                    authKey=authKey, \n                    privKey=privKey, \n                    authProtocol=authProto, \n                    privProtocol=privProto, \n                    securityEngineId=None\n            ),\n    cmdgen.UdpTransportTarget((sHost, 161)),\n    0 , 25, \n    *[cmdgen.MibVariable(oid) for oid in sOID] )\n</code></pre>\n\n<p>However the results returned from the agent are over the 255 character limit imposed by the MIB lookup. I have found two workarounds to this problem:</p>\n\n<ol>\n<li>Change the value of the max length of <code>DisplayString</code> in pysnmp/smi/mibs/SNMPv2-TC.py: <code>subtypeSpec = OctetString.subtypeSpec + ValueSizeConstraint(0, 512)</code></li>\n<li>Disabling MIB lookups in the cmdgen with <code>lookupMib=False</code></li>\n</ol>\n\n<p>However both these fixes, while allowing the script to complete, seem to truncate the output. For example:</p>\n\n<pre><code>[ObjectType(ObjectIdentity(&lt;ObjectName value object at 0x7f1c04686cd0 tagSet &lt;TagSet object at 0x7f1c0c88dad0 tags 0:0:6&gt; payload [1.3.6.1.2.1.1.9.1.3.106]&gt;), &lt;DisplayString value object at 0x7f1c04623150 subtypeSpec &lt;ConstraintsIntersection object at 0x7f1c04a64490 consts &lt;ValueSizeConstraint object at 0x7f1c0756c510 consts 0, 65535&gt;, &lt;ValueSizeConstraint object at 0x7f1c04a64450 consts 0, 512&gt;&gt; tagSet &lt;TagSet object at 0x7f1c0c88d5d0 tags 0:0:4&gt; encoding iso-8859-1 payload [Agent capabiliti...B\nFile name: sys]&gt;)]\n</code></pre>\n\n<p>Note the ellipsis and the line break.</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>How do I fix the truncating of output?</li>\n<li>What format is this message in and how do I unpack it? (quite different from a standard get output with a key and a value)</li>\n</ol>\n"}, "answerContent": {"type": "literal", "value": "<p>First of, this seems like a bug in your SNMP agent - they should not overflow the string. In that sense pysnmp is doing just fine. ;-)</p>\n\n<p>To answer your questions:</p>\n\n<ol>\n<li>The ellipsis is only present in <code>repr()</code>, it won't happen if you do <code>str</code> or <code>.prettyPrint()</code> on the value</li>\n<li>Essentially, it a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">sequence of tuples</a>. Each tuple is (name, value). So to print stuff out you could do this:</li>\n</ol>\n\n<p>:</p>\n\n<pre><code>for varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>The example can be found <a href=\"http://snmplabs.com/pysnmp/examples/hlapi/asyncore/sync/manager/cmdgen/table-operations.html#fetch-scalar-and-table-variables\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>First of, this seems like a bug in your SNMP agent - they should not overflow the string. In that sense pysnmp is doing just fine. ;-)</p>\n\n<p>To answer your questions:</p>\n\n<ol>\n<li>The ellipsis is only present in <code>repr()</code>, it won't happen if you do <code>str</code> or <code>.prettyPrint()</code> on the value</li>\n<li>Essentially, it a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">sequence of tuples</a>. Each tuple is (name, value). So to print stuff out you could do this:</li>\n</ol>\n\n<p>:</p>\n\n<pre><code>for varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>The example can be found <a href=\"http://snmplabs.com/pysnmp/examples/hlapi/asyncore/sync/manager/cmdgen/table-operations.html#fetch-scalar-and-table-variables\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am trying to walk the sysORTable using the bulkget commandgenerator using the following code based from the samples:</p>\n\n<pre> </pre>\n\n<p>However the results returned from the agent are over the 255 character limit imposed by the MIB lookup. I have found two workarounds to this problem:</p>\n\n<ol>\n<li>Change the value of the max length of   in pysnmp/smi/mibs/SNMPv2-TC.py:  </li>\n<li>Disabling MIB lookups in the cmdgen with  </li>\n</ol>\n\n<p>However both these fixes, while allowing the script to complete, seem to truncate the output. For example:</p>\n\n<pre> </pre>\n\n<p>Note the ellipsis and the line break.</p>\n\n<p>Two questions:</p>\n\n<ol>\n<li>How do I fix the truncating of output?</li>\n<li>What format is this message in and how do I unpack it? (quite different from a standard get output with a key and a value)</li>\n</ol>\n", "answer_wo_code": "<p>First of, this seems like a bug in your SNMP agent - they should not overflow the string. In that sense pysnmp is doing just fine. ;-)</p>\n\n<p>To answer your questions:</p>\n\n<ol>\n<li>The ellipsis is only present in  , it won't happen if you do   or   on the value</li>\n<li>Essentially, it a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">sequence of tuples</a>. Each tuple is (name, value). So to print stuff out you could do this:</li>\n</ol>\n\n<p>:</p>\n\n<pre> </pre>\n\n<p>The example can be found <a href=\"http://snmplabs.com/pysnmp/examples/hlapi/asyncore/sync/manager/cmdgen/table-operations.html#fetch-scalar-and-table-variables\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/46963051"}, "title": {"type": "literal", "value": "pysnmp.smi.error.SmiError: No symbol error while trying to power on the apc outlet with pysnmp"}, "content": {"type": "literal", "value": "<p>I am trying to use <code>pysnmp</code> module to power on apc outlet.</p>\n\n<p>This is the manual command which works fine:</p>\n\n<pre><code># snmpset -v1 -c comstring 10.x.x.x SNMPv2-SMI::enterprises.318.1.1.26.9.2.4.1.5.27 i 1\nPowerNet-MIB::rPDU2OutletSwitchedControlCommand.27 = INTEGER: immediateOn(1)\n</code></pre>\n\n<p>Here is the code:</p>\n\n<pre><code>from pysnmp.entity.rfc3413.oneliner import cmdgen\n\ncmdGen = cmdgen.CommandGenerator()\n\nerrorIndication, errorStatus, errorIndex, varBinds = cmdGen.setCmd(\n    cmdgen.CommunityData('comstring'),\n    cmdgen.UdpTransportTarget(('10.x.x.x', 161)),\n    (cmdgen.MibVariable('SNMPv2-SMI', 'enterprises.318.1.1.26.9.2.4.1.5', \"27\"), 1)\n)\n\n# Check for errors and print out results\nif errorIndication:\n    print(errorIndication)\nelse:\n    if errorStatus:\n        print('%s at %s' % (\n            errorStatus.prettyPrint(),\n            errorIndex and varBinds[int(errorIndex)-1] or '?'\n            )\n        )\n    else:\n        for name, val in varBinds:\n            print('%s = %s' % (name.prettyPrint(), val.prettyPrint()))\n</code></pre>\n\n<p>I have copied mibs file to following paths:</p>\n\n<pre><code>/usr/lib/python2.7/site-packages/pysnmp/smi/mibs\n/usr/lib/python2.7/site-packages/pysnmp/smi/mibs/instances\n</code></pre>\n\n<p>I am seeing below error when I try to run the script:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"snmp3.py\", line 128, in &lt;module&gt;\n    (cmdgen.MibVariable('SNMPv2-SMI', 'enterprises.318.1.1.26.9.2.4.1.5', \"27\"), 1)\n  File \"/usr/lib/python2.7/site-packages/pysnmp/entity/rfc3413/oneliner/cmdgen.py\", line 200, in setCmd\n    **kwargs):\n  File \"/usr/lib/python2.7/site-packages/pysnmp/hlapi/asyncore/sync/cmdgen.py\", line 217, in setCmd\n    lookupMib=options.get('lookupMib', True)))\n  File \"/usr/lib/python2.7/site-packages/pysnmp/hlapi/asyncore/cmdgen.py\", line 239, in setCmd\n    contextData.contextName, vbProcessor.makeVarBinds(snmpEngine, varBinds),\n  File \"/usr/lib/python2.7/site-packages/pysnmp/hlapi/varbinds.py\", line 39, in makeVarBinds\n    __varBinds.append(varBind.resolveWithMib(mibViewController))\n  File \"/usr/lib/python2.7/site-packages/pysnmp/smi/rfc1902.py\", line 845, in resolveWithMib\n    self.__args[0].resolveWithMib(mibViewController)\n  File \"/usr/lib/python2.7/site-packages/pysnmp/smi/rfc1902.py\", line 481, in resolveWithMib\n    self.__modName, self.__symName\n  File \"/usr/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 407, in importSymbols\n    'No symbol %s::%s at %s' % (modName, symName, self)\npysnmp.smi.error.SmiError: No symbol SNMPv2-SMI::enterprises.318.1.1.26.9.2.4.1.5 at &lt;pysnmp.smi.builder.MibBuilder object at 0x367f550&gt;\n</code></pre>\n\n<p>Can someone please let me know if I am missing anything here? How can I resolve this error?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I guess one problem is with wrong <code>MibVariable</code> initialization parameters. On top of that, the MIB object you are trying to refer to is not defined in the SNMPv2-SMI MIB.</p>\n\n<p>The <code>MibVariable</code> (AKA <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#mib-variables\" rel=\"nofollow noreferrer\">ObjectIdentity</a>) type expects <code>MIB-name</code>, <code>object-name</code>, <code>indices</code>. With your code you pass <code>object-name</code> + <code>indices</code> glued together as <code>object-name</code>. This is why it fails to resolve the MIB object.</p>\n\n<p>How about this:</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysnmp import debug\n\ndebug.setLogger(debug.Debug('msgproc'))\n\nsnmpEngine = SnmpEngine()\n\n(errorIndication,\n errorStatus,\n errorIndex,\n varBinds) = next(\n    setCmd(\n      snmpEngine,\n      CommunityData('public'),\n      UdpTransportTarget(('demo.snmplabs.com', 161)),\n      ContextData(),\n      ObjectType(ObjectIdentity('SNMPv2-SMI', 'enterprises', '318.1.1.26.9.2.4.1.5.27'), Integer32(1))\n    )\n)\n</code></pre>\n\n<p>Or much better approach would be to actually use the <code>PowerNet-MIB</code>:</p>\n\n<pre><code>ObjectType(ObjectIdentity('PowerNet-MIB', 'rPDU2OutletSwitchedControlCommand', 27), 1) \n</code></pre>\n\n<p>Or you can just pass that bare OID along with value type to pysnmp:</p>\n\n<pre><code>ObjectType(ObjectIdentity('1.3.6.1.4.1.318.1.1.26.9.2.4.1.5.27'), Integer32(1))\n</code></pre>\n\n<p>Finally, you should not copy your MIBs into pysnmp installation directory. Consider <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">referring</a> to your own MIBs directory instead. Alternatively, you can ask pysnmp to automatically search for and download requested MIBs from <a href=\"http://mibs.snmplabs.com/asn1\" rel=\"nofollow noreferrer\">the web</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I guess one problem is with wrong <code>MibVariable</code> initialization parameters. On top of that, the MIB object you are trying to refer to is not defined in the SNMPv2-SMI MIB.</p>\n\n<p>The <code>MibVariable</code> (AKA <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#mib-variables\" rel=\"nofollow noreferrer\">ObjectIdentity</a>) type expects <code>MIB-name</code>, <code>object-name</code>, <code>indices</code>. With your code you pass <code>object-name</code> + <code>indices</code> glued together as <code>object-name</code>. This is why it fails to resolve the MIB object.</p>\n\n<p>How about this:</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysnmp import debug\n\ndebug.setLogger(debug.Debug('msgproc'))\n\nsnmpEngine = SnmpEngine()\n\n(errorIndication,\n errorStatus,\n errorIndex,\n varBinds) = next(\n    setCmd(\n      snmpEngine,\n      CommunityData('public'),\n      UdpTransportTarget(('demo.snmplabs.com', 161)),\n      ContextData(),\n      ObjectType(ObjectIdentity('SNMPv2-SMI', 'enterprises', '318.1.1.26.9.2.4.1.5.27'), Integer32(1))\n    )\n)\n</code></pre>\n\n<p>Or much better approach would be to actually use the <code>PowerNet-MIB</code>:</p>\n\n<pre><code>ObjectType(ObjectIdentity('PowerNet-MIB', 'rPDU2OutletSwitchedControlCommand', 27), 1) \n</code></pre>\n\n<p>Or you can just pass that bare OID along with value type to pysnmp:</p>\n\n<pre><code>ObjectType(ObjectIdentity('1.3.6.1.4.1.318.1.1.26.9.2.4.1.5.27'), Integer32(1))\n</code></pre>\n\n<p>Finally, you should not copy your MIBs into pysnmp installation directory. Consider <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">referring</a> to your own MIBs directory instead. Alternatively, you can ask pysnmp to automatically search for and download requested MIBs from <a href=\"http://mibs.snmplabs.com/asn1\" rel=\"nofollow noreferrer\">the web</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am trying to use   module to power on apc outlet.</p>\n\n<p>This is the manual command which works fine:</p>\n\n<pre> </pre>\n\n<p>Here is the code:</p>\n\n<pre> </pre>\n\n<p>I have copied mibs file to following paths:</p>\n\n<pre> </pre>\n\n<p>I am seeing below error when I try to run the script:</p>\n\n<pre> </pre>\n\n<p>Can someone please let me know if I am missing anything here? How can I resolve this error?</p>\n", "answer_wo_code": "<p>I guess one problem is with wrong   initialization parameters. On top of that, the MIB object you are trying to refer to is not defined in the SNMPv2-SMI MIB.</p>\n\n<p>The   (AKA <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#mib-variables\" rel=\"nofollow noreferrer\">ObjectIdentity</a>) type expects  ,  ,  . With your code you pass   +   glued together as  . This is why it fails to resolve the MIB object.</p>\n\n<p>How about this:</p>\n\n<pre> </pre>\n\n<p>Or much better approach would be to actually use the  :</p>\n\n<pre> </pre>\n\n<p>Or you can just pass that bare OID along with value type to pysnmp:</p>\n\n<pre> </pre>\n\n<p>Finally, you should not copy your MIBs into pysnmp installation directory. Consider <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">referring</a> to your own MIBs directory instead. Alternatively, you can ask pysnmp to automatically search for and download requested MIBs from <a href=\"http://mibs.snmplabs.com/asn1\" rel=\"nofollow noreferrer\">the web</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/40138763"}, "title": {"type": "literal", "value": "Can pysnmp return octectstring values only"}, "content": {"type": "literal", "value": "<p>I am doing a small script to get SNMP traps with PySnmp. </p>\n\n<p>I am able to get the <code>oid = value pairs</code>, but the value is too long with a small information in the end. How can I <strong>access the octectstring only</strong> which comes in the end of the value. Is there a way other than string manipulations? Please comment.</p>\n\n<blockquote>\n  <p><strong>OID =</strong>_BindValue(componentType=NamedTypes(NamedType('value', ObjectSyntax------------------------------------------------(DELETED)-----------------(None, OctetString(b'<strong>New Alarm</strong>'))))</p>\n</blockquote>\n\n<p>Is it possible to get the output like the following, as is available from another SNMP client:</p>\n\n<blockquote>\n  <p>.iso.org.dod.internet.private.enterprises.xxxx.1.1.2.2.14:  CM_DAS Alarm Traps: </p>\n</blockquote>\n\n<p>Edit - the codes  are :</p>\n\n<pre><code>**for oid, val in varBinds:\n      print('%s = %s' % (oid.prettyPrint(), val.prettyPrint()))\n      target.write(str(val))**\n</code></pre>\n\n<p>On screen, it shows short, but on file, the val is so long.</p>\n\n<p>Usage of <code>target.write( str(val[0][1][2]))</code> does not work for all (program stops with error), but the 1st oid(time tick) gets it fine.</p>\n\n<p>How can I get the value from tail as the actual value is found there for all oids.</p>\n\n<p>Thanks.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Thanks...\nthe following codes works like somwwhat I was looking for.</p>\n\n<pre><code>            if str(oid)==\"1.3.6.1.2.1.1.3.0\":\n                target.write(\" = str(val[0][1]['timeticks-value']) = \" +str(val[0][1]['timeticks-value'])) # time ticks\n\n            else:\n                target.write(\"= val[0][0]['string-value']= \" + str(val[0][0]['string-value']))\n</code></pre>\n\n\n<p>SNMP transfers information in form of a sequence of OID-value pairs called variable-bindings:</p>\n\n<pre><code>variable_bindings = [[oid1, value1], [oid2, value2], ...]\n</code></pre>\n\n<p>Once you get the variable-bindings sequence from SNMP PDU, to access value1, for example, you might do:</p>\n\n<pre><code>variable_binding1 = variable_bindings[0]\nvalue1 = variable_binding1[1]\n</code></pre>\n\n<p>To access the tail part of value1 (assuming it's a string) you could simply subscribe it:</p>\n\n<pre><code>tail_of_value1 = value1[-10:]\n</code></pre>\n\n<p>I guess in your question you operate on a single variable_binding, not a sequence of them.</p>\n\n<p>If you want pysnmp to translate oid-value pair into a human-friendly representation (of MIB object name, MIB object value), you'd have to pass original OID-value pair to the <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow\">ObjectType</a> class and run it through MIB resolver as explained in the documentation. </p>\n"}, "answer_1": {"type": "literal", "value": "<p>Thanks...\nthe following codes works like somwwhat I was looking for.</p>\n\n<pre><code>            if str(oid)==\"1.3.6.1.2.1.1.3.0\":\n                target.write(\" = str(val[0][1]['timeticks-value']) = \" +str(val[0][1]['timeticks-value'])) # time ticks\n\n            else:\n                target.write(\"= val[0][0]['string-value']= \" + str(val[0][0]['string-value']))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>SNMP transfers information in form of a sequence of OID-value pairs called variable-bindings:</p>\n\n<pre><code>variable_bindings = [[oid1, value1], [oid2, value2], ...]\n</code></pre>\n\n<p>Once you get the variable-bindings sequence from SNMP PDU, to access value1, for example, you might do:</p>\n\n<pre><code>variable_binding1 = variable_bindings[0]\nvalue1 = variable_binding1[1]\n</code></pre>\n\n<p>To access the tail part of value1 (assuming it's a string) you could simply subscribe it:</p>\n\n<pre><code>tail_of_value1 = value1[-10:]\n</code></pre>\n\n<p>I guess in your question you operate on a single variable_binding, not a sequence of them.</p>\n\n<p>If you want pysnmp to translate oid-value pair into a human-friendly representation (of MIB object name, MIB object value), you'd have to pass original OID-value pair to the <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow\">ObjectType</a> class and run it through MIB resolver as explained in the documentation. </p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am doing a small script to get SNMP traps with PySnmp. </p>\n\n<p>I am able to get the  , but the value is too long with a small information in the end. How can I <strong>access the octectstring only</strong> which comes in the end of the value. Is there a way other than string manipulations? Please comment.</p>\n\n<blockquote>\n  <p><strong>OID =</strong>_BindValue(componentType=NamedTypes(NamedType('value', ObjectSyntax------------------------------------------------(DELETED)-----------------(None, OctetString(b'<strong>New Alarm</strong>'))))</p>\n</blockquote>\n\n<p>Is it possible to get the output like the following, as is available from another SNMP client:</p>\n\n<blockquote>\n  <p>.iso.org.dod.internet.private.enterprises.xxxx.1.1.2.2.14:  CM_DAS Alarm Traps: </p>\n</blockquote>\n\n<p>Edit - the codes  are :</p>\n\n<pre> </pre>\n\n<p>On screen, it shows short, but on file, the val is so long.</p>\n\n<p>Usage of   does not work for all (program stops with error), but the 1st oid(time tick) gets it fine.</p>\n\n<p>How can I get the value from tail as the actual value is found there for all oids.</p>\n\n<p>Thanks.</p>\n", "answer_wo_code": "<p>Thanks...\nthe following codes works like somwwhat I was looking for.</p>\n\n<pre> </pre>\n\n\n<p>SNMP transfers information in form of a sequence of OID-value pairs called variable-bindings:</p>\n\n<pre> </pre>\n\n<p>Once you get the variable-bindings sequence from SNMP PDU, to access value1, for example, you might do:</p>\n\n<pre> </pre>\n\n<p>To access the tail part of value1 (assuming it's a string) you could simply subscribe it:</p>\n\n<pre> </pre>\n\n<p>I guess in your question you operate on a single variable_binding, not a sequence of them.</p>\n\n<p>If you want pysnmp to translate oid-value pair into a human-friendly representation (of MIB object name, MIB object value), you'd have to pass original OID-value pair to the <a href=\"http://pysnmp.sourceforge.net/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow\">ObjectType</a> class and run it through MIB resolver as explained in the documentation. </p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/53893588"}, "title": {"type": "literal", "value": "How to access data in MibTableColumn"}, "content": {"type": "literal", "value": "<p>I need to extract value's name from mib's syntax and I don't know how to do it. When my script receive trap from device with oid \"ccmHistoryEventCommandSource\" with value \"1\" I want to get it's name which is \"commandLine\"...</p>\n\n<p>Part of mib (CISCO-CONFIG-MAN-MIB):</p>\n\n<pre><code>ccmHistoryEventCommandSource OBJECT-TYPE\n    SYNTAX          INTEGER  {\n                        commandLine(1),\n                        snmp(2)\n                    }\n    MAX-ACCESS      read-only\n    STATUS          current\n    DESCRIPTION\n            \"The source of the command that instigated the event.\"\n    ::= { ccmHistoryEventEntry 3 }\n</code></pre>\n\n<p>This is part of my code:</p>\n\n<pre><code>mib_obj = rfc1902.ObjectIdentity(oid).resolveWithMib(mibViewController)\nmn = mib_obj.getMibNode()\nprint(\"&gt;&gt;&gt; \", mn)\nprint(\"&gt;&gt;&gt; \", mn.syntax)\n</code></pre>\n\n<p>And this is output:</p>\n\n<pre><code>&gt;&gt;&gt;  MibTableColumn((1, 3, 6, 1, 4, 1, 9, 9, 43, 1, 1, 6, 1, 3), Integer32(subtypeSpec=ConstraintsUnion(ConstraintsUnion(SingleValueConstraint(1, 2)), ConstraintsIntersection(ConstraintsIntersection(), ValueRangeConstraint(-2147483648, 2147483647))), NamedValues(('commandLine', 1), ('snmp', 2))))\n&gt;&gt;&gt;  NoValue()\n</code></pre>\n\n<p>I have compiled CISCO-CONFIG-MAN-MIB.py with code:</p>\n\n<pre><code>ccmHistoryEventCommandSource = MibTableColumn((1, 3, 6, 1, 4, 1, 9, 9, 43, 1, 1, 6, 1, 3), Integer32().subtype(subtypeSpec=ConstraintsUnion(SingleValueConstraint(1, 2))).clone(namedValues=NamedValues((\"commandLine\", 1), (\"snmp\", 2)))).setMaxAccess(\"readonly\")\n</code></pre>\n\n<p>As we can see, output has some NamedValues I need, but I don't know how to access this data ...</p>\n"}, "answerContent": {"type": "literal", "value": "<p>The <code>ObjectIdentity</code> object never carries any value, it's just an ID of a MIB object. But  <code>ObjectType</code> object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the <code>ObjectIdentity</code> one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre><code>resolved_var_binds = [\n    ObjectType(ObjectIdentity(oid), value).resolveWithMib(mibViewController)\n    for oid, value in var_binds]\n\nresolved_values = [value for oid, value in resolved_varbinds]\n</code></pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, "answer_1": {"type": "literal", "value": "<p>The <code>ObjectIdentity</code> object never carries any value, it's just an ID of a MIB object. But  <code>ObjectType</code> object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the <code>ObjectIdentity</code> one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre><code>resolved_var_binds = [\n    ObjectType(ObjectIdentity(oid), value).resolveWithMib(mibViewController)\n    for oid, value in var_binds]\n\nresolved_values = [value for oid, value in resolved_varbinds]\n</code></pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I need to extract value's name from mib's syntax and I don't know how to do it. When my script receive trap from device with oid \"ccmHistoryEventCommandSource\" with value \"1\" I want to get it's name which is \"commandLine\"...</p>\n\n<p>Part of mib (CISCO-CONFIG-MAN-MIB):</p>\n\n<pre> </pre>\n\n<p>This is part of my code:</p>\n\n<pre> </pre>\n\n<p>And this is output:</p>\n\n<pre> </pre>\n\n<p>I have compiled CISCO-CONFIG-MAN-MIB.py with code:</p>\n\n<pre> </pre>\n\n<p>As we can see, output has some NamedValues I need, but I don't know how to access this data ...</p>\n", "answer_wo_code": "<p>The   object never carries any value, it's just an ID of a MIB object. But    object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the   one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre> </pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/53893588"}, "title": {"type": "literal", "value": "How to access data in MibTableColumn"}, "content": {"type": "literal", "value": "<p>I need to extract value's name from mib's syntax and I don't know how to do it. When my script receive trap from device with oid \"ccmHistoryEventCommandSource\" with value \"1\" I want to get it's name which is \"commandLine\"...</p>\n\n<p>Part of mib (CISCO-CONFIG-MAN-MIB):</p>\n\n<pre><code>ccmHistoryEventCommandSource OBJECT-TYPE\n    SYNTAX          INTEGER  {\n                        commandLine(1),\n                        snmp(2)\n                    }\n    MAX-ACCESS      read-only\n    STATUS          current\n    DESCRIPTION\n            \"The source of the command that instigated the event.\"\n    ::= { ccmHistoryEventEntry 3 }\n</code></pre>\n\n<p>This is part of my code:</p>\n\n<pre><code>mib_obj = rfc1902.ObjectIdentity(oid).resolveWithMib(mibViewController)\nmn = mib_obj.getMibNode()\nprint(\"&gt;&gt;&gt; \", mn)\nprint(\"&gt;&gt;&gt; \", mn.syntax)\n</code></pre>\n\n<p>And this is output:</p>\n\n<pre><code>&gt;&gt;&gt;  MibTableColumn((1, 3, 6, 1, 4, 1, 9, 9, 43, 1, 1, 6, 1, 3), Integer32(subtypeSpec=ConstraintsUnion(ConstraintsUnion(SingleValueConstraint(1, 2)), ConstraintsIntersection(ConstraintsIntersection(), ValueRangeConstraint(-2147483648, 2147483647))), NamedValues(('commandLine', 1), ('snmp', 2))))\n&gt;&gt;&gt;  NoValue()\n</code></pre>\n\n<p>I have compiled CISCO-CONFIG-MAN-MIB.py with code:</p>\n\n<pre><code>ccmHistoryEventCommandSource = MibTableColumn((1, 3, 6, 1, 4, 1, 9, 9, 43, 1, 1, 6, 1, 3), Integer32().subtype(subtypeSpec=ConstraintsUnion(SingleValueConstraint(1, 2))).clone(namedValues=NamedValues((\"commandLine\", 1), (\"snmp\", 2)))).setMaxAccess(\"readonly\")\n</code></pre>\n\n<p>As we can see, output has some NamedValues I need, but I don't know how to access this data ...</p>\n"}, "answerContent": {"type": "literal", "value": "<p>The <code>ObjectIdentity</code> object never carries any value, it's just an ID of a MIB object. But  <code>ObjectType</code> object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the <code>ObjectIdentity</code> one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre><code>resolved_var_binds = [\n    ObjectType(ObjectIdentity(oid), value).resolveWithMib(mibViewController)\n    for oid, value in var_binds]\n\nresolved_values = [value for oid, value in resolved_varbinds]\n</code></pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, "answer_1": {"type": "literal", "value": "<p>The <code>ObjectIdentity</code> object never carries any value, it's just an ID of a MIB object. But  <code>ObjectType</code> object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the <code>ObjectIdentity</code> one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre><code>resolved_var_binds = [\n    ObjectType(ObjectIdentity(oid), value).resolveWithMib(mibViewController)\n    for oid, value in var_binds]\n\nresolved_values = [value for oid, value in resolved_varbinds]\n</code></pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I need to extract value's name from mib's syntax and I don't know how to do it. When my script receive trap from device with oid \"ccmHistoryEventCommandSource\" with value \"1\" I want to get it's name which is \"commandLine\"...</p>\n\n<p>Part of mib (CISCO-CONFIG-MAN-MIB):</p>\n\n<pre> </pre>\n\n<p>This is part of my code:</p>\n\n<pre> </pre>\n\n<p>And this is output:</p>\n\n<pre> </pre>\n\n<p>I have compiled CISCO-CONFIG-MAN-MIB.py with code:</p>\n\n<pre> </pre>\n\n<p>As we can see, output has some NamedValues I need, but I don't know how to access this data ...</p>\n", "answer_wo_code": "<p>The   object never carries any value, it's just an ID of a MIB object. But    object accommodates both, the ID and the value.</p>\n\n<p>So you need to feed the variable-bindings you receive in the TRAP message to the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> object rather than to the   one.</p>\n\n<p>Perhaps something like this:</p>\n\n<pre> </pre>\n\n<p>Make sure you <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">load the MIB</a> that defines the object you are trying to resolve. You may also need to <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">configure the source</a> of the ASN.1 MIBs (or you could pre-compile them and configure their source if you wish).</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54028927"}, "title": {"type": "literal", "value": "Pysnmp problem loading MIB text and py files"}, "content": {"type": "literal", "value": "<p>I'm having problems with the pysnmp package.</p>\n\n<p>I want to connect to the OLT device from Huawei MA5600. I have his MIBs in text format * .mib the idea is to get the temperature, energy consumption among others</p>\n\n<p>I have the following code</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysmi import debug\n#debug.setLogger(debug.Debug('compiler'))\n\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('MyCommunity'),\n           UdpTransportTarget(('192.168.1.2', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.1.6.0')),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0')),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')),\n           ObjectType(ObjectIdentity('1.3.6.1.4.1.2011.2.6.7.1.1.1.1.11'))\\\n           .addAsn1MibSource('file:///home/devel/mib/',\n                 'file:///home/devel/mib'\n                 'http://mibs.snmplabs.com/asn1/@mib@',\n                 'http://mibs.snmplabs.com/asn1/',\n                 'file:///home/devel/mib/public/',\n                 'file:///home/devel/mib/public')\n           )\n\n)\n\n\n\nif errorIndication:\n    print(errorIndication)\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex) - 1][0] or '?'))\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>In this case I have the following answer:</p>\n\n<pre><code>(sent) C:\\DESARROLLOS\\system&gt;python v2c-get.py\nSNMPv2-MIB::sysLocation.0 = EPL\nSNMPv2-MIB::sysDescr.0 = Huawei Integrated Access Software\nSNMPv2-SMI::mib-2.2.2.1.10 = No Such Instance currently exists at this OID\nSNMPv2-SMI::enterprises.2011.2.6.7.1.1.1.1.11 = No Such Instance currently exists at this OID\n</code></pre>\n\n<p>I have also tried to use the addMibSource function to add the python mibs, I have the pysnmp_mibs package but it does not work either.</p>\n\n<p>It should be noted that with the command I get the information I request:</p>\n\n<pre><code>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1\n</code></pre>\n\n<p>It should be noted that with the command I get the information I request.</p>\n\n<p>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1</p>\n\n<p>I would be grateful if someone happened to something similar and knows what to do. I am using python 3.6, pysmi == 0.3.2, pysnmp == 4.4.6\nand pysnmp-mibs == 0.1.6</p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre><code>ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysLocation', 0))\n</code></pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre><code>ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysLocation', 0))\n</code></pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I'm having problems with the pysnmp package.</p>\n\n<p>I want to connect to the OLT device from Huawei MA5600. I have his MIBs in text format * .mib the idea is to get the temperature, energy consumption among others</p>\n\n<p>I have the following code</p>\n\n<pre> </pre>\n\n<p>In this case I have the following answer:</p>\n\n<pre> </pre>\n\n<p>I have also tried to use the addMibSource function to add the python mibs, I have the pysnmp_mibs package but it does not work either.</p>\n\n<p>It should be noted that with the command I get the information I request:</p>\n\n<pre> </pre>\n\n<p>It should be noted that with the command I get the information I request.</p>\n\n<p>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1</p>\n\n<p>I would be grateful if someone happened to something similar and knows what to do. I am using python 3.6, pysmi == 0.3.2, pysnmp == 4.4.6\nand pysnmp-mibs == 0.1.6</p>\n", "answer_wo_code": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre> </pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54028927"}, "title": {"type": "literal", "value": "Pysnmp problem loading MIB text and py files"}, "content": {"type": "literal", "value": "<p>I'm having problems with the pysnmp package.</p>\n\n<p>I want to connect to the OLT device from Huawei MA5600. I have his MIBs in text format * .mib the idea is to get the temperature, energy consumption among others</p>\n\n<p>I have the following code</p>\n\n<pre><code>from pysnmp.hlapi import *\nfrom pysmi import debug\n#debug.setLogger(debug.Debug('compiler'))\n\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('MyCommunity'),\n           UdpTransportTarget(('192.168.1.2', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.1.6.0')),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0')),\n           ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')),\n           ObjectType(ObjectIdentity('1.3.6.1.4.1.2011.2.6.7.1.1.1.1.11'))\\\n           .addAsn1MibSource('file:///home/devel/mib/',\n                 'file:///home/devel/mib'\n                 'http://mibs.snmplabs.com/asn1/@mib@',\n                 'http://mibs.snmplabs.com/asn1/',\n                 'file:///home/devel/mib/public/',\n                 'file:///home/devel/mib/public')\n           )\n\n)\n\n\n\nif errorIndication:\n    print(errorIndication)\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex) - 1][0] or '?'))\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>In this case I have the following answer:</p>\n\n<pre><code>(sent) C:\\DESARROLLOS\\system&gt;python v2c-get.py\nSNMPv2-MIB::sysLocation.0 = EPL\nSNMPv2-MIB::sysDescr.0 = Huawei Integrated Access Software\nSNMPv2-SMI::mib-2.2.2.1.10 = No Such Instance currently exists at this OID\nSNMPv2-SMI::enterprises.2011.2.6.7.1.1.1.1.11 = No Such Instance currently exists at this OID\n</code></pre>\n\n<p>I have also tried to use the addMibSource function to add the python mibs, I have the pysnmp_mibs package but it does not work either.</p>\n\n<p>It should be noted that with the command I get the information I request:</p>\n\n<pre><code>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1\n</code></pre>\n\n<p>It should be noted that with the command I get the information I request.</p>\n\n<p>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1</p>\n\n<p>I would be grateful if someone happened to something similar and knows what to do. I am using python 3.6, pysmi == 0.3.2, pysnmp == 4.4.6\nand pysnmp-mibs == 0.1.6</p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre><code>ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysLocation', 0))\n</code></pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre><code>ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysLocation', 0))\n</code></pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I'm having problems with the pysnmp package.</p>\n\n<p>I want to connect to the OLT device from Huawei MA5600. I have his MIBs in text format * .mib the idea is to get the temperature, energy consumption among others</p>\n\n<p>I have the following code</p>\n\n<pre> </pre>\n\n<p>In this case I have the following answer:</p>\n\n<pre> </pre>\n\n<p>I have also tried to use the addMibSource function to add the python mibs, I have the pysnmp_mibs package but it does not work either.</p>\n\n<p>It should be noted that with the command I get the information I request:</p>\n\n<pre> </pre>\n\n<p>It should be noted that with the command I get the information I request.</p>\n\n<p>snmpwalk -v 2c -c MyCommunity 192.168.1.2 1.3.6.1.4.1.2011.2.6.7.1.1.1.1.1</p>\n\n<p>I would be grateful if someone happened to something similar and knows what to do. I am using python 3.6, pysmi == 0.3.2, pysnmp == 4.4.6\nand pysnmp-mibs == 0.1.6</p>\n", "answer_wo_code": "<p>If you think that you should get some value in response instead of 'No such instance', then the problem is that you are querying the object (OID) which does not exist at the device or you do not have access to it.</p>\n\n<p>If you want the OIDs in response to be resolved into human-friendly names, you should pre-load the MIB(s) that define those OIDs e.g. ObjectType(ObjectIdentity('1.3.6.1.2.1.2.2.1.10')).<a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">loadMibs</a>('MY-MIB', 'MY-OTHER-MIB').</p>\n\n<p>Either way, probably the best solution is to query the object(s) by name(s):</p>\n\n<pre> </pre>\n\n<p>...rather than by OID. Querying by name would automatically load the MIB you reference.</p>\n\n<p>The quick doc is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">here</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/56089528"}, "title": {"type": "literal", "value": "How to add new MIB text file in pysnmp?"}, "content": {"type": "literal", "value": "<p>Unable to MIB of external device. </p>\n\n<p>I have a IP power bar which can be controlled using SNMP. I am trying to switch off and on outlets using Python script. I have saved new mib at d:\\mib\nI am trying to set on of the OID to 0. I am calling setCmd function as below </p>\n\n<pre><code>errorIndication, errorStatus, errorIndex, varBinds = next(setCmd(SnmpEngine(),CommunityData('write_public'),UdpTransportTarget(('xx.xx.xx.xx', 161)),ContextData(),ObjectType(ObjectIdentity('1.3.6.1.4.1.13742.6.4.1.2.1.2.1.3',0).addAsn1MibSource('d:/mib'))))\n</code></pre>\n\n<p>I am getting errors \nTraceback (most recent call last):</p>\n\n<pre><code>  File \"&lt;pyshell#11&gt;\", line 1, in &lt;module&gt;\n    errorIndication, errorStatus, errorIndex, varBinds = next(setCmd(SnmpEngine(),CommunityData('write_public'),UdpTransportTarget(('xx.xxx.xx.x', 161)),ContextData(),ObjectType(ObjectIdentity('SNMPv2-MIB','1.3.6.1.4.1.13742.6.4.1.2.1.2.1.3',0).addAsn1MibSource('d:/mib'))))\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\hlapi\\asyncore\\sync\\cmdgen.py\", line 217, in setCmd\n    lookupMib=options.get('lookupMib', True)))\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\hlapi\\asyncore\\cmdgen.py\", line 241, in setCmd\n    contextData.contextName, vbProcessor.makeVarBinds(snmpEngine, varBinds),\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\hlapi\\varbinds.py\", line 39, in makeVarBinds\n    __varBinds.append(varBind.resolveWithMib(mibViewController))\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\smi\\rfc1902.py\", line 847, in resolveWithMib\n    self.__args[0].resolveWithMib(mibViewController)\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\smi\\rfc1902.py\", line 368, in resolveWithMib\n    ifNotAdded=self.__asn1SourcesOptions.get('ifNotAdded')\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysnmp\\smi\\compiler.py\", line 55, in addMibCompiler\n    compiler.addSources(*getReadersFromUrls(*kwargs.get('sources') or defaultSources))\n  File \"C:\\Users\\mahemad\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pysmi\\reader\\url.py\", line 68, in getReadersFromUrls\n    raise error.PySmiError('Unsupported URL scheme %s' % sourceUrl)\npysmi.error.PySmiError: Unsupported URL scheme d:/mib\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>First of all, MIBs are not <em>required</em> with SNMP for as long as you use bare OIDs and values (this is what you seem to do).</p>\n\n<p>If you want to set that OID to value <em>0</em> the following managed object specification  should produce SNMP SET PDU you might need:</p>\n\n<pre><code>ObjectType(ObjectIdentity('1.3.6.1.4.1.13742.6.4.1.2.1.2.1.3'), 0)\n</code></pre>\n\n<p>If you want to use MIBs you should <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">addAsn1MibSource</a> method, but pass it a <a href=\"https://blogs.msdn.microsoft.com/ie/2006/12/06/file-uris-in-windows/\" rel=\"nofollow noreferrer\">valid URL</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>First of all, MIBs are not <em>required</em> with SNMP for as long as you use bare OIDs and values (this is what you seem to do).</p>\n\n<p>If you want to set that OID to value <em>0</em> the following managed object specification  should produce SNMP SET PDU you might need:</p>\n\n<pre><code>ObjectType(ObjectIdentity('1.3.6.1.4.1.13742.6.4.1.2.1.2.1.3'), 0)\n</code></pre>\n\n<p>If you want to use MIBs you should <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">addAsn1MibSource</a> method, but pass it a <a href=\"https://blogs.msdn.microsoft.com/ie/2006/12/06/file-uris-in-windows/\" rel=\"nofollow noreferrer\">valid URL</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>Unable to MIB of external device. </p>\n\n<p>I have a IP power bar which can be controlled using SNMP. I am trying to switch off and on outlets using Python script. I have saved new mib at d:\\mib\nI am trying to set on of the OID to 0. I am calling setCmd function as below </p>\n\n<pre> </pre>\n\n<p>I am getting errors \nTraceback (most recent call last):</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>First of all, MIBs are not <em>required</em> with SNMP for as long as you use bare OIDs and values (this is what you seem to do).</p>\n\n<p>If you want to set that OID to value <em>0</em> the following managed object specification  should produce SNMP SET PDU you might need:</p>\n\n<pre> </pre>\n\n<p>If you want to use MIBs you should <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">addAsn1MibSource</a> method, but pass it a <a href=\"https://blogs.msdn.microsoft.com/ie/2006/12/06/file-uris-in-windows/\" rel=\"nofollow noreferrer\">valid URL</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/TypeError"}, "class_func_label": {"type": "literal", "value": "TypeError"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "Inappropriate argument type."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47165894"}, "title": {"type": "literal", "value": "PySNMP returns TypeError with Anaconda install even with example code"}, "content": {"type": "literal", "value": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre><code>#!/root/anaconda2/python\n\nfrom pysnmp.hlapi import *\n\nfor (errorIndication,\n     errorStatus,\n     errorIndex,\n     varBinds) in nextCmd(SnmpEngine(),\n                      CommunityData('public', mpModel=0),\n                      UdpTransportTarget(('demo.snmplabs.com', 161)),\n                      ContextData(),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifDescr')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifMtu')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifSpeed')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifPhysAddress')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      lexicographicMode=False):\n\nif errorIndication:\n    print(errorIndication)\n    break\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex)-1][0] or '?'))\n    break\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n\n\n<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre> </pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n", "answer_wo_code": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases ( ) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n\n\n<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases ( ) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sys.exc_info"}, "class_func_label": {"type": "literal", "value": "sys.exc_info"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "exc_info() -> (type, value, traceback)\n\nReturn information about the most recent exception caught by an except\nclause in the current stack frame or in an older stack frame."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47165894"}, "title": {"type": "literal", "value": "PySNMP returns TypeError with Anaconda install even with example code"}, "content": {"type": "literal", "value": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre><code>#!/root/anaconda2/python\n\nfrom pysnmp.hlapi import *\n\nfor (errorIndication,\n     errorStatus,\n     errorIndex,\n     varBinds) in nextCmd(SnmpEngine(),\n                      CommunityData('public', mpModel=0),\n                      UdpTransportTarget(('demo.snmplabs.com', 161)),\n                      ContextData(),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifDescr')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifMtu')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifSpeed')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifPhysAddress')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      lexicographicMode=False):\n\nif errorIndication:\n    print(errorIndication)\n    break\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex)-1][0] or '?'))\n    break\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre> </pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n", "answer_wo_code": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases ( ) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/traceback.format_exc"}, "class_func_label": {"type": "literal", "value": "traceback.format_exc"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "Like print_exc() but return a string."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47165894"}, "title": {"type": "literal", "value": "PySNMP returns TypeError with Anaconda install even with example code"}, "content": {"type": "literal", "value": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre><code>#!/root/anaconda2/python\n\nfrom pysnmp.hlapi import *\n\nfor (errorIndication,\n     errorStatus,\n     errorIndex,\n     varBinds) in nextCmd(SnmpEngine(),\n                      CommunityData('public', mpModel=0),\n                      UdpTransportTarget(('demo.snmplabs.com', 161)),\n                      ContextData(),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifDescr')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifMtu')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifSpeed')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifPhysAddress')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      lexicographicMode=False):\n\nif errorIndication:\n    print(errorIndication)\n    break\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex)-1][0] or '?'))\n    break\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre> </pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n", "answer_wo_code": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases ( ) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/traceback.format_exception"}, "class_func_label": {"type": "literal", "value": "traceback.format_exception"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "Format a stack trace and the exception information.\n\nThe arguments have the same meaning as the corresponding arguments\nto print_exception().  The return value is a list of strings, each\nending in a newline and some containing internal newlines.  When\nthese lines are concatenated and printed, exactly the same text is\nprinted as does print_exception()."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47165894"}, "title": {"type": "literal", "value": "PySNMP returns TypeError with Anaconda install even with example code"}, "content": {"type": "literal", "value": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre><code>#!/root/anaconda2/python\n\nfrom pysnmp.hlapi import *\n\nfor (errorIndication,\n     errorStatus,\n     errorIndex,\n     varBinds) in nextCmd(SnmpEngine(),\n                      CommunityData('public', mpModel=0),\n                      UdpTransportTarget(('demo.snmplabs.com', 161)),\n                      ContextData(),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifDescr')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifMtu')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifSpeed')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifPhysAddress')),\n                      ObjectType(ObjectIdentity('IF-MIB', 'ifType')),\n                      lexicographicMode=False):\n\nif errorIndication:\n    print(errorIndication)\n    break\nelif errorStatus:\n    print('%s at %s' % (errorStatus.prettyPrint(),\n                        errorIndex and varBinds[int(errorIndex)-1][0] or '?'))\n    break\nelse:\n    for varBind in varBinds:\n        print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases (<code>pip install --upgrade pysnmp pyasn1</code>) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have installed Anaconda2.5 and it gives me Python 2.7.  I have installed the latest PySNMP. Initially, I attempted to run the Python script from another server using Python 2.6 on this one and got a TypeError (see below). I thought it must be because of my Python version, maybe something changed with PySNMP, but when I ran their example code (see below), it returned the exact same error.  I am baffled as to why their example code would return an error and can only suspect Anaconda installing PySNMP would be at fault?<br>\nThis is the example code:</p>\n\n<pre> </pre>\n\n<p>This is my error: </p>\n\n<pre>\nTraceback (most recent call last):\n  File \"get_aqx_intersite_bw.py\", line 8, in \n    varBinds) in nextCmd(SnmpEngine(),\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/entity/engine.py\", line 61, in __init__\n    self.msgAndPduDsp = MsgAndPduDispatcher()\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc3412.py\", line 29, in __init__\n    'SNMP-TARGET-MIB', 'SNMP-USER-BASED-SM-MIB'\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 337, in loadModules\n    self.loadModule(modName, **userCtx)\n  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 306, in loadModule\n    'MIB module \\\"%s\\\" load error: %s' % (modPath, traceback.format_exception(*sys.exc_info()))\npysnmp.smi.error.MibLoadError: MIB module \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.pyc\" load error: ['Traceback (most recent call last):\\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/smi/builder.py\", line 301, in loadModule\\n    exec(modData, g)\\n', '  File \"/home/ilan/minonda/envs/_build/lib/python2.7/site-packages/pysnmp/smi/mibs/SNMPv2-MIB.py\", line 26, in \\n', '  File \"/root/anaconda2/lib/python2.7/site-packages/pysnmp/proto/rfc1902.py\", line 201, in subtype\\n    self, value, implicitTag, explicitTag, subtypeSpec\\n', 'TypeError: subtype() takes at most 2 arguments (5 given)\\n']\n</pre>\n", "answer_wo_code": "<p>It must have something to do with the incompatible pysnmp/pyasn1 versions.</p>\n\n<p>Depending on your situation you may want to either push both packages to their latest PyPI releases ( ) or, if you are stuck with your current pyasn1 version, downgrade pysnmp to perhaps 4.3.10.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectIdentity"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable ID.\n\n    At the protocol level, MIB variable is only identified by an OID.\n    However, when interacting with humans, MIB variable can also be referred\n    to by its MIB name. The *ObjectIdentity* class supports various forms\n    of MIB variable identification, providing automatic conversion from\n    one to others. At the same time *ObjectIdentity* objects behave like\n    :py:obj:`tuples` of py:obj:`int` sub-OIDs.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-IDENTITY\n    SMI definitions.\n\n    Parameters\n    ----------\n    args\n        initial MIB variable identity. Recognized variants:\n\n        * single :py:obj:`tuple` or integers representing OID\n        * single :py:obj:`str` representing OID in dot-separated\n          integers form\n        * single :py:obj:`str` representing MIB variable in\n          dot-separated labels form\n        * single :py:obj:`str` representing MIB name. First variable\n          defined in MIB is assumed.\n        * pair of :py:obj:`str` representing MIB name and variable name\n        * pair of :py:obj:`str` representing MIB name and variable name\n          followed by an arbitrary number of :py:obj:`str` and/or\n          :py:obj:`int` values representing MIB variable instance\n          identification.\n\n    Other parameters\n    ----------------\n    kwargs\n        MIB resolution options(object):\n\n        * whenever only MIB name is given, resolve into last variable defined\n          in MIB if last=True.  Otherwise resolves to first variable (default).\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectIdentity.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import ObjectIdentity\n    >>> ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    ObjectIdentity((1, 3, 6, 1, 2, 1, 1, 1, 0))\n    >>> ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    ObjectIdentity('1.3.6.1.2.1.1.1.0')\n    >>> ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    ObjectIdentity('iso.org.dod.internet.mgmt.mib-2.system.sysDescr.0')\n    >>> ObjectIdentity('SNMPv2-MIB', 'system')\n    ObjectIdentity('SNMPv2-MIB', 'system')\n    >>> ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n    >>> ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n    ObjectIdentity('IP-MIB', 'ipAdEntAddr', '127.0.0.1', 123)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/53603783"}, "title": {"type": "literal", "value": "pysnmp OID resolution"}, "content": {"type": "literal", "value": "<p>Using pysnmp, how you perform resolution on queries that return an OID instead of value?</p>\n\n<p>I wrote a lookup tool using pysnmp, here are the inputs and results :</p>\n\n<pre><code>./run_snmp_discovery.py --host 1.1.1.1 --community XXXXXX --command get --mib_oid_index  '{ \"mib\" : \"SNMPv2-MIB\", \"oid\" : \"sysObjectID\", \"index\" : \"0\"  }' --verbose\nDebug: 'varBind': SNMPv2-MIB::sysObjectID.0 = SNMPv2-SMI::enterprises.9.1.222\n{\"0\": {\"sysObjectID\": \"SNMPv2-SMI::enterprises.9.1.222\"}}\n</code></pre>\n\n<p>How can the result be converted to the text value <code>cisco7206VXR</code> (reference <a href=\"http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR\" rel=\"nofollow noreferrer\">http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR</a>)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre><code>from pysnmp.hlapi import *\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('public'),\n           UdpTransportTarget(('demo.snmplabs.com', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)))\n)\n\nfor varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>And you want MIB object to be represented as an OID, then the <code>varBind</code> in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre><code>&gt;&gt;&gt; objectIdentity = ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n&gt;&gt;&gt; objectIdentity.resolveWithMib(mibViewController)\n&gt;&gt;&gt; objectIdentity.getOid()\nObjectName('1.3.6.1.2.1.1.1.0')\n</code></pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre><code>from pysnmp.hlapi import *\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('public'),\n           UdpTransportTarget(('demo.snmplabs.com', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)))\n)\n\nfor varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>And you want MIB object to be represented as an OID, then the <code>varBind</code> in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre><code>&gt;&gt;&gt; objectIdentity = ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n&gt;&gt;&gt; objectIdentity.resolveWithMib(mibViewController)\n&gt;&gt;&gt; objectIdentity.getOid()\nObjectName('1.3.6.1.2.1.1.1.0')\n</code></pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>Using pysnmp, how you perform resolution on queries that return an OID instead of value?</p>\n\n<p>I wrote a lookup tool using pysnmp, here are the inputs and results :</p>\n\n<pre> </pre>\n\n<p>How can the result be converted to the text value   (reference <a href=\"http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR\" rel=\"nofollow noreferrer\">http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR</a>)</p>\n", "answer_wo_code": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre> </pre>\n\n<p>And you want MIB object to be represented as an OID, then the   in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre> </pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pysnmp.smi.rfc1902.ObjectType"}, "class_func_label": {"type": "literal", "value": "pysnmp.smi.rfc1902.ObjectType"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Create an object representing MIB variable.\n\n    Instances of :py:class:`~pysnmp.smi.rfc1902.ObjectType` class are\n    containers incorporating :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n    class instance (identifying MIB variable) and optional value belonging\n    to one of SNMP types (:RFC:`1902`).\n\n    Typical MIB variable is defined like this (from *SNMPv2-MIB.txt*):\n\n    .. code-block:: bash\n\n       sysDescr OBJECT-TYPE\n           SYNTAX      DisplayString (SIZE (0..255))\n           MAX-ACCESS  read-only\n           STATUS      current\n           DESCRIPTION\n                   \"A textual description of the entity.  This value should...\"\n           ::= { system 1 }\n\n    Corresponding ObjectType instantiation would look like this:\n\n    .. code-block:: python\n\n        ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr'), 'Linux i386 box')\n\n    In order to behave like SNMP variable-binding (:RFC:`1157#section-4.1.1`),\n    :py:class:`~pysnmp.smi.rfc1902.ObjectType` objects also support\n    sequence protocol addressing `objectIdentity` as its 0-th element\n    and `objectSyntax` as 1-st.\n\n    See :RFC:`1902#section-2` for more information on OBJECT-TYPE SMI\n    definitions.\n\n    Parameters\n    ----------\n    objectIdentity : :py:class:`~pysnmp.smi.rfc1902.ObjectIdentity`\n        Class instance representing MIB variable identification.\n    objectSyntax :\n        Represents a value associated with this MIB variable. Values of\n        built-in Python types will be automatically converted into SNMP\n        object as specified in OBJECT-TYPE->SYNTAX field.\n\n    Notes\n    -----\n        Actual conversion between MIB variable representation formats occurs\n        upon :py:meth:`~pysnmp.smi.rfc1902.ObjectType.resolveWithMib`\n        invocation.\n\n    Examples\n    --------\n    >>> from pysnmp.smi.rfc1902 import *\n    >>> ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'))\n    ObjectType(ObjectIdentity('1.3.6.1.2.1.1.1.0'), Null(''))\n    >>> ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n    ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0), 'Linux i386')\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/53603783"}, "title": {"type": "literal", "value": "pysnmp OID resolution"}, "content": {"type": "literal", "value": "<p>Using pysnmp, how you perform resolution on queries that return an OID instead of value?</p>\n\n<p>I wrote a lookup tool using pysnmp, here are the inputs and results :</p>\n\n<pre><code>./run_snmp_discovery.py --host 1.1.1.1 --community XXXXXX --command get --mib_oid_index  '{ \"mib\" : \"SNMPv2-MIB\", \"oid\" : \"sysObjectID\", \"index\" : \"0\"  }' --verbose\nDebug: 'varBind': SNMPv2-MIB::sysObjectID.0 = SNMPv2-SMI::enterprises.9.1.222\n{\"0\": {\"sysObjectID\": \"SNMPv2-SMI::enterprises.9.1.222\"}}\n</code></pre>\n\n<p>How can the result be converted to the text value <code>cisco7206VXR</code> (reference <a href=\"http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR\" rel=\"nofollow noreferrer\">http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR</a>)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre><code>from pysnmp.hlapi import *\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('public'),\n           UdpTransportTarget(('demo.snmplabs.com', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)))\n)\n\nfor varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>And you want MIB object to be represented as an OID, then the <code>varBind</code> in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre><code>&gt;&gt;&gt; objectIdentity = ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n&gt;&gt;&gt; objectIdentity.resolveWithMib(mibViewController)\n&gt;&gt;&gt; objectIdentity.getOid()\nObjectName('1.3.6.1.2.1.1.1.0')\n</code></pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre><code>from pysnmp.hlapi import *\n\nerrorIndication, errorStatus, errorIndex, varBinds = next(\n    getCmd(SnmpEngine(),\n           CommunityData('public'),\n           UdpTransportTarget(('demo.snmplabs.com', 161)),\n           ContextData(),\n           ObjectType(ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)))\n)\n\nfor varBind in varBinds:\n    print(' = '.join([x.prettyPrint() for x in varBind]))\n</code></pre>\n\n<p>And you want MIB object to be represented as an OID, then the <code>varBind</code> in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre><code>&gt;&gt;&gt; objectIdentity = ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)\n&gt;&gt;&gt; objectIdentity.resolveWithMib(mibViewController)\n&gt;&gt;&gt; objectIdentity.getOid()\nObjectName('1.3.6.1.2.1.1.1.0')\n</code></pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>Using pysnmp, how you perform resolution on queries that return an OID instead of value?</p>\n\n<p>I wrote a lookup tool using pysnmp, here are the inputs and results :</p>\n\n<pre> </pre>\n\n<p>How can the result be converted to the text value   (reference <a href=\"http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR\" rel=\"nofollow noreferrer\">http://www.circitor.fr/Mibs/Html/C/CISCO-PRODUCTS-MIB.php#cisco7206VXR</a>)</p>\n", "answer_wo_code": "<p>If you are using code <a href=\"http://snmplabs.com/pysnmp/quick-start.html#fetch-snmp-variable\" rel=\"nofollow noreferrer\">like this</a>:</p>\n\n<pre> </pre>\n\n<p>And you want MIB object to be represented as an OID, then the   in the code above is actually an <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectType\" rel=\"nofollow noreferrer\">ObjectType</a> class instance which behaves like a tuple of two elements. The first element is <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity\" rel=\"nofollow noreferrer\">ObjectIdentity</a> which has the <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.getOid\" rel=\"nofollow noreferrer\">.getOid</a> method:</p>\n\n<pre> </pre>\n\n<p>If you want MIB object and its value to be fully represented in MIB terms (i.e. value resolved into an enumeration), then you just need to load the MIB that defines that MIB object (perhaps, CISCO-PRODUCTS-MIB) using <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.loadMibs\" rel=\"nofollow noreferrer\">.loadMibs()</a> method. You might also need to set up a <a href=\"http://snmplabs.com/pysnmp/docs/api-reference.html#pysnmp.smi.rfc1902.ObjectIdentity.addAsn1MibSource\" rel=\"nofollow noreferrer\">search path</a> to let pysnmp find the MIB you refer.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/lineno"}, "class_func_label": {"type": "literal", "value": "lineno"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nReturn the cumulative line number of the line that has just been read.\nBefore the first line has been read, returns 0. After the last line\nof the last file has been read, returns the line number of that line."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/768634"}, "title": {"type": "literal", "value": "Parse a .py file, read the AST, modify it, then write back the modified source code"}, "content": {"type": "literal", "value": "<p>I want to programmatically edit python source code. Basically I want to read a <code>.py</code> file, generate the <a href=\"http://en.wikipedia.org/wiki/Abstract_syntax_tree\" rel=\"noreferrer\">AST</a>, and then write back the modified python source code (i.e. another <code>.py</code> file).</p>\n\n<p>There are ways to parse/compile python source code using standard python modules, such as <a href=\"http://docs.python.org/library/ast.html\" rel=\"noreferrer\"><code>ast</code></a> or <a href=\"http://docs.python.org/library/compiler.html\" rel=\"noreferrer\"><code>compiler</code></a>.  However, I don't think any of them support ways to modify the source code (e.g. delete this function declaration) and then write back the modifying python source code.</p>\n\n<p>UPDATE: The reason I want to do this is I'd like to write a <a href=\"http://en.wikipedia.org/wiki/Mutation_testing\" rel=\"noreferrer\">Mutation testing library</a> for python, mostly by deleting statements / expressions, rerunning tests and seeing what breaks.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I used to use baron for this, but have now switched to parso because it's up to date with modern python. It works great. </p>\n\n<p>I also needed this for a mutation tester. It's really quite simple to make one with parso, check out my code at <a href=\"https://github.com/boxed/mutmut\" rel=\"nofollow noreferrer\">https://github.com/boxed/mutmut</a></p>\n\n\n<p>I used to use baron for this, but have now switched to parso because it's up to date with modern python. It works great. </p>\n\n<p>I also needed this for a mutation tester. It's really quite simple to make one with parso, check out my code at <a href=\"https://github.com/boxed/mutmut\" rel=\"nofollow noreferrer\">https://github.com/boxed/mutmut</a></p>\n\n\n<p>A <a href=\"http://en.wikipedia.org/wiki/Program_transformation\" rel=\"nofollow noreferrer\">Program Transformation System</a> is a tool that parses source text, builds ASTs, allows you to modify them using source-to-source transformations (\"if you see this pattern, replace it by that pattern\").   Such tools are ideal for doing mutation of existing source codes, which are just \"if you see this pattern, replace by a pattern variant\".</p>\n\n<p>Of course, you need a program transformation engine that can parse the language of interest to you, and still do the pattern-directed transformations.  Our <a href=\"http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\" rel=\"nofollow noreferrer\">DMS Software Reengineering Toolkit</a> is a system that can do that, and handles Python, and a variety of other languages. </p>\n\n<p>See this <a href=\"https://stackoverflow.com/a/22118379/120163\">SO answer for an example of a DMS-parsed AST for Python capturing comments</a> accurately.  DMS can make changes to the AST, and regenerate valid text, including the comments.  You can ask it to prettyprint the AST, using its own formatting conventions (you can changes these), or do \"fidelity printing\", which uses the original line and column information to maximally preserve the original layout (some change in layout where new code is inserted is unavoidable).</p>\n\n<p>To implement a \"mutation\" rule for Python with DMS, you could write the following:</p>\n\n<pre><code>rule mutate_addition(s:sum, p:product):sum-&gt;sum =\n  \" \\s + \\p \" -&gt; \" \\s - \\p\"\n if mutate_this_place(s);\n</code></pre>\n\n<p>This rule replace  \"+\" with \"-\" in a syntactically correct way; it operates on the AST and thus won't touch strings or comments that happen to look right.  The extra condition on \"mutate_this_place\" is to let you control how often this occurs; you don't want to mutate <em>every</em> place in the program.</p>\n\n<p>You'd obviously want a bunch more rules like this that detect various code structures, and replace them by the mutated versions.   DMS is happy to apply a set of rules.  The mutated AST is then prettyprinted.</p>\n\n\n<p>A <a href=\"http://en.wikipedia.org/wiki/Program_transformation\" rel=\"nofollow noreferrer\">Program Transformation System</a> is a tool that parses source text, builds ASTs, allows you to modify them using source-to-source transformations (\"if you see this pattern, replace it by that pattern\").   Such tools are ideal for doing mutation of existing source codes, which are just \"if you see this pattern, replace by a pattern variant\".</p>\n\n<p>Of course, you need a program transformation engine that can parse the language of interest to you, and still do the pattern-directed transformations.  Our <a href=\"http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\" rel=\"nofollow noreferrer\">DMS Software Reengineering Toolkit</a> is a system that can do that, and handles Python, and a variety of other languages. </p>\n\n<p>See this <a href=\"https://stackoverflow.com/a/22118379/120163\">SO answer for an example of a DMS-parsed AST for Python capturing comments</a> accurately.  DMS can make changes to the AST, and regenerate valid text, including the comments.  You can ask it to prettyprint the AST, using its own formatting conventions (you can changes these), or do \"fidelity printing\", which uses the original line and column information to maximally preserve the original layout (some change in layout where new code is inserted is unavoidable).</p>\n\n<p>To implement a \"mutation\" rule for Python with DMS, you could write the following:</p>\n\n<pre><code>rule mutate_addition(s:sum, p:product):sum-&gt;sum =\n  \" \\s + \\p \" -&gt; \" \\s - \\p\"\n if mutate_this_place(s);\n</code></pre>\n\n<p>This rule replace  \"+\" with \"-\" in a syntactically correct way; it operates on the AST and thus won't touch strings or comments that happen to look right.  The extra condition on \"mutate_this_place\" is to let you control how often this occurs; you don't want to mutate <em>every</em> place in the program.</p>\n\n<p>You'd obviously want a bunch more rules like this that detect various code structures, and replace them by the mutated versions.   DMS is happy to apply a set of rules.  The mutated AST is then prettyprinted.</p>\n\n\n<p>You might not need to re-generate source code.  That's a bit dangerous for me to say, of course, since you have not actually explained why you think you need to generate a .py file full of code; but:</p>\n\n<ul>\n<li><p>If you want to generate a .py file that people will actually use, maybe so that they can fill out a form and get a useful .py file to insert into their project, then you don't want to change it into an AST and back because you'll lose <strike>all formatting (think of the blank lines that make Python so readable by grouping related sets of lines together)</strike> (<a href=\"http://docs.python.org/py3k/library/ast.html#ast.AST.lineno\" rel=\"noreferrer\">ast nodes have <code>lineno</code> and <code>col_offset</code> attributes</a>) comments.  Instead, you'll probably want to use a templating engine (the <a href=\"http://docs.djangoproject.com/en/dev/topics/templates/#topics-templates\" rel=\"noreferrer\">Django template language</a>, for example, is designed to make templating even text files easy) to customize the .py file, or else use Rick Copeland's <a href=\"http://code.google.com/p/metapython/\" rel=\"noreferrer\">MetaPython</a> extension.</p></li>\n<li><p>If you are trying to make a change during compilation of a module, note that you don't have to go all the way back to text; you can just compile the AST directly instead of turning it back into a .py file.</p></li>\n<li><p>But in almost any and every case, you are probably trying to do something dynamic that a language like Python actually makes very easy, without writing new .py files! If you expand your question to let us know what you actually want to accomplish, new .py files will probably not be involved in the answer at all; I have seen hundreds of Python projects doing hundreds of real-world things, and not a single one of them needed to ever writer a .py file.  So, I must admit, I'm a bit of a skeptic that you've found the first good use-case. :-)</p></li>\n</ul>\n\n<p><strong>Update:</strong> now that you've explained what you're trying to do, I'd be tempted to just operate on the AST anyway. You will want to mutate by removing, not lines of a file (which could result in half-statements that simply die with a SyntaxError), but whole statements \u2014 and what better place to do that than in the AST?</p>\n\n\n<p>You might not need to re-generate source code.  That's a bit dangerous for me to say, of course, since you have not actually explained why you think you need to generate a .py file full of code; but:</p>\n\n<ul>\n<li><p>If you want to generate a .py file that people will actually use, maybe so that they can fill out a form and get a useful .py file to insert into their project, then you don't want to change it into an AST and back because you'll lose <strike>all formatting (think of the blank lines that make Python so readable by grouping related sets of lines together)</strike> (<a href=\"http://docs.python.org/py3k/library/ast.html#ast.AST.lineno\" rel=\"noreferrer\">ast nodes have <code>lineno</code> and <code>col_offset</code> attributes</a>) comments.  Instead, you'll probably want to use a templating engine (the <a href=\"http://docs.djangoproject.com/en/dev/topics/templates/#topics-templates\" rel=\"noreferrer\">Django template language</a>, for example, is designed to make templating even text files easy) to customize the .py file, or else use Rick Copeland's <a href=\"http://code.google.com/p/metapython/\" rel=\"noreferrer\">MetaPython</a> extension.</p></li>\n<li><p>If you are trying to make a change during compilation of a module, note that you don't have to go all the way back to text; you can just compile the AST directly instead of turning it back into a .py file.</p></li>\n<li><p>But in almost any and every case, you are probably trying to do something dynamic that a language like Python actually makes very easy, without writing new .py files! If you expand your question to let us know what you actually want to accomplish, new .py files will probably not be involved in the answer at all; I have seen hundreds of Python projects doing hundreds of real-world things, and not a single one of them needed to ever writer a .py file.  So, I must admit, I'm a bit of a skeptic that you've found the first good use-case. :-)</p></li>\n</ul>\n\n<p><strong>Update:</strong> now that you've explained what you're trying to do, I'd be tempted to just operate on the AST anyway. You will want to mutate by removing, not lines of a file (which could result in half-statements that simply die with a SyntaxError), but whole statements \u2014 and what better place to do that than in the AST?</p>\n\n\n<p>The builtin ast module doesn't seem to have a method to convert back to source.  However, the <a href=\"https://pypi.python.org/pypi/codegen/1.0\" rel=\"noreferrer\">codegen</a> module here provides a pretty printer for the ast that would enable you do do so.\neg.</p>\n\n<pre><code>import ast\nimport codegen\n\nexpr=\"\"\"\ndef foo():\n   print(\"hello world\")\n\"\"\"\np=ast.parse(expr)\n\np.body[0].body = [ ast.parse(\"return 42\").body[0] ] # Replace function body with \"return 42\"\n\nprint(codegen.to_source(p))\n</code></pre>\n\n<p>This will print:</p>\n\n<pre><code>def foo():\n    return 42\n</code></pre>\n\n<p>Note that you may lose the exact formatting and comments, as these are not preserved.</p>\n\n<p>However, you may not need to.  If all you require is to execute the replaced AST, you can do so simply by calling compile() on the ast, and execing the resulting code object.</p>\n\n\n<p>The builtin ast module doesn't seem to have a method to convert back to source.  However, the <a href=\"https://pypi.python.org/pypi/codegen/1.0\" rel=\"noreferrer\">codegen</a> module here provides a pretty printer for the ast that would enable you do do so.\neg.</p>\n\n<pre><code>import ast\nimport codegen\n\nexpr=\"\"\"\ndef foo():\n   print(\"hello world\")\n\"\"\"\np=ast.parse(expr)\n\np.body[0].body = [ ast.parse(\"return 42\").body[0] ] # Replace function body with \"return 42\"\n\nprint(codegen.to_source(p))\n</code></pre>\n\n<p>This will print:</p>\n\n<pre><code>def foo():\n    return 42\n</code></pre>\n\n<p>Note that you may lose the exact formatting and comments, as these are not preserved.</p>\n\n<p>However, you may not need to.  If all you require is to execute the replaced AST, you can do so simply by calling compile() on the ast, and execing the resulting code object.</p>\n\n\n<p>In a different answer I suggested using the <code>astor</code> package, but I have since found a more up-to-date AST un-parsing package called <a href=\"https://github.com/simonpercivall/astunparse\"><code>astunparse</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astunparse\n&gt;&gt;&gt; print(astunparse.unparse(ast.parse('def foo(x): return 2 * x')))\n\n\ndef foo(x):\n    return (2 * x)\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>In a different answer I suggested using the <code>astor</code> package, but I have since found a more up-to-date AST un-parsing package called <a href=\"https://github.com/simonpercivall/astunparse\"><code>astunparse</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astunparse\n&gt;&gt;&gt; print(astunparse.unparse(ast.parse('def foo(x): return 2 * x')))\n\n\ndef foo(x):\n    return (2 * x)\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>Parsing and modifying the code structure is certainly possible with the help of <code>ast</code> module and I will show it in an example in a moment. However, writing back the modified source code is not possible with <code>ast</code> module alone. There are other modules available for this job such as one <a href=\"http://svn.python.org/view/python/trunk/Demo/parser/unparse.py?view=markup\" rel=\"noreferrer\">here</a>. </p>\n\n<p>NOTE: Example below can be treated as an introductory tutorial on the usage of <code>ast</code> module but a more comprehensive guide on using <code>ast</code> module is available here at <a href=\"https://greentreesnakes.readthedocs.io/en/latest/tofrom.html\" rel=\"noreferrer\">Green Tree snakes tutorial</a> and <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"noreferrer\">official documentation on <code>ast</code> module</a>.   </p>\n\n<p><strong>Introduction to <code>ast</code>:</strong> </p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; exec(compile(tree, filename=\"&lt;ast&gt;\", mode=\"exec\"))\nHello Python!!\n</code></pre>\n\n<p>You can parse the python code (represented in string) by simply calling the API <code>ast.parse()</code>. This returns the handle to Abstract Syntax Tree (AST) structure. Interestingly you can compile back this structure and execute it as shown above.</p>\n\n<p>Another very useful API is <code>ast.dump()</code> which dumps the whole AST in a string form. It can be used to inspect the tree structure and is very helpful in debugging. For example,</p>\n\n<p><em>On Python 2.7:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Print(dest=None, values=[Str(s='Hello Python!!')], nl=True)])\"\n</code></pre>\n\n<p><em>On Python 3.5:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print ('Hello Python!!')\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Expr(value=Call(func=Name(id='print', ctx=Load()), args=[Str(s='Hello Python!!')], keywords=[]))])\"\n</code></pre>\n\n<p>Notice the difference in syntax for print statement in Python 2.7 vs. Python 3.5 and the difference in type of AST node in respective trees.</p>\n\n<hr>\n\n<p><strong>How to modify code using <code>ast</code>:</strong></p>\n\n<p>Now, let's a have a look at an example of modification of python code by <code>ast</code> module. The main tool for modifying AST structure is <code>ast.NodeTransformer</code> class. Whenever one needs to modify the AST, he/she needs to subclass from it and write Node Transformation(s) accordingly. </p>\n\n<p>For our example, let's try to write a simple utility which transforms the Python 2 , print statements to Python 3 function calls. </p>\n\n<p><strong>Print statement to Fun call converter utility:  print2to3.py:</strong></p>\n\n<pre><code>#!/usr/bin/env python\n'''\nThis utility converts the python (2.7) statements to Python 3 alike function calls before running the code.\n\nUSAGE:\n     python print2to3.py &lt;filename&gt;\n'''\nimport ast\nimport sys\n\nclass P2to3(ast.NodeTransformer):\n    def visit_Print(self, node):\n        new_node = ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n            args=node.values,\n            keywords=[], starargs=None, kwargs=None))\n        ast.copy_location(new_node, node)\n        return new_node\n\ndef main(filename=None):\n    if not filename:\n        return\n\n    with open(filename, 'r') as fp:\n        data = fp.readlines()\n    data = ''.join(data)\n    tree = ast.parse(data)\n\n    print \"Converting python 2 print statements to Python 3 function calls\"\n    print \"-\" * 35\n    P2to3().visit(tree)\n    ast.fix_missing_locations(tree)\n    # print ast.dump(tree)\n\n    exec(compile(tree, filename=\"p23\", mode=\"exec\"))\n\nif __name__ == '__main__':\n    if len(sys.argv) &lt;=1:\n        print (\"\\nUSAGE:\\n\\t print2to3.py &lt;filename&gt;\")\n        sys.exit(1)\n    else:\n        main(sys.argv[1])\n</code></pre>\n\n<p>This utility can be tried on small example file, such as one below, and it should work fine. </p>\n\n<p><strong>Test Input file : py2.py</strong></p>\n\n<pre><code>class A(object):\n    def __init__(self):\n        pass\n\ndef good():\n    print \"I am good\"\n\nmain = good\n\nif __name__ == '__main__':\n    print \"I am in main\"\n    main()\n</code></pre>\n\n<p>Please note that above transformation is only for <code>ast</code> tutorial purpose and in real case scenario one will have to look at all different scenarios such as <code>print \" x is %s\" % (\"Hello Python\")</code>.</p>\n\n\n<p>Parsing and modifying the code structure is certainly possible with the help of <code>ast</code> module and I will show it in an example in a moment. However, writing back the modified source code is not possible with <code>ast</code> module alone. There are other modules available for this job such as one <a href=\"http://svn.python.org/view/python/trunk/Demo/parser/unparse.py?view=markup\" rel=\"noreferrer\">here</a>. </p>\n\n<p>NOTE: Example below can be treated as an introductory tutorial on the usage of <code>ast</code> module but a more comprehensive guide on using <code>ast</code> module is available here at <a href=\"https://greentreesnakes.readthedocs.io/en/latest/tofrom.html\" rel=\"noreferrer\">Green Tree snakes tutorial</a> and <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"noreferrer\">official documentation on <code>ast</code> module</a>.   </p>\n\n<p><strong>Introduction to <code>ast</code>:</strong> </p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; exec(compile(tree, filename=\"&lt;ast&gt;\", mode=\"exec\"))\nHello Python!!\n</code></pre>\n\n<p>You can parse the python code (represented in string) by simply calling the API <code>ast.parse()</code>. This returns the handle to Abstract Syntax Tree (AST) structure. Interestingly you can compile back this structure and execute it as shown above.</p>\n\n<p>Another very useful API is <code>ast.dump()</code> which dumps the whole AST in a string form. It can be used to inspect the tree structure and is very helpful in debugging. For example,</p>\n\n<p><em>On Python 2.7:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Print(dest=None, values=[Str(s='Hello Python!!')], nl=True)])\"\n</code></pre>\n\n<p><em>On Python 3.5:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print ('Hello Python!!')\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Expr(value=Call(func=Name(id='print', ctx=Load()), args=[Str(s='Hello Python!!')], keywords=[]))])\"\n</code></pre>\n\n<p>Notice the difference in syntax for print statement in Python 2.7 vs. Python 3.5 and the difference in type of AST node in respective trees.</p>\n\n<hr>\n\n<p><strong>How to modify code using <code>ast</code>:</strong></p>\n\n<p>Now, let's a have a look at an example of modification of python code by <code>ast</code> module. The main tool for modifying AST structure is <code>ast.NodeTransformer</code> class. Whenever one needs to modify the AST, he/she needs to subclass from it and write Node Transformation(s) accordingly. </p>\n\n<p>For our example, let's try to write a simple utility which transforms the Python 2 , print statements to Python 3 function calls. </p>\n\n<p><strong>Print statement to Fun call converter utility:  print2to3.py:</strong></p>\n\n<pre><code>#!/usr/bin/env python\n'''\nThis utility converts the python (2.7) statements to Python 3 alike function calls before running the code.\n\nUSAGE:\n     python print2to3.py &lt;filename&gt;\n'''\nimport ast\nimport sys\n\nclass P2to3(ast.NodeTransformer):\n    def visit_Print(self, node):\n        new_node = ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n            args=node.values,\n            keywords=[], starargs=None, kwargs=None))\n        ast.copy_location(new_node, node)\n        return new_node\n\ndef main(filename=None):\n    if not filename:\n        return\n\n    with open(filename, 'r') as fp:\n        data = fp.readlines()\n    data = ''.join(data)\n    tree = ast.parse(data)\n\n    print \"Converting python 2 print statements to Python 3 function calls\"\n    print \"-\" * 35\n    P2to3().visit(tree)\n    ast.fix_missing_locations(tree)\n    # print ast.dump(tree)\n\n    exec(compile(tree, filename=\"p23\", mode=\"exec\"))\n\nif __name__ == '__main__':\n    if len(sys.argv) &lt;=1:\n        print (\"\\nUSAGE:\\n\\t print2to3.py &lt;filename&gt;\")\n        sys.exit(1)\n    else:\n        main(sys.argv[1])\n</code></pre>\n\n<p>This utility can be tried on small example file, such as one below, and it should work fine. </p>\n\n<p><strong>Test Input file : py2.py</strong></p>\n\n<pre><code>class A(object):\n    def __init__(self):\n        pass\n\ndef good():\n    print \"I am good\"\n\nmain = good\n\nif __name__ == '__main__':\n    print \"I am in main\"\n    main()\n</code></pre>\n\n<p>Please note that above transformation is only for <code>ast</code> tutorial purpose and in real case scenario one will have to look at all different scenarios such as <code>print \" x is %s\" % (\"Hello Python\")</code>.</p>\n\n\n<p><a href=\"https://stackoverflow.com/a/769202\">One of the other answers</a> recommends <code>codegen</code>, which seems to have been superceded by <a href=\"https://github.com/berkerpeksag/astor\" rel=\"nofollow noreferrer\"><code>astor</code></a>. The version of <a href=\"https://pypi.python.org/pypi/astor\" rel=\"nofollow noreferrer\"><code>astor</code> on PyPI</a> (version 0.5 as of this writing) seems to be a little outdated as well, so you can install the development version of <code>astor</code> as follows.</p>\n\n<pre><code>pip install git+https://github.com/berkerpeksag/astor.git#egg=astor\n</code></pre>\n\n<p>Then you can use <code>astor.to_source</code> to convert a Python AST to human-readable Python source code:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astor\n&gt;&gt;&gt; print(astor.to_source(ast.parse('def foo(x): return 2 * x')))\ndef foo(x):\n    return 2 * x\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p><a href=\"https://stackoverflow.com/a/769202\">One of the other answers</a> recommends <code>codegen</code>, which seems to have been superceded by <a href=\"https://github.com/berkerpeksag/astor\" rel=\"nofollow noreferrer\"><code>astor</code></a>. The version of <a href=\"https://pypi.python.org/pypi/astor\" rel=\"nofollow noreferrer\"><code>astor</code> on PyPI</a> (version 0.5 as of this writing) seems to be a little outdated as well, so you can install the development version of <code>astor</code> as follows.</p>\n\n<pre><code>pip install git+https://github.com/berkerpeksag/astor.git#egg=astor\n</code></pre>\n\n<p>Then you can use <code>astor.to_source</code> to convert a Python AST to human-readable Python source code:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astor\n&gt;&gt;&gt; print(astor.to_source(ast.parse('def foo(x): return 2 * x')))\ndef foo(x):\n    return 2 * x\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>We had a similar need, which wasn't solved by other answers here. So we created a library for this, <a href=\"https://github.com/gristlabs/asttokens\" rel=\"nofollow noreferrer\">ASTTokens</a>, which takes an AST tree produced with the <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> or <a href=\"https://www.astroid.org/\" rel=\"nofollow noreferrer\">astroid</a> modules, and marks it with the ranges of text in the original source code.</p>\n\n<p>It doesn't do modifications of code directly, but that's not hard to add on top, since it does tell you the range of text you need to modify.</p>\n\n<p>For example, this wraps a function call in <code>WRAP(...)</code>, preserving comments and everything else:</p>\n\n<pre><code>example = \"\"\"\ndef foo(): # Test\n  '''My func'''\n  log(\"hello world\")  # Print\n\"\"\"\n\nimport ast, asttokens\natok = asttokens.ASTTokens(example, parse=True)\n\ncall = next(n for n in ast.walk(atok.tree) if isinstance(n, ast.Call))\nstart, end = atok.get_text_range(call)\nprint(atok.text[:start] + ('WRAP(%s)' % atok.text[start:end])  + atok.text[end:])\n</code></pre>\n\n<p>Produces:</p>\n\n<pre><code>def foo(): # Test\n  '''My func'''\n  WRAP(log(\"hello world\"))  # Print\n</code></pre>\n\n<p>Hope this helps!</p>\n\n\n<p>We had a similar need, which wasn't solved by other answers here. So we created a library for this, <a href=\"https://github.com/gristlabs/asttokens\" rel=\"nofollow noreferrer\">ASTTokens</a>, which takes an AST tree produced with the <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> or <a href=\"https://www.astroid.org/\" rel=\"nofollow noreferrer\">astroid</a> modules, and marks it with the ranges of text in the original source code.</p>\n\n<p>It doesn't do modifications of code directly, but that's not hard to add on top, since it does tell you the range of text you need to modify.</p>\n\n<p>For example, this wraps a function call in <code>WRAP(...)</code>, preserving comments and everything else:</p>\n\n<pre><code>example = \"\"\"\ndef foo(): # Test\n  '''My func'''\n  log(\"hello world\")  # Print\n\"\"\"\n\nimport ast, asttokens\natok = asttokens.ASTTokens(example, parse=True)\n\ncall = next(n for n in ast.walk(atok.tree) if isinstance(n, ast.Call))\nstart, end = atok.get_text_range(call)\nprint(atok.text[:start] + ('WRAP(%s)' % atok.text[start:end])  + atok.text[end:])\n</code></pre>\n\n<p>Produces:</p>\n\n<pre><code>def foo(): # Test\n  '''My func'''\n  WRAP(log(\"hello world\"))  # Print\n</code></pre>\n\n<p>Hope this helps!</p>\n\n\n<p><a href=\"https://github.com/mkwiatkowski/pythoscope\" rel=\"nofollow noreferrer\">Pythoscope</a> does this to the test cases it automatically generates as does the <a href=\"http://docs.python.org/library/2to3.html\" rel=\"nofollow noreferrer\">2to3</a> tool for python 2.6 (it converts python 2.x source into python 3.x source). </p>\n\n<p>Both these tools uses the <a href=\"http://svn.python.org/projects/python/trunk/Lib/lib2to3/\" rel=\"nofollow noreferrer\">lib2to3</a> library which is a implementation of the python parser/compiler machinery that can preserve comments in source when it's round tripped from source -> AST -> source.</p>\n\n<p>The <a href=\"https://github.com/python-rope/rope\" rel=\"nofollow noreferrer\">rope project</a> may meet your needs if you want to do more refactoring like transforms.</p>\n\n<p>The <a href=\"http://docs.python.org/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> module is your other option, and <a href=\"https://svn.python.org/projects/python/trunk/Demo/parser/unparse.py\" rel=\"nofollow noreferrer\">there's an older example of how to \"unparse\" syntax trees back into code</a> (using the parser module). But the <code>ast</code> module is more useful when doing an AST transform on code that is then transformed into a code object.</p>\n\n<p>The <a href=\"https://redbaron.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">redbaron</a> project also may be a good fit (ht Xavier Combelle)</p>\n\n\n<p><a href=\"https://github.com/mkwiatkowski/pythoscope\" rel=\"nofollow noreferrer\">Pythoscope</a> does this to the test cases it automatically generates as does the <a href=\"http://docs.python.org/library/2to3.html\" rel=\"nofollow noreferrer\">2to3</a> tool for python 2.6 (it converts python 2.x source into python 3.x source). </p>\n\n<p>Both these tools uses the <a href=\"http://svn.python.org/projects/python/trunk/Lib/lib2to3/\" rel=\"nofollow noreferrer\">lib2to3</a> library which is a implementation of the python parser/compiler machinery that can preserve comments in source when it's round tripped from source -> AST -> source.</p>\n\n<p>The <a href=\"https://github.com/python-rope/rope\" rel=\"nofollow noreferrer\">rope project</a> may meet your needs if you want to do more refactoring like transforms.</p>\n\n<p>The <a href=\"http://docs.python.org/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> module is your other option, and <a href=\"https://svn.python.org/projects/python/trunk/Demo/parser/unparse.py\" rel=\"nofollow noreferrer\">there's an older example of how to \"unparse\" syntax trees back into code</a> (using the parser module). But the <code>ast</code> module is more useful when doing an AST transform on code that is then transformed into a code object.</p>\n\n<p>The <a href=\"https://redbaron.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">redbaron</a> project also may be a good fit (ht Xavier Combelle)</p>\n\n\n<p>I've created recently quite stable (core is really well tested) and extensible piece of code which generates code from <code>ast</code> tree: <a href=\"https://github.com/paluh/code-formatter\" rel=\"nofollow\">https://github.com/paluh/code-formatter</a> .</p>\n\n<p>I'm using my project as a base for a small vim plugin (which I'm using every day), so my goal is to generate really nice and readable python code.</p>\n\n<p>P.S.\nI've tried to extend <code>codegen</code> but it's architecture is based on <code>ast.NodeVisitor</code> interface, so formatters (<code>visitor_</code> methods) are just functions. I've found this structure quite limiting and hard to optimize (in case of long and nested expressions it's easier to keep objects tree and cache some partial results - in other way you can hit exponential complexity if you want to search for best layout). <strong>BUT</strong> <code>codegen</code> as every piece of mitsuhiko's work (which I've read) is very well written and concise.</p>\n\n\n<p>I've created recently quite stable (core is really well tested) and extensible piece of code which generates code from <code>ast</code> tree: <a href=\"https://github.com/paluh/code-formatter\" rel=\"nofollow\">https://github.com/paluh/code-formatter</a> .</p>\n\n<p>I'm using my project as a base for a small vim plugin (which I'm using every day), so my goal is to generate really nice and readable python code.</p>\n\n<p>P.S.\nI've tried to extend <code>codegen</code> but it's architecture is based on <code>ast.NodeVisitor</code> interface, so formatters (<code>visitor_</code> methods) are just functions. I've found this structure quite limiting and hard to optimize (in case of long and nested expressions it's easier to keep objects tree and cache some partial results - in other way you can hit exponential complexity if you want to search for best layout). <strong>BUT</strong> <code>codegen</code> as every piece of mitsuhiko's work (which I've read) is very well written and concise.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I used to use baron for this, but have now switched to parso because it's up to date with modern python. It works great. </p>\n\n<p>I also needed this for a mutation tester. It's really quite simple to make one with parso, check out my code at <a href=\"https://github.com/boxed/mutmut\" rel=\"nofollow noreferrer\">https://github.com/boxed/mutmut</a></p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>A <a href=\"http://en.wikipedia.org/wiki/Program_transformation\" rel=\"nofollow noreferrer\">Program Transformation System</a> is a tool that parses source text, builds ASTs, allows you to modify them using source-to-source transformations (\"if you see this pattern, replace it by that pattern\").   Such tools are ideal for doing mutation of existing source codes, which are just \"if you see this pattern, replace by a pattern variant\".</p>\n\n<p>Of course, you need a program transformation engine that can parse the language of interest to you, and still do the pattern-directed transformations.  Our <a href=\"http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\" rel=\"nofollow noreferrer\">DMS Software Reengineering Toolkit</a> is a system that can do that, and handles Python, and a variety of other languages. </p>\n\n<p>See this <a href=\"https://stackoverflow.com/a/22118379/120163\">SO answer for an example of a DMS-parsed AST for Python capturing comments</a> accurately.  DMS can make changes to the AST, and regenerate valid text, including the comments.  You can ask it to prettyprint the AST, using its own formatting conventions (you can changes these), or do \"fidelity printing\", which uses the original line and column information to maximally preserve the original layout (some change in layout where new code is inserted is unavoidable).</p>\n\n<p>To implement a \"mutation\" rule for Python with DMS, you could write the following:</p>\n\n<pre><code>rule mutate_addition(s:sum, p:product):sum-&gt;sum =\n  \" \\s + \\p \" -&gt; \" \\s - \\p\"\n if mutate_this_place(s);\n</code></pre>\n\n<p>This rule replace  \"+\" with \"-\" in a syntactically correct way; it operates on the AST and thus won't touch strings or comments that happen to look right.  The extra condition on \"mutate_this_place\" is to let you control how often this occurs; you don't want to mutate <em>every</em> place in the program.</p>\n\n<p>You'd obviously want a bunch more rules like this that detect various code structures, and replace them by the mutated versions.   DMS is happy to apply a set of rules.  The mutated AST is then prettyprinted.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "9"}, "answer_3": {"type": "literal", "value": "<p>You might not need to re-generate source code.  That's a bit dangerous for me to say, of course, since you have not actually explained why you think you need to generate a .py file full of code; but:</p>\n\n<ul>\n<li><p>If you want to generate a .py file that people will actually use, maybe so that they can fill out a form and get a useful .py file to insert into their project, then you don't want to change it into an AST and back because you'll lose <strike>all formatting (think of the blank lines that make Python so readable by grouping related sets of lines together)</strike> (<a href=\"http://docs.python.org/py3k/library/ast.html#ast.AST.lineno\" rel=\"noreferrer\">ast nodes have <code>lineno</code> and <code>col_offset</code> attributes</a>) comments.  Instead, you'll probably want to use a templating engine (the <a href=\"http://docs.djangoproject.com/en/dev/topics/templates/#topics-templates\" rel=\"noreferrer\">Django template language</a>, for example, is designed to make templating even text files easy) to customize the .py file, or else use Rick Copeland's <a href=\"http://code.google.com/p/metapython/\" rel=\"noreferrer\">MetaPython</a> extension.</p></li>\n<li><p>If you are trying to make a change during compilation of a module, note that you don't have to go all the way back to text; you can just compile the AST directly instead of turning it back into a .py file.</p></li>\n<li><p>But in almost any and every case, you are probably trying to do something dynamic that a language like Python actually makes very easy, without writing new .py files! If you expand your question to let us know what you actually want to accomplish, new .py files will probably not be involved in the answer at all; I have seen hundreds of Python projects doing hundreds of real-world things, and not a single one of them needed to ever writer a .py file.  So, I must admit, I'm a bit of a skeptic that you've found the first good use-case. :-)</p></li>\n</ul>\n\n<p><strong>Update:</strong> now that you've explained what you're trying to do, I'd be tempted to just operate on the AST anyway. You will want to mutate by removing, not lines of a file (which could result in half-statements that simply die with a SyntaxError), but whole statements \u2014 and what better place to do that than in the AST?</p>\n"}, "answer_3_votes": {"type": "literal", "value": "21"}, "answer_4": {"type": "literal", "value": "<p>The builtin ast module doesn't seem to have a method to convert back to source.  However, the <a href=\"https://pypi.python.org/pypi/codegen/1.0\" rel=\"noreferrer\">codegen</a> module here provides a pretty printer for the ast that would enable you do do so.\neg.</p>\n\n<pre><code>import ast\nimport codegen\n\nexpr=\"\"\"\ndef foo():\n   print(\"hello world\")\n\"\"\"\np=ast.parse(expr)\n\np.body[0].body = [ ast.parse(\"return 42\").body[0] ] # Replace function body with \"return 42\"\n\nprint(codegen.to_source(p))\n</code></pre>\n\n<p>This will print:</p>\n\n<pre><code>def foo():\n    return 42\n</code></pre>\n\n<p>Note that you may lose the exact formatting and comments, as these are not preserved.</p>\n\n<p>However, you may not need to.  If all you require is to execute the replaced AST, you can do so simply by calling compile() on the ast, and execing the resulting code object.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "56"}, "answer_5": {"type": "literal", "value": "<p>In a different answer I suggested using the <code>astor</code> package, but I have since found a more up-to-date AST un-parsing package called <a href=\"https://github.com/simonpercivall/astunparse\"><code>astunparse</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astunparse\n&gt;&gt;&gt; print(astunparse.unparse(ast.parse('def foo(x): return 2 * x')))\n\n\ndef foo(x):\n    return (2 * x)\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "17"}, "answer_6": {"type": "literal", "value": "<p>Parsing and modifying the code structure is certainly possible with the help of <code>ast</code> module and I will show it in an example in a moment. However, writing back the modified source code is not possible with <code>ast</code> module alone. There are other modules available for this job such as one <a href=\"http://svn.python.org/view/python/trunk/Demo/parser/unparse.py?view=markup\" rel=\"noreferrer\">here</a>. </p>\n\n<p>NOTE: Example below can be treated as an introductory tutorial on the usage of <code>ast</code> module but a more comprehensive guide on using <code>ast</code> module is available here at <a href=\"https://greentreesnakes.readthedocs.io/en/latest/tofrom.html\" rel=\"noreferrer\">Green Tree snakes tutorial</a> and <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"noreferrer\">official documentation on <code>ast</code> module</a>.   </p>\n\n<p><strong>Introduction to <code>ast</code>:</strong> </p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; exec(compile(tree, filename=\"&lt;ast&gt;\", mode=\"exec\"))\nHello Python!!\n</code></pre>\n\n<p>You can parse the python code (represented in string) by simply calling the API <code>ast.parse()</code>. This returns the handle to Abstract Syntax Tree (AST) structure. Interestingly you can compile back this structure and execute it as shown above.</p>\n\n<p>Another very useful API is <code>ast.dump()</code> which dumps the whole AST in a string form. It can be used to inspect the tree structure and is very helpful in debugging. For example,</p>\n\n<p><em>On Python 2.7:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print 'Hello Python!!'\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Print(dest=None, values=[Str(s='Hello Python!!')], nl=True)])\"\n</code></pre>\n\n<p><em>On Python 3.5:</em></p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; tree = ast.parse(\"print ('Hello Python!!')\")\n&gt;&gt;&gt; ast.dump(tree)\n\"Module(body=[Expr(value=Call(func=Name(id='print', ctx=Load()), args=[Str(s='Hello Python!!')], keywords=[]))])\"\n</code></pre>\n\n<p>Notice the difference in syntax for print statement in Python 2.7 vs. Python 3.5 and the difference in type of AST node in respective trees.</p>\n\n<hr>\n\n<p><strong>How to modify code using <code>ast</code>:</strong></p>\n\n<p>Now, let's a have a look at an example of modification of python code by <code>ast</code> module. The main tool for modifying AST structure is <code>ast.NodeTransformer</code> class. Whenever one needs to modify the AST, he/she needs to subclass from it and write Node Transformation(s) accordingly. </p>\n\n<p>For our example, let's try to write a simple utility which transforms the Python 2 , print statements to Python 3 function calls. </p>\n\n<p><strong>Print statement to Fun call converter utility:  print2to3.py:</strong></p>\n\n<pre><code>#!/usr/bin/env python\n'''\nThis utility converts the python (2.7) statements to Python 3 alike function calls before running the code.\n\nUSAGE:\n     python print2to3.py &lt;filename&gt;\n'''\nimport ast\nimport sys\n\nclass P2to3(ast.NodeTransformer):\n    def visit_Print(self, node):\n        new_node = ast.Expr(value=ast.Call(func=ast.Name(id='print', ctx=ast.Load()),\n            args=node.values,\n            keywords=[], starargs=None, kwargs=None))\n        ast.copy_location(new_node, node)\n        return new_node\n\ndef main(filename=None):\n    if not filename:\n        return\n\n    with open(filename, 'r') as fp:\n        data = fp.readlines()\n    data = ''.join(data)\n    tree = ast.parse(data)\n\n    print \"Converting python 2 print statements to Python 3 function calls\"\n    print \"-\" * 35\n    P2to3().visit(tree)\n    ast.fix_missing_locations(tree)\n    # print ast.dump(tree)\n\n    exec(compile(tree, filename=\"p23\", mode=\"exec\"))\n\nif __name__ == '__main__':\n    if len(sys.argv) &lt;=1:\n        print (\"\\nUSAGE:\\n\\t print2to3.py &lt;filename&gt;\")\n        sys.exit(1)\n    else:\n        main(sys.argv[1])\n</code></pre>\n\n<p>This utility can be tried on small example file, such as one below, and it should work fine. </p>\n\n<p><strong>Test Input file : py2.py</strong></p>\n\n<pre><code>class A(object):\n    def __init__(self):\n        pass\n\ndef good():\n    print \"I am good\"\n\nmain = good\n\nif __name__ == '__main__':\n    print \"I am in main\"\n    main()\n</code></pre>\n\n<p>Please note that above transformation is only for <code>ast</code> tutorial purpose and in real case scenario one will have to look at all different scenarios such as <code>print \" x is %s\" % (\"Hello Python\")</code>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "8"}, "answer_7": {"type": "literal", "value": "<p><a href=\"https://stackoverflow.com/a/769202\">One of the other answers</a> recommends <code>codegen</code>, which seems to have been superceded by <a href=\"https://github.com/berkerpeksag/astor\" rel=\"nofollow noreferrer\"><code>astor</code></a>. The version of <a href=\"https://pypi.python.org/pypi/astor\" rel=\"nofollow noreferrer\"><code>astor</code> on PyPI</a> (version 0.5 as of this writing) seems to be a little outdated as well, so you can install the development version of <code>astor</code> as follows.</p>\n\n<pre><code>pip install git+https://github.com/berkerpeksag/astor.git#egg=astor\n</code></pre>\n\n<p>Then you can use <code>astor.to_source</code> to convert a Python AST to human-readable Python source code:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; import astor\n&gt;&gt;&gt; print(astor.to_source(ast.parse('def foo(x): return 2 * x')))\ndef foo(x):\n    return 2 * x\n</code></pre>\n\n<p>I have tested this on Python 3.5.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>We had a similar need, which wasn't solved by other answers here. So we created a library for this, <a href=\"https://github.com/gristlabs/asttokens\" rel=\"nofollow noreferrer\">ASTTokens</a>, which takes an AST tree produced with the <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> or <a href=\"https://www.astroid.org/\" rel=\"nofollow noreferrer\">astroid</a> modules, and marks it with the ranges of text in the original source code.</p>\n\n<p>It doesn't do modifications of code directly, but that's not hard to add on top, since it does tell you the range of text you need to modify.</p>\n\n<p>For example, this wraps a function call in <code>WRAP(...)</code>, preserving comments and everything else:</p>\n\n<pre><code>example = \"\"\"\ndef foo(): # Test\n  '''My func'''\n  log(\"hello world\")  # Print\n\"\"\"\n\nimport ast, asttokens\natok = asttokens.ASTTokens(example, parse=True)\n\ncall = next(n for n in ast.walk(atok.tree) if isinstance(n, ast.Call))\nstart, end = atok.get_text_range(call)\nprint(atok.text[:start] + ('WRAP(%s)' % atok.text[start:end])  + atok.text[end:])\n</code></pre>\n\n<p>Produces:</p>\n\n<pre><code>def foo(): # Test\n  '''My func'''\n  WRAP(log(\"hello world\"))  # Print\n</code></pre>\n\n<p>Hope this helps!</p>\n"}, "answer_8_votes": {"type": "literal", "value": "2"}, "answer_9": {"type": "literal", "value": "<p><a href=\"https://github.com/mkwiatkowski/pythoscope\" rel=\"nofollow noreferrer\">Pythoscope</a> does this to the test cases it automatically generates as does the <a href=\"http://docs.python.org/library/2to3.html\" rel=\"nofollow noreferrer\">2to3</a> tool for python 2.6 (it converts python 2.x source into python 3.x source). </p>\n\n<p>Both these tools uses the <a href=\"http://svn.python.org/projects/python/trunk/Lib/lib2to3/\" rel=\"nofollow noreferrer\">lib2to3</a> library which is a implementation of the python parser/compiler machinery that can preserve comments in source when it's round tripped from source -> AST -> source.</p>\n\n<p>The <a href=\"https://github.com/python-rope/rope\" rel=\"nofollow noreferrer\">rope project</a> may meet your needs if you want to do more refactoring like transforms.</p>\n\n<p>The <a href=\"http://docs.python.org/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> module is your other option, and <a href=\"https://svn.python.org/projects/python/trunk/Demo/parser/unparse.py\" rel=\"nofollow noreferrer\">there's an older example of how to \"unparse\" syntax trees back into code</a> (using the parser module). But the <code>ast</code> module is more useful when doing an AST transform on code that is then transformed into a code object.</p>\n\n<p>The <a href=\"https://redbaron.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">redbaron</a> project also may be a good fit (ht Xavier Combelle)</p>\n"}, "answer_9_votes": {"type": "literal", "value": "69"}, "answer_10": {"type": "literal", "value": "<p>I've created recently quite stable (core is really well tested) and extensible piece of code which generates code from <code>ast</code> tree: <a href=\"https://github.com/paluh/code-formatter\" rel=\"nofollow\">https://github.com/paluh/code-formatter</a> .</p>\n\n<p>I'm using my project as a base for a small vim plugin (which I'm using every day), so my goal is to generate really nice and readable python code.</p>\n\n<p>P.S.\nI've tried to extend <code>codegen</code> but it's architecture is based on <code>ast.NodeVisitor</code> interface, so formatters (<code>visitor_</code> methods) are just functions. I've found this structure quite limiting and hard to optimize (in case of long and nested expressions it's easier to keep objects tree and cache some partial results - in other way you can hit exponential complexity if you want to search for best layout). <strong>BUT</strong> <code>codegen</code> as every piece of mitsuhiko's work (which I've read) is very well written and concise.</p>\n"}, "answer_10_votes": {"type": "literal", "value": "6"}, "content_wo_code": "<p>I want to programmatically edit python source code. Basically I want to read a   file, generate the <a href=\"http://en.wikipedia.org/wiki/Abstract_syntax_tree\" rel=\"noreferrer\">AST</a>, and then write back the modified python source code (i.e. another   file).</p>\n\n<p>There are ways to parse/compile python source code using standard python modules, such as <a href=\"http://docs.python.org/library/ast.html\" rel=\"noreferrer\"> </a> or <a href=\"http://docs.python.org/library/compiler.html\" rel=\"noreferrer\"> </a>.  However, I don't think any of them support ways to modify the source code (e.g. delete this function declaration) and then write back the modifying python source code.</p>\n\n<p>UPDATE: The reason I want to do this is I'd like to write a <a href=\"http://en.wikipedia.org/wiki/Mutation_testing\" rel=\"noreferrer\">Mutation testing library</a> for python, mostly by deleting statements / expressions, rerunning tests and seeing what breaks.</p>\n", "answer_wo_code": "<p>I used to use baron for this, but have now switched to parso because it's up to date with modern python. It works great. </p>\n\n<p>I also needed this for a mutation tester. It's really quite simple to make one with parso, check out my code at <a href=\"https://github.com/boxed/mutmut\" rel=\"nofollow noreferrer\">https://github.com/boxed/mutmut</a></p>\n\n\n<p>I used to use baron for this, but have now switched to parso because it's up to date with modern python. It works great. </p>\n\n<p>I also needed this for a mutation tester. It's really quite simple to make one with parso, check out my code at <a href=\"https://github.com/boxed/mutmut\" rel=\"nofollow noreferrer\">https://github.com/boxed/mutmut</a></p>\n\n\n<p>A <a href=\"http://en.wikipedia.org/wiki/Program_transformation\" rel=\"nofollow noreferrer\">Program Transformation System</a> is a tool that parses source text, builds ASTs, allows you to modify them using source-to-source transformations (\"if you see this pattern, replace it by that pattern\").   Such tools are ideal for doing mutation of existing source codes, which are just \"if you see this pattern, replace by a pattern variant\".</p>\n\n<p>Of course, you need a program transformation engine that can parse the language of interest to you, and still do the pattern-directed transformations.  Our <a href=\"http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\" rel=\"nofollow noreferrer\">DMS Software Reengineering Toolkit</a> is a system that can do that, and handles Python, and a variety of other languages. </p>\n\n<p>See this <a href=\"https://stackoverflow.com/a/22118379/120163\">SO answer for an example of a DMS-parsed AST for Python capturing comments</a> accurately.  DMS can make changes to the AST, and regenerate valid text, including the comments.  You can ask it to prettyprint the AST, using its own formatting conventions (you can changes these), or do \"fidelity printing\", which uses the original line and column information to maximally preserve the original layout (some change in layout where new code is inserted is unavoidable).</p>\n\n<p>To implement a \"mutation\" rule for Python with DMS, you could write the following:</p>\n\n<pre> </pre>\n\n<p>This rule replace  \"+\" with \"-\" in a syntactically correct way; it operates on the AST and thus won't touch strings or comments that happen to look right.  The extra condition on \"mutate_this_place\" is to let you control how often this occurs; you don't want to mutate <em>every</em> place in the program.</p>\n\n<p>You'd obviously want a bunch more rules like this that detect various code structures, and replace them by the mutated versions.   DMS is happy to apply a set of rules.  The mutated AST is then prettyprinted.</p>\n\n\n<p>A <a href=\"http://en.wikipedia.org/wiki/Program_transformation\" rel=\"nofollow noreferrer\">Program Transformation System</a> is a tool that parses source text, builds ASTs, allows you to modify them using source-to-source transformations (\"if you see this pattern, replace it by that pattern\").   Such tools are ideal for doing mutation of existing source codes, which are just \"if you see this pattern, replace by a pattern variant\".</p>\n\n<p>Of course, you need a program transformation engine that can parse the language of interest to you, and still do the pattern-directed transformations.  Our <a href=\"http://www.semanticdesigns.com/Products/DMS/DMSToolkit.html\" rel=\"nofollow noreferrer\">DMS Software Reengineering Toolkit</a> is a system that can do that, and handles Python, and a variety of other languages. </p>\n\n<p>See this <a href=\"https://stackoverflow.com/a/22118379/120163\">SO answer for an example of a DMS-parsed AST for Python capturing comments</a> accurately.  DMS can make changes to the AST, and regenerate valid text, including the comments.  You can ask it to prettyprint the AST, using its own formatting conventions (you can changes these), or do \"fidelity printing\", which uses the original line and column information to maximally preserve the original layout (some change in layout where new code is inserted is unavoidable).</p>\n\n<p>To implement a \"mutation\" rule for Python with DMS, you could write the following:</p>\n\n<pre> </pre>\n\n<p>This rule replace  \"+\" with \"-\" in a syntactically correct way; it operates on the AST and thus won't touch strings or comments that happen to look right.  The extra condition on \"mutate_this_place\" is to let you control how often this occurs; you don't want to mutate <em>every</em> place in the program.</p>\n\n<p>You'd obviously want a bunch more rules like this that detect various code structures, and replace them by the mutated versions.   DMS is happy to apply a set of rules.  The mutated AST is then prettyprinted.</p>\n\n\n<p>You might not need to re-generate source code.  That's a bit dangerous for me to say, of course, since you have not actually explained why you think you need to generate a .py file full of code; but:</p>\n\n<ul>\n<li><p>If you want to generate a .py file that people will actually use, maybe so that they can fill out a form and get a useful .py file to insert into their project, then you don't want to change it into an AST and back because you'll lose <strike>all formatting (think of the blank lines that make Python so readable by grouping related sets of lines together)</strike> (<a href=\"http://docs.python.org/py3k/library/ast.html#ast.AST.lineno\" rel=\"noreferrer\">ast nodes have   and   attributes</a>) comments.  Instead, you'll probably want to use a templating engine (the <a href=\"http://docs.djangoproject.com/en/dev/topics/templates/#topics-templates\" rel=\"noreferrer\">Django template language</a>, for example, is designed to make templating even text files easy) to customize the .py file, or else use Rick Copeland's <a href=\"http://code.google.com/p/metapython/\" rel=\"noreferrer\">MetaPython</a> extension.</p></li>\n<li><p>If you are trying to make a change during compilation of a module, note that you don't have to go all the way back to text; you can just compile the AST directly instead of turning it back into a .py file.</p></li>\n<li><p>But in almost any and every case, you are probably trying to do something dynamic that a language like Python actually makes very easy, without writing new .py files! If you expand your question to let us know what you actually want to accomplish, new .py files will probably not be involved in the answer at all; I have seen hundreds of Python projects doing hundreds of real-world things, and not a single one of them needed to ever writer a .py file.  So, I must admit, I'm a bit of a skeptic that you've found the first good use-case. :-)</p></li>\n</ul>\n\n<p><strong>Update:</strong> now that you've explained what you're trying to do, I'd be tempted to just operate on the AST anyway. You will want to mutate by removing, not lines of a file (which could result in half-statements that simply die with a SyntaxError), but whole statements \u2014 and what better place to do that than in the AST?</p>\n\n\n<p>You might not need to re-generate source code.  That's a bit dangerous for me to say, of course, since you have not actually explained why you think you need to generate a .py file full of code; but:</p>\n\n<ul>\n<li><p>If you want to generate a .py file that people will actually use, maybe so that they can fill out a form and get a useful .py file to insert into their project, then you don't want to change it into an AST and back because you'll lose <strike>all formatting (think of the blank lines that make Python so readable by grouping related sets of lines together)</strike> (<a href=\"http://docs.python.org/py3k/library/ast.html#ast.AST.lineno\" rel=\"noreferrer\">ast nodes have   and   attributes</a>) comments.  Instead, you'll probably want to use a templating engine (the <a href=\"http://docs.djangoproject.com/en/dev/topics/templates/#topics-templates\" rel=\"noreferrer\">Django template language</a>, for example, is designed to make templating even text files easy) to customize the .py file, or else use Rick Copeland's <a href=\"http://code.google.com/p/metapython/\" rel=\"noreferrer\">MetaPython</a> extension.</p></li>\n<li><p>If you are trying to make a change during compilation of a module, note that you don't have to go all the way back to text; you can just compile the AST directly instead of turning it back into a .py file.</p></li>\n<li><p>But in almost any and every case, you are probably trying to do something dynamic that a language like Python actually makes very easy, without writing new .py files! If you expand your question to let us know what you actually want to accomplish, new .py files will probably not be involved in the answer at all; I have seen hundreds of Python projects doing hundreds of real-world things, and not a single one of them needed to ever writer a .py file.  So, I must admit, I'm a bit of a skeptic that you've found the first good use-case. :-)</p></li>\n</ul>\n\n<p><strong>Update:</strong> now that you've explained what you're trying to do, I'd be tempted to just operate on the AST anyway. You will want to mutate by removing, not lines of a file (which could result in half-statements that simply die with a SyntaxError), but whole statements \u2014 and what better place to do that than in the AST?</p>\n\n\n<p>The builtin ast module doesn't seem to have a method to convert back to source.  However, the <a href=\"https://pypi.python.org/pypi/codegen/1.0\" rel=\"noreferrer\">codegen</a> module here provides a pretty printer for the ast that would enable you do do so.\neg.</p>\n\n<pre> </pre>\n\n<p>This will print:</p>\n\n<pre> </pre>\n\n<p>Note that you may lose the exact formatting and comments, as these are not preserved.</p>\n\n<p>However, you may not need to.  If all you require is to execute the replaced AST, you can do so simply by calling compile() on the ast, and execing the resulting code object.</p>\n\n\n<p>The builtin ast module doesn't seem to have a method to convert back to source.  However, the <a href=\"https://pypi.python.org/pypi/codegen/1.0\" rel=\"noreferrer\">codegen</a> module here provides a pretty printer for the ast that would enable you do do so.\neg.</p>\n\n<pre> </pre>\n\n<p>This will print:</p>\n\n<pre> </pre>\n\n<p>Note that you may lose the exact formatting and comments, as these are not preserved.</p>\n\n<p>However, you may not need to.  If all you require is to execute the replaced AST, you can do so simply by calling compile() on the ast, and execing the resulting code object.</p>\n\n\n<p>In a different answer I suggested using the   package, but I have since found a more up-to-date AST un-parsing package called <a href=\"https://github.com/simonpercivall/astunparse\"> </a>:</p>\n\n<pre> </pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>In a different answer I suggested using the   package, but I have since found a more up-to-date AST un-parsing package called <a href=\"https://github.com/simonpercivall/astunparse\"> </a>:</p>\n\n<pre> </pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>Parsing and modifying the code structure is certainly possible with the help of   module and I will show it in an example in a moment. However, writing back the modified source code is not possible with   module alone. There are other modules available for this job such as one <a href=\"http://svn.python.org/view/python/trunk/Demo/parser/unparse.py?view=markup\" rel=\"noreferrer\">here</a>. </p>\n\n<p>NOTE: Example below can be treated as an introductory tutorial on the usage of   module but a more comprehensive guide on using   module is available here at <a href=\"https://greentreesnakes.readthedocs.io/en/latest/tofrom.html\" rel=\"noreferrer\">Green Tree snakes tutorial</a> and <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"noreferrer\">official documentation on   module</a>.   </p>\n\n<p><strong>Introduction to  :</strong> </p>\n\n<pre> </pre>\n\n<p>You can parse the python code (represented in string) by simply calling the API  . This returns the handle to Abstract Syntax Tree (AST) structure. Interestingly you can compile back this structure and execute it as shown above.</p>\n\n<p>Another very useful API is   which dumps the whole AST in a string form. It can be used to inspect the tree structure and is very helpful in debugging. For example,</p>\n\n<p><em>On Python 2.7:</em></p>\n\n<pre> </pre>\n\n<p><em>On Python 3.5:</em></p>\n\n<pre> </pre>\n\n<p>Notice the difference in syntax for print statement in Python 2.7 vs. Python 3.5 and the difference in type of AST node in respective trees.</p>\n\n<hr>\n\n<p><strong>How to modify code using  :</strong></p>\n\n<p>Now, let's a have a look at an example of modification of python code by   module. The main tool for modifying AST structure is   class. Whenever one needs to modify the AST, he/she needs to subclass from it and write Node Transformation(s) accordingly. </p>\n\n<p>For our example, let's try to write a simple utility which transforms the Python 2 , print statements to Python 3 function calls. </p>\n\n<p><strong>Print statement to Fun call converter utility:  print2to3.py:</strong></p>\n\n<pre> </pre>\n\n<p>This utility can be tried on small example file, such as one below, and it should work fine. </p>\n\n<p><strong>Test Input file : py2.py</strong></p>\n\n<pre> </pre>\n\n<p>Please note that above transformation is only for   tutorial purpose and in real case scenario one will have to look at all different scenarios such as  .</p>\n\n\n<p>Parsing and modifying the code structure is certainly possible with the help of   module and I will show it in an example in a moment. However, writing back the modified source code is not possible with   module alone. There are other modules available for this job such as one <a href=\"http://svn.python.org/view/python/trunk/Demo/parser/unparse.py?view=markup\" rel=\"noreferrer\">here</a>. </p>\n\n<p>NOTE: Example below can be treated as an introductory tutorial on the usage of   module but a more comprehensive guide on using   module is available here at <a href=\"https://greentreesnakes.readthedocs.io/en/latest/tofrom.html\" rel=\"noreferrer\">Green Tree snakes tutorial</a> and <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"noreferrer\">official documentation on   module</a>.   </p>\n\n<p><strong>Introduction to  :</strong> </p>\n\n<pre> </pre>\n\n<p>You can parse the python code (represented in string) by simply calling the API  . This returns the handle to Abstract Syntax Tree (AST) structure. Interestingly you can compile back this structure and execute it as shown above.</p>\n\n<p>Another very useful API is   which dumps the whole AST in a string form. It can be used to inspect the tree structure and is very helpful in debugging. For example,</p>\n\n<p><em>On Python 2.7:</em></p>\n\n<pre> </pre>\n\n<p><em>On Python 3.5:</em></p>\n\n<pre> </pre>\n\n<p>Notice the difference in syntax for print statement in Python 2.7 vs. Python 3.5 and the difference in type of AST node in respective trees.</p>\n\n<hr>\n\n<p><strong>How to modify code using  :</strong></p>\n\n<p>Now, let's a have a look at an example of modification of python code by   module. The main tool for modifying AST structure is   class. Whenever one needs to modify the AST, he/she needs to subclass from it and write Node Transformation(s) accordingly. </p>\n\n<p>For our example, let's try to write a simple utility which transforms the Python 2 , print statements to Python 3 function calls. </p>\n\n<p><strong>Print statement to Fun call converter utility:  print2to3.py:</strong></p>\n\n<pre> </pre>\n\n<p>This utility can be tried on small example file, such as one below, and it should work fine. </p>\n\n<p><strong>Test Input file : py2.py</strong></p>\n\n<pre> </pre>\n\n<p>Please note that above transformation is only for   tutorial purpose and in real case scenario one will have to look at all different scenarios such as  .</p>\n\n\n<p><a href=\"https://stackoverflow.com/a/769202\">One of the other answers</a> recommends  , which seems to have been superceded by <a href=\"https://github.com/berkerpeksag/astor\" rel=\"nofollow noreferrer\"> </a>. The version of <a href=\"https://pypi.python.org/pypi/astor\" rel=\"nofollow noreferrer\">  on PyPI</a> (version 0.5 as of this writing) seems to be a little outdated as well, so you can install the development version of   as follows.</p>\n\n<pre> </pre>\n\n<p>Then you can use   to convert a Python AST to human-readable Python source code:</p>\n\n<pre> </pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p><a href=\"https://stackoverflow.com/a/769202\">One of the other answers</a> recommends  , which seems to have been superceded by <a href=\"https://github.com/berkerpeksag/astor\" rel=\"nofollow noreferrer\"> </a>. The version of <a href=\"https://pypi.python.org/pypi/astor\" rel=\"nofollow noreferrer\">  on PyPI</a> (version 0.5 as of this writing) seems to be a little outdated as well, so you can install the development version of   as follows.</p>\n\n<pre> </pre>\n\n<p>Then you can use   to convert a Python AST to human-readable Python source code:</p>\n\n<pre> </pre>\n\n<p>I have tested this on Python 3.5.</p>\n\n\n<p>We had a similar need, which wasn't solved by other answers here. So we created a library for this, <a href=\"https://github.com/gristlabs/asttokens\" rel=\"nofollow noreferrer\">ASTTokens</a>, which takes an AST tree produced with the <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> or <a href=\"https://www.astroid.org/\" rel=\"nofollow noreferrer\">astroid</a> modules, and marks it with the ranges of text in the original source code.</p>\n\n<p>It doesn't do modifications of code directly, but that's not hard to add on top, since it does tell you the range of text you need to modify.</p>\n\n<p>For example, this wraps a function call in  , preserving comments and everything else:</p>\n\n<pre> </pre>\n\n<p>Produces:</p>\n\n<pre> </pre>\n\n<p>Hope this helps!</p>\n\n\n<p>We had a similar need, which wasn't solved by other answers here. So we created a library for this, <a href=\"https://github.com/gristlabs/asttokens\" rel=\"nofollow noreferrer\">ASTTokens</a>, which takes an AST tree produced with the <a href=\"https://docs.python.org/2/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> or <a href=\"https://www.astroid.org/\" rel=\"nofollow noreferrer\">astroid</a> modules, and marks it with the ranges of text in the original source code.</p>\n\n<p>It doesn't do modifications of code directly, but that's not hard to add on top, since it does tell you the range of text you need to modify.</p>\n\n<p>For example, this wraps a function call in  , preserving comments and everything else:</p>\n\n<pre> </pre>\n\n<p>Produces:</p>\n\n<pre> </pre>\n\n<p>Hope this helps!</p>\n\n\n<p><a href=\"https://github.com/mkwiatkowski/pythoscope\" rel=\"nofollow noreferrer\">Pythoscope</a> does this to the test cases it automatically generates as does the <a href=\"http://docs.python.org/library/2to3.html\" rel=\"nofollow noreferrer\">2to3</a> tool for python 2.6 (it converts python 2.x source into python 3.x source). </p>\n\n<p>Both these tools uses the <a href=\"http://svn.python.org/projects/python/trunk/Lib/lib2to3/\" rel=\"nofollow noreferrer\">lib2to3</a> library which is a implementation of the python parser/compiler machinery that can preserve comments in source when it's round tripped from source -> AST -> source.</p>\n\n<p>The <a href=\"https://github.com/python-rope/rope\" rel=\"nofollow noreferrer\">rope project</a> may meet your needs if you want to do more refactoring like transforms.</p>\n\n<p>The <a href=\"http://docs.python.org/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> module is your other option, and <a href=\"https://svn.python.org/projects/python/trunk/Demo/parser/unparse.py\" rel=\"nofollow noreferrer\">there's an older example of how to \"unparse\" syntax trees back into code</a> (using the parser module). But the   module is more useful when doing an AST transform on code that is then transformed into a code object.</p>\n\n<p>The <a href=\"https://redbaron.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">redbaron</a> project also may be a good fit (ht Xavier Combelle)</p>\n\n\n<p><a href=\"https://github.com/mkwiatkowski/pythoscope\" rel=\"nofollow noreferrer\">Pythoscope</a> does this to the test cases it automatically generates as does the <a href=\"http://docs.python.org/library/2to3.html\" rel=\"nofollow noreferrer\">2to3</a> tool for python 2.6 (it converts python 2.x source into python 3.x source). </p>\n\n<p>Both these tools uses the <a href=\"http://svn.python.org/projects/python/trunk/Lib/lib2to3/\" rel=\"nofollow noreferrer\">lib2to3</a> library which is a implementation of the python parser/compiler machinery that can preserve comments in source when it's round tripped from source -> AST -> source.</p>\n\n<p>The <a href=\"https://github.com/python-rope/rope\" rel=\"nofollow noreferrer\">rope project</a> may meet your needs if you want to do more refactoring like transforms.</p>\n\n<p>The <a href=\"http://docs.python.org/library/ast.html\" rel=\"nofollow noreferrer\">ast</a> module is your other option, and <a href=\"https://svn.python.org/projects/python/trunk/Demo/parser/unparse.py\" rel=\"nofollow noreferrer\">there's an older example of how to \"unparse\" syntax trees back into code</a> (using the parser module). But the   module is more useful when doing an AST transform on code that is then transformed into a code object.</p>\n\n<p>The <a href=\"https://redbaron.readthedocs.org/en/latest/\" rel=\"nofollow noreferrer\">redbaron</a> project also may be a good fit (ht Xavier Combelle)</p>\n\n\n<p>I've created recently quite stable (core is really well tested) and extensible piece of code which generates code from   tree: <a href=\"https://github.com/paluh/code-formatter\" rel=\"nofollow\">https://github.com/paluh/code-formatter</a> .</p>\n\n<p>I'm using my project as a base for a small vim plugin (which I'm using every day), so my goal is to generate really nice and readable python code.</p>\n\n<p>P.S.\nI've tried to extend   but it's architecture is based on   interface, so formatters (  methods) are just functions. I've found this structure quite limiting and hard to optimize (in case of long and nested expressions it's easier to keep objects tree and cache some partial results - in other way you can hit exponential complexity if you want to search for best layout). <strong>BUT</strong>   as every piece of mitsuhiko's work (which I've read) is very well written and concise.</p>\n\n\n<p>I've created recently quite stable (core is really well tested) and extensible piece of code which generates code from   tree: <a href=\"https://github.com/paluh/code-formatter\" rel=\"nofollow\">https://github.com/paluh/code-formatter</a> .</p>\n\n<p>I'm using my project as a base for a small vim plugin (which I'm using every day), so my goal is to generate really nice and readable python code.</p>\n\n<p>P.S.\nI've tried to extend   but it's architecture is based on   interface, so formatters (  methods) are just functions. I've found this structure quite limiting and hard to optimize (in case of long and nested expressions it's easier to keep objects tree and cache some partial results - in other way you can hit exponential complexity if you want to search for best layout). <strong>BUT</strong>   as every piece of mitsuhiko's work (which I've read) is very well written and concise.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/scipy.linalg.eig"}, "class_func_label": {"type": "literal", "value": "scipy.linalg.eig"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nSolve an ordinary or generalized eigenvalue problem of a square matrix.\n\nFind eigenvalues w and right or left eigenvectors of a general matrix::\n\n    a   vr[:,i] = w[i]        b   vr[:,i]\n    a.H vl[:,i] = w[i].conj() b.H vl[:,i]\n\nwhere ``.H`` is the Hermitian conjugation.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/40050369"}, "title": {"type": "literal", "value": "Different eigenvalues between scipy.sparse.linalg.eigs and numpy/scipy.eig"}, "content": {"type": "literal", "value": "<h2>Context:</h2>\n\n<p>My goal is to create a Python3 program to operate differential operations on a vector V of size N. I did so, test it for basic operation and it works (differentiation, gradient...).</p>\n\n<p>I tried to write with that basis more complex equations (Navier-Stokes, Orr-Sommerfeld,...) and I tried to validate my work by calculating the eigenvalues of these equations.</p>\n\n<p>As these eigenvalues were completely unexpected, I simplify my problem and I am currently trying to calculate the eigenvalues only for the differentiation matrix (see below). But the results seem wrong...</p>\n\n<p>Thanks in advance for your help, because I do not find any solution to my problem...</p>\n\n<h2>Definition of DM:</h2>\n\n<p>I use Chebyshev spectral method to operate the differentiation of vectors.\nI use the following Chebyshev package (translated from Matlab to Python):\n<a href=\"http://dip.sun.ac.za/%7Eweideman/research/differ.html\" rel=\"nofollow\">http://dip.sun.ac.za/%7Eweideman/research/differ.html</a></p>\n\n<p>That package allow me to create a differentiation matrix DM, obtained with:</p>\n\n<pre><code>nodes, DM = chebyshev.chebdiff(N, maximal_order)\n</code></pre>\n\n<p>To obtain the 1st, 2nd, 3th... order differentiation, I write for example:</p>\n\n<pre><code>dVdx1 = np.dot(DM[0,:,:], V)\nd2Vdx2 = np.dot(DM[1,:,:], V)\nd3Vdx3 = np.dot(DM[2,:,:], V)\n</code></pre>\n\n<p>I tested that and it works.\nI've build different operators based on that differentiation process.\nI've tried to validate them by finding their eigenvalues. It didn't go well so I am just trying right now with DM only.\nI do not manage to find the right eigenvalues of DM.</p>\n\n<p>I've tried with different functions:</p>\n\n<pre><code>numpy.linalg.eigvals(DM)\nscipy.linalg.eig(DM)\nscipy.sparse.linalg.eigs(DM)\nsympy.solve( (DM-x*np.eye).det(), x) [for snall size only]\n</code></pre>\n\n<h2>Why I use scipy.sparse.LinearOperator:</h2>\n\n<p>I do not want to directly use the matrix DM, so I wrapped into a function which operates the differentiation (see code below) like that:</p>\n\n<pre><code>dVdx1 = derivative(V)\n</code></pre>\n\n<p>The reason why I do that comes from the global project itself.\nThis is useful for more complicated equations.</p>\n\n<p>Creating such a function prevents me from using directly the matrix DM to find its eigenvalues (because DM stay inside the function).\nFor that reason, I use a scipy.sparse.LinearOperator to wrap my method derivative() and use it as an input of scipy.sparse.eig().</p>\n\n<h2>Code and results:</h2>\n\n<p>Here is the code to compute these eigenvalues:</p>\n\n<pre><code>import numpy as np\nimport scipy\nimport sympy\n\nfrom scipy.sparse.linalg import aslinearoperator\nfrom scipy.sparse.linalg import eigs\nfrom scipy.sparse.linalg import LinearOperator\n\nimport chebyshev\n\nN = 20 # should be 4, 20, 50, 100, 300\nmax_order = 4\n\noption = 1\n#option 1: building the differentiation matrix DM for a given order\nif option == 1:\n    if 0:\n        # usage of package chebyshev, but I add a file with the matrix inside\n        nodes, DM = chebyshev.chebdiff(N, max_order)\n        order = 1\n        DM = DM[order-1,:,:]\n        #outfile = TemporaryFile()\n        np.save('DM20', DM)\n    if 1:\n        # loading the matrix from the file\n        # uncomment depending on N\n        #DM = np.load('DM4.npy')\n        DM = np.load('DM20.npy')\n        #DM = np.load('DM50.npy')\n        #DM = np.load('DM100.npy')\n        #DM = np.load('DM300.npy')\n\n#option 2: building a random matrix\nelif option == 2:\n    j = np.complex(0,1)\n    np.random.seed(0)\n    Real = np.random.random((N, N)) - 0.5\n    Im = np.random.random((N,N)) - 0.5\n\n    # If I want DM symmetric:\n    #Real = np.dot(Real, Real.T)\n    #Im = np.dot(Im, Im.T)\n\n    DM = Real + j*Im\n\n    # If I want DM singular:\n    #DM[0,:] = DM[1,:]\n\n# Test DM symmetric\nprint('Is DM symmetric ? \\n', (DM.transpose() == DM).all() )        \n# Test DM Hermitian\nprint('Is DM hermitian ? \\n', (DM.transpose().real == DM.real).all() and\n                                        (DM.transpose().imag == -DM.imag).all() )  \n\n# building a linear operator which wrap matrix DM\ndef derivative(v):\n    return np.dot(DM, v)\n\nlinop_DM = LinearOperator( (N, N), matvec = derivative)\n\n# building a linear operator directly from a matrix DM with asLinearOperator\naslinop_DM = aslinearoperator(DM)\n\n# comparison of LinearOperator and direct Dot Product\nV = np.random.random((N))\ndiff_lo = linop_DM.matvec(V)\ndiff_mat = np.dot(DM, V)\n# diff_lo and diff_mat are equals\n\n# FINDING EIGENVALUES\n\n#number of eigenvalues to find\nk = 1\nif 1:\n    # SCIPY SPARSE LINALG LINEAR OPERATOR\n    vals_sparse, vecs = scipy.sparse.linalg.eigs(linop_DM, k, which='SR',\n                            maxiter = 10000,\n                            tol = 1E-3)\n    vals_sparse = np.sort(vals_sparse)\n    print('\\nEigenvalues (scipy.sparse.linalg Linear Operator) : \\n', vals_sparse)\n\nif 1:\n    # SCIPY SPARSE ARRAY\n    vals_sparse2, vecs2 = scipy.sparse.linalg.eigs(DM, k, which='SR',\n                        maxiter = 10000,\n                        tol = 1E-3)\n    vals_sparse2 = np.sort(vals_sparse2)\n    print('\\nEigenvalues (scipy.sparse.linalg with matrix DM) : \\n', vals_sparse2)\n\nif 1:\n    # SCIPY SPARSE AS LINEAR OPERATOR\n    vals_sparse3, vecs3 = scipy.sparse.linalg.eigs(aslinop_DM, k, which='SR',\n                        maxiter = 10000,\n                        tol = 1E-3)\n    vals_sparse3 = np.sort(vals_sparse3)\n    print('\\nEigenvalues (scipy.sparse.linalg AS linear Operator) : \\n', vals_sparse3)\n\nif 0:\n    # NUMPY LINALG / SAME RESULT AS SCIPY LINALG\n    vals_np = np.linalg.eigvals(DM)\n    vals_np = np.sort(vals_np)\n    print('\\nEigenvalues (numpy.linalg) : \\n', vals_np)\n\nif 1:\n    # SCIPY LINALG\n    vals_sp = scipy.linalg.eig(DM)\n    vals_sp = np.sort(vals_sp[0])\n    print('\\nEigenvalues (scipy.linalg.eig) : \\n', vals_sp)\n\nif 0:\n    x = sympy.Symbol('x')\n    D = sympy.Matrix(DM)\n    print('\\ndet D (sympy):', D.det() )\n    E = D - x*np.eye(DM.shape[0])\n    eig_sympy = sympy.solve(E.det(), x)\n    print('\\nEigenvalues (sympy) : \\n', eig_sympy)\n</code></pre>\n\n<p>Here are my results (for N=20):</p>\n\n<pre><code>Is DM symmetric ? \n False\nIs DM hermitian ? \n False\n\nEigenvalues (scipy.sparse.linalg Linear Operator) : \n [-2.5838015+0.j]\n\nEigenvalues (scipy.sparse.linalg with matrix DM) : \n [-2.58059801+0.j]\n\nEigenvalues (scipy.sparse.linalg AS linear Operator) : \n [-2.36137671+0.j]\n\nEigenvalues (scipy.linalg.eig) : \n [-2.92933791+0.j         -2.72062839-1.01741142j -2.72062839+1.01741142j\n -2.15314244-1.84770128j -2.15314244+1.84770128j -1.36473659-2.38021351j\n -1.36473659+2.38021351j -0.49536645-2.59716913j -0.49536645+2.59716913j\n  0.38136094-2.53335888j  0.38136094+2.53335888j  0.55256471-1.68108134j\n  0.55256471+1.68108134j  1.26425751-2.25101241j  1.26425751+2.25101241j\n  2.03390489-1.74122287j  2.03390489+1.74122287j  2.57770573-0.95982011j\n  2.57770573+0.95982011j  2.77749810+0.j        ]\n</code></pre>\n\n<p>The values returned by scipy.sparse should be included in the ones found by scipy/numpy, which is not the case. (idem for sympy)</p>\n\n<p>I've tried with different random matrices instead of DM (see option 2) (symmetric, non-symmetric, real, imaginary, etc...), which had small size N (4,5,6..) and also bigger ones (100,...).\nThat worked</p>\n\n<p>By changing parameters like 'which' (LM, SM, LR...), 'tol' (10E-3, 10E-6..), 'maxiter', 'sigma' (0) in scipy.sparse... scipy.sparse.linalg.eigs always worked for random matrices but never for my matrix DM. In best cases, found eigenvalues are close to the ones found by scipy, but never match.</p>\n\n<p>I really do not know what is so particular in my matrix.\nI also dont know why using scipy.sparse.linagl.eig with a matrix, a LinearOperator or a AsLinearOperator gives different results.</p>\n\n<p>I DO NOT KNOW HOW I COULD INCLUDE MY FILES CONTAINING MATRICES DM...</p>\n\n<p>For N = 4 :</p>\n\n<pre><code>[[ 3.16666667 -4.          1.33333333 -0.5       ]\n [ 1.         -0.33333333 -1.          0.33333333]\n [-0.33333333  1.          0.33333333 -1.        ]\n [ 0.5        -1.33333333  4.         -3.16666667]]\n</code></pre>\n\n<p>Every idea is welcome.</p>\n\n<p>May a moderator could tag my question with :\nscipy.sparse.linalg.eigs / weideman / eigenvalues / scipy.eig /scipy.sparse.lingalg.linearOperator</p>\n\n<p>Geoffroy.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre><code>DM[0,:] = 0\nDM[:,0] = 0\nDM[N-1,:] = 0\nDM[:,N-1] = 0\n</code></pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre><code>[[ 0     0               0               0]\n [ 0     -0.33333333     -1.             0]\n [ 0      1.             0.33333333      0]\n [ 0      0              0               0]]\n</code></pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre><code>DM[0,:] = 0\nDM[:,0] = 0\nDM[N-1,:] = 0\nDM[:,N-1] = 0\n</code></pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre><code>[[ 0     0               0               0]\n [ 0     -0.33333333     -1.             0]\n [ 0      1.             0.33333333      0]\n [ 0      0              0               0]]\n</code></pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<h2>Context:</h2>\n\n<p>My goal is to create a Python3 program to operate differential operations on a vector V of size N. I did so, test it for basic operation and it works (differentiation, gradient...).</p>\n\n<p>I tried to write with that basis more complex equations (Navier-Stokes, Orr-Sommerfeld,...) and I tried to validate my work by calculating the eigenvalues of these equations.</p>\n\n<p>As these eigenvalues were completely unexpected, I simplify my problem and I am currently trying to calculate the eigenvalues only for the differentiation matrix (see below). But the results seem wrong...</p>\n\n<p>Thanks in advance for your help, because I do not find any solution to my problem...</p>\n\n<h2>Definition of DM:</h2>\n\n<p>I use Chebyshev spectral method to operate the differentiation of vectors.\nI use the following Chebyshev package (translated from Matlab to Python):\n<a href=\"http://dip.sun.ac.za/%7Eweideman/research/differ.html\" rel=\"nofollow\">http://dip.sun.ac.za/%7Eweideman/research/differ.html</a></p>\n\n<p>That package allow me to create a differentiation matrix DM, obtained with:</p>\n\n<pre> </pre>\n\n<p>To obtain the 1st, 2nd, 3th... order differentiation, I write for example:</p>\n\n<pre> </pre>\n\n<p>I tested that and it works.\nI've build different operators based on that differentiation process.\nI've tried to validate them by finding their eigenvalues. It didn't go well so I am just trying right now with DM only.\nI do not manage to find the right eigenvalues of DM.</p>\n\n<p>I've tried with different functions:</p>\n\n<pre> </pre>\n\n<h2>Why I use scipy.sparse.LinearOperator:</h2>\n\n<p>I do not want to directly use the matrix DM, so I wrapped into a function which operates the differentiation (see code below) like that:</p>\n\n<pre> </pre>\n\n<p>The reason why I do that comes from the global project itself.\nThis is useful for more complicated equations.</p>\n\n<p>Creating such a function prevents me from using directly the matrix DM to find its eigenvalues (because DM stay inside the function).\nFor that reason, I use a scipy.sparse.LinearOperator to wrap my method derivative() and use it as an input of scipy.sparse.eig().</p>\n\n<h2>Code and results:</h2>\n\n<p>Here is the code to compute these eigenvalues:</p>\n\n<pre> </pre>\n\n<p>Here are my results (for N=20):</p>\n\n<pre> </pre>\n\n<p>The values returned by scipy.sparse should be included in the ones found by scipy/numpy, which is not the case. (idem for sympy)</p>\n\n<p>I've tried with different random matrices instead of DM (see option 2) (symmetric, non-symmetric, real, imaginary, etc...), which had small size N (4,5,6..) and also bigger ones (100,...).\nThat worked</p>\n\n<p>By changing parameters like 'which' (LM, SM, LR...), 'tol' (10E-3, 10E-6..), 'maxiter', 'sigma' (0) in scipy.sparse... scipy.sparse.linalg.eigs always worked for random matrices but never for my matrix DM. In best cases, found eigenvalues are close to the ones found by scipy, but never match.</p>\n\n<p>I really do not know what is so particular in my matrix.\nI also dont know why using scipy.sparse.linagl.eig with a matrix, a LinearOperator or a AsLinearOperator gives different results.</p>\n\n<p>I DO NOT KNOW HOW I COULD INCLUDE MY FILES CONTAINING MATRICES DM...</p>\n\n<p>For N = 4 :</p>\n\n<pre> </pre>\n\n<p>Every idea is welcome.</p>\n\n<p>May a moderator could tag my question with :\nscipy.sparse.linalg.eigs / weideman / eigenvalues / scipy.eig /scipy.sparse.lingalg.linearOperator</p>\n\n<p>Geoffroy.</p>\n", "answer_wo_code": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre> </pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre> </pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/scipy.sparse.linalg.eigs"}, "class_func_label": {"type": "literal", "value": "scipy.sparse.linalg.eigs"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nFind k eigenvalues and eigenvectors of the square matrix A.\n\nSolves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem\nfor w[i] eigenvalues with corresponding eigenvectors x[i].\n\nIf M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the\ngeneralized eigenvalue problem for w[i] eigenvalues\nwith corresponding eigenvectors x[i]\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/40050369"}, "title": {"type": "literal", "value": "Different eigenvalues between scipy.sparse.linalg.eigs and numpy/scipy.eig"}, "content": {"type": "literal", "value": "<h2>Context:</h2>\n\n<p>My goal is to create a Python3 program to operate differential operations on a vector V of size N. I did so, test it for basic operation and it works (differentiation, gradient...).</p>\n\n<p>I tried to write with that basis more complex equations (Navier-Stokes, Orr-Sommerfeld,...) and I tried to validate my work by calculating the eigenvalues of these equations.</p>\n\n<p>As these eigenvalues were completely unexpected, I simplify my problem and I am currently trying to calculate the eigenvalues only for the differentiation matrix (see below). But the results seem wrong...</p>\n\n<p>Thanks in advance for your help, because I do not find any solution to my problem...</p>\n\n<h2>Definition of DM:</h2>\n\n<p>I use Chebyshev spectral method to operate the differentiation of vectors.\nI use the following Chebyshev package (translated from Matlab to Python):\n<a href=\"http://dip.sun.ac.za/%7Eweideman/research/differ.html\" rel=\"nofollow\">http://dip.sun.ac.za/%7Eweideman/research/differ.html</a></p>\n\n<p>That package allow me to create a differentiation matrix DM, obtained with:</p>\n\n<pre><code>nodes, DM = chebyshev.chebdiff(N, maximal_order)\n</code></pre>\n\n<p>To obtain the 1st, 2nd, 3th... order differentiation, I write for example:</p>\n\n<pre><code>dVdx1 = np.dot(DM[0,:,:], V)\nd2Vdx2 = np.dot(DM[1,:,:], V)\nd3Vdx3 = np.dot(DM[2,:,:], V)\n</code></pre>\n\n<p>I tested that and it works.\nI've build different operators based on that differentiation process.\nI've tried to validate them by finding their eigenvalues. It didn't go well so I am just trying right now with DM only.\nI do not manage to find the right eigenvalues of DM.</p>\n\n<p>I've tried with different functions:</p>\n\n<pre><code>numpy.linalg.eigvals(DM)\nscipy.linalg.eig(DM)\nscipy.sparse.linalg.eigs(DM)\nsympy.solve( (DM-x*np.eye).det(), x) [for snall size only]\n</code></pre>\n\n<h2>Why I use scipy.sparse.LinearOperator:</h2>\n\n<p>I do not want to directly use the matrix DM, so I wrapped into a function which operates the differentiation (see code below) like that:</p>\n\n<pre><code>dVdx1 = derivative(V)\n</code></pre>\n\n<p>The reason why I do that comes from the global project itself.\nThis is useful for more complicated equations.</p>\n\n<p>Creating such a function prevents me from using directly the matrix DM to find its eigenvalues (because DM stay inside the function).\nFor that reason, I use a scipy.sparse.LinearOperator to wrap my method derivative() and use it as an input of scipy.sparse.eig().</p>\n\n<h2>Code and results:</h2>\n\n<p>Here is the code to compute these eigenvalues:</p>\n\n<pre><code>import numpy as np\nimport scipy\nimport sympy\n\nfrom scipy.sparse.linalg import aslinearoperator\nfrom scipy.sparse.linalg import eigs\nfrom scipy.sparse.linalg import LinearOperator\n\nimport chebyshev\n\nN = 20 # should be 4, 20, 50, 100, 300\nmax_order = 4\n\noption = 1\n#option 1: building the differentiation matrix DM for a given order\nif option == 1:\n    if 0:\n        # usage of package chebyshev, but I add a file with the matrix inside\n        nodes, DM = chebyshev.chebdiff(N, max_order)\n        order = 1\n        DM = DM[order-1,:,:]\n        #outfile = TemporaryFile()\n        np.save('DM20', DM)\n    if 1:\n        # loading the matrix from the file\n        # uncomment depending on N\n        #DM = np.load('DM4.npy')\n        DM = np.load('DM20.npy')\n        #DM = np.load('DM50.npy')\n        #DM = np.load('DM100.npy')\n        #DM = np.load('DM300.npy')\n\n#option 2: building a random matrix\nelif option == 2:\n    j = np.complex(0,1)\n    np.random.seed(0)\n    Real = np.random.random((N, N)) - 0.5\n    Im = np.random.random((N,N)) - 0.5\n\n    # If I want DM symmetric:\n    #Real = np.dot(Real, Real.T)\n    #Im = np.dot(Im, Im.T)\n\n    DM = Real + j*Im\n\n    # If I want DM singular:\n    #DM[0,:] = DM[1,:]\n\n# Test DM symmetric\nprint('Is DM symmetric ? \\n', (DM.transpose() == DM).all() )        \n# Test DM Hermitian\nprint('Is DM hermitian ? \\n', (DM.transpose().real == DM.real).all() and\n                                        (DM.transpose().imag == -DM.imag).all() )  \n\n# building a linear operator which wrap matrix DM\ndef derivative(v):\n    return np.dot(DM, v)\n\nlinop_DM = LinearOperator( (N, N), matvec = derivative)\n\n# building a linear operator directly from a matrix DM with asLinearOperator\naslinop_DM = aslinearoperator(DM)\n\n# comparison of LinearOperator and direct Dot Product\nV = np.random.random((N))\ndiff_lo = linop_DM.matvec(V)\ndiff_mat = np.dot(DM, V)\n# diff_lo and diff_mat are equals\n\n# FINDING EIGENVALUES\n\n#number of eigenvalues to find\nk = 1\nif 1:\n    # SCIPY SPARSE LINALG LINEAR OPERATOR\n    vals_sparse, vecs = scipy.sparse.linalg.eigs(linop_DM, k, which='SR',\n                            maxiter = 10000,\n                            tol = 1E-3)\n    vals_sparse = np.sort(vals_sparse)\n    print('\\nEigenvalues (scipy.sparse.linalg Linear Operator) : \\n', vals_sparse)\n\nif 1:\n    # SCIPY SPARSE ARRAY\n    vals_sparse2, vecs2 = scipy.sparse.linalg.eigs(DM, k, which='SR',\n                        maxiter = 10000,\n                        tol = 1E-3)\n    vals_sparse2 = np.sort(vals_sparse2)\n    print('\\nEigenvalues (scipy.sparse.linalg with matrix DM) : \\n', vals_sparse2)\n\nif 1:\n    # SCIPY SPARSE AS LINEAR OPERATOR\n    vals_sparse3, vecs3 = scipy.sparse.linalg.eigs(aslinop_DM, k, which='SR',\n                        maxiter = 10000,\n                        tol = 1E-3)\n    vals_sparse3 = np.sort(vals_sparse3)\n    print('\\nEigenvalues (scipy.sparse.linalg AS linear Operator) : \\n', vals_sparse3)\n\nif 0:\n    # NUMPY LINALG / SAME RESULT AS SCIPY LINALG\n    vals_np = np.linalg.eigvals(DM)\n    vals_np = np.sort(vals_np)\n    print('\\nEigenvalues (numpy.linalg) : \\n', vals_np)\n\nif 1:\n    # SCIPY LINALG\n    vals_sp = scipy.linalg.eig(DM)\n    vals_sp = np.sort(vals_sp[0])\n    print('\\nEigenvalues (scipy.linalg.eig) : \\n', vals_sp)\n\nif 0:\n    x = sympy.Symbol('x')\n    D = sympy.Matrix(DM)\n    print('\\ndet D (sympy):', D.det() )\n    E = D - x*np.eye(DM.shape[0])\n    eig_sympy = sympy.solve(E.det(), x)\n    print('\\nEigenvalues (sympy) : \\n', eig_sympy)\n</code></pre>\n\n<p>Here are my results (for N=20):</p>\n\n<pre><code>Is DM symmetric ? \n False\nIs DM hermitian ? \n False\n\nEigenvalues (scipy.sparse.linalg Linear Operator) : \n [-2.5838015+0.j]\n\nEigenvalues (scipy.sparse.linalg with matrix DM) : \n [-2.58059801+0.j]\n\nEigenvalues (scipy.sparse.linalg AS linear Operator) : \n [-2.36137671+0.j]\n\nEigenvalues (scipy.linalg.eig) : \n [-2.92933791+0.j         -2.72062839-1.01741142j -2.72062839+1.01741142j\n -2.15314244-1.84770128j -2.15314244+1.84770128j -1.36473659-2.38021351j\n -1.36473659+2.38021351j -0.49536645-2.59716913j -0.49536645+2.59716913j\n  0.38136094-2.53335888j  0.38136094+2.53335888j  0.55256471-1.68108134j\n  0.55256471+1.68108134j  1.26425751-2.25101241j  1.26425751+2.25101241j\n  2.03390489-1.74122287j  2.03390489+1.74122287j  2.57770573-0.95982011j\n  2.57770573+0.95982011j  2.77749810+0.j        ]\n</code></pre>\n\n<p>The values returned by scipy.sparse should be included in the ones found by scipy/numpy, which is not the case. (idem for sympy)</p>\n\n<p>I've tried with different random matrices instead of DM (see option 2) (symmetric, non-symmetric, real, imaginary, etc...), which had small size N (4,5,6..) and also bigger ones (100,...).\nThat worked</p>\n\n<p>By changing parameters like 'which' (LM, SM, LR...), 'tol' (10E-3, 10E-6..), 'maxiter', 'sigma' (0) in scipy.sparse... scipy.sparse.linalg.eigs always worked for random matrices but never for my matrix DM. In best cases, found eigenvalues are close to the ones found by scipy, but never match.</p>\n\n<p>I really do not know what is so particular in my matrix.\nI also dont know why using scipy.sparse.linagl.eig with a matrix, a LinearOperator or a AsLinearOperator gives different results.</p>\n\n<p>I DO NOT KNOW HOW I COULD INCLUDE MY FILES CONTAINING MATRICES DM...</p>\n\n<p>For N = 4 :</p>\n\n<pre><code>[[ 3.16666667 -4.          1.33333333 -0.5       ]\n [ 1.         -0.33333333 -1.          0.33333333]\n [-0.33333333  1.          0.33333333 -1.        ]\n [ 0.5        -1.33333333  4.         -3.16666667]]\n</code></pre>\n\n<p>Every idea is welcome.</p>\n\n<p>May a moderator could tag my question with :\nscipy.sparse.linalg.eigs / weideman / eigenvalues / scipy.eig /scipy.sparse.lingalg.linearOperator</p>\n\n<p>Geoffroy.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre><code>DM[0,:] = 0\nDM[:,0] = 0\nDM[N-1,:] = 0\nDM[:,N-1] = 0\n</code></pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre><code>[[ 0     0               0               0]\n [ 0     -0.33333333     -1.             0]\n [ 0      1.             0.33333333      0]\n [ 0      0              0               0]]\n</code></pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre><code>DM[0,:] = 0\nDM[:,0] = 0\nDM[N-1,:] = 0\nDM[:,N-1] = 0\n</code></pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre><code>[[ 0     0               0               0]\n [ 0     -0.33333333     -1.             0]\n [ 0      1.             0.33333333      0]\n [ 0      0              0               0]]\n</code></pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<h2>Context:</h2>\n\n<p>My goal is to create a Python3 program to operate differential operations on a vector V of size N. I did so, test it for basic operation and it works (differentiation, gradient...).</p>\n\n<p>I tried to write with that basis more complex equations (Navier-Stokes, Orr-Sommerfeld,...) and I tried to validate my work by calculating the eigenvalues of these equations.</p>\n\n<p>As these eigenvalues were completely unexpected, I simplify my problem and I am currently trying to calculate the eigenvalues only for the differentiation matrix (see below). But the results seem wrong...</p>\n\n<p>Thanks in advance for your help, because I do not find any solution to my problem...</p>\n\n<h2>Definition of DM:</h2>\n\n<p>I use Chebyshev spectral method to operate the differentiation of vectors.\nI use the following Chebyshev package (translated from Matlab to Python):\n<a href=\"http://dip.sun.ac.za/%7Eweideman/research/differ.html\" rel=\"nofollow\">http://dip.sun.ac.za/%7Eweideman/research/differ.html</a></p>\n\n<p>That package allow me to create a differentiation matrix DM, obtained with:</p>\n\n<pre> </pre>\n\n<p>To obtain the 1st, 2nd, 3th... order differentiation, I write for example:</p>\n\n<pre> </pre>\n\n<p>I tested that and it works.\nI've build different operators based on that differentiation process.\nI've tried to validate them by finding their eigenvalues. It didn't go well so I am just trying right now with DM only.\nI do not manage to find the right eigenvalues of DM.</p>\n\n<p>I've tried with different functions:</p>\n\n<pre> </pre>\n\n<h2>Why I use scipy.sparse.LinearOperator:</h2>\n\n<p>I do not want to directly use the matrix DM, so I wrapped into a function which operates the differentiation (see code below) like that:</p>\n\n<pre> </pre>\n\n<p>The reason why I do that comes from the global project itself.\nThis is useful for more complicated equations.</p>\n\n<p>Creating such a function prevents me from using directly the matrix DM to find its eigenvalues (because DM stay inside the function).\nFor that reason, I use a scipy.sparse.LinearOperator to wrap my method derivative() and use it as an input of scipy.sparse.eig().</p>\n\n<h2>Code and results:</h2>\n\n<p>Here is the code to compute these eigenvalues:</p>\n\n<pre> </pre>\n\n<p>Here are my results (for N=20):</p>\n\n<pre> </pre>\n\n<p>The values returned by scipy.sparse should be included in the ones found by scipy/numpy, which is not the case. (idem for sympy)</p>\n\n<p>I've tried with different random matrices instead of DM (see option 2) (symmetric, non-symmetric, real, imaginary, etc...), which had small size N (4,5,6..) and also bigger ones (100,...).\nThat worked</p>\n\n<p>By changing parameters like 'which' (LM, SM, LR...), 'tol' (10E-3, 10E-6..), 'maxiter', 'sigma' (0) in scipy.sparse... scipy.sparse.linalg.eigs always worked for random matrices but never for my matrix DM. In best cases, found eigenvalues are close to the ones found by scipy, but never match.</p>\n\n<p>I really do not know what is so particular in my matrix.\nI also dont know why using scipy.sparse.linagl.eig with a matrix, a LinearOperator or a AsLinearOperator gives different results.</p>\n\n<p>I DO NOT KNOW HOW I COULD INCLUDE MY FILES CONTAINING MATRICES DM...</p>\n\n<p>For N = 4 :</p>\n\n<pre> </pre>\n\n<p>Every idea is welcome.</p>\n\n<p>May a moderator could tag my question with :\nscipy.sparse.linalg.eigs / weideman / eigenvalues / scipy.eig /scipy.sparse.lingalg.linearOperator</p>\n\n<p>Geoffroy.</p>\n", "answer_wo_code": "<p>I spoke with a few colleague and solve partly my problem.\nMy conclusion is that my matrix is simply very ill conditioned...</p>\n\n<p>In my project, I can simplify my matrix by imposing boundary condition as follow:</p>\n\n<pre> </pre>\n\n<p>which produces a matrix similar to that for N=4:</p>\n\n<pre> </pre>\n\n<p>By using such condition, I obtain eigenvalues for scipy.sparse.linalg.eigs which are equal to the one in scipy.linalg.eig.\nI also tried using Matlab, and it return the same values.</p>\n\n<p>To continue my work, I actually needed to use the generalized eigenvalue problem in the standard form</p>\n\n<blockquote>\n  <p>\u03bb B x= DM x</p>\n</blockquote>\n\n<p>It seems that it does not work in my case because of my matrix B (which represents a Laplacian operator matrix).\nIf you have a similar problem, I advise you to visit that question:\n<a href=\"https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem\">https://scicomp.stackexchange.com/questions/10940/solving-a-generalised-eigenvalue-problem</a></p>\n\n<p>(I think that) the matrix B needs to be positive definite to use scipy.sparse.\nA solution would be to change B, to use scipy.linalg.eig or to use Matlab.\nI will confirm that later.</p>\n\n<h2>EDIT:</h2>\n\n<p>I wrote a solution to the stack exchange question I post above which explains how I solve my problem.\nI appears that scipy.sparse.linalg.eigs has indeed a bug if matrix B is not positive definite, and will return bad eigenvalues.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/numpy.linalg.eig"}, "class_func_label": {"type": "literal", "value": "numpy.linalg.eig"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nCompute the eigenvalues and right eigenvectors of a square array.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/48338430"}, "title": {"type": "literal", "value": "Find zeros of the characteristic polynomial of a matrix with Python"}, "content": {"type": "literal", "value": "<p>Given an <em>N x N</em> symmetric matrix <code>C</code> and an <em>N x N</em> diagonal matrix <code>I</code>, find the solutions of the equation <code>det(\u03bbI-C)=0</code>. In other words, the (generalized) eigenvalues of <code>C</code> are to be found.</p>\n\n<p>I know few ways how to solve this in MATLAB using build-in functions:</p>\n\n<p><strong>1st way:</strong></p>\n\n<pre><code>function lambdas=eigenValues(C,I)\n    syms x;\n    lambdas=sort(roots(double(fliplr(coeffs(det(C-I*x))))));\n</code></pre>\n\n<p><strong>2nd way:</strong></p>\n\n<pre><code>[V,D]=eig(C,I);\n</code></pre>\n\n<p>However, I need to use Python. There are similar function in NumPy and SymPy, but, according to docs (<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig\" rel=\"nofollow noreferrer\">numpy</a>, <a href=\"http://docs.sympy.org/latest/modules/matrices/matrices.html\" rel=\"nofollow noreferrer\">sympy</a>), they take only one matrix <em>C</em>, as the input. Though, the result's different from the result produced by Matlab. Also, symbolic solutions produced by SymPy aren't helpful. Maybe I am doing something wrong? How to find solution?</p>\n\n<hr>\n\n<h2>Example</h2>\n\n<ul>\n<li>MATLAB:</li>\n</ul>\n\n<p>%INPUT</p>\n\n<pre><code>I =\n\n     2     0     0\n     0     6     0\n     0     0     5\nC =\n\n     4     7     0\n     7     8    -4\n     0    -4     1\n\n[v,d]=eig(C,I)\n</code></pre>\n\n<p>%RESULT</p>\n\n<pre><code>v =\n\n    -0.3558   -0.3109   -0.5261\n     0.2778    0.1344   -0.2673\n     0.2383   -0.3737    0.0598\n\n\nd =\n\n       -0.7327    0         0\n        0    0.4876         0\n        0         0    3.7784\n</code></pre>\n\n<ul>\n<li>Python 3.5:</li>\n</ul>\n\n<p>%INPUT</p>\n\n<pre><code>I=np.matrix([[2,0,0],\n                 [0,6,0],\n                 [0,0,5]])\n\nC=np.matrix([[4,7,0],[7,8,-4],[0,-4,1]])\n\n\nnp.linalg.eigh(C)\n</code></pre>\n\n<p>%RESULT</p>\n\n<pre><code>(array([-3., 1.91723747, 14.08276253]),\n\n\nmatrix(\n\n       [[-0.57735027,  0.60061066, -0.55311256],\n\n        [ 0.57735027, -0.1787042 , -0.79670037],\n\n        [ 0.57735027,  0.77931486,  0.24358781]]))\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>At least if <code>I</code> has positive diagonal entries you can simply solve a transformed system:</p>\n\n<pre><code># example problem\n&gt;&gt;&gt; A = np.random.random((3, 3))\n&gt;&gt;&gt; A = A.T @ A\n&gt;&gt;&gt; I = np.identity(3) * np.random.random((3,))\n\n# transform \n&gt;&gt;&gt; J = np.sqrt(np.einsum('ii-&gt;i', I))\n&gt;&gt;&gt; B = A / np.outer(J, J)\n\n# solve\n&gt;&gt;&gt; eval_, evec = np.linalg.eigh(B)\n\n# back transform result\n&gt;&gt;&gt; evec /= J[:, None]\n\n# check\n&gt;&gt;&gt; A @ evec\narray([[ -1.43653725e-02,   4.14643550e-01,  -2.42340866e+00],\n       [ -1.75615960e-03,  -4.17347693e-01,  -8.19546081e-01],\n       [  1.90178603e-02,   1.34837899e-01,  -1.69999003e+00]])\n&gt;&gt;&gt; eval_ * (I @ evec)\narray([[ -1.43653725e-02,   4.14643550e-01,  -2.42340866e+00],\n       [ -1.75615960e-03,  -4.17347693e-01,  -8.19546081e-01],\n       [  1.90178603e-02,   1.34837899e-01,  -1.69999003e+00]])\n</code></pre>\n\n<p>OP's example. IMPORTANT: must use <code>np.array</code>s for <code>I</code> and <code>C</code>, <code>np.matrix</code> will not work. </p>\n\n<pre><code>&gt;&gt;&gt; I=np.array([[2,0,0],[0,6,0],[0,0,5]])\n&gt;&gt;&gt; C=np.array([[4,7,0],[7,8,-4],[0,-4,1]])\n&gt;&gt;&gt; \n&gt;&gt;&gt; J = np.sqrt(np.einsum('ii-&gt;i', I))\n&gt;&gt;&gt; B = C / np.outer(J, J)\n&gt;&gt;&gt; eval_, evec = np.linalg.eigh(B)\n&gt;&gt;&gt; evec /= J[:, None]\n&gt;&gt;&gt; \n&gt;&gt;&gt; evec\narray([[-0.35578356, -0.31094779, -0.52605088],\n       [ 0.27778714,  0.1343625 , -0.267297  ],\n       [ 0.23826117, -0.37371199,  0.05975754]])\n&gt;&gt;&gt; eval_\narray([-0.73271478,  0.48762792,  3.7784202 ])\n</code></pre>\n\n<p>If <code>I</code> has positive and negative entries use <code>eig</code> instead of <code>eigh</code> and before taking the square root cast to <code>complex</code> <code>dtype</code>.</p>\n\n\n<p>Differing from other answers, I assume that by the symbol <em>I</em> you mean the identity matrix, <em>Ix=x</em>.</p>\n\n<p>What you want to solve, <em>Cx=\u03bbIx</em>, is the so-called <em>standard</em> eigenvalue problem,\nand most eigenvalue solvers tackle the problem described in that format, hence the\nNumpy function has the signature <code>eig(C)</code>.</p>\n\n<p>If your <em>C</em> matrix is a symmetric matrix and your problem is indeed a standard eigenvalue problem I'd recommend the use of <code>numpy.linalg.eigh</code>, that is optimized for this type of problems.</p>\n\n<p>On the contrary if your problem is really a generalized eigenvalue problem, as, e.g., the frequency equation <em>Kx=\u03c9\u00b2Mx</em> you could use <code>scipy.linalg.eigh</code>, that supports that type of problem statement for symmetric matrices.</p>\n\n<pre><code>eigvals, eigvecs = scipy.linalg.eigh(C, I)\n</code></pre>\n\n<p>With respect to the discrepancies in eigenvalues, the Numpy implementation gives no guarantees w/r to their ordering, so it could be just a different ordering, but if your problem is indeed a generalized problem (<em>I</em> not being the identity matrix...) the solution is of course different and you have to use the Scipy implementation of <code>eigh</code>.</p>\n\n<p>If the discrepancies is within the eigenvectors, please remember that the eigenvectors are known within an arbitrary scale factor and, again, the ordering could be undefined (but, of course, their order is the same order in which you have the eigenvalues) \u2014 the situation is a little different for <code>scipy.linalg.eigh</code> because in this case the eigenvalues are sorted and the eigenvectors are normalized with respect to the second matrix argument (<em>I</em> in your example).</p>\n\n<hr>\n\n<p>Ps: <code>scipy.linalg.eigh</code> behaviour (i.e., sorted eigenvalues and normalized eigenvectors) is so convenient for <strong>my</strong> use cases that I use to use it also to solve standard eigenvalue problems.</p>\n\n\n<p>Using <a href=\"http://www.sympy.org\" rel=\"nofollow noreferrer\">SymPy</a>:</p>\n\n<pre><code>&gt;&gt;&gt; from sympy import *\n&gt;&gt;&gt; t = Symbol('t')\n&gt;&gt;&gt; D = diag(2,6,5)\n&gt;&gt;&gt; S = Matrix([[ 4, 7, 0],\n                [ 7, 8,-4],\n                [ 0,-4, 1]])\n&gt;&gt;&gt; (t*D - S).det()\n60*t**3 - 212*t**2 - 77*t + 81\n</code></pre>\n\n<p>Computing the <strong>exact</strong> roots:</p>\n\n<pre><code>&gt;&gt;&gt; roots = solve(60*t**3 - 212*t**2 - 77*t + 81,t)\n&gt;&gt;&gt; roots\n[53/45 + (-1/2 - sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3) + 14701/(8100*(-1/2 - sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)), 53/45 + 14701/(8100*(-1/2 + sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3), 53/45 + 14701/(8100*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)) + (312469/182250 + sqrt(797521629)*I/16200)**(1/3)]\n</code></pre>\n\n<p>Computing <strong>floating-point</strong> approximations of the roots:</p>\n\n<pre><code>&gt;&gt;&gt; for r in roots:\n...     r.evalf()\n... \n0.487627918145732 + 0.e-22*I\n-0.73271478047926 - 0.e-22*I\n3.77842019566686 - 0.e-21*I\n</code></pre>\n\n<p>Note that the roots are real.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>At least if <code>I</code> has positive diagonal entries you can simply solve a transformed system:</p>\n\n<pre><code># example problem\n&gt;&gt;&gt; A = np.random.random((3, 3))\n&gt;&gt;&gt; A = A.T @ A\n&gt;&gt;&gt; I = np.identity(3) * np.random.random((3,))\n\n# transform \n&gt;&gt;&gt; J = np.sqrt(np.einsum('ii-&gt;i', I))\n&gt;&gt;&gt; B = A / np.outer(J, J)\n\n# solve\n&gt;&gt;&gt; eval_, evec = np.linalg.eigh(B)\n\n# back transform result\n&gt;&gt;&gt; evec /= J[:, None]\n\n# check\n&gt;&gt;&gt; A @ evec\narray([[ -1.43653725e-02,   4.14643550e-01,  -2.42340866e+00],\n       [ -1.75615960e-03,  -4.17347693e-01,  -8.19546081e-01],\n       [  1.90178603e-02,   1.34837899e-01,  -1.69999003e+00]])\n&gt;&gt;&gt; eval_ * (I @ evec)\narray([[ -1.43653725e-02,   4.14643550e-01,  -2.42340866e+00],\n       [ -1.75615960e-03,  -4.17347693e-01,  -8.19546081e-01],\n       [  1.90178603e-02,   1.34837899e-01,  -1.69999003e+00]])\n</code></pre>\n\n<p>OP's example. IMPORTANT: must use <code>np.array</code>s for <code>I</code> and <code>C</code>, <code>np.matrix</code> will not work. </p>\n\n<pre><code>&gt;&gt;&gt; I=np.array([[2,0,0],[0,6,0],[0,0,5]])\n&gt;&gt;&gt; C=np.array([[4,7,0],[7,8,-4],[0,-4,1]])\n&gt;&gt;&gt; \n&gt;&gt;&gt; J = np.sqrt(np.einsum('ii-&gt;i', I))\n&gt;&gt;&gt; B = C / np.outer(J, J)\n&gt;&gt;&gt; eval_, evec = np.linalg.eigh(B)\n&gt;&gt;&gt; evec /= J[:, None]\n&gt;&gt;&gt; \n&gt;&gt;&gt; evec\narray([[-0.35578356, -0.31094779, -0.52605088],\n       [ 0.27778714,  0.1343625 , -0.267297  ],\n       [ 0.23826117, -0.37371199,  0.05975754]])\n&gt;&gt;&gt; eval_\narray([-0.73271478,  0.48762792,  3.7784202 ])\n</code></pre>\n\n<p>If <code>I</code> has positive and negative entries use <code>eig</code> instead of <code>eigh</code> and before taking the square root cast to <code>complex</code> <code>dtype</code>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>Differing from other answers, I assume that by the symbol <em>I</em> you mean the identity matrix, <em>Ix=x</em>.</p>\n\n<p>What you want to solve, <em>Cx=\u03bbIx</em>, is the so-called <em>standard</em> eigenvalue problem,\nand most eigenvalue solvers tackle the problem described in that format, hence the\nNumpy function has the signature <code>eig(C)</code>.</p>\n\n<p>If your <em>C</em> matrix is a symmetric matrix and your problem is indeed a standard eigenvalue problem I'd recommend the use of <code>numpy.linalg.eigh</code>, that is optimized for this type of problems.</p>\n\n<p>On the contrary if your problem is really a generalized eigenvalue problem, as, e.g., the frequency equation <em>Kx=\u03c9\u00b2Mx</em> you could use <code>scipy.linalg.eigh</code>, that supports that type of problem statement for symmetric matrices.</p>\n\n<pre><code>eigvals, eigvecs = scipy.linalg.eigh(C, I)\n</code></pre>\n\n<p>With respect to the discrepancies in eigenvalues, the Numpy implementation gives no guarantees w/r to their ordering, so it could be just a different ordering, but if your problem is indeed a generalized problem (<em>I</em> not being the identity matrix...) the solution is of course different and you have to use the Scipy implementation of <code>eigh</code>.</p>\n\n<p>If the discrepancies is within the eigenvectors, please remember that the eigenvectors are known within an arbitrary scale factor and, again, the ordering could be undefined (but, of course, their order is the same order in which you have the eigenvalues) \u2014 the situation is a little different for <code>scipy.linalg.eigh</code> because in this case the eigenvalues are sorted and the eigenvectors are normalized with respect to the second matrix argument (<em>I</em> in your example).</p>\n\n<hr>\n\n<p>Ps: <code>scipy.linalg.eigh</code> behaviour (i.e., sorted eigenvalues and normalized eigenvectors) is so convenient for <strong>my</strong> use cases that I use to use it also to solve standard eigenvalue problems.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Using <a href=\"http://www.sympy.org\" rel=\"nofollow noreferrer\">SymPy</a>:</p>\n\n<pre><code>&gt;&gt;&gt; from sympy import *\n&gt;&gt;&gt; t = Symbol('t')\n&gt;&gt;&gt; D = diag(2,6,5)\n&gt;&gt;&gt; S = Matrix([[ 4, 7, 0],\n                [ 7, 8,-4],\n                [ 0,-4, 1]])\n&gt;&gt;&gt; (t*D - S).det()\n60*t**3 - 212*t**2 - 77*t + 81\n</code></pre>\n\n<p>Computing the <strong>exact</strong> roots:</p>\n\n<pre><code>&gt;&gt;&gt; roots = solve(60*t**3 - 212*t**2 - 77*t + 81,t)\n&gt;&gt;&gt; roots\n[53/45 + (-1/2 - sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3) + 14701/(8100*(-1/2 - sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)), 53/45 + 14701/(8100*(-1/2 + sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(312469/182250 + sqrt(797521629)*I/16200)**(1/3), 53/45 + 14701/(8100*(312469/182250 + sqrt(797521629)*I/16200)**(1/3)) + (312469/182250 + sqrt(797521629)*I/16200)**(1/3)]\n</code></pre>\n\n<p>Computing <strong>floating-point</strong> approximations of the roots:</p>\n\n<pre><code>&gt;&gt;&gt; for r in roots:\n...     r.evalf()\n... \n0.487627918145732 + 0.e-22*I\n-0.73271478047926 - 0.e-22*I\n3.77842019566686 - 0.e-21*I\n</code></pre>\n\n<p>Note that the roots are real.</p>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>Given an <em>N x N</em> symmetric matrix   and an <em>N x N</em> diagonal matrix  , find the solutions of the equation  . In other words, the (generalized) eigenvalues of   are to be found.</p>\n\n<p>I know few ways how to solve this in MATLAB using build-in functions:</p>\n\n<p><strong>1st way:</strong></p>\n\n<pre> </pre>\n\n<p><strong>2nd way:</strong></p>\n\n<pre> </pre>\n\n<p>However, I need to use Python. There are similar function in NumPy and SymPy, but, according to docs (<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig\" rel=\"nofollow noreferrer\">numpy</a>, <a href=\"http://docs.sympy.org/latest/modules/matrices/matrices.html\" rel=\"nofollow noreferrer\">sympy</a>), they take only one matrix <em>C</em>, as the input. Though, the result's different from the result produced by Matlab. Also, symbolic solutions produced by SymPy aren't helpful. Maybe I am doing something wrong? How to find solution?</p>\n\n<hr>\n\n<h2>Example</h2>\n\n<ul>\n<li>MATLAB:</li>\n</ul>\n\n<p>%INPUT</p>\n\n<pre> </pre>\n\n<p>%RESULT</p>\n\n<pre> </pre>\n\n<ul>\n<li>Python 3.5:</li>\n</ul>\n\n<p>%INPUT</p>\n\n<pre> </pre>\n\n<p>%RESULT</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>At least if   has positive diagonal entries you can simply solve a transformed system:</p>\n\n<pre> </pre>\n\n<p>OP's example. IMPORTANT: must use  s for   and  ,   will not work. </p>\n\n<pre> </pre>\n\n<p>If   has positive and negative entries use   instead of   and before taking the square root cast to    .</p>\n\n\n<p>Differing from other answers, I assume that by the symbol <em>I</em> you mean the identity matrix, <em>Ix=x</em>.</p>\n\n<p>What you want to solve, <em>Cx=\u03bbIx</em>, is the so-called <em>standard</em> eigenvalue problem,\nand most eigenvalue solvers tackle the problem described in that format, hence the\nNumpy function has the signature  .</p>\n\n<p>If your <em>C</em> matrix is a symmetric matrix and your problem is indeed a standard eigenvalue problem I'd recommend the use of  , that is optimized for this type of problems.</p>\n\n<p>On the contrary if your problem is really a generalized eigenvalue problem, as, e.g., the frequency equation <em>Kx=\u03c9\u00b2Mx</em> you could use  , that supports that type of problem statement for symmetric matrices.</p>\n\n<pre> </pre>\n\n<p>With respect to the discrepancies in eigenvalues, the Numpy implementation gives no guarantees w/r to their ordering, so it could be just a different ordering, but if your problem is indeed a generalized problem (<em>I</em> not being the identity matrix...) the solution is of course different and you have to use the Scipy implementation of  .</p>\n\n<p>If the discrepancies is within the eigenvectors, please remember that the eigenvectors are known within an arbitrary scale factor and, again, the ordering could be undefined (but, of course, their order is the same order in which you have the eigenvalues) \u2014 the situation is a little different for   because in this case the eigenvalues are sorted and the eigenvectors are normalized with respect to the second matrix argument (<em>I</em> in your example).</p>\n\n<hr>\n\n<p>Ps:   behaviour (i.e., sorted eigenvalues and normalized eigenvectors) is so convenient for <strong>my</strong> use cases that I use to use it also to solve standard eigenvalue problems.</p>\n\n\n<p>Using <a href=\"http://www.sympy.org\" rel=\"nofollow noreferrer\">SymPy</a>:</p>\n\n<pre> </pre>\n\n<p>Computing the <strong>exact</strong> roots:</p>\n\n<pre> </pre>\n\n<p>Computing <strong>floating-point</strong> approximations of the roots:</p>\n\n<pre> </pre>\n\n<p>Note that the roots are real.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.poly"}, "class_func_label": {"type": "literal", "value": "sympy.poly"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nEfficiently transform an expression into a polynomial.\n\n.. rubric:: Examples\n\n>>> from sympy import poly\n>>> from sympy.abc import x\n\n>>> poly(x*(x**2 + x - 1)**2)\nPoly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/38126043"}, "title": {"type": "literal", "value": "Sympy never returns when processing complicated determinants"}, "content": {"type": "literal", "value": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre><code>    deter = mat.det('method':'berkowitz')\n    #self.resultFileHandler.writeLogStr(str(deter))\n    a = sy_polys.Poly(deter, k)\n</code></pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre><code>MainThread - pid_135368_id_42197520 \n  _print [printer.py:262]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  doprint [printer.py:233]  \n  sstr [str.py:748] \n  __str__ [basic.py:396]    \n  _getRoots_sympy_Poly_nroots [__init__.py:91]  \n  getRoots [__init__.py:68] \n  findPolyRoots [__init__.py:697]   \n  _getNroots [polefinder.py:97] \n  _doForN [polefinder.py:60]    \n  _incN [polefinder.py:52]  \n  __init__ [polefinder.py:39]   \n  _doPoleFind [polefinderwrap.py:33]    \n  _polesForPos [polefinderwrap.py:47]   \n  &lt;module&gt; [polefinderwrap.py:60]   \n  run [pydevd.py:937]   \n  &lt;module&gt; [pydevd.py:1530] \n</code></pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 60, in _doForN\n    roots = self._getNroots(N)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 97, in _getNroots\n    roots = ratSmat.findPolyRoots(False)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 697, in findPolyRoots\n    roots = self.polyRootSolve.getRoots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 68, in getRoots\n    ret = self._getRoots_sympy_Poly_nroots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 91, in _getRoots_sympy_Poly_nroots\n    self.resultFileHandler.writeLogStr(str(deter))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\core\\basic.py\", line 396, in __str__\n    return sstr(self, order=None)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 748, in sstr\n    s = p.doprint(expr)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 233, in doprint\n    return self._str(self._print(expr))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 69, in _print_Add\n    return sign + ' '.join(l)\nMemoryError\n</code></pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n"}, "answerContent": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre> </pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre> </pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre> </pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n", "answer_wo_code": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre> </pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre> </pre>\n\n<p>3:  Above but with  . det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre> </pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class  </a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list  , tuple  , or dictionary  ); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose   as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use   or   internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.matrices.matrices.MatrixBase"}, "class_func_label": {"type": "literal", "value": "sympy.matrices.matrices.MatrixBase"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Base class for matrix objects."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/38126043"}, "title": {"type": "literal", "value": "Sympy never returns when processing complicated determinants"}, "content": {"type": "literal", "value": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre><code>    deter = mat.det('method':'berkowitz')\n    #self.resultFileHandler.writeLogStr(str(deter))\n    a = sy_polys.Poly(deter, k)\n</code></pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre><code>MainThread - pid_135368_id_42197520 \n  _print [printer.py:262]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  doprint [printer.py:233]  \n  sstr [str.py:748] \n  __str__ [basic.py:396]    \n  _getRoots_sympy_Poly_nroots [__init__.py:91]  \n  getRoots [__init__.py:68] \n  findPolyRoots [__init__.py:697]   \n  _getNroots [polefinder.py:97] \n  _doForN [polefinder.py:60]    \n  _incN [polefinder.py:52]  \n  __init__ [polefinder.py:39]   \n  _doPoleFind [polefinderwrap.py:33]    \n  _polesForPos [polefinderwrap.py:47]   \n  &lt;module&gt; [polefinderwrap.py:60]   \n  run [pydevd.py:937]   \n  &lt;module&gt; [pydevd.py:1530] \n</code></pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 60, in _doForN\n    roots = self._getNroots(N)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 97, in _getNroots\n    roots = ratSmat.findPolyRoots(False)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 697, in findPolyRoots\n    roots = self.polyRootSolve.getRoots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 68, in getRoots\n    ret = self._getRoots_sympy_Poly_nroots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 91, in _getRoots_sympy_Poly_nroots\n    self.resultFileHandler.writeLogStr(str(deter))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\core\\basic.py\", line 396, in __str__\n    return sstr(self, order=None)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 748, in sstr\n    s = p.doprint(expr)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 233, in doprint\n    return self._str(self._print(expr))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 69, in _print_Add\n    return sign + ' '.join(l)\nMemoryError\n</code></pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n"}, "answerContent": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre> </pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre> </pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre> </pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n", "answer_wo_code": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre> </pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre> </pre>\n\n<p>3:  Above but with  . det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre> </pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class  </a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list  , tuple  , or dictionary  ); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose   as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use   or   internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.polys.Poly"}, "class_func_label": {"type": "literal", "value": "sympy.polys.Poly"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Generic class for representing and operating on polynomial expressions.\n    Subclasses Expr class.\n\n    Examples\n    ========\n\n    >>> from sympy import Poly\n    >>> from sympy.abc import x, y\n\n    Create a univariate polynomial:\n\n    >>> Poly(x*(x**2 + x - 1)**2)\n    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n\n    Create a univariate polynomial with specific domain:\n\n    >>> from sympy import sqrt\n    >>> Poly(x**2 + 2*x + sqrt(3), domain='R')\n    Poly(1.0*x**2 + 2.0*x + 1.73205080756888, x, domain='RR')\n\n    Create a multivariate polynomial:\n\n    >>> Poly(y*x**2 + x*y + 1)\n    Poly(x**2*y + x*y + 1, x, y, domain='ZZ')\n\n    Create a univariate polynomial, where y is a constant:\n\n    >>> Poly(y*x**2 + x*y + 1,x)\n    Poly(y*x**2 + y*x + 1, x, domain='ZZ[y]')\n\n    You can evaluate the above polynomial as a function of y:\n\n    >>> Poly(y*x**2 + x*y + 1,x).eval(2)\n    6*y + 1\n\n    See Also\n    ========\n\n    sympy.core.expr.Expr\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/38126043"}, "title": {"type": "literal", "value": "Sympy never returns when processing complicated determinants"}, "content": {"type": "literal", "value": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre><code>    deter = mat.det('method':'berkowitz')\n    #self.resultFileHandler.writeLogStr(str(deter))\n    a = sy_polys.Poly(deter, k)\n</code></pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre><code>MainThread - pid_135368_id_42197520 \n  _print [printer.py:262]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  parenthesize [str.py:29]  \n  _print_Mul [str.py:290]   \n  _print [printer.py:257]   \n  _print_Add [str.py:56]    \n  _print [printer.py:257]   \n  doprint [printer.py:233]  \n  sstr [str.py:748] \n  __str__ [basic.py:396]    \n  _getRoots_sympy_Poly_nroots [__init__.py:91]  \n  getRoots [__init__.py:68] \n  findPolyRoots [__init__.py:697]   \n  _getNroots [polefinder.py:97] \n  _doForN [polefinder.py:60]    \n  _incN [polefinder.py:52]  \n  __init__ [polefinder.py:39]   \n  _doPoleFind [polefinderwrap.py:33]    \n  _polesForPos [polefinderwrap.py:47]   \n  &lt;module&gt; [polefinderwrap.py:60]   \n  run [pydevd.py:937]   \n  &lt;module&gt; [pydevd.py:1530] \n</code></pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 60, in _doForN\n    roots = self._getNroots(N)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\ratsmat.\\polefinder.py\", line 97, in _getNroots\n    roots = ratSmat.findPolyRoots(False)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 697, in findPolyRoots\n    roots = self.polyRootSolve.getRoots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 68, in getRoots\n    ret = self._getRoots_sympy_Poly_nroots(mat, k)\n  File \"E:\\Peter's Documents\\PhD\\Code\\Git\\ProtoQScat\\multichannel\\qscat\\numerical/..\\ratsmat\\__init__.py\", line 91, in _getRoots_sympy_Poly_nroots\n    self.resultFileHandler.writeLogStr(str(deter))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\core\\basic.py\", line 396, in __str__\n    return sstr(self, order=None)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 748, in sstr\n    s = p.doprint(expr)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 233, in doprint\n    return self._str(self._print(expr))\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 56, in _print_Add\n    t = self._print(term)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 290, in _print_Mul\n    a_str = [self.parenthesize(x, prec) for x in a]\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 29, in parenthesize\n    return \"(%s)\" % self._print(item)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\printer.py\", line 257, in _print\n    return getattr(self, printmethod)(expr, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\sympy\\printing\\str.py\", line 69, in _print_Add\n    return sign + ' '.join(l)\nMemoryError\n</code></pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n"}, "answerContent": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre><code>from sympy import MatrixSymbol, Matrix\n\nX = MatrixSymbol('X', 10, 10)\nXmat = Matrix(X)\n\ndeter = Xmat.det(method='berkowitz')\nstr(deter)\n</code></pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = 2.0*float(i+1)*x**2 + 2.0*float(j+1)*x - 5.0*float(i+1)\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n\n<p>3:  Above but with <code>m[i,j] = poly(</code>. det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre><code>import sympy\nfrom sympy import Matrix, I\nfrom sympy import poly\nfrom sympy.polys import Poly\n\nmatSz = 10\n\nm = Matrix([[0.0]*matSz]*matSz)\n\nx = sympy.symbols('x')\nfor i in range(matSz):\n  for j in range(matSz):\n    m[i,j] = poly(2.0*float(i+1)*x**2 + 2.0*float(j+1)*x*I - 5.0*float(i+1))\n\ndeter = m.det(method='berkowitz')\ndeter_poly = Poly(deter, x)  #Required or exception\nroots = deter_poly.nroots()\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class <code>NC^2</code></a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list <code>[1]</code>, tuple <code>[2]</code>, or dictionary <code>[3]</code>); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose <code>[1]</code> as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use <code>[2]</code> or <code>[3]</code> internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>At least this it how it appears. The following code behaved correctly for 3x3 and 6x6 matrices.</p>\n\n<pre> </pre>\n\n<p>For 3x3 it takes ~0.8s to execute this code, for 6x6 it takes ~288s (with only 650ms for the det function, the rest for the Poly). For 10x10, either the complexity has ramped at a colossal rate or some other reason is preventing it returning from the Poly call (I waited a week). No exceptions are thrown.</p>\n\n<p>The elements of the determinants consist of large symbolic polynomials.</p>\n\n<p>I was on 0.7.1 and just upgraded to 1.0 (problem in both versions).</p>\n\n<p>I added the logging to try and get the determinant to file but it sticks again in the str(deter) function call. If I break my debugger can't display the deter (prob too large for the debugger). </p>\n\n<p>Here is a stack:</p>\n\n<pre> </pre>\n\n<p>OK, I've got an exception from the str function. Seems likely that the polynomial has become too large.</p>\n\n<pre> </pre>\n\n<p>EDIT:\nFollowing from answer below here is a profile plot with the determinant size (channel). Ignore N (on y axis) it is another parameter of the calculation (governs the size of the polys in the elements).\n<a href=\"https://i.stack.imgur.com/2lyBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2lyBO.png\" alt=\"enter image description here\"></a></p>\n", "answer_wo_code": "<p>OK, so I returned to this after reading literature and feeling (emphasis here) that the Berkowitz should perform between O(n^2) and O(n^3). </p>\n\n<p>What I found was that the input type to the det and Poly makes a huge difference to the performance (I admit that the input type was not shown in my question). Wrapping the original expression in a poly drastically improves performance.</p>\n\n<p>Consider the following three codes</p>\n\n<p>1: using MatrixSymbol. det takes 1.1s then still stuck at str after 30min</p>\n\n<pre> </pre>\n\n<p>2:  Represents my original problem code. det takes 1.8s then still stuck at Poly after 30min</p>\n\n<pre> </pre>\n\n<p>3:  Above but with  . det takes 3.0s, Poly 0.04 and nroots 0.27</p>\n\n<pre> </pre>\n\n\n<p><strong>The algorithm is just slow.</strong></p>\n\n<p>Sympy explains <a href=\"http://docs.sympy.org/0.7.2/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">the Berkowitz method in its documentation</a>, and references <a href=\"http://www.sciencedirect.com/science/article/pii/0020019084900188\" rel=\"nofollow noreferrer\">\"On computing the determinant in small parallel time using a small number of processors\"</a> ; for its implementation, <a href=\"http://docs.sympy.org/0.7.2/_modules/sympy/matrices/matrices.html#MatrixBase.berkowitz\" rel=\"nofollow noreferrer\">look at the open-source sympy code</a>. </p>\n\n<p>The complexity of Berkowitz is pretty hard to understand, and it looks like if you don't want to brute force the proof of its correctness <a href=\"http://arxiv.org/pdf/math/0201315.pdf\" rel=\"nofollow noreferrer\">then you need to get involved in some pretty hairy combinatorics</a>.</p>\n\n<p><em>The algorithm is fast for highly parrallized architectures</em>; it's mainly motivated by the fact that Gaussian Ellimination doesn't parralelize well. Formally, its in the <a href=\"https://en.wikipedia.org/wiki/NC_%28complexity%29#The_NC_hierarchy\" rel=\"nofollow noreferrer\">class  </a>. I might guess that your tests weren't being run on such an architecture. Some researchers into the algorithm <a href=\"https://cstheory.stackexchange.com/questions/12448/smallest-known-formula-for-the-determinant\">seem to be contributors on CS.SE</a>, if you have more questions on that topic.</p>\n\n<p><strong>The Polynomial Call is Slow</strong></p>\n\n<p><a href=\"http://docs.sympy.org/0.6.7/modules/polynomials.html#sympy.polys.Poly\" rel=\"nofollow noreferrer\">From the docs</a>, there are multiple ways of constructing a polynomial, dependent on what type of collection is passed into the constructor (list  , tuple  , or dictionary  ); they result in different validation and have very different performance. I would point you to this note in that documentation (emphasis mine, capitalization their's):</p>\n\n<blockquote>\n  <p>For interactive usage choose   as it\u2019s safe and relatively fast.</p>\n  \n  <p>CAUTION: Use   or   internally for time critical algorithms, when\n  you know that coefficients and monomials will be valid sympy\n  expressions. Use them with caution! If the coefficients are integers\n  instead of sympy integers (e.g. 1 instead of S(1)) <strong>the polynomial will\n  be created but you may run into problems if you try to print the\n  polynomial</strong>. If the monomials are not given as tuples of integers you\n  will have problems.</p>\n</blockquote>\n\n<hr>\n\n<p>Sympy also reserves the right to lazily evaluate expressions until their output is needed. This is a significant part of the benefit of symbolic calculations - mathematical simplification can result in precision gains and performance gains, but it also may mean that the actual evaluation of complex expressions may be delayed until unexpected times.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/numpy.linalg.solve"}, "class_func_label": {"type": "literal", "value": "numpy.linalg.solve"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nSolve a linear matrix equation, or system of linear scalar equations.\n\nComputes the \"exact\" solution, `x`, of the well-determined, i.e., full\nrank, linear matrix equation `ax = b`.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/14367331"}, "title": {"type": "literal", "value": "How to set up and solve simultaneous equations in python"}, "content": {"type": "literal", "value": "<p>For a fixed integer <code>n</code>, I have a set of <code>2(n-1)</code> simultaneous equations as follows.  </p>\n\n<pre><code>M(p) = 1+((n-p-1)/n)*M(n-1) + (2/n)*N(p-1) + ((p-1)/n)*M(p-1)\n\nN(p) = 1+((n-p-1)/n)*M(n-1) + (p/n)*N(p-1)\n\nM(1) = 1+((n-2)/n)*M(n-1) + (2/n)*N(0)\n\nN(0) = 1+((n-1)/n)*M(n-1)\n</code></pre>\n\n<p><code>M(p)</code> is defined for <code>1 &lt;= p &lt;= n-1</code>.  <code>N(p)</code> is defined for <code>0 &lt;= p &lt;= n-2</code>.  Notice also that <code>p</code> is just a constant integer in every equation so the whole system is linear.</p>\n\n<p>I have been using Maple but I would like to set these up and solve them in python now, maybe using <code>numpy.linalg.solve</code> (or any other better method).  I actually only want the value of <code>M(n-1)</code>. For example, when <code>n=2</code> the answer should be <code>M(1) = 4</code>, I believe.  This is because the equations become </p>\n\n<pre><code>M(1) = 1+(2/2)*N(0)\nN(0) = 1 + (1/2)*M(1)\n</code></pre>\n\n<p>Therefore</p>\n\n<pre><code>M(1)/2 = 1+1\n</code></pre>\n\n<p>and so</p>\n\n<pre><code>M(1) = 4. \n</code></pre>\n\n<p>If I want to plug in <code>n=50</code>, say, how can you set up this system of simultaneous equations in python so that numpy.linalg.solve can solve them?</p>\n\n<p><strong>Update</strong>  The answers are great but they use dense solvers where the system of equations is sparse.  Posted follow up to <a href=\"https://stackoverflow.com/questions/14380899/using-scipy-sparse-matrices-to-solve-system-of-equations\">Using scipy sparse matrices to solve system of equations</a> .</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Updated: added implementation using scipy.sparse</p>\n\n<hr>\n\n<p>This gives the solution in the order <code>N_max,...,N_0,M_max,...,M_1</code>.</p>\n\n<p>The linear system to solve is of the shape <code>A dot x == const 1-vector</code>.\n<code>x</code> is the sought after solution vector.<br>\nHere I ordered the equations such that <code>x</code> is <code>N_max,...,N_0,M_max,...,M_1</code>.<br>\nThen I build up the <code>A</code>-coefficient matrix from 4 block matrices.</p>\n\n<p>Here's a snapshot for the example case <code>n=50</code> showing how you can derive the coefficient matrix and understand the block structure. The coefficient matrix <code>A</code> is light blue, the constant right side is orange. The sought after solution vector <code>x</code> is here light green and used to label the columns. The first column show from which of the above given eqs. the row (= eq.) has been derived:\n<img src=\"https://i.stack.imgur.com/qTygY.png\" alt=\"enter image description here\"></p>\n\n<p>As Jaime suggested, multiplying by <code>n</code> improves the code. This is not reflected in the spreadsheet above but has been implemented in the code below: </p>\n\n<p><strong>Implementation using numpy:</strong></p>\n\n<pre><code>import numpy as np\nimport numpy.linalg as linalg\n\n\ndef solve(n):\n    # upper left block\n    n_to_M = -2. * np.eye(n-1) \n\n    # lower left block\n    n_to_N = (n * np.eye(n-1)) - np.diag(np.arange(n-2, 0, -1), 1)\n\n    # upper right block\n    m_to_M = n_to_N.copy()\n    m_to_M[1:, 0] = -np.arange(1, n-1)\n\n    # lower right block\n    m_to_N = np.zeros((n-1, n-1))\n    m_to_N[:,0] = -np.arange(1,n)\n\n    # build A, combine all blocks\n    coeff_mat = np.hstack(\n                          (np.vstack((n_to_M, n_to_N)),\n                           np.vstack((m_to_M, m_to_N))))\n\n    # const vector, right side of eq.\n    const = n * np.ones((2 * (n-1),1))\n\n    return linalg.solve(coeff_mat, const)\n</code></pre>\n\n<p><strong>Solution using scipy.sparse:</strong></p>\n\n<pre><code>from scipy.sparse import spdiags, lil_matrix, vstack, hstack\nfrom scipy.sparse.linalg import spsolve\nimport numpy as np\n\n\ndef solve(n):\n    nrange = np.arange(n)\n    diag = np.ones(n-1)\n\n    # upper left block\n    n_to_M = spdiags(-2. * diag, 0, n-1, n-1)\n\n    # lower left block\n    n_to_N = spdiags([n * diag, -nrange[-1:0:-1]], [0, 1], n-1, n-1)\n\n    # upper right block\n    m_to_M = lil_matrix(n_to_N)\n    m_to_M[1:, 0] = -nrange[1:-1].reshape((n-2, 1))\n\n    # lower right block\n    m_to_N = lil_matrix((n-1, n-1))\n    m_to_N[:, 0] = -nrange[1:].reshape((n-1, 1))\n\n    # build A, combine all blocks\n    coeff_mat = hstack(\n                       (vstack((n_to_M, n_to_N)),\n                        vstack((m_to_M, m_to_N))))\n\n    # const vector, right side of eq.\n    const = n * np.ones((2 * (n-1),1))\n\n    return spsolve(coeff_mat.tocsr(), const).reshape((-1,1))\n</code></pre>\n\n<p>Example for <code>n=4</code>:</p>\n\n<pre><code>[[ 7.25      ]\n [ 7.76315789]\n [ 8.10526316]\n [ 9.47368421]   # &lt;&lt;&lt; your result\n [ 9.69736842]\n [ 9.78947368]]\n</code></pre>\n\n<p>Example for <code>n=10</code>:</p>\n\n<pre><code>[[ 24.778976  ]\n [ 25.85117842]\n [ 26.65015984]\n [ 27.26010007]\n [ 27.73593401]\n [ 28.11441922]\n [ 28.42073207]\n [ 28.67249606]\n [ 28.88229939]\n [ 30.98033266]  # &lt;&lt;&lt; your result\n [ 31.28067182]\n [ 31.44628982]\n [ 31.53365219]\n [ 31.57506477]\n [ 31.58936225]\n [ 31.58770694]\n [ 31.57680467]\n [ 31.560726  ]]\n</code></pre>\n\n\n<p>Here's an entirely different approach, using <code>sympy</code>.  It's not fast, but it allows me to copy the RHS of your equations exactly, limiting the thinking I need to do (always a plus), and gives fractional answers.</p>\n\n<pre><code>from sympy import Integer, Symbol, Eq, solve\n\ndef build_equations(n):\n    ni = n\n    n = Integer(n)\n    Ms = {p: Symbol(\"M{}\".format(p)) for p in range(ni)}\n    Ns = {p: Symbol(\"N{}\".format(p)) for p in range(ni-1)}\n    M = lambda i: Ms[int(i)] if i &gt;= 1 else 0\n    N = lambda i: Ns[int(i)]\n\n    M_eqs = {}\n    M_eqs[1] = Eq(M(1), 1+((n-2)/n)*M(n-1) + (2/n)*N(0))\n    for p in range(2, ni):\n        M_eqs[p] = Eq(M(p), 1+((n-p-1)/n)*M(n-1) + (2/n)*N(p-1) + ((p-1)/n)*M(p-1))\n\n    N_eqs = {}\n    N_eqs[0] = Eq(N(0), 1+((n-1)/n)*M(n-1))\n    for p in range(1, ni-1):\n        N_eqs[p] = Eq(N(p), 1+((n-p-1)/n)*M(n-1) + (p/n)*N(p-1))\n\n    return M_eqs.values() + N_eqs.values()\n\ndef solve_system(n, show=False):\n\n    eqs = build_equations(n)\n    sol = solve(eqs)\n\n    if show:\n        print 'equations:'\n        for eq in sorted(eqs):\n            print eq\n        print 'solution:'\n        for var, val in sorted(sol.items()):\n            print var, val, float(val)\n\n    return sol\n</code></pre>\n\n<p>which gives</p>\n\n<pre><code>&gt;&gt;&gt; solve_system(2, True)\nequations:\nM1 == N0 + 1\nN0 == M1/2 + 1\nsolution:\nM1 4 4.0\nN0 3 3.0\n{M1: 4, N0: 3}\n&gt;&gt;&gt; solve_system(3, True)\nequations:\nM1 == M2/3 + 2*N0/3 + 1\nM2 == M1/3 + 2*N1/3 + 1\nN0 == 2*M2/3 + 1\nN1 == M2/3 + N0/3 + 1\nsolution:\nM1 34/5 6.8\nM2 33/5 6.6\nN0 27/5 5.4\nN1 5 5.0\n{M2: 33/5, M1: 34/5, N1: 5, N0: 27/5}        \n</code></pre>\n\n<p>and</p>\n\n<pre><code>&gt;&gt;&gt; solve_system(4, True)\nequations:\nM1 == M3/2 + N0/2 + 1\nM2 == M1/4 + M3/4 + N1/2 + 1\nM3 == M2/2 + N2/2 + 1\nN0 == 3*M3/4 + 1\nN1 == M3/2 + N0/4 + 1\nN2 == M3/4 + N1/2 + 1\nsolution:\nM1 186/19 9.78947368421\nM2 737/76 9.69736842105\nM3 180/19 9.47368421053\nN0 154/19 8.10526315789\nN1 295/38 7.76315789474\nN2 29/4 7.25\n{N2: 29/4, N1: 295/38, M1: 186/19, M3: 180/19, N0: 154/19, M2: 737/76}\n</code></pre>\n\n<p>which seems to match the other answers.</p>\n\n\n<p>This is messy, but solves your problem, barring a very probable mistake transcribing the coefficients:</p>\n\n<pre><code>from __future__ import division\nimport numpy as np    \nn = 2\n# Solution vector is [N[0], N[1], ..., N[n - 2], M[1], M[2], ..., M[n - 1]]\nn_pos = lambda p : p\nm_pos = lambda p : p + n - 2\nA = np.zeros((2 * (n - 1), 2 * (n - 1)))\n# p = 0\n# N[0] + (1 - n) / n * M[n-1] = 1\nA[n_pos(0), n_pos(0)] = 1 # N[0]\nA[n_pos(0), m_pos(n - 1)] = (1 - n) / n #M[n - 1]\nfor p in xrange(1, n - 1) :\n    # M[p] + (1 + p - n) /n * M[n - 1] - 2 / n * N[p - 1] +\n    #  (1 - p) / n * M[p - 1] = 1\n    A[m_pos(p), m_pos(p)] = 1 # M[p]\n    A[m_pos(p), m_pos(n - 1)] =  (1 + p - n) / n # M[n - 1]\n    A[m_pos(p), n_pos(p - 1)] = -2 / n # N[p - 1]\n    if p &gt; 1 :\n        A[m_pos(p), m_pos(p - 1)] = (1 - p) / n # M[p - 1]\n    # N[p] + (1 + p -n) / n * M[n - 1] - p / n * N[p - 1] = 1\n    A[n_pos(p), n_pos(p)] = 1 # N[p]\n    A[n_pos(p), m_pos(n - 1)] = (1 + p - n) / n # M[n - 1]\n    A[n_pos(p), n_pos(p - 1)] = -p / n # N[p - 1]\nif n &gt; 2 :\n    # p = n - 1\n    # M[n - 1] - 2 / n * N[n - 2] + (2 - n) / n * M[n - 2] = 1\n    A[m_pos(n - 1), m_pos(n - 1)] = 1 # M[n - 1]\n    A[m_pos(n - 1), n_pos(n - 2)] = -2 / n # N[n - 2]\n    A[m_pos(n - 1), m_pos(n - 2)] = (2 - n) / n # M[n - 2]\nelse :\n    # p = 1 \n    #M[1] - 2 / n * N[0] = 1\n    A[m_pos(n - 1), m_pos(n - 1)] = 1\n    A[m_pos(n - 1), n_pos(n - 2)] = -2 / n\n\nX = np.linalg.solve(A, np.ones((2 * (n - 1),)))\n</code></pre>\n\n<p>But it gives a solution of</p>\n\n<pre><code>&gt;&gt;&gt; X[-1]\n6.5999999999999979\n</code></pre>\n\n<p>for <code>M(2)</code> when <code>n=3</code>, which is not what you came up with.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Updated: added implementation using scipy.sparse</p>\n\n<hr>\n\n<p>This gives the solution in the order <code>N_max,...,N_0,M_max,...,M_1</code>.</p>\n\n<p>The linear system to solve is of the shape <code>A dot x == const 1-vector</code>.\n<code>x</code> is the sought after solution vector.<br>\nHere I ordered the equations such that <code>x</code> is <code>N_max,...,N_0,M_max,...,M_1</code>.<br>\nThen I build up the <code>A</code>-coefficient matrix from 4 block matrices.</p>\n\n<p>Here's a snapshot for the example case <code>n=50</code> showing how you can derive the coefficient matrix and understand the block structure. The coefficient matrix <code>A</code> is light blue, the constant right side is orange. The sought after solution vector <code>x</code> is here light green and used to label the columns. The first column show from which of the above given eqs. the row (= eq.) has been derived:\n<img src=\"https://i.stack.imgur.com/qTygY.png\" alt=\"enter image description here\"></p>\n\n<p>As Jaime suggested, multiplying by <code>n</code> improves the code. This is not reflected in the spreadsheet above but has been implemented in the code below: </p>\n\n<p><strong>Implementation using numpy:</strong></p>\n\n<pre><code>import numpy as np\nimport numpy.linalg as linalg\n\n\ndef solve(n):\n    # upper left block\n    n_to_M = -2. * np.eye(n-1) \n\n    # lower left block\n    n_to_N = (n * np.eye(n-1)) - np.diag(np.arange(n-2, 0, -1), 1)\n\n    # upper right block\n    m_to_M = n_to_N.copy()\n    m_to_M[1:, 0] = -np.arange(1, n-1)\n\n    # lower right block\n    m_to_N = np.zeros((n-1, n-1))\n    m_to_N[:,0] = -np.arange(1,n)\n\n    # build A, combine all blocks\n    coeff_mat = np.hstack(\n                          (np.vstack((n_to_M, n_to_N)),\n                           np.vstack((m_to_M, m_to_N))))\n\n    # const vector, right side of eq.\n    const = n * np.ones((2 * (n-1),1))\n\n    return linalg.solve(coeff_mat, const)\n</code></pre>\n\n<p><strong>Solution using scipy.sparse:</strong></p>\n\n<pre><code>from scipy.sparse import spdiags, lil_matrix, vstack, hstack\nfrom scipy.sparse.linalg import spsolve\nimport numpy as np\n\n\ndef solve(n):\n    nrange = np.arange(n)\n    diag = np.ones(n-1)\n\n    # upper left block\n    n_to_M = spdiags(-2. * diag, 0, n-1, n-1)\n\n    # lower left block\n    n_to_N = spdiags([n * diag, -nrange[-1:0:-1]], [0, 1], n-1, n-1)\n\n    # upper right block\n    m_to_M = lil_matrix(n_to_N)\n    m_to_M[1:, 0] = -nrange[1:-1].reshape((n-2, 1))\n\n    # lower right block\n    m_to_N = lil_matrix((n-1, n-1))\n    m_to_N[:, 0] = -nrange[1:].reshape((n-1, 1))\n\n    # build A, combine all blocks\n    coeff_mat = hstack(\n                       (vstack((n_to_M, n_to_N)),\n                        vstack((m_to_M, m_to_N))))\n\n    # const vector, right side of eq.\n    const = n * np.ones((2 * (n-1),1))\n\n    return spsolve(coeff_mat.tocsr(), const).reshape((-1,1))\n</code></pre>\n\n<p>Example for <code>n=4</code>:</p>\n\n<pre><code>[[ 7.25      ]\n [ 7.76315789]\n [ 8.10526316]\n [ 9.47368421]   # &lt;&lt;&lt; your result\n [ 9.69736842]\n [ 9.78947368]]\n</code></pre>\n\n<p>Example for <code>n=10</code>:</p>\n\n<pre><code>[[ 24.778976  ]\n [ 25.85117842]\n [ 26.65015984]\n [ 27.26010007]\n [ 27.73593401]\n [ 28.11441922]\n [ 28.42073207]\n [ 28.67249606]\n [ 28.88229939]\n [ 30.98033266]  # &lt;&lt;&lt; your result\n [ 31.28067182]\n [ 31.44628982]\n [ 31.53365219]\n [ 31.57506477]\n [ 31.58936225]\n [ 31.58770694]\n [ 31.57680467]\n [ 31.560726  ]]\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "7"}, "answer_2": {"type": "literal", "value": "<p>Here's an entirely different approach, using <code>sympy</code>.  It's not fast, but it allows me to copy the RHS of your equations exactly, limiting the thinking I need to do (always a plus), and gives fractional answers.</p>\n\n<pre><code>from sympy import Integer, Symbol, Eq, solve\n\ndef build_equations(n):\n    ni = n\n    n = Integer(n)\n    Ms = {p: Symbol(\"M{}\".format(p)) for p in range(ni)}\n    Ns = {p: Symbol(\"N{}\".format(p)) for p in range(ni-1)}\n    M = lambda i: Ms[int(i)] if i &gt;= 1 else 0\n    N = lambda i: Ns[int(i)]\n\n    M_eqs = {}\n    M_eqs[1] = Eq(M(1), 1+((n-2)/n)*M(n-1) + (2/n)*N(0))\n    for p in range(2, ni):\n        M_eqs[p] = Eq(M(p), 1+((n-p-1)/n)*M(n-1) + (2/n)*N(p-1) + ((p-1)/n)*M(p-1))\n\n    N_eqs = {}\n    N_eqs[0] = Eq(N(0), 1+((n-1)/n)*M(n-1))\n    for p in range(1, ni-1):\n        N_eqs[p] = Eq(N(p), 1+((n-p-1)/n)*M(n-1) + (p/n)*N(p-1))\n\n    return M_eqs.values() + N_eqs.values()\n\ndef solve_system(n, show=False):\n\n    eqs = build_equations(n)\n    sol = solve(eqs)\n\n    if show:\n        print 'equations:'\n        for eq in sorted(eqs):\n            print eq\n        print 'solution:'\n        for var, val in sorted(sol.items()):\n            print var, val, float(val)\n\n    return sol\n</code></pre>\n\n<p>which gives</p>\n\n<pre><code>&gt;&gt;&gt; solve_system(2, True)\nequations:\nM1 == N0 + 1\nN0 == M1/2 + 1\nsolution:\nM1 4 4.0\nN0 3 3.0\n{M1: 4, N0: 3}\n&gt;&gt;&gt; solve_system(3, True)\nequations:\nM1 == M2/3 + 2*N0/3 + 1\nM2 == M1/3 + 2*N1/3 + 1\nN0 == 2*M2/3 + 1\nN1 == M2/3 + N0/3 + 1\nsolution:\nM1 34/5 6.8\nM2 33/5 6.6\nN0 27/5 5.4\nN1 5 5.0\n{M2: 33/5, M1: 34/5, N1: 5, N0: 27/5}        \n</code></pre>\n\n<p>and</p>\n\n<pre><code>&gt;&gt;&gt; solve_system(4, True)\nequations:\nM1 == M3/2 + N0/2 + 1\nM2 == M1/4 + M3/4 + N1/2 + 1\nM3 == M2/2 + N2/2 + 1\nN0 == 3*M3/4 + 1\nN1 == M3/2 + N0/4 + 1\nN2 == M3/4 + N1/2 + 1\nsolution:\nM1 186/19 9.78947368421\nM2 737/76 9.69736842105\nM3 180/19 9.47368421053\nN0 154/19 8.10526315789\nN1 295/38 7.76315789474\nN2 29/4 7.25\n{N2: 29/4, N1: 295/38, M1: 186/19, M3: 180/19, N0: 154/19, M2: 737/76}\n</code></pre>\n\n<p>which seems to match the other answers.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "answer_3": {"type": "literal", "value": "<p>This is messy, but solves your problem, barring a very probable mistake transcribing the coefficients:</p>\n\n<pre><code>from __future__ import division\nimport numpy as np    \nn = 2\n# Solution vector is [N[0], N[1], ..., N[n - 2], M[1], M[2], ..., M[n - 1]]\nn_pos = lambda p : p\nm_pos = lambda p : p + n - 2\nA = np.zeros((2 * (n - 1), 2 * (n - 1)))\n# p = 0\n# N[0] + (1 - n) / n * M[n-1] = 1\nA[n_pos(0), n_pos(0)] = 1 # N[0]\nA[n_pos(0), m_pos(n - 1)] = (1 - n) / n #M[n - 1]\nfor p in xrange(1, n - 1) :\n    # M[p] + (1 + p - n) /n * M[n - 1] - 2 / n * N[p - 1] +\n    #  (1 - p) / n * M[p - 1] = 1\n    A[m_pos(p), m_pos(p)] = 1 # M[p]\n    A[m_pos(p), m_pos(n - 1)] =  (1 + p - n) / n # M[n - 1]\n    A[m_pos(p), n_pos(p - 1)] = -2 / n # N[p - 1]\n    if p &gt; 1 :\n        A[m_pos(p), m_pos(p - 1)] = (1 - p) / n # M[p - 1]\n    # N[p] + (1 + p -n) / n * M[n - 1] - p / n * N[p - 1] = 1\n    A[n_pos(p), n_pos(p)] = 1 # N[p]\n    A[n_pos(p), m_pos(n - 1)] = (1 + p - n) / n # M[n - 1]\n    A[n_pos(p), n_pos(p - 1)] = -p / n # N[p - 1]\nif n &gt; 2 :\n    # p = n - 1\n    # M[n - 1] - 2 / n * N[n - 2] + (2 - n) / n * M[n - 2] = 1\n    A[m_pos(n - 1), m_pos(n - 1)] = 1 # M[n - 1]\n    A[m_pos(n - 1), n_pos(n - 2)] = -2 / n # N[n - 2]\n    A[m_pos(n - 1), m_pos(n - 2)] = (2 - n) / n # M[n - 2]\nelse :\n    # p = 1 \n    #M[1] - 2 / n * N[0] = 1\n    A[m_pos(n - 1), m_pos(n - 1)] = 1\n    A[m_pos(n - 1), n_pos(n - 2)] = -2 / n\n\nX = np.linalg.solve(A, np.ones((2 * (n - 1),)))\n</code></pre>\n\n<p>But it gives a solution of</p>\n\n<pre><code>&gt;&gt;&gt; X[-1]\n6.5999999999999979\n</code></pre>\n\n<p>for <code>M(2)</code> when <code>n=3</code>, which is not what you came up with.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>For a fixed integer  , I have a set of   simultaneous equations as follows.  </p>\n\n<pre> </pre>\n\n<p>  is defined for  .    is defined for  .  Notice also that   is just a constant integer in every equation so the whole system is linear.</p>\n\n<p>I have been using Maple but I would like to set these up and solve them in python now, maybe using   (or any other better method).  I actually only want the value of  . For example, when   the answer should be  , I believe.  This is because the equations become </p>\n\n<pre> </pre>\n\n<p>Therefore</p>\n\n<pre> </pre>\n\n<p>and so</p>\n\n<pre> </pre>\n\n<p>If I want to plug in  , say, how can you set up this system of simultaneous equations in python so that numpy.linalg.solve can solve them?</p>\n\n<p><strong>Update</strong>  The answers are great but they use dense solvers where the system of equations is sparse.  Posted follow up to <a href=\"https://stackoverflow.com/questions/14380899/using-scipy-sparse-matrices-to-solve-system-of-equations\">Using scipy sparse matrices to solve system of equations</a> .</p>\n", "answer_wo_code": "<p>Updated: added implementation using scipy.sparse</p>\n\n<hr>\n\n<p>This gives the solution in the order  .</p>\n\n<p>The linear system to solve is of the shape  .\n  is the sought after solution vector.<br>\nHere I ordered the equations such that   is  .<br>\nThen I build up the  -coefficient matrix from 4 block matrices.</p>\n\n<p>Here's a snapshot for the example case   showing how you can derive the coefficient matrix and understand the block structure. The coefficient matrix   is light blue, the constant right side is orange. The sought after solution vector   is here light green and used to label the columns. The first column show from which of the above given eqs. the row (= eq.) has been derived:\n<img src=\"https://i.stack.imgur.com/qTygY.png\" alt=\"enter image description here\"></p>\n\n<p>As Jaime suggested, multiplying by   improves the code. This is not reflected in the spreadsheet above but has been implemented in the code below: </p>\n\n<p><strong>Implementation using numpy:</strong></p>\n\n<pre> </pre>\n\n<p><strong>Solution using scipy.sparse:</strong></p>\n\n<pre> </pre>\n\n<p>Example for  :</p>\n\n<pre> </pre>\n\n<p>Example for  :</p>\n\n<pre> </pre>\n\n\n<p>Here's an entirely different approach, using  .  It's not fast, but it allows me to copy the RHS of your equations exactly, limiting the thinking I need to do (always a plus), and gives fractional answers.</p>\n\n<pre> </pre>\n\n<p>which gives</p>\n\n<pre> </pre>\n\n<p>and</p>\n\n<pre> </pre>\n\n<p>which seems to match the other answers.</p>\n\n\n<p>This is messy, but solves your problem, barring a very probable mistake transcribing the coefficients:</p>\n\n<pre> </pre>\n\n<p>But it gives a solution of</p>\n\n<pre> </pre>\n\n<p>for   when  , which is not what you came up with.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.exp"}, "class_func_label": {"type": "literal", "value": "sympy.exp"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    The exponential function, :math:`e^x`.\n\n    See Also\n    ========\n\n    log\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/32171509"}, "title": {"type": "literal", "value": "sympy: 'Transpose' object has no attribute tolist"}, "content": {"type": "literal", "value": "<p>I'm trying to do some symbolic matrix calculations with sympy. My goal is to obtain a symbolic representation of the result of some matrix computations. I've run into some problems which I have boiled down to this simple example, in which I try to evaluate the result of a exponentiating a specified matrix and multiplying it by an arbitrary vector.</p>\n\n<pre><code>&gt;&gt;&gt; import sympy\n&gt;&gt;&gt; v = sympy.MatrixSymbol('v', 2, 1)\n&gt;&gt;&gt; Z = sympy.zeros(2, 2)  # create 2x2 zero matrix\n&gt;&gt;&gt; I = sympy.exp(Z)  # exponentiate zero matrix to get identity matrix\n&gt;&gt;&gt; I * v\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"sympy/matrices/matrices.py\", line 507, in __mul__\n    blst = B.T.tolist()\nAttributeError: 'Transpose' object has no attribute 'tolist'\n</code></pre>\n\n<p>In contrast, if I directly create the identity matrix and then multiply it by v, then there is no problem:</p>\n\n<pre><code>&gt;&gt;&gt; I_ = sympy.eye(2)  # directly create the identity matrix\n&gt;&gt;&gt; I_ == I  # check the two matrices are equal\nTrue\n&gt;&gt;&gt; I_ * v\nv\n</code></pre>\n\n<p>One thing that I've noted is that the two identity matrices are of different classes:</p>\n\n<pre><code>&gt;&gt;&gt; I.__class__\nsympy.matrices.immutable.ImmutableMatrix\n&gt;&gt;&gt; I_.__class__\nsympy.matrices.dense.MutableDenseMatrix\n</code></pre>\n\n<p>I also found that calling the <code>as_mutable()</code> method provided a work-around.</p>\n\n<pre><code>&gt;&gt;&gt; I.as_mutable() * v\nv\n</code></pre>\n\n<p>Is it always necessary to put <code>as_mutable()</code> calls throughout one's linear algebra calculations? I'm guessing not, and that instead these errors suggest that I'm using the wrong strategy to solve my problem, but I can't figure out what the right strategy would be. Does anyone have any pointers?</p>\n\n<p>I have read the documentation page on <a href=\"http://docs.sympy.org/dev/modules/matrices/immutablematrices.html\">Immutable Matrices</a> but I could still use some help understanding how their differences with standard mutable matrices are important here, and why some operations (e.g. sympy.exp) convert between these different classes.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I'd claim that this is a bug in Sympy:</p>\n\n<p>In Python, you can <a href=\"https://stackoverflow.com/a/5182501/1881610\">overload the multiplication operator from both sides</a>. <code>A*B</code> may internally be handled by either calling <code>A.__mul__(B)</code>, or <code>B.__rmul__(A)</code>. Python first calls <code>A.__mul__</code>, and if this method does not exist or returns <code>NotImplemented</code>,  then Python tries <code>B.__rmul__</code> automatically. SymPy instead uses a decorator called <a href=\"https://github.com/sympy/sympy/blob/master/sympy/core/decorators.py\" rel=\"nofollow noreferrer\">call_highest_priority</a> to decide which of both implementations to use. It looks up the <code>_op_priority</code> of the involved classes and calls the function of the implementation with higher priority.  The priorities in your case are 11 for <code>v</code> and <code>I</code> and 10.01 for <code>I_</code>, so <code>I</code> is preferred. Also, the base implementation of <code>__mul__</code>, which <code>I</code> uses, lacks the decorator.</p>\n\n<p>Long story short, <code>I*v</code> ends up always calling <code>I.__mul__</code>, and <code>__mul__</code> cannot handle <code>MatrixSymbol</code>s but does not return <code>NotImplemented</code> either. <code>v.__rmul__(I)</code> works as expected.</p>\n\n<p>The proper fix would be to capture the <code>AttributeError</code> in <code>matrices.py</code> and return <code>NotImplemented</code>, i.e.</p>\n\n<pre><code>try:\n    blst = B.T.tolist()\nexcept AttributeError:\n    return NotImplemented\n</code></pre>\n\n<p>Python would then automatically fallback to <code>__rmul__</code>. The hack'ish fix would be to adjust <code>_op_priority</code>. Either way, you should file a bug report: If the error was by design (that is, if you accidentally tried something that's not supposed to work), then the error message would say so.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I'd claim that this is a bug in Sympy:</p>\n\n<p>In Python, you can <a href=\"https://stackoverflow.com/a/5182501/1881610\">overload the multiplication operator from both sides</a>. <code>A*B</code> may internally be handled by either calling <code>A.__mul__(B)</code>, or <code>B.__rmul__(A)</code>. Python first calls <code>A.__mul__</code>, and if this method does not exist or returns <code>NotImplemented</code>,  then Python tries <code>B.__rmul__</code> automatically. SymPy instead uses a decorator called <a href=\"https://github.com/sympy/sympy/blob/master/sympy/core/decorators.py\" rel=\"nofollow noreferrer\">call_highest_priority</a> to decide which of both implementations to use. It looks up the <code>_op_priority</code> of the involved classes and calls the function of the implementation with higher priority.  The priorities in your case are 11 for <code>v</code> and <code>I</code> and 10.01 for <code>I_</code>, so <code>I</code> is preferred. Also, the base implementation of <code>__mul__</code>, which <code>I</code> uses, lacks the decorator.</p>\n\n<p>Long story short, <code>I*v</code> ends up always calling <code>I.__mul__</code>, and <code>__mul__</code> cannot handle <code>MatrixSymbol</code>s but does not return <code>NotImplemented</code> either. <code>v.__rmul__(I)</code> works as expected.</p>\n\n<p>The proper fix would be to capture the <code>AttributeError</code> in <code>matrices.py</code> and return <code>NotImplemented</code>, i.e.</p>\n\n<pre><code>try:\n    blst = B.T.tolist()\nexcept AttributeError:\n    return NotImplemented\n</code></pre>\n\n<p>Python would then automatically fallback to <code>__rmul__</code>. The hack'ish fix would be to adjust <code>_op_priority</code>. Either way, you should file a bug report: If the error was by design (that is, if you accidentally tried something that's not supposed to work), then the error message would say so.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "5"}, "content_wo_code": "<p>I'm trying to do some symbolic matrix calculations with sympy. My goal is to obtain a symbolic representation of the result of some matrix computations. I've run into some problems which I have boiled down to this simple example, in which I try to evaluate the result of a exponentiating a specified matrix and multiplying it by an arbitrary vector.</p>\n\n<pre> </pre>\n\n<p>In contrast, if I directly create the identity matrix and then multiply it by v, then there is no problem:</p>\n\n<pre> </pre>\n\n<p>One thing that I've noted is that the two identity matrices are of different classes:</p>\n\n<pre> </pre>\n\n<p>I also found that calling the   method provided a work-around.</p>\n\n<pre> </pre>\n\n<p>Is it always necessary to put   calls throughout one's linear algebra calculations? I'm guessing not, and that instead these errors suggest that I'm using the wrong strategy to solve my problem, but I can't figure out what the right strategy would be. Does anyone have any pointers?</p>\n\n<p>I have read the documentation page on <a href=\"http://docs.sympy.org/dev/modules/matrices/immutablematrices.html\">Immutable Matrices</a> but I could still use some help understanding how their differences with standard mutable matrices are important here, and why some operations (e.g. sympy.exp) convert between these different classes.</p>\n", "answer_wo_code": "<p>I'd claim that this is a bug in Sympy:</p>\n\n<p>In Python, you can <a href=\"https://stackoverflow.com/a/5182501/1881610\">overload the multiplication operator from both sides</a>.   may internally be handled by either calling  , or  . Python first calls  , and if this method does not exist or returns  ,  then Python tries   automatically. SymPy instead uses a decorator called <a href=\"https://github.com/sympy/sympy/blob/master/sympy/core/decorators.py\" rel=\"nofollow noreferrer\">call_highest_priority</a> to decide which of both implementations to use. It looks up the   of the involved classes and calls the function of the implementation with higher priority.  The priorities in your case are 11 for   and   and 10.01 for  , so   is preferred. Also, the base implementation of  , which   uses, lacks the decorator.</p>\n\n<p>Long story short,   ends up always calling  , and   cannot handle  s but does not return   either.   works as expected.</p>\n\n<p>The proper fix would be to capture the   in   and return  , i.e.</p>\n\n<pre> </pre>\n\n<p>Python would then automatically fallback to  . The hack'ish fix would be to adjust  . Either way, you should file a bug report: If the error was by design (that is, if you accidentally tried something that's not supposed to work), then the error message would say so.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.poly"}, "class_func_label": {"type": "literal", "value": "sympy.poly"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nEfficiently transform an expression into a polynomial.\n\n.. rubric:: Examples\n\n>>> from sympy import poly\n>>> from sympy.abc import x\n\n>>> poly(x*(x**2 + x - 1)**2)\nPoly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/31070921"}, "title": {"type": "literal", "value": "Incorrect results with Sympy when utilizing (numpy's) floats"}, "content": {"type": "literal", "value": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix <code>RE(t)</code> (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix <code>SE(t) = dRE(t)/dt * RE.T</code>. I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre><code>from IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\nt, lat = sy.symbols(\"t, lat\", real=True)  # time and latitude\nomE = 7.292115e-5  # rad/s -- earth rotation rate (15.04107 \u00b0/h)\nlat_sy = 48.232*sy.pi/180  # latitude in rad\nlat_fl = float(lat_sy)  # latitude as float\nprint(\"\\nlat_sy - lat_fl = {}\".format((lat_sy - lat_fl).evalf()))\n\n# earth rotation matrix at latitiude 48.232\u00b0:\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n# substitute latitude with sympy and float value:\nRE_sy, RE_fl = RE.subs(lat, lat_sy), RE.subs(lat, lat_fl)\n\n# Angular velocity in world coordinates as skew symmetric matrix:\nSE_sy = sy.simplify(RE_sy.diff(t) * RE_sy.T)\nSE_fl = sy.simplify(RE_fl.diff(t) * RE_fl.T)\n\nprint(\"\\nAngular velocity with Sympy latitude ({}):\".format(lat_sy))\ndisplay(SE_sy)  # correct result\nprint(\"\\nAngular velocity with float latitude ({}):\".format(lat_fl))\ndisplay(SE_fl)  # incorrect result\n</code></pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "4"}, "content_wo_code": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix   (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix  . I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre> </pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n", "answer_wo_code": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre> </pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre> </pre>\n\n<p>I get</p>\n\n<pre> </pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list ( ) doesn't match the coefficient in the summand list ( ), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre> </pre>\n\n<p>This gives:</p>\n\n<pre> </pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter   in   was changed from   to   in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater,   was reverted back to   to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify   at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"> </a> in  , to  , you will get the correct result for  .</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>The change of   can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre> </pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre> </pre>\n\n<p>When we call  , in this case, it's almost equivalent to a combination of   and  .</p>\n\n<pre> </pre>\n\n<p>Now, we know wrong results would be produced during the invocation of  .<br>\nActually,   is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>.</p>\n\n<pre> </pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>. The key part is:</p>\n\n<pre> </pre>\n\n<p>We use the above   to simulate this procedure.</p>\n\n<pre> </pre>\n\n<p>By design, the constant polynomial should execute   suite, while the other polynomials should execute   suite.<br>\n , which is a number after  , and   are both constant polynomials, thus  s were thrown.<br>\n  is  , namely,  , a constant polynomial, whereas   is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing  ?<br>\nLet us recall:   was reverted back to  , which means automatic reduction to zero in   is enabled again.<br>\n  was reduced to zero. Thus,   became a constant polynomial.<br>\nIf   is  , this won't happen.</p>\n\n<pre> </pre>\n\n<p>Now, we can explain why you've got  . In the   suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"> </a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p>  is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate  . The numerator is kept, while the denominator is discarded.</p>\n\n<pre> </pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre> </pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.polys.polytools.Poly"}, "class_func_label": {"type": "literal", "value": "sympy.polys.polytools.Poly"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Generic class for representing and operating on polynomial expressions.\n    Subclasses Expr class.\n\n    Examples\n    ========\n\n    >>> from sympy import Poly\n    >>> from sympy.abc import x, y\n\n    Create a univariate polynomial:\n\n    >>> Poly(x*(x**2 + x - 1)**2)\n    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n\n    Create a univariate polynomial with specific domain:\n\n    >>> from sympy import sqrt\n    >>> Poly(x**2 + 2*x + sqrt(3), domain='R')\n    Poly(1.0*x**2 + 2.0*x + 1.73205080756888, x, domain='RR')\n\n    Create a multivariate polynomial:\n\n    >>> Poly(y*x**2 + x*y + 1)\n    Poly(x**2*y + x*y + 1, x, y, domain='ZZ')\n\n    Create a univariate polynomial, where y is a constant:\n\n    >>> Poly(y*x**2 + x*y + 1,x)\n    Poly(y*x**2 + y*x + 1, x, domain='ZZ[y]')\n\n    You can evaluate the above polynomial as a function of y:\n\n    >>> Poly(y*x**2 + x*y + 1,x).eval(2)\n    6*y + 1\n\n    See Also\n    ========\n\n    sympy.core.expr.Expr\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/31070921"}, "title": {"type": "literal", "value": "Incorrect results with Sympy when utilizing (numpy's) floats"}, "content": {"type": "literal", "value": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix <code>RE(t)</code> (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix <code>SE(t) = dRE(t)/dt * RE.T</code>. I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre><code>from IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\nt, lat = sy.symbols(\"t, lat\", real=True)  # time and latitude\nomE = 7.292115e-5  # rad/s -- earth rotation rate (15.04107 \u00b0/h)\nlat_sy = 48.232*sy.pi/180  # latitude in rad\nlat_fl = float(lat_sy)  # latitude as float\nprint(\"\\nlat_sy - lat_fl = {}\".format((lat_sy - lat_fl).evalf()))\n\n# earth rotation matrix at latitiude 48.232\u00b0:\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n# substitute latitude with sympy and float value:\nRE_sy, RE_fl = RE.subs(lat, lat_sy), RE.subs(lat, lat_fl)\n\n# Angular velocity in world coordinates as skew symmetric matrix:\nSE_sy = sy.simplify(RE_sy.diff(t) * RE_sy.T)\nSE_fl = sy.simplify(RE_fl.diff(t) * RE_fl.T)\n\nprint(\"\\nAngular velocity with Sympy latitude ({}):\".format(lat_sy))\ndisplay(SE_sy)  # correct result\nprint(\"\\nAngular velocity with float latitude ({}):\".format(lat_fl))\ndisplay(SE_fl)  # incorrect result\n</code></pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "4"}, "content_wo_code": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix   (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix  . I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre> </pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n", "answer_wo_code": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre> </pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre> </pre>\n\n<p>I get</p>\n\n<pre> </pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list ( ) doesn't match the coefficient in the summand list ( ), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre> </pre>\n\n<p>This gives:</p>\n\n<pre> </pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter   in   was changed from   to   in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater,   was reverted back to   to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify   at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"> </a> in  , to  , you will get the correct result for  .</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>The change of   can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre> </pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre> </pre>\n\n<p>When we call  , in this case, it's almost equivalent to a combination of   and  .</p>\n\n<pre> </pre>\n\n<p>Now, we know wrong results would be produced during the invocation of  .<br>\nActually,   is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>.</p>\n\n<pre> </pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>. The key part is:</p>\n\n<pre> </pre>\n\n<p>We use the above   to simulate this procedure.</p>\n\n<pre> </pre>\n\n<p>By design, the constant polynomial should execute   suite, while the other polynomials should execute   suite.<br>\n , which is a number after  , and   are both constant polynomials, thus  s were thrown.<br>\n  is  , namely,  , a constant polynomial, whereas   is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing  ?<br>\nLet us recall:   was reverted back to  , which means automatic reduction to zero in   is enabled again.<br>\n  was reduced to zero. Thus,   became a constant polynomial.<br>\nIf   is  , this won't happen.</p>\n\n<pre> </pre>\n\n<p>Now, we can explain why you've got  . In the   suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"> </a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p>  is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate  . The numerator is kept, while the denominator is discarded.</p>\n\n<pre> </pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre> </pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.polys.poly"}, "class_func_label": {"type": "literal", "value": "sympy.polys.poly"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nEfficiently transform an expression into a polynomial.\n\n.. rubric:: Examples\n\n>>> from sympy import poly\n>>> from sympy.abc import x\n\n>>> poly(x*(x**2 + x - 1)**2)\nPoly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/31070921"}, "title": {"type": "literal", "value": "Incorrect results with Sympy when utilizing (numpy's) floats"}, "content": {"type": "literal", "value": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix <code>RE(t)</code> (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix <code>SE(t) = dRE(t)/dt * RE.T</code>. I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre><code>from IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\nt, lat = sy.symbols(\"t, lat\", real=True)  # time and latitude\nomE = 7.292115e-5  # rad/s -- earth rotation rate (15.04107 \u00b0/h)\nlat_sy = 48.232*sy.pi/180  # latitude in rad\nlat_fl = float(lat_sy)  # latitude as float\nprint(\"\\nlat_sy - lat_fl = {}\".format((lat_sy - lat_fl).evalf()))\n\n# earth rotation matrix at latitiude 48.232\u00b0:\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n# substitute latitude with sympy and float value:\nRE_sy, RE_fl = RE.subs(lat, lat_sy), RE.subs(lat, lat_fl)\n\n# Angular velocity in world coordinates as skew symmetric matrix:\nSE_sy = sy.simplify(RE_sy.diff(t) * RE_sy.T)\nSE_fl = sy.simplify(RE_fl.diff(t) * RE_fl.T)\n\nprint(\"\\nAngular velocity with Sympy latitude ({}):\".format(lat_sy))\ndisplay(SE_sy)  # correct result\nprint(\"\\nAngular velocity with float latitude ({}):\".format(lat_fl))\ndisplay(SE_fl)  # incorrect result\n</code></pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre><code>lat_sy - lat_fl = -2.61291277482447e-17\n\nAngular velocity with Sympy latitude (0.267955555555556*pi):\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n\nAngular velocity with float latitude (0.841807204822):\nMatrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre><code>def matrix_product_element(a, b, i, j):\n    v = a[3*i:3*i+3]\n    w = b[j::3]\n    summand_list = [v[k]*w[k]\n                    for k in range(3)]\n\n    print('element ({},{})'.format(i, j))\n    print('  summand_list: {}'.format(summand_list))\n    print('  sum(summand_list): {}'.format(sum(summand_list)))\n    print('  sum(summand_list).simplify(): {}'.format(sum(summand_list)))\n\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 0, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 1, 0)\nmatrix_product_element(RE_fl.diff(t), RE_fl.T, 2, 0)\n\nsumlist=[sy.Float(-4.05652668591092e-5,15), sy.Float(7.292115e-5,15), sy.Float(-3.23558831408908e-5,14)]\ndisplay(sumlist)\ndisplay(sum(sumlist))\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>element (0,0)\n  summand_list: [-4.05652668591092e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), 7.292115e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t), -3.23558831408908e-5*sin(7.292115e-5*t)*cos(7.292115e-5*t)]\n  sum(summand_list): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\n  sum(summand_list).simplify(): 6.7762635780344e-21*sin(7.292115e-5*t)*cos(7.292115e-5*t)\nelement (1,0)\n  summand_list: [4.05652668591092e-5*cos(7.292115e-5*t)**2, 7.292115e-5*sin(7.292115e-5*t)**2, 3.23558831408908e-5*cos(7.292115e-5*t)**2]\n  sum(summand_list): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\n  sum(summand_list).simplify(): 7.292115e-5*sin(7.292115e-5*t)**2 + 7.292115e-5*cos(7.292115e-5*t)**2\nelement (2,0)\n  summand_list: [0, 0, 0]\n  sum(summand_list): 0\n  sum(summand_list).simplify(): 0\n[-4.05652668591092e-5, 7.29211500000000e-5, -3.2355883140891e-5]\n6.77626357803440e-21\n</code></pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list (<code>-3.2355883140891e-5</code>) doesn't match the coefficient in the summand list (<code>-3.23558831408908e-5</code>), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre><code># encoding:utf-8\nfrom IPython.display import display\nimport sympy as sy\n\nsy.init_printing()  # LaTeX like pretty printing for IPython\n\n\ndef mk_rotmatrix(alpha, coord_ax=\"x\"):\n    \"\"\" Rotation matrix around coordinate axis \"\"\"\n    ca, sa = sy.cos(alpha), sy.sin(alpha)\n    if coord_ax == \"x\":\n        return sy.Matrix([[1,  0,   0],\n                          [0, ca, -sa],\n                          [0, sa, +ca]])\n    elif coord_ax == 'y':\n        return sy.Matrix([[+ca, 0, sa],\n                          [0,   1,  0],\n                          [-sa, 0, ca]])\n    elif coord_ax == 'z':\n        return sy.Matrix([[ca, -sa, 0],\n                          [sa, +ca, 0],\n                          [0,    0, 1]])\n    else:\n        raise ValueError(\"Parameter coord_ax='\" + coord_ax +\n                         \"' is not in ['x', 'y', 'z']!\")\n\n\n# time [s], latitude [rad], earth rate [rad/s]\nt, lat, omE = sy.symbols(\"t, lat, omE\", real=True)\n\nRE = (mk_rotmatrix(omE*t, \"z\") * mk_rotmatrix(lat - sy.pi/2, \"y\"))\n\nSE = sy.simplify(RE.diff(t) * RE.T)\n\ndisplay(SE)\ndisplay(SE.subs({lat: 48.232*sy.pi/180, omE: 7.292115e-5}))\n</code></pre>\n\n<p>This gives:</p>\n\n<pre><code>Matrix([\n[  0, -omE, 0],\n[omE,    0, 0],\n[  0,    0, 0]])\nMatrix([\n[          0, -7.292115e-5, 0],\n[7.292115e-5,            0, 0],\n[          0,            0, 0]])\n</code></pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre><code>In [1]: from sympy import factor, Symbol\n\nIn [2]: factor(1e-20*Symbol('t')-7.292115e-5)\nOut[2]: -2785579325.00000\n</code></pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter <code>tol</code> in <code>RealField.__init__</code> was changed from <code>None</code> to <code>False</code> in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater, <code>tol</code> was reverted back to <code>None</code> to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify <code>tol=None</code> at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"><code>RealField.__init__</code></a> in <code>realfield.py</code>, to <code>tol=False</code>, you will get the correct result for <code>SE_fl</code>.</p>\n\n<pre><code>Matrix([\n[3.3881317890172e-21*sin(0.0001458423*t),                     -7.29211495242194e-5, 0],\n[                    7.29211495242194e-5, -3.3881317890172e-21*sin(0.0001458423*t), 0],\n[                                      0,                                        0, 0]])\n</code></pre>\n\n<hr>\n\n<p>The change of <code>tol</code> can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre><code>from sympy import simplify, expand, S\nfrom sympy.polys import factor\nfrom sympy.polys.domains import QQ, RR, RealField\nfrom sympy.polys.factortools import dup_convert\nfrom sympy.polys.polytools import Poly\nfrom sympy.polys.polytools import _symbolic_factor_list, _poly_from_expr\nfrom sympy.polys.polyerrors import PolificationFailed\nfrom sympy.polys import polyoptions as options\nfrom sympy.simplify.fu import TR6\n\ndef new_opt():\n    args = dict()\n    options.allowed_flags(args, [])\n    opt = options.build_options((), args)\n    return opt\n\ndef my_symbolic_factor_list(base):\n    opt = new_opt()\n    try:\n        poly, _ = _poly_from_expr(base, opt)\n    except PolificationFailed as exc:\n        print(exc)\n        print(exc.expr)\n    else:\n        _coeff, _factors = poly.factor_list()\n        print(poly)\n        print(_coeff, _factors)\n        return poly\n</code></pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre><code>In [8]: elm_sy = (RE_sy.diff(t) * RE_sy.T)[1]\n\nIn [9]: elm_fl = (RE_fl.diff(t) * RE_fl.T)[1]\n\nIn [10]: elm_sy\nOut[10]: -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 - 7.292115e-5*sin(7.292115e\n-5*t)**2*cos(0.267955555555556*pi)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [11]: elm_fl\nOut[11]: -7.292115e-5*sin(7.292115e-5*t)**2 - 7.292115e-5*cos(7.292115e-5*t)**2\n\nIn [12]: simplify(elm_sy)\nOut[12]: -7.29211500000000e-5\n\nIn [13]: simplify(elm_fl)\nOut[13]: -2785579325.00000\n</code></pre>\n\n<p>When we call <code>simplify</code>, in this case, it's almost equivalent to a combination of <code>TR6</code> and <code>factor</code>.</p>\n\n<pre><code>In [15]: expr_sy = TR6(elm_sy)\n\nIn [16]: expr_fl = TR6(elm_fl)\n\nIn [17]: expr_fl\nOut[17]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5\n\nIn [18]: factor(expr_fl)\nOut[18]: -2785579325.00000\n</code></pre>\n\n<p>Now, we know wrong results would be produced during the invocation of <code>factor()</code>.<br>\nActually, <code>factor</code> is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>.</p>\n\n<pre><code>In [20]: _symbolic_factor_list(expr_fl, opt, 'factor')\nOut[20]: (-2785579325.00000, [])\n</code></pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"><code>_symbolic_factor_list</code></a>. The key part is:</p>\n\n<pre><code>        try:\n            poly, _ = _poly_from_expr(base, opt)\n        except PolificationFailed as exc:\n            factors.append((exc.expr, exp))\n        else:\n            func = getattr(poly, method + '_list')\n\n            _coeff, _factors = func()\n</code></pre>\n\n<p>We use the above <code>my_symbolic_factor_list</code> to simulate this procedure.</p>\n\n<pre><code>In [22]: expand(expr_sy)\nOut[22]: -7.29211500000000e-5\n\nIn [23]: my_symbolic_factor_list(expr_sy)\ncan't construct a polynomial from -7.292115e-5*sin(0.267955555555556*pi)**2*sin(7.292115e-5*t)**2 -\n7.292115e-5*(-sin(0.267955555555556*pi)**2 + 1)*sin(7.292115e-5*t)**2 + 7.292115e-5*sin(7.292115e-5*\nt)**2 - 7.292115e-5\n-7.29211500000000e-5\n\nIn [24]: my_symbolic_factor_list(S(1))\ncan't construct a polynomial from 1\n1\n\nIn [25]: expr_fl\nOut[25]: 1.35525271560688e-20*sin(7.292115e-5*t)**2 - 7.292115e-5    \n\nIn [26]: poly_fl = my_symbolic_factor_list(expr_fl)\nPoly(-7.292115e-5, sin(7.292115e-5*t), domain='RR')\n(-2785579325.00000, [])\n</code></pre>\n\n<p>By design, the constant polynomial should execute <code>except PolificationFailed as exc:</code> suite, while the other polynomials should execute <code>else:</code> suite.<br>\n<code>expr_sy</code>, which is a number after <code>expand()</code>, and <code>1</code> are both constant polynomials, thus <code>PolificationFailed</code>s were thrown.<br>\n<code>poly_fl</code> is <code>-7.292115e-5 * sin(7.292115e-5*t) ** 0</code>, namely, <code>-7.292115e-5</code>, a constant polynomial, whereas <code>expr_fl</code> is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing <code>1.35525271560688e-20*sin(7.292115e-5*t)**2</code>?<br>\nLet us recall: <code>tol</code> was reverted back to <code>None</code>, which means automatic reduction to zero in <code>RR</code> is enabled again.<br>\n<code>1.35525271560688e-20</code> was reduced to zero. Thus, <code>poly_fl</code> became a constant polynomial.<br>\nIf <code>tol</code> is <code>False</code>, this won't happen.</p>\n\n<pre><code>In [31]: arg2 = expr_fl.args[1].args[0]\n\nIn [32]: arg2\nOut[32]: 1.35525271560688e-20\n\nIn [33]: RR.from_sympy(arg2)\nOut[33]: 0.0\n\nIn [34]: R = RealField(tol=False)\n\nIn [35]: R.from_sympy(arg2)\nOut[35]: 1.35525271560688e-20\n</code></pre>\n\n<p>Now, we can explain why you've got <code>-2785579325.0</code>. In the <code>else:</code> suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"><code>Poly.factor_list</code></a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p><code>poly_fl</code> is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate <code>poly_fl</code>. The numerator is kept, while the denominator is discarded.</p>\n\n<pre><code>In [42]: poly_fl.factor_list()\nOut[42]: (-2785579325.00000, [])\n\nIn [43]: dup_convert(poly_fl.coeffs(), RR, QQ)\nOut[43]: [-2785579325/38199881995827]\n\nIn [44]: Poly([S(1.25)], t, domain='RR').factor_list()\nOut[44]: (5.00000000000000, [])\n\nIn [45]: dup_convert(Poly([S(1.25)], t, domain='RR').coeffs(), RR, QQ)\nOut[45]: [5/4]\n\nIn [46]: Poly((RE_fl.diff(t) * RE_fl.T)[3].args[0].args[0], t).factor_list()\nOut[46]: (1767051195.00000, [])\n</code></pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre><code>In [47]: factor(1e-20*t-1.2345e-5)\nOut[47]: -539023891.000000\n\nIn [48]: factor(S(1e-20)*t-S(1.2345e-5))\nOut[48]: -539023891.000000\n</code></pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "4"}, "content_wo_code": "<p>I am trying to calculate a <a href=\"https://en.wikipedia.org/wiki/Angular_velocity#Properties_of_angular_velocity_tensors\" rel=\"nofollow noreferrer\">velocity tensor</a> from a time dependent rotationmatrix   (Namely the earth rotation at latitude 48.3\u00b0). This is achieved by determining the skew symmetric matrix  . I am obtaining incorrect results when utilizing a float instead of a Sympy expression, as shown in the following example:</p>\n\n<pre> </pre>\n\n<p>The result is:</p>\n\n<p><img src=\"https://i.stack.imgur.com/c1Wke.png\" alt=\"calculation results\"></p>\n\n<p>For the float latitude the result is totally wrong in spite of the difference of only -3e-17 to the Sympy value. It is not clear to me, why this happens. Numerically, this calculation does not seem to be problematic.</p>\n\n<p>My question is, how to work around such deficits. Should I avoid mixing Sympy and float/Numpy data types? They are quite difficult to detect for more complex settings.</p>\n\n<p>PS: The Sympy version is 0.7.6.</p>\n", "answer_wo_code": "<p>I think this might be a bug in Sympy; when I run your script on my system (Ubuntu 14.04 64-bit, Python 2.7, Sympy 0.7.4.1), I get</p>\n\n<pre> </pre>\n\n<p>which looks OK.</p>\n\n<p>I'm not sure what to suggest: you could try an older version of Sympy than 0.7.6, or the latest revision from Github.</p>\n\n<p>[In answer to comment] As to why the diagonals are non-zero, my first comment is that 3e-21/7e-5 is about 4e-17; IEEE754 64-bit (\"float\")\nnumerical precision is around 2e-16.  At 3e-21 rad/s one revolution will take 60 trillion years (about 2e21 s).  Don't worry about it.</p>\n\n<p>I'm not entirely sure what is happening here, but after adding this to your script</p>\n\n<pre> </pre>\n\n<p>I get</p>\n\n<pre> </pre>\n\n<p>The coefficients of the first summation should sum to zero, but don't.  I've managed to sort-of fake this effect in the last few lines by recreating the coefficients with lower precision (this was just luck, and probably not that signicant).  It's \"sort-of\" since the third value in the list ( ) doesn't match the coefficient in the summand list ( ), which is given to 15 places.</p>\n\n<p>The Sympy docs discuss these sorts of issue here <a href=\"http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals\" rel=\"nofollow\">http://docs.sympy.org/dev/gotchas.html#evaluating-expressions-with-floats-and-rationals</a> , with some suggestions on how to mitigate the problem.  Here's a straightforward variation on your code, deferring substitution of floats right to the end:</p>\n\n<pre> </pre>\n\n<p>This gives:</p>\n\n<pre> </pre>\n\n<p>I prefer this regardless of numerical advantages, since one may learn something from the form of the symbolic solution.</p>\n\n\n<p><strong>TL; DR</strong></p>\n\n<p>It is a bug. If you don't believe it, try this:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Two years ago, the default value for the parameter   in   was changed from   to   in commit <a href=\"https://github.com/sympy/sympy/commit/24649916b2c6552dd42f20f1e3b575ed8231c433#diff-8756675b7147afd7ae0afab7c6c7b936\" rel=\"nofollow\">polys: Disabled automatic reduction to zero in RR and CC</a>.<br>\nLater,   was reverted back to   to fix a simplification issue, in commit <a href=\"https://github.com/sympy/sympy/commit/f09894593aca1558375285dec30b7fb2879fb4c8#diff-7dc8bbd13c2a551135f6c559f707c68c\" rel=\"nofollow\">Changed tol on Complex and Real field to None</a>.<br>\nIt seems the developers didn't expect this reversion would bring some other issue.</p>\n\n<p>If you modify   at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/domains/realfield.py#L48\" rel=\"nofollow\"> </a> in  , to  , you will get the correct result for  .</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>The change of   can explain why you've got a wrong result, but I don't thint it is the root of the issue.<br>\nIMHO, there is a deficiency in the polynomial factorization in SymPy. I'll illustrate this deficiency.<br>\nFor convenience, let us do some preparation work.<br>\nAdd the followings to your example.</p>\n\n<pre> </pre>\n\n<p>We don't need to study the whole matrices. Let us focus on one element, element at row 1 and column 2. It has already shown the result is incorrect.</p>\n\n<pre> </pre>\n\n<p>When we call  , in this case, it's almost equivalent to a combination of   and  .</p>\n\n<pre> </pre>\n\n<p>Now, we know wrong results would be produced during the invocation of  .<br>\nActually,   is just a wrapper, the major work is done by <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>.</p>\n\n<pre> </pre>\n\n<p>Let us take a look at <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L5548\" rel=\"nofollow\"> </a>. The key part is:</p>\n\n<pre> </pre>\n\n<p>We use the above   to simulate this procedure.</p>\n\n<pre> </pre>\n\n<p>By design, the constant polynomial should execute   suite, while the other polynomials should execute   suite.<br>\n , which is a number after  , and   are both constant polynomials, thus  s were thrown.<br>\n  is  , namely,  , a constant polynomial, whereas   is not. They were supposed to be the same polynomial, just different representation. Now they are not.<br>\nThis is the <strong>deficiency</strong> I mentioned.  </p>\n\n<p>Where is the missing  ?<br>\nLet us recall:   was reverted back to  , which means automatic reduction to zero in   is enabled again.<br>\n  was reduced to zero. Thus,   became a constant polynomial.<br>\nIf   is  , this won't happen.</p>\n\n<pre> </pre>\n\n<p>Now, we can explain why you've got  . In the   suite, <a href=\"https://github.com/sympy/sympy/blob/sympy-0.7.6/sympy/polys/polytools.py#L3068\" rel=\"nofollow\"> </a> is called.<br>\nAccording to <a href=\"http://docs.sympy.org/dev/modules/polys/reference.html#sympy.polys.polytools.Poly.factor_list\" rel=\"nofollow\">docs</a>:</p>\n\n<blockquote>\n  <p>factor_list(f)[source]</p>\n  \n  <p>Returns a list of irreducible factors of f.</p>\n</blockquote>\n\n<p>  is supposed to be a non constant polynomial, but it is just a number.\nThus, SymPy was tring to use a rational number to approximate  . The numerator is kept, while the denominator is discarded.</p>\n\n<pre> </pre>\n\n<p>I don't think we should blame mixing Sympy and float/Numpy data types. This problem is not caused by those <a href=\"http://docs.sympy.org/latest/gotchas.html\" rel=\"nofollow\">pitfalls</a> SymPy mentioned.<br>\nEven a very simple factorization can produce a counterintuitive result. </p>\n\n<pre> </pre>\n\n<p>So it is a bug. Just let the developers fix it.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.python"}, "class_func_label": {"type": "literal", "value": "sympy.python"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nReturn Python interpretation of passed expression\n(can be passed to the exec() function without any modifications)"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/27304590"}, "title": {"type": "literal", "value": "Generate python code from a sympy expression?"}, "content": {"type": "literal", "value": "<p><strong>The Question</strong>:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.</p>\n\n<p><strong>Why</strong></p>\n\n<p>I find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  </p>\n\n<p>I can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:</p>\n\n<ol>\n<li>Call str(sympyResult)</li>\n<li>Do custom things with regular expressions to get this string to look like valid python code</li>\n<li>write this python code to a file</li>\n</ol>\n\n<p>I note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  </p>\n\n<p>I know of several possible but problematic ways around this problem:</p>\n\n<ol>\n<li><p>I know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:</p>\n\n<ul>\n<li>dependency: my code now depends on sympy to run.  This is bad.</li>\n<li>efficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need <code>evalf</code> every time.</li>\n</ul></li>\n<li><p>I also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.</p></li>\n</ol>\n\n<p><strong>Edit: Summary</strong>\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.</p>\n\n<p>That doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  </p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"><code>lambdify</code></a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"><code>evalf</code></a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"><code>sympy.printing.lambdarepr</code></a> directly, which is what <code>lambdify</code> uses to convert an expression into a lambda function. </p>\n\n\n<p>The function you are looking for to generate python code is <code>python</code>. Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre><code>&gt;&gt;&gt; import sympy as sp\n&gt;&gt;&gt; x = sp.Symbol('x')\n&gt;&gt;&gt; y = sp.Symbol('y')\n&gt;&gt;&gt; print(sp.python(sp.Matrix([[x**2,sp.exp(y) + x]]).jacobian([x, y])))\nx = Symbol('x')\ny = Symbol('y')\ne = MutableDenseMatrix([[2*x, 0], [1, exp(y)]])\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"><code>lambdify</code></a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"><code>evalf</code></a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"><code>sympy.printing.lambdarepr</code></a> directly, which is what <code>lambdify</code> uses to convert an expression into a lambda function. </p>\n"}, "answer_1_votes": {"type": "literal", "value": "8"}, "answer_2": {"type": "literal", "value": "<p>The function you are looking for to generate python code is <code>python</code>. Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre><code>&gt;&gt;&gt; import sympy as sp\n&gt;&gt;&gt; x = sp.Symbol('x')\n&gt;&gt;&gt; y = sp.Symbol('y')\n&gt;&gt;&gt; print(sp.python(sp.Matrix([[x**2,sp.exp(y) + x]]).jacobian([x, y])))\nx = Symbol('x')\ny = Symbol('y')\ne = MutableDenseMatrix([[2*x, 0], [1, exp(y)]])\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p><strong>The Question</strong>:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.</p>\n\n<p><strong>Why</strong></p>\n\n<p>I find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  </p>\n\n<p>I can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:</p>\n\n<ol>\n<li>Call str(sympyResult)</li>\n<li>Do custom things with regular expressions to get this string to look like valid python code</li>\n<li>write this python code to a file</li>\n</ol>\n\n<p>I note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  </p>\n\n<p>I know of several possible but problematic ways around this problem:</p>\n\n<ol>\n<li><p>I know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:</p>\n\n<ul>\n<li>dependency: my code now depends on sympy to run.  This is bad.</li>\n<li>efficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need   every time.</li>\n</ul></li>\n<li><p>I also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.</p></li>\n</ol>\n\n<p><strong>Edit: Summary</strong>\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.</p>\n\n<p>That doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  </p>\n", "answer_wo_code": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"> </a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"> </a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"> </a> directly, which is what   uses to convert an expression into a lambda function. </p>\n\n\n<p>The function you are looking for to generate python code is  . Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sympy.printing.print_ccode"}, "class_func_label": {"type": "literal", "value": "sympy.printing.print_ccode"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nPrints C representation of the given expression."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/27304590"}, "title": {"type": "literal", "value": "Generate python code from a sympy expression?"}, "content": {"type": "literal", "value": "<p><strong>The Question</strong>:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.</p>\n\n<p><strong>Why</strong></p>\n\n<p>I find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  </p>\n\n<p>I can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:</p>\n\n<ol>\n<li>Call str(sympyResult)</li>\n<li>Do custom things with regular expressions to get this string to look like valid python code</li>\n<li>write this python code to a file</li>\n</ol>\n\n<p>I note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  </p>\n\n<p>I know of several possible but problematic ways around this problem:</p>\n\n<ol>\n<li><p>I know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:</p>\n\n<ul>\n<li>dependency: my code now depends on sympy to run.  This is bad.</li>\n<li>efficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need <code>evalf</code> every time.</li>\n</ul></li>\n<li><p>I also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.</p></li>\n</ol>\n\n<p><strong>Edit: Summary</strong>\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.</p>\n\n<p>That doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  </p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"><code>lambdify</code></a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"><code>evalf</code></a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"><code>sympy.printing.lambdarepr</code></a> directly, which is what <code>lambdify</code> uses to convert an expression into a lambda function. </p>\n\n\n<p>The function you are looking for to generate python code is <code>python</code>. Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre><code>&gt;&gt;&gt; import sympy as sp\n&gt;&gt;&gt; x = sp.Symbol('x')\n&gt;&gt;&gt; y = sp.Symbol('y')\n&gt;&gt;&gt; print(sp.python(sp.Matrix([[x**2,sp.exp(y) + x]]).jacobian([x, y])))\nx = Symbol('x')\ny = Symbol('y')\ne = MutableDenseMatrix([[2*x, 0], [1, exp(y)]])\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"><code>lambdify</code></a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"><code>evalf</code></a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"><code>sympy.printing.lambdarepr</code></a> directly, which is what <code>lambdify</code> uses to convert an expression into a lambda function. </p>\n"}, "answer_1_votes": {"type": "literal", "value": "8"}, "answer_2": {"type": "literal", "value": "<p>The function you are looking for to generate python code is <code>python</code>. Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre><code>&gt;&gt;&gt; import sympy as sp\n&gt;&gt;&gt; x = sp.Symbol('x')\n&gt;&gt;&gt; y = sp.Symbol('y')\n&gt;&gt;&gt; print(sp.python(sp.Matrix([[x**2,sp.exp(y) + x]]).jacobian([x, y])))\nx = Symbol('x')\ny = Symbol('y')\ne = MutableDenseMatrix([[2*x, 0], [1, exp(y)]])\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p><strong>The Question</strong>:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.</p>\n\n<p><strong>Why</strong></p>\n\n<p>I find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  </p>\n\n<p>I can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:</p>\n\n<ol>\n<li>Call str(sympyResult)</li>\n<li>Do custom things with regular expressions to get this string to look like valid python code</li>\n<li>write this python code to a file</li>\n</ol>\n\n<p>I note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  </p>\n\n<p>I know of several possible but problematic ways around this problem:</p>\n\n<ol>\n<li><p>I know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:</p>\n\n<ul>\n<li>dependency: my code now depends on sympy to run.  This is bad.</li>\n<li>efficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need   every time.</li>\n</ul></li>\n<li><p>I also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.</p></li>\n</ol>\n\n<p><strong>Edit: Summary</strong>\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.</p>\n\n<p>That doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  </p>\n", "answer_wo_code": "<p>If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use <a href=\"https://docs.sympy.org/latest/modules/utilities/lambdify.html\" rel=\"nofollow noreferrer\"> </a> to evaluate it.  This will be much faster than using <a href=\"https://docs.sympy.org/latest/tutorial/basic_operations.html#evalf\" rel=\"nofollow noreferrer\"> </a>, especially if you use numpy. </p>\n\n<p>You could also look at using the printer in <a href=\"https://docs.sympy.org/latest/_modules/sympy/printing/lambdarepr.html\" rel=\"nofollow noreferrer\"> </a> directly, which is what   uses to convert an expression into a lambda function. </p>\n\n\n<p>The function you are looking for to generate python code is  . Although it generates python code, that code will need some tweaking to remove dependence on SymPy objects as Oliver W pointed out.</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/cos"}, "class_func_label": {"type": "literal", "value": "cos"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\n    The cosine function.\n\n    Returns the cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cos, pi\n    >>> from sympy.abc import x\n    >>> cos(x**2).diff(x)\n    -2*x*sin(x**2)\n    >>> cos(1).diff(x)\n    0\n    >>> cos(pi)\n    -1\n    >>> cos(pi/2)\n    0\n    >>> cos(2*pi/3)\n    -1/2\n    >>> cos(pi/12)\n    sqrt(2)/4 + sqrt(6)/4\n\n    See Also\n    ========\n\n    sin, csc, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cos\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37980497"}, "title": {"type": "literal", "value": "Sympy cancelling terms in the Jacobian of polar coordinates transformation"}, "content": {"type": "literal", "value": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre><code>J1 = Matrix([[(r*cos(theta)).diff(r), (r*cos(theta)).diff(theta)],[(r*sin(theta)).diff(r),(r*sin(theta)).diff(theta)]])\nJ2 = Matrix([[((x**2+y**2)**(1/2.)).diff(x),((x**2+y**2)**(1/2.)).diff(y)],[atan(y/x).diff(x),atan(y/x).diff(y)]])\n#substitute for x,y to have same variables for J1 and J2\nJ2 = trigsimp(J2.subs({x:r*cos(theta), y:r*sin(theta)}))\nJ2\n</code></pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre><code>simplify(r/(r**2)**(1/2)*(sin(theta)**2+cos(theta)**2))\n</code></pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre><code>J = J1*J2\nsimplify(J[0,0])\n</code></pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre> </pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre> </pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre> </pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n", "answer_wo_code": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sin"}, "class_func_label": {"type": "literal", "value": "sin"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\n    The sine function.\n\n    Returns the sine of x (measured in radians).\n\n    Notes\n    =====\n\n    This function will evaluate automatically in the\n    case x/pi is some rational number [4]_.  For example,\n    if x is a multiple of pi, pi/2, pi/3, pi/4 and pi/6.\n\n    Examples\n    ========\n\n    >>> from sympy import sin, pi\n    >>> from sympy.abc import x\n    >>> sin(x**2).diff(x)\n    2*x*cos(x**2)\n    >>> sin(1).diff(x)\n    0\n    >>> sin(pi)\n    0\n    >>> sin(pi/2)\n    1\n    >>> sin(pi/6)\n    1/2\n    >>> sin(pi/12)\n    -sqrt(2)/4 + sqrt(6)/4\n\n\n    See Also\n    ========\n\n    csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sin\n    .. [4] http://mathworld.wolfram.com/TrigonometryAngles.html\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37980497"}, "title": {"type": "literal", "value": "Sympy cancelling terms in the Jacobian of polar coordinates transformation"}, "content": {"type": "literal", "value": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre><code>J1 = Matrix([[(r*cos(theta)).diff(r), (r*cos(theta)).diff(theta)],[(r*sin(theta)).diff(r),(r*sin(theta)).diff(theta)]])\nJ2 = Matrix([[((x**2+y**2)**(1/2.)).diff(x),((x**2+y**2)**(1/2.)).diff(y)],[atan(y/x).diff(x),atan(y/x).diff(y)]])\n#substitute for x,y to have same variables for J1 and J2\nJ2 = trigsimp(J2.subs({x:r*cos(theta), y:r*sin(theta)}))\nJ2\n</code></pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre><code>simplify(r/(r**2)**(1/2)*(sin(theta)**2+cos(theta)**2))\n</code></pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre><code>J = J1*J2\nsimplify(J[0,0])\n</code></pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre> </pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre> </pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre> </pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n", "answer_wo_code": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/rational"}, "class_func_label": {"type": "literal", "value": "rational"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "Fixed precision rational numbers"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37980497"}, "title": {"type": "literal", "value": "Sympy cancelling terms in the Jacobian of polar coordinates transformation"}, "content": {"type": "literal", "value": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre><code>J1 = Matrix([[(r*cos(theta)).diff(r), (r*cos(theta)).diff(theta)],[(r*sin(theta)).diff(r),(r*sin(theta)).diff(theta)]])\nJ2 = Matrix([[((x**2+y**2)**(1/2.)).diff(x),((x**2+y**2)**(1/2.)).diff(y)],[atan(y/x).diff(x),atan(y/x).diff(y)]])\n#substitute for x,y to have same variables for J1 and J2\nJ2 = trigsimp(J2.subs({x:r*cos(theta), y:r*sin(theta)}))\nJ2\n</code></pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre><code>simplify(r/(r**2)**(1/2)*(sin(theta)**2+cos(theta)**2))\n</code></pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre><code>J = J1*J2\nsimplify(J[0,0])\n</code></pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre><code>r = Symbol('r', nonnegative=True) \ntheta, x, y = symbols('theta x y')\n</code></pre>\n\n<p>(Mathematically, you can even assume r strictly positive, <code>positive=True</code>, since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable: <code>[[1.0*cos(theta), 1.0*sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use <code>Rational(1, 2)</code> to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use <code>sqrt</code> which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre><code>sqrt(x**2+y**2).diff(x)\n</code></pre>\n\n<p>The end result is <code>[[cos(theta), sin(theta)], [-sin(theta)/r, cos(theta)/r]]</code> </p>\n\n<p>As for trigonometric simplification, <code>trigsimp(J1*J2)</code> does return the identity matrix.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am preparing examples of how a Jacobian is arrived at using transformations from polar/cartesian parametrisations. My question is twofold. First, I want to know how to force sympy to cancel out terms following substitution. \nThe two matrices are: </p>\n\n<pre> </pre>\n\n<p>I expected that using cancel(), or even evalf() would have removed the r/(r^2)^1/2= 1 term, but it did not. </p>\n\n<p>Second, how or can one require sympy to recognise simple identities, in this case sin^2+cos^2 = 1? This is so that the result is an evaluated identity matrix from J1*J2.</p>\n\n<p>This works, as per documentation: </p>\n\n<pre> </pre>\n\n<p>This equivalent(ish) equation does not. </p>\n\n<pre> </pre>\n\n<p>It seems that the second error is a consequence of the first.</p>\n", "answer_wo_code": "<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n\n\n<p>Mathematically, r/(r^2)^1/2= 1 is not always true. It's true if r is a nonnegative number, which it is in polar coordinates. So you should tell SymPy this: </p>\n\n<pre> </pre>\n\n<p>(Mathematically, you can even assume r strictly positive,  , since at the origin derivatives in polar coordinates don't work anyway.) </p>\n\n<p>The output will be much more agreeable:   </p>\n\n<p>Only this 1.0 is annoying, where does it come from? It comes from 1/2. being a float instead of a rational number. Use   to have a rational number in the exponent (important for simplification). In this case, the exponent being 1/2, it's more natural to use   which has the same effect of making the exponent rational and is easier to type.</p>\n\n<pre> </pre>\n\n<p>The end result is   </p>\n\n<p>As for trigonometric simplification,   does return the identity matrix.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/Matrix"}, "class_func_label": {"type": "literal", "value": "Matrix"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": ".. class:: Matrix([rows])\n\n   This object gives access to Matrices in Blender, supporting square and rectangular\n   matrices from 2x2 up to 4x4.\n\n   :param rows: Sequence of rows.\n      When omitted, a 4x4 identity matrix is constructed.\n   :type rows: 2d number sequence\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/43010422"}, "title": {"type": "literal", "value": "Preserving dimensions when slicing symbolic block matrices in sympy"}, "content": {"type": "literal", "value": "<p>I am using sympy (python 3.6, sympy 1.0) to facilitate the calculation of matrix-transformations in mathematical proofs. </p>\n\n<p>To calculate the Schur complements it is necessary to slice a block-matrix consisting of symbolic matrices. </p>\n\n<p>As directly addressing the matrix with:</p>\n\n<pre><code>    M[0:1,1]\n</code></pre>\n\n<p>is not working I tried <em>sympy.matrices.expressions.blockmatrix.blocks</em> Unfortunately <em>blocks</em> is messing up the dimensions of the matrices when addressing a range of blocks:</p>\n\n<pre><code>    from sympy import *\n    n = Symbol('n')\n    Aj = MatrixSymbol('Aj', n,n)\n\n    M = BlockMatrix([[Aj, Aj],[Aj, Aj]])\n    M1 = M.blocks[0:1,0:1]\n    M2 = M.blocks[0,0]\n\n    print(M1.shape)\n    print(M2.shape)\n</code></pre>\n\n<p>M.blocks returns a matrix with the dimension 1,1 for the matrix M1 while the matrix M2 has the right dimension n,n.</p>\n\n<p>Any suggestion how to get the right dimensions when using an interval ? </p>\n"}, "answerContent": {"type": "literal", "value": "<p>The method <code>blocks</code> returns an ImmutableMatrix object, not a BlockMatrix object. Here it is for reference: </p>\n\n<pre><code>def blocks(self):\n    from sympy.matrices.immutable import ImmutableMatrix\n    mats = self.args\n    data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                    for j in range(len(mats))]\n                    for i in range(len(mats))]\n    return ImmutableMatrix(data)\n</code></pre>\n\n<p>The shape of an ImmutableMatrix object is determined by the number of symbols it contains; the structure of symbols is not taken into account. Hence, you get (1,1). </p>\n\n<p>When using <code>M.blocks[0,0]</code> you access an element of the matrix, which is Aj. This is known as a MatrixSymbol, so the shape works as expected. </p>\n\n<p>When using <code>M.blocks[0:1, 0:1]</code> you slice a SymPy matrix. Slicing always returns a submatrix, even if the size of the slice is 1 by 1. So you get an ImmutableMatrix with one entry, <code>Matrix([[Aj]])</code>. As said above, the shape of this thing is (1,1) since there is no recognition of the block structure. </p>\n\n<p>As user2357112 suggested, converting the sliced output of blocks into a BlockMatrix causes the shape to be determined on the basis of the shape of <code>Aj</code>:</p>\n\n<pre><code>&gt;&gt;&gt; M3 = BlockMatrix(M.blocks[0:, 0:1])\n&gt;&gt;&gt; M3.shape\n(2*n, n)  \n</code></pre>\n\n<p>It's often useful to check the type of objects that behave in unexpected way: e.g.,  <code>type(M1)</code> and <code>type(M2)</code>. </p>\n\n\n<p>The method <code>blocks</code> returns an ImmutableMatrix object, not a BlockMatrix object. Here it is for reference: </p>\n\n<pre><code>def blocks(self):\n    from sympy.matrices.immutable import ImmutableMatrix\n    mats = self.args\n    data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                    for j in range(len(mats))]\n                    for i in range(len(mats))]\n    return ImmutableMatrix(data)\n</code></pre>\n\n<p>The shape of an ImmutableMatrix object is determined by the number of symbols it contains; the structure of symbols is not taken into account. Hence, you get (1,1). </p>\n\n<p>When using <code>M.blocks[0,0]</code> you access an element of the matrix, which is Aj. This is known as a MatrixSymbol, so the shape works as expected. </p>\n\n<p>When using <code>M.blocks[0:1, 0:1]</code> you slice a SymPy matrix. Slicing always returns a submatrix, even if the size of the slice is 1 by 1. So you get an ImmutableMatrix with one entry, <code>Matrix([[Aj]])</code>. As said above, the shape of this thing is (1,1) since there is no recognition of the block structure. </p>\n\n<p>As user2357112 suggested, converting the sliced output of blocks into a BlockMatrix causes the shape to be determined on the basis of the shape of <code>Aj</code>:</p>\n\n<pre><code>&gt;&gt;&gt; M3 = BlockMatrix(M.blocks[0:, 0:1])\n&gt;&gt;&gt; M3.shape\n(2*n, n)  \n</code></pre>\n\n<p>It's often useful to check the type of objects that behave in unexpected way: e.g.,  <code>type(M1)</code> and <code>type(M2)</code>. </p>\n"}, "answer_1": {"type": "literal", "value": "<p>The method <code>blocks</code> returns an ImmutableMatrix object, not a BlockMatrix object. Here it is for reference: </p>\n\n<pre><code>def blocks(self):\n    from sympy.matrices.immutable import ImmutableMatrix\n    mats = self.args\n    data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                    for j in range(len(mats))]\n                    for i in range(len(mats))]\n    return ImmutableMatrix(data)\n</code></pre>\n\n<p>The shape of an ImmutableMatrix object is determined by the number of symbols it contains; the structure of symbols is not taken into account. Hence, you get (1,1). </p>\n\n<p>When using <code>M.blocks[0,0]</code> you access an element of the matrix, which is Aj. This is known as a MatrixSymbol, so the shape works as expected. </p>\n\n<p>When using <code>M.blocks[0:1, 0:1]</code> you slice a SymPy matrix. Slicing always returns a submatrix, even if the size of the slice is 1 by 1. So you get an ImmutableMatrix with one entry, <code>Matrix([[Aj]])</code>. As said above, the shape of this thing is (1,1) since there is no recognition of the block structure. </p>\n\n<p>As user2357112 suggested, converting the sliced output of blocks into a BlockMatrix causes the shape to be determined on the basis of the shape of <code>Aj</code>:</p>\n\n<pre><code>&gt;&gt;&gt; M3 = BlockMatrix(M.blocks[0:, 0:1])\n&gt;&gt;&gt; M3.shape\n(2*n, n)  \n</code></pre>\n\n<p>It's often useful to check the type of objects that behave in unexpected way: e.g.,  <code>type(M1)</code> and <code>type(M2)</code>. </p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am using sympy (python 3.6, sympy 1.0) to facilitate the calculation of matrix-transformations in mathematical proofs. </p>\n\n<p>To calculate the Schur complements it is necessary to slice a block-matrix consisting of symbolic matrices. </p>\n\n<p>As directly addressing the matrix with:</p>\n\n<pre> </pre>\n\n<p>is not working I tried <em>sympy.matrices.expressions.blockmatrix.blocks</em> Unfortunately <em>blocks</em> is messing up the dimensions of the matrices when addressing a range of blocks:</p>\n\n<pre> </pre>\n\n<p>M.blocks returns a matrix with the dimension 1,1 for the matrix M1 while the matrix M2 has the right dimension n,n.</p>\n\n<p>Any suggestion how to get the right dimensions when using an interval ? </p>\n", "answer_wo_code": "<p>The method   returns an ImmutableMatrix object, not a BlockMatrix object. Here it is for reference: </p>\n\n<pre> </pre>\n\n<p>The shape of an ImmutableMatrix object is determined by the number of symbols it contains; the structure of symbols is not taken into account. Hence, you get (1,1). </p>\n\n<p>When using   you access an element of the matrix, which is Aj. This is known as a MatrixSymbol, so the shape works as expected. </p>\n\n<p>When using   you slice a SymPy matrix. Slicing always returns a submatrix, even if the size of the slice is 1 by 1. So you get an ImmutableMatrix with one entry,  . As said above, the shape of this thing is (1,1) since there is no recognition of the block structure. </p>\n\n<p>As user2357112 suggested, converting the sliced output of blocks into a BlockMatrix causes the shape to be determined on the basis of the shape of  :</p>\n\n<pre> </pre>\n\n<p>It's often useful to check the type of objects that behave in unexpected way: e.g.,    and  . </p>\n\n\n<p>The method   returns an ImmutableMatrix object, not a BlockMatrix object. Here it is for reference: </p>\n\n<pre> </pre>\n\n<p>The shape of an ImmutableMatrix object is determined by the number of symbols it contains; the structure of symbols is not taken into account. Hence, you get (1,1). </p>\n\n<p>When using   you access an element of the matrix, which is Aj. This is known as a MatrixSymbol, so the shape works as expected. </p>\n\n<p>When using   you slice a SymPy matrix. Slicing always returns a submatrix, even if the size of the slice is 1 by 1. So you get an ImmutableMatrix with one entry,  . As said above, the shape of this thing is (1,1) since there is no recognition of the block structure. </p>\n\n<p>As user2357112 suggested, converting the sliced output of blocks into a BlockMatrix causes the shape to be determined on the basis of the shape of  :</p>\n\n<pre> </pre>\n\n<p>It's often useful to check the type of objects that behave in unexpected way: e.g.,    and  . </p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/create_archive"}, "class_func_label": {"type": "literal", "value": "create_archive"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nCreate an application archive from SOURCE.\n\nThe SOURCE can be the name of a directory, or a filename or a file-like\nobject referring to an existing archive.\n\nThe content of SOURCE is packed into an application archive in TARGET,\nwhich can be a filename or a file-like object.  If SOURCE is a directory,\nTARGET can be omitted and will default to the name of SOURCE with .pyz\nappended.\n\nThe created application archive will have a shebang line specifying\nthat it should run with INTERPRETER (there will be no shebang line if\nINTERPRETER is None), and a __main__.py which runs MAIN (if MAIN is\nnot specified, an existing __main__.py will be used).  It is an error\nto specify MAIN for anything other than a directory source with no\n__main__.py, and it is an error to omit MAIN if the directory has no\n__main__.py."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/14127095"}, "title": {"type": "literal", "value": "Boto Glacier - Upload file larger than 4 GB using multipart upload"}, "content": {"type": "literal", "value": "<p>I am periodically uploading a file to AWS Glacier using boto as follows:</p>\n\n<pre><code># Import boto's layer2\nimport boto.glacier.layer2\n\n# Create a Layer2 object to connect to Glacier\nl = boto.glacier.layer2.Layer2(aws_access_key_id=awsAccess, aws_secret_access_key=awsSecret)\n\n# Get a vault based on vault name (assuming you created it already)\nv = l.get_vault(vaultName)\n\n# Create an archive from a local file on the vault\narchiveID = v.create_archive_from_file(fileName)\n</code></pre>\n\n<p>However this fails for files that are larger than 4 GB in size.</p>\n\n<p>I'm assuming that this is because as specified in the <a href=\"http://aws.amazon.com/glacier/faqs/\" rel=\"nofollow\">Amazon Glacier FAQ</a>: \"The largest archive that can be uploaded in a single Upload request is 4 gigabytes. For items larger than 100 megabytes, customers should consider using the Multipart upload capability.\"</p>\n\n<p>How do I use the Multipart upload capability with boto and AWS Glacier?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I've just looked into sources and is seems that <strong>boto.glacier.vault.Vault.upload_archive()</strong> does all the magic automatically:</p>\n\n<blockquote>\n  <p>Adds an archive to a vault. \n  For archives greater than 100MB the multipart upload will be used.</p>\n</blockquote>\n\n<pre><code>def upload_archive(self, filename, description=None):\n    if os.path.getsize(filename) &gt; self.SingleOperationThreshold:\n        return self.create_archive_from_file(filename, description=description)\n    return self._upload_archive_single_operation(filename, description)\n</code></pre>\n\n\n<p>I've just looked into sources and is seems that <strong>boto.glacier.vault.Vault.upload_archive()</strong> does all the magic automatically:</p>\n\n<blockquote>\n  <p>Adds an archive to a vault. \n  For archives greater than 100MB the multipart upload will be used.</p>\n</blockquote>\n\n<pre><code>def upload_archive(self, filename, description=None):\n    if os.path.getsize(filename) &gt; self.SingleOperationThreshold:\n        return self.create_archive_from_file(filename, description=description)\n    return self._upload_archive_single_operation(filename, description)\n</code></pre>\n\n\n<p>Amazon Glacier uses the term archive to describe files. In other words, you cannot upload a file larger than 4GB to Glacier. If you'd like to try the multipart uploader anyway, look at vault.concurrent_create_archive_from_file or vault.create_archive_writer</p>\n\n\n<p>Amazon Glacier uses the term archive to describe files. In other words, you cannot upload a file larger than 4GB to Glacier. If you'd like to try the multipart uploader anyway, look at vault.concurrent_create_archive_from_file or vault.create_archive_writer</p>\n\n\n<p>The <a href=\"http://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\" rel=\"nofollow\">glacier docs</a> clearly state:</p>\n\n<p>Depending on the size of the data you are uploading, Amazon Glacier offers the following options:</p>\n\n<ul>\n<li><p><strong>Upload archives in a single operation</strong>\u2014In a single operation, you can\nupload archives from 1 byte to up to 4 GB in size. However, we\nencourage Amazon Glacier customers to use Multipart Upload to upload\narchives greater than 100 MB.</p></li>\n<li><p><strong>Upload</strong> archives in parts\u2014Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).</p>\n\n<p>The Multipart Upload API call is designed to improve the upload experience for larger archives. You can upload archives in parts. These parts can be uploaded independently, in any order, and in parallel. If a part upload fails, you only need to upload that part again and not the entire archive. You can use Multipart Upload for archives from 1 byte to about 40,000 GB in size.</p></li>\n</ul>\n\n<p>In <code>boto</code> layer 2 this means that you have to use one of the following methods from <code>boto.glacier.vault.Vault</code></p>\n\n<ul>\n<li><code>concurrent_create_archive_from_file</code></li>\n<li><code>create_archive_writer</code></li>\n<li><code>upload_archive</code></li>\n</ul>\n\n\n<p>The <a href=\"http://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\" rel=\"nofollow\">glacier docs</a> clearly state:</p>\n\n<p>Depending on the size of the data you are uploading, Amazon Glacier offers the following options:</p>\n\n<ul>\n<li><p><strong>Upload archives in a single operation</strong>\u2014In a single operation, you can\nupload archives from 1 byte to up to 4 GB in size. However, we\nencourage Amazon Glacier customers to use Multipart Upload to upload\narchives greater than 100 MB.</p></li>\n<li><p><strong>Upload</strong> archives in parts\u2014Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).</p>\n\n<p>The Multipart Upload API call is designed to improve the upload experience for larger archives. You can upload archives in parts. These parts can be uploaded independently, in any order, and in parallel. If a part upload fails, you only need to upload that part again and not the entire archive. You can use Multipart Upload for archives from 1 byte to about 40,000 GB in size.</p></li>\n</ul>\n\n<p>In <code>boto</code> layer 2 this means that you have to use one of the following methods from <code>boto.glacier.vault.Vault</code></p>\n\n<ul>\n<li><code>concurrent_create_archive_from_file</code></li>\n<li><code>create_archive_writer</code></li>\n<li><code>upload_archive</code></li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>I've just looked into sources and is seems that <strong>boto.glacier.vault.Vault.upload_archive()</strong> does all the magic automatically:</p>\n\n<blockquote>\n  <p>Adds an archive to a vault. \n  For archives greater than 100MB the multipart upload will be used.</p>\n</blockquote>\n\n<pre><code>def upload_archive(self, filename, description=None):\n    if os.path.getsize(filename) &gt; self.SingleOperationThreshold:\n        return self.create_archive_from_file(filename, description=description)\n    return self._upload_archive_single_operation(filename, description)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p>Amazon Glacier uses the term archive to describe files. In other words, you cannot upload a file larger than 4GB to Glacier. If you'd like to try the multipart uploader anyway, look at vault.concurrent_create_archive_from_file or vault.create_archive_writer</p>\n"}, "answer_2_votes": {"type": "literal", "value": "4"}, "answer_3": {"type": "literal", "value": "<p>The <a href=\"http://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\" rel=\"nofollow\">glacier docs</a> clearly state:</p>\n\n<p>Depending on the size of the data you are uploading, Amazon Glacier offers the following options:</p>\n\n<ul>\n<li><p><strong>Upload archives in a single operation</strong>\u2014In a single operation, you can\nupload archives from 1 byte to up to 4 GB in size. However, we\nencourage Amazon Glacier customers to use Multipart Upload to upload\narchives greater than 100 MB.</p></li>\n<li><p><strong>Upload</strong> archives in parts\u2014Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).</p>\n\n<p>The Multipart Upload API call is designed to improve the upload experience for larger archives. You can upload archives in parts. These parts can be uploaded independently, in any order, and in parallel. If a part upload fails, you only need to upload that part again and not the entire archive. You can use Multipart Upload for archives from 1 byte to about 40,000 GB in size.</p></li>\n</ul>\n\n<p>In <code>boto</code> layer 2 this means that you have to use one of the following methods from <code>boto.glacier.vault.Vault</code></p>\n\n<ul>\n<li><code>concurrent_create_archive_from_file</code></li>\n<li><code>create_archive_writer</code></li>\n<li><code>upload_archive</code></li>\n</ul>\n"}, "answer_3_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am periodically uploading a file to AWS Glacier using boto as follows:</p>\n\n<pre> </pre>\n\n<p>However this fails for files that are larger than 4 GB in size.</p>\n\n<p>I'm assuming that this is because as specified in the <a href=\"http://aws.amazon.com/glacier/faqs/\" rel=\"nofollow\">Amazon Glacier FAQ</a>: \"The largest archive that can be uploaded in a single Upload request is 4 gigabytes. For items larger than 100 megabytes, customers should consider using the Multipart upload capability.\"</p>\n\n<p>How do I use the Multipart upload capability with boto and AWS Glacier?</p>\n", "answer_wo_code": "<p>I've just looked into sources and is seems that <strong>boto.glacier.vault.Vault.upload_archive()</strong> does all the magic automatically:</p>\n\n<blockquote>\n  <p>Adds an archive to a vault. \n  For archives greater than 100MB the multipart upload will be used.</p>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>I've just looked into sources and is seems that <strong>boto.glacier.vault.Vault.upload_archive()</strong> does all the magic automatically:</p>\n\n<blockquote>\n  <p>Adds an archive to a vault. \n  For archives greater than 100MB the multipart upload will be used.</p>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Amazon Glacier uses the term archive to describe files. In other words, you cannot upload a file larger than 4GB to Glacier. If you'd like to try the multipart uploader anyway, look at vault.concurrent_create_archive_from_file or vault.create_archive_writer</p>\n\n\n<p>Amazon Glacier uses the term archive to describe files. In other words, you cannot upload a file larger than 4GB to Glacier. If you'd like to try the multipart uploader anyway, look at vault.concurrent_create_archive_from_file or vault.create_archive_writer</p>\n\n\n<p>The <a href=\"http://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\" rel=\"nofollow\">glacier docs</a> clearly state:</p>\n\n<p>Depending on the size of the data you are uploading, Amazon Glacier offers the following options:</p>\n\n<ul>\n<li><p><strong>Upload archives in a single operation</strong>\u2014In a single operation, you can\nupload archives from 1 byte to up to 4 GB in size. However, we\nencourage Amazon Glacier customers to use Multipart Upload to upload\narchives greater than 100 MB.</p></li>\n<li><p><strong>Upload</strong> archives in parts\u2014Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).</p>\n\n<p>The Multipart Upload API call is designed to improve the upload experience for larger archives. You can upload archives in parts. These parts can be uploaded independently, in any order, and in parallel. If a part upload fails, you only need to upload that part again and not the entire archive. You can use Multipart Upload for archives from 1 byte to about 40,000 GB in size.</p></li>\n</ul>\n\n<p>In   layer 2 this means that you have to use one of the following methods from  </p>\n\n<ul>\n<li> </li>\n<li> </li>\n<li> </li>\n</ul>\n\n\n<p>The <a href=\"http://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\" rel=\"nofollow\">glacier docs</a> clearly state:</p>\n\n<p>Depending on the size of the data you are uploading, Amazon Glacier offers the following options:</p>\n\n<ul>\n<li><p><strong>Upload archives in a single operation</strong>\u2014In a single operation, you can\nupload archives from 1 byte to up to 4 GB in size. However, we\nencourage Amazon Glacier customers to use Multipart Upload to upload\narchives greater than 100 MB.</p></li>\n<li><p><strong>Upload</strong> archives in parts\u2014Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).</p>\n\n<p>The Multipart Upload API call is designed to improve the upload experience for larger archives. You can upload archives in parts. These parts can be uploaded independently, in any order, and in parallel. If a part upload fails, you only need to upload that part again and not the entire archive. You can use Multipart Upload for archives from 1 byte to about 40,000 GB in size.</p></li>\n</ul>\n\n<p>In   layer 2 this means that you have to use one of the following methods from  </p>\n\n<ul>\n<li> </li>\n<li> </li>\n<li> </li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.dialects.postgresql.JSON"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.dialects.postgresql.JSON"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Represent the PostgreSQL JSON type.\n\n    This type is a specialization of the Core-level :class:`.types.JSON`\n    type.   Be sure to read the documentation for :class:`.types.JSON` for\n    important tips regarding treatment of NULL values and ORM use.\n\n    .. versionchanged:: 1.1 :class:`.postgresql.JSON` is now a PostgreSQL-\n       specific specialization of the new :class:`.types.JSON` type.\n\n    The operators provided by the PostgreSQL version of :class:`.JSON`\n    include:\n\n    * Index operations (the ``->`` operator)::\n\n        data_table.c.data['some key']\n\n        data_table.c.data[5]\n\n\n    * Index operations returning text (the ``->>`` operator)::\n\n        data_table.c.data['some key'].astext == 'some value'\n\n      Note that equivalent functionality is available via the\n      :attr:`.JSON.Comparator.as_string` accessor.\n\n    * Index operations with CAST\n      (equivalent to ``CAST(col ->> ['some key'] AS <type>)``)::\n\n        data_table.c.data['some key'].astext.cast(Integer) == 5\n\n      Note that equivalent functionality is available via the\n      :attr:`.JSON.Comparator.as_integer` and similar accessors.\n\n    * Path index operations (the ``#>`` operator)::\n\n        data_table.c.data[('key_1', 'key_2', 5, ..., 'key_n')]\n\n    * Path index operations returning text (the ``#>>`` operator)::\n\n        data_table.c.data[('key_1', 'key_2', 5, ..., 'key_n')].astext == 'some value'\n\n    .. versionchanged:: 1.1  The :meth:`.ColumnElement.cast` operator on\n       JSON objects now requires that the :attr:`.JSON.Comparator.astext`\n       modifier be called explicitly, if the cast works only from a textual\n       string.\n\n    Index operations return an expression object whose type defaults to\n    :class:`.JSON` by default, so that further JSON-oriented instructions\n    may be called upon the result type.\n\n    Custom serializers and deserializers are specified at the dialect level,\n    that is using :func:`.create_engine`.  The reason for this is that when\n    using psycopg2, the DBAPI only allows serializers at the per-cursor\n    or per-connection level.   E.g.::\n\n        engine = create_engine(\"postgresql://scott:tiger@localhost/test\",\n                                json_serializer=my_serialize_fn,\n                                json_deserializer=my_deserialize_fn\n                        )\n\n    When using the psycopg2 dialect, the json_deserializer is registered\n    against the database using ``psycopg2.extras.register_default_json``.\n\n    .. seealso::\n\n        :class:`.types.JSON` - Core level JSON type\n\n        :class:`.JSONB`\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54654705"}, "title": {"type": "literal", "value": "Using Postgres @> Operator and json_build_object in SQLAlchemy"}, "content": {"type": "literal", "value": "<p>I have a query using several Postgres-specific operations:</p>\n\n<pre><code>SELECT \n    a.row_id, \n    a.name\nFROM\n    a \nJOIN\n    b\nON\n    b.json_record @&gt; json_build_object('path', json_build_object('to', a.name))::jsonb\n</code></pre>\n\n<p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only <strong>keys</strong>, not <strong>values</strong>.</p>\n\n<p><a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator</a></p>\n\n<p>I'm not clear on how I could design this query through SQLAlchemy short of using a raw query.</p>\n\n<h2>Edit 1</h2>\n\n<p><a href=\"https://stackoverflow.com/a/39467149/1886901\">Based on this answer</a>, I gave the below a try.</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .join(\n      B_Table.json_record.contains({\n         'path': {\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>It, however, resulted in an error from the line <code>'to': A_Table.name</code>:</p>\n\n<pre><code>AttributeError: Neither 'BinaryExpression' object nor 'Comparator' object has an attribute 'selectable'\nsqlalchemy/orm/query.py\", line 2206, in join\nfrom_joinpoint=from_joinpoint,\nFile \"&lt;string&gt;\", line 2, in _join\n</code></pre>\n\n<p>So I instead attempted</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .filter(\n      B_Table.json_record.contains({\n         'path': {\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>Which at least resulted in a different error, this one with some SQL generation from SQLAlchemy:</p>\n\n<pre><code>sqlalchemy.exc.StatementError: (builtins.TypeError) \nObject of type 'InstrumentedAttribute' is not JSON serializable \n[SQL: 'SELECT a.row_id AS a_id, a.name AS a_name FROM a, b \nWHERE b.json_record @&gt; %(json_record_1)s'] [parameters: [{}]]\n</code></pre>\n\n<p>This SQL is close to what I was aiming for, and may be acceptable, but the example provided in the answer presumes I know the value ahead of time, when what I want to do is a comparison against the row value. I typically would do:</p>\n\n<pre><code>.filter([a.name == b.json_record['path']['to'].astext])\n</code></pre>\n\n<p>But I'm also trying to leverage an optimization from a <code>gin</code> index on this <code>JSONB</code> column, which leaves me needing the <code>@&gt;</code> operator.</p>\n\n<h2>Edit 2</h2>\n\n<p>Based on the answer from Ilja Everil\u00e4, I was able to track down the SQLAlchemy method <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/_modules/sqlalchemy_utils/functions/database.html#json_sql\" rel=\"nofollow noreferrer\">implemented in the source code</a>, and <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/database_helpers.html#json-sql\" rel=\"nofollow noreferrer\">using the <code>sql-json</code> method</a> was able to get the SQL almost there.</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .join(\n      B_Table.json_record.contains({\n         json_sql({'path': json_sql({\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>Giving me the SQL:</p>\n\n<pre><code>SELECT \n    a.row_id, \n    a.name\nFROM\n    a \nJOIN\n    b\nON\n    b.json_record @&gt; json_build_object('path', json_build_object('to', a.name))\n</code></pre>\n\n<p>The problem with this output is that instead of:</p>\n\n<pre><code>json_build_object(..., json_build_object(...))\n</code></pre>\n\n<p>Valid Postgres syntax should be:</p>\n\n<pre><code>json_build_object(..., json_build_object(...))::jsonb\n</code></pre>\n\n<p>Both the answer's and the source code's approach <a href=\"https://docs.sqlalchemy.org/en/latest/core/sqlelement.html?highlight=_functiongenerator\" rel=\"nofollow noreferrer\">relies on the <code>_FunctionGenerator</code></a>, which can build the function, but it's not clear how something can get appended to the end of the method during the <code>compile</code> when going that route.</p>\n\n<h2>Edit 3</h2>\n\n<p>NVM - the answer's author pointed out <code>jsonb_build_object(...)</code> would fit with this model w/o the flag.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using <code>contains()</code> in SQLA and <code>jsonb_build_object()</code> in Postgresql as you were trying before:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            func.jsonb_build_object( \n                'path',\n                func.jsonb_build_object('to', A_Table.name)\n            )\n        )\n    )\n</code></pre>\n\n<blockquote>\n  <p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"><code>JSONB.Comparator.contains()</code></a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for <code>@&gt;</code>:</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building <code>jsonb</code> in a helper function:</p>\n\n<pre><code>def build_jsonb(obj):\n    if isinstance(obj, dict):\n        pairs = [(k, build_jsonb(v)) for k, v in obj.items()]\n        return func.jsonb_build_object(*[arg for p in pairs for arg in p])\n\n    elif isinstance(obj, list):\n        return func.jsonb_build_array(*[build_jsonb(v) for v in obj])\n\n    else:\n        return obj\n</code></pre>\n\n<p>and then use it in the original query:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n\n<p>If you wish to use the explicit <code>JOIN</code> syntax:</p>\n\n<pre><code>session.query(A_Table).\\\n    join(B_Table, B_Table.json_record.contains(\n        build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using <code>contains()</code> in SQLA and <code>jsonb_build_object()</code> in Postgresql as you were trying before:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            func.jsonb_build_object( \n                'path',\n                func.jsonb_build_object('to', A_Table.name)\n            )\n        )\n    )\n</code></pre>\n\n<blockquote>\n  <p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"><code>JSONB.Comparator.contains()</code></a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for <code>@&gt;</code>:</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building <code>jsonb</code> in a helper function:</p>\n\n<pre><code>def build_jsonb(obj):\n    if isinstance(obj, dict):\n        pairs = [(k, build_jsonb(v)) for k, v in obj.items()]\n        return func.jsonb_build_object(*[arg for p in pairs for arg in p])\n\n    elif isinstance(obj, list):\n        return func.jsonb_build_array(*[build_jsonb(v) for v in obj])\n\n    else:\n        return obj\n</code></pre>\n\n<p>and then use it in the original query:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n\n<p>If you wish to use the explicit <code>JOIN</code> syntax:</p>\n\n<pre><code>session.query(A_Table).\\\n    join(B_Table, B_Table.json_record.contains(\n        build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I have a query using several Postgres-specific operations:</p>\n\n<pre> </pre>\n\n<p>My understanding is that the   operator acts as a comparison, but the comparison methods for   in the SQLAlchemy docs reference only <strong>keys</strong>, not <strong>values</strong>.</p>\n\n<p><a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator</a></p>\n\n<p>I'm not clear on how I could design this query through SQLAlchemy short of using a raw query.</p>\n\n<h2>Edit 1</h2>\n\n<p><a href=\"https://stackoverflow.com/a/39467149/1886901\">Based on this answer</a>, I gave the below a try.</p>\n\n<pre> </pre>\n\n<p>It, however, resulted in an error from the line  :</p>\n\n<pre> </pre>\n\n<p>So I instead attempted</p>\n\n<pre> </pre>\n\n<p>Which at least resulted in a different error, this one with some SQL generation from SQLAlchemy:</p>\n\n<pre> </pre>\n\n<p>This SQL is close to what I was aiming for, and may be acceptable, but the example provided in the answer presumes I know the value ahead of time, when what I want to do is a comparison against the row value. I typically would do:</p>\n\n<pre> </pre>\n\n<p>But I'm also trying to leverage an optimization from a   index on this   column, which leaves me needing the   operator.</p>\n\n<h2>Edit 2</h2>\n\n<p>Based on the answer from Ilja Everil\u00e4, I was able to track down the SQLAlchemy method <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/_modules/sqlalchemy_utils/functions/database.html#json_sql\" rel=\"nofollow noreferrer\">implemented in the source code</a>, and <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/database_helpers.html#json-sql\" rel=\"nofollow noreferrer\">using the   method</a> was able to get the SQL almost there.</p>\n\n<pre> </pre>\n\n<p>Giving me the SQL:</p>\n\n<pre> </pre>\n\n<p>The problem with this output is that instead of:</p>\n\n<pre> </pre>\n\n<p>Valid Postgres syntax should be:</p>\n\n<pre> </pre>\n\n<p>Both the answer's and the source code's approach <a href=\"https://docs.sqlalchemy.org/en/latest/core/sqlelement.html?highlight=_functiongenerator\" rel=\"nofollow noreferrer\">relies on the  </a>, which can build the function, but it's not clear how something can get appended to the end of the method during the   when going that route.</p>\n\n<h2>Edit 3</h2>\n\n<p>NVM - the answer's author pointed out   would fit with this model w/o the flag.</p>\n", "answer_wo_code": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using   in SQLA and   in Postgresql as you were trying before:</p>\n\n<pre> </pre>\n\n<blockquote>\n  <p>My understanding is that the   operator acts as a comparison, but the comparison methods for   in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"> </a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for  :</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building   in a helper function:</p>\n\n<pre> </pre>\n\n<p>and then use it in the original query:</p>\n\n<pre> </pre>\n\n<p>If you wish to use the explicit   syntax:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.dialects.postgresql.JSONB"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.dialects.postgresql.JSONB"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Represent the PostgreSQL JSONB type.\n\n    The :class:`.JSONB` type stores arbitrary JSONB format data, e.g.::\n\n        data_table = Table('data_table', metadata,\n            Column('id', Integer, primary_key=True),\n            Column('data', JSONB)\n        )\n\n        with engine.connect() as conn:\n            conn.execute(\n                data_table.insert(),\n                data = {\"key1\": \"value1\", \"key2\": \"value2\"}\n            )\n\n    The :class:`.JSONB` type includes all operations provided by\n    :class:`.JSON`, including the same behaviors for indexing operations.\n    It also adds additional operators specific to JSONB, including\n    :meth:`.JSONB.Comparator.has_key`, :meth:`.JSONB.Comparator.has_all`,\n    :meth:`.JSONB.Comparator.has_any`, :meth:`.JSONB.Comparator.contains`,\n    and :meth:`.JSONB.Comparator.contained_by`.\n\n    Like the :class:`.JSON` type, the :class:`.JSONB` type does not detect\n    in-place changes when used with the ORM, unless the\n    :mod:`sqlalchemy.ext.mutable` extension is used.\n\n    Custom serializers and deserializers\n    are shared with the :class:`.JSON` class, using the ``json_serializer``\n    and ``json_deserializer`` keyword arguments.  These must be specified\n    at the dialect level using :func:`.create_engine`.  When using\n    psycopg2, the serializers are associated with the jsonb type using\n    ``psycopg2.extras.register_default_jsonb`` on a per-connection basis,\n    in the same way that ``psycopg2.extras.register_default_json`` is used\n    to register these handlers with the json type.\n\n    .. versionadded:: 0.9.7\n\n    .. seealso::\n\n        :class:`.JSON`\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54654705"}, "title": {"type": "literal", "value": "Using Postgres @> Operator and json_build_object in SQLAlchemy"}, "content": {"type": "literal", "value": "<p>I have a query using several Postgres-specific operations:</p>\n\n<pre><code>SELECT \n    a.row_id, \n    a.name\nFROM\n    a \nJOIN\n    b\nON\n    b.json_record @&gt; json_build_object('path', json_build_object('to', a.name))::jsonb\n</code></pre>\n\n<p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only <strong>keys</strong>, not <strong>values</strong>.</p>\n\n<p><a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator</a></p>\n\n<p>I'm not clear on how I could design this query through SQLAlchemy short of using a raw query.</p>\n\n<h2>Edit 1</h2>\n\n<p><a href=\"https://stackoverflow.com/a/39467149/1886901\">Based on this answer</a>, I gave the below a try.</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .join(\n      B_Table.json_record.contains({\n         'path': {\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>It, however, resulted in an error from the line <code>'to': A_Table.name</code>:</p>\n\n<pre><code>AttributeError: Neither 'BinaryExpression' object nor 'Comparator' object has an attribute 'selectable'\nsqlalchemy/orm/query.py\", line 2206, in join\nfrom_joinpoint=from_joinpoint,\nFile \"&lt;string&gt;\", line 2, in _join\n</code></pre>\n\n<p>So I instead attempted</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .filter(\n      B_Table.json_record.contains({\n         'path': {\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>Which at least resulted in a different error, this one with some SQL generation from SQLAlchemy:</p>\n\n<pre><code>sqlalchemy.exc.StatementError: (builtins.TypeError) \nObject of type 'InstrumentedAttribute' is not JSON serializable \n[SQL: 'SELECT a.row_id AS a_id, a.name AS a_name FROM a, b \nWHERE b.json_record @&gt; %(json_record_1)s'] [parameters: [{}]]\n</code></pre>\n\n<p>This SQL is close to what I was aiming for, and may be acceptable, but the example provided in the answer presumes I know the value ahead of time, when what I want to do is a comparison against the row value. I typically would do:</p>\n\n<pre><code>.filter([a.name == b.json_record['path']['to'].astext])\n</code></pre>\n\n<p>But I'm also trying to leverage an optimization from a <code>gin</code> index on this <code>JSONB</code> column, which leaves me needing the <code>@&gt;</code> operator.</p>\n\n<h2>Edit 2</h2>\n\n<p>Based on the answer from Ilja Everil\u00e4, I was able to track down the SQLAlchemy method <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/_modules/sqlalchemy_utils/functions/database.html#json_sql\" rel=\"nofollow noreferrer\">implemented in the source code</a>, and <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/database_helpers.html#json-sql\" rel=\"nofollow noreferrer\">using the <code>sql-json</code> method</a> was able to get the SQL almost there.</p>\n\n<pre><code>session \\\n   .query(A_Table) \\\n   .join(\n      B_Table.json_record.contains({\n         json_sql({'path': json_sql({\n            'to': A_Table.name\n         }\n      })\n   )\n</code></pre>\n\n<p>Giving me the SQL:</p>\n\n<pre><code>SELECT \n    a.row_id, \n    a.name\nFROM\n    a \nJOIN\n    b\nON\n    b.json_record @&gt; json_build_object('path', json_build_object('to', a.name))\n</code></pre>\n\n<p>The problem with this output is that instead of:</p>\n\n<pre><code>json_build_object(..., json_build_object(...))\n</code></pre>\n\n<p>Valid Postgres syntax should be:</p>\n\n<pre><code>json_build_object(..., json_build_object(...))::jsonb\n</code></pre>\n\n<p>Both the answer's and the source code's approach <a href=\"https://docs.sqlalchemy.org/en/latest/core/sqlelement.html?highlight=_functiongenerator\" rel=\"nofollow noreferrer\">relies on the <code>_FunctionGenerator</code></a>, which can build the function, but it's not clear how something can get appended to the end of the method during the <code>compile</code> when going that route.</p>\n\n<h2>Edit 3</h2>\n\n<p>NVM - the answer's author pointed out <code>jsonb_build_object(...)</code> would fit with this model w/o the flag.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using <code>contains()</code> in SQLA and <code>jsonb_build_object()</code> in Postgresql as you were trying before:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            func.jsonb_build_object( \n                'path',\n                func.jsonb_build_object('to', A_Table.name)\n            )\n        )\n    )\n</code></pre>\n\n<blockquote>\n  <p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"><code>JSONB.Comparator.contains()</code></a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for <code>@&gt;</code>:</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building <code>jsonb</code> in a helper function:</p>\n\n<pre><code>def build_jsonb(obj):\n    if isinstance(obj, dict):\n        pairs = [(k, build_jsonb(v)) for k, v in obj.items()]\n        return func.jsonb_build_object(*[arg for p in pairs for arg in p])\n\n    elif isinstance(obj, list):\n        return func.jsonb_build_array(*[build_jsonb(v) for v in obj])\n\n    else:\n        return obj\n</code></pre>\n\n<p>and then use it in the original query:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n\n<p>If you wish to use the explicit <code>JOIN</code> syntax:</p>\n\n<pre><code>session.query(A_Table).\\\n    join(B_Table, B_Table.json_record.contains(\n        build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using <code>contains()</code> in SQLA and <code>jsonb_build_object()</code> in Postgresql as you were trying before:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            func.jsonb_build_object( \n                'path',\n                func.jsonb_build_object('to', A_Table.name)\n            )\n        )\n    )\n</code></pre>\n\n<blockquote>\n  <p>My understanding is that the <code>@&gt;</code> operator acts as a comparison, but the comparison methods for <code>JSONB</code> in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"><code>JSONB.Comparator.contains()</code></a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for <code>@&gt;</code>:</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building <code>jsonb</code> in a helper function:</p>\n\n<pre><code>def build_jsonb(obj):\n    if isinstance(obj, dict):\n        pairs = [(k, build_jsonb(v)) for k, v in obj.items()]\n        return func.jsonb_build_object(*[arg for p in pairs for arg in p])\n\n    elif isinstance(obj, list):\n        return func.jsonb_build_array(*[build_jsonb(v) for v in obj])\n\n    else:\n        return obj\n</code></pre>\n\n<p>and then use it in the original query:</p>\n\n<pre><code>session.query(A_Table) \\\n    .filter(\n        B_Table.json_record.contains(\n            build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n\n<p>If you wish to use the explicit <code>JOIN</code> syntax:</p>\n\n<pre><code>session.query(A_Table).\\\n    join(B_Table, B_Table.json_record.contains(\n        build_jsonb({'path': {'to': A_Table.name}})))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I have a query using several Postgres-specific operations:</p>\n\n<pre> </pre>\n\n<p>My understanding is that the   operator acts as a comparison, but the comparison methods for   in the SQLAlchemy docs reference only <strong>keys</strong>, not <strong>values</strong>.</p>\n\n<p><a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator</a></p>\n\n<p>I'm not clear on how I could design this query through SQLAlchemy short of using a raw query.</p>\n\n<h2>Edit 1</h2>\n\n<p><a href=\"https://stackoverflow.com/a/39467149/1886901\">Based on this answer</a>, I gave the below a try.</p>\n\n<pre> </pre>\n\n<p>It, however, resulted in an error from the line  :</p>\n\n<pre> </pre>\n\n<p>So I instead attempted</p>\n\n<pre> </pre>\n\n<p>Which at least resulted in a different error, this one with some SQL generation from SQLAlchemy:</p>\n\n<pre> </pre>\n\n<p>This SQL is close to what I was aiming for, and may be acceptable, but the example provided in the answer presumes I know the value ahead of time, when what I want to do is a comparison against the row value. I typically would do:</p>\n\n<pre> </pre>\n\n<p>But I'm also trying to leverage an optimization from a   index on this   column, which leaves me needing the   operator.</p>\n\n<h2>Edit 2</h2>\n\n<p>Based on the answer from Ilja Everil\u00e4, I was able to track down the SQLAlchemy method <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/_modules/sqlalchemy_utils/functions/database.html#json_sql\" rel=\"nofollow noreferrer\">implemented in the source code</a>, and <a href=\"https://sqlalchemy-utils.readthedocs.io/en/latest/database_helpers.html#json-sql\" rel=\"nofollow noreferrer\">using the   method</a> was able to get the SQL almost there.</p>\n\n<pre> </pre>\n\n<p>Giving me the SQL:</p>\n\n<pre> </pre>\n\n<p>The problem with this output is that instead of:</p>\n\n<pre> </pre>\n\n<p>Valid Postgres syntax should be:</p>\n\n<pre> </pre>\n\n<p>Both the answer's and the source code's approach <a href=\"https://docs.sqlalchemy.org/en/latest/core/sqlelement.html?highlight=_functiongenerator\" rel=\"nofollow noreferrer\">relies on the  </a>, which can build the function, but it's not clear how something can get appended to the end of the method during the   when going that route.</p>\n\n<h2>Edit 3</h2>\n\n<p>NVM - the answer's author pointed out   would fit with this model w/o the flag.</p>\n", "answer_wo_code": "<p>The linked Q/A handles the case of using literal values, as you've noticed. The solution is to combine using   in SQLA and   in Postgresql as you were trying before:</p>\n\n<pre> </pre>\n\n<blockquote>\n  <p>My understanding is that the   operator acts as a comparison, but the comparison methods for   in the SQLAlchemy docs reference only keys, not values.</p>\n</blockquote>\n\n<p>The SQLAlchemy documentation for <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#sqlalchemy.dialects.postgresql.JSONB.Comparator.contains\" rel=\"nofollow noreferrer\"> </a> seems to be a bit poorly written. Compare</p>\n\n<blockquote>\n  <p>Boolean expression. Test if keys (or array) are a superset of/contained the keys of the argument jsonb expression.</p>\n</blockquote>\n\n<p>to Postgresql documentation for  :</p>\n\n<blockquote>\n  <p>Does the left JSON value contain the right JSON path/value entries at the top level?</p>\n</blockquote>\n\n<hr>\n\n<p>You could hide the details of building   in a helper function:</p>\n\n<pre> </pre>\n\n<p>and then use it in the original query:</p>\n\n<pre> </pre>\n\n<p>If you wish to use the explicit   syntax:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sklearn.tree.export_graphviz"}, "class_func_label": {"type": "literal", "value": "sklearn.tree.export_graphviz"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nExport a decision tree in DOT format.\n\nThis function generates a GraphViz representation of the decision tree,\nwhich is then written into `out_file`. Once exported, graphical renderings\ncan be generated using, for example::\n\n    $ dot -Tps tree.dot -o tree.ps      (PostScript format)\n    $ dot -Tpng tree.dot -o tree.png    (PNG format)\n\nThe sample counts that are shown are weighted with any sample_weights that\nmight be present.\n\nRead more in the :ref:`User Guide <tree>`.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/18495850"}, "title": {"type": "literal", "value": "Can you extract scoring algorithm from Scikit-learn RandomForestClassifier and Load coefficients into Oracle?"}, "content": {"type": "literal", "value": "<p>I have run a RandomForestClassifier model in Python using the sklearn module. I saved the model in a pickle file. I then extract data from Oracle, save it as a .csv file, send this .csv file to a machine that can open the model's pickle file in Python, and score the data. Once the data is scored I send the results back to Oracle.</p>\n\n<p>Is it possible to extract the scoring coefficients from the RandomForestClassifier(.predict_proba) function so I can load that data into Oracle and score the data solely inside of Oracle? </p>\n\n<p>After reading the documentation, it appears the scoring algorithm is too complex to perform the above suggestion given that it has to push each new record through each tree before it can arrive at a final scored probability. Is this correct?</p>\n\n<p>I appreciate your help in advance.</p>\n\n<p>Matt</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I was in a situation where I had to run a random forest model on an Oracle database. It's possible to generate a PL/SQL package that performs the same functions as the Python Sk-learn RF model.</p>\n\n<p>This is pretty trivial to do once you have something like Daniele's answer from <a href=\"https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree\">this SO</a></p>\n\n<p>First you have this file: <strong>rforest_to_plsql.py</strong></p>\n\n<pre><code>def t(n):\n    return \" \" * 4 * n\n\ndef get_est_code(tree, feature_names):\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    value = tree.tree_.value\n    def recurse(left, right, threshold, features, node, depth, code):\n        if (threshold[node] != -2):\n            code += t(depth) + \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) then\\n\"\n            depth += 1\n            if left[node] != -1:\n                code = recurse (left, right, threshold, features,left[node], depth, code)                 \n            code += t(depth - 1) + \"else\\n\"\n            if right[node] != -1:\n                code = recurse (left, right, threshold, features,right[node], depth, code)\n            code += t(depth - 1) + \"end if;\\n\"\n            depth -= 1\n        else:\n            code +=  t(depth) + \"return two_values(\" + str(value[node][0][0]) + \", \" + str(value[node][0][1]) + \");\\n\"\n        return code\n    return recurse(left, right, threshold, features, 0, 2, \"\")\n\n\ndef get_pkg_header_code(clf, feature_names):\n    pkg_h_code = \"\"\"create or replace package pkg_rforest_model as\n    function predict_proba (\\n\"\"\"\n    for feat in feature_names:\n        pkg_h_code += t(2) + feat + \"   number,\\n\"\n    pkg_h_code = pkg_h_code[:-2] + \")  return number;\\n\"\n    pkg_h_code += \"end pkg_rforest_model;\"\n    return pkg_h_code\n\ndef get_pkg_body_code(clf, feature_names):\n    pkg_b_code = \"create or replace package body pkg_rforest_model as\\n\"        \n    #code for each estimator\n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(1) + \"function \" + func_name + \" (\\n\"\n        for feat in feature_names:\n            pkg_b_code += t(2) + feat + \"   number,\\n\"\n        pkg_b_code = pkg_b_code[:-2] + \") return two_values as\\n    begin\\n\"\n        pkg_b_code += get_est_code(clf.estimators_[index], [\"f\" + str(i) for i in range(7)])\n        pkg_b_code += \"    end \" + func_name + \";\\n\"\n    #this function calls all each estimator function and returns a weighted probability\n    pkg_b_code += \"    function predict_proba (\\n\"\n    for feat in feature_names:\n        pkg_b_code += t(2) + feat + \"   number,\\n\"\n    pkg_b_code = pkg_b_code[:-2] + \")  return number as\\n    v_prob    number;\\n\"    \n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_a number;\\n\"\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_b number;\\n\"\n        pkg_b_code += t(2) + \"pr_est_\" + str(index).zfill(3) + \" number;\\n\"\n\n    pkg_b_code += t(1) + \"begin\\n\"    \n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_a := \" + func_name+ \"(\" + \", \".join(feature_names) + \").a;\\n\"\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_b := \" + func_name+ \"(\" + \", \".join(feature_names) + \").b;\\n\"\n        pkg_b_code += t(2) + \"pr_est_\" + str(index).zfill(3) + \" := v_\" + func_name + \"_a / ( v_\" + \\\n                      func_name + \"_a + v_\" + func_name + \"_b);\\n\"\n    pkg_b_code += t(2) + \"return  (\"\n    for index, estimator in enumerate(clf.estimators_):\n        pkg_b_code += \"pr_est_\" + str(index).zfill(3) + \" + \"\n    pkg_b_code = pkg_b_code[:-2] + \") / \" + str(len(clf.estimators_)) + \";\\n\"\n    pkg_b_code += t(1) + \"end predict_proba;\\n\"   \n    pkg_b_code += \"end pkg_rforest_model;\"\n    return pkg_b_code\n</code></pre>\n\n<p>Then you train your model, and get the PL/SQL code back the file's functions:</p>\n\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nimport rforest_to_plsql\nn_features = 4\nX, y = make_classification(n_samples=1000, n_features=n_features,\n                            n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\nfeatures = [\"f\" + str(i) for i in range(n_features)]\npkg_h_code = rforest_to_plsql.get_pkg_header_code(clf, features)\npkg_b_code = rforest_to_plsql.get_pkg_body_code(clf, features)\nprint pkg_h_code\nprint pkg_b_code\n</code></pre>\n\n<p>Once you've created that package on the database you can do something like:</p>\n\n<pre><code>select pkg_rforest_model.predict_proba(0.513889 , 0.511111 , 0.491667 ,  0)\nfrom   dual;\n</code></pre>\n\n<p>This is pure PL/SQL and should run very quickly. If you have a very big RF, then you could compile the package natively for more performance. Be warned - the package could be 10s of 1000s of LOC. </p>\n\n\n<p>AFAIK there is no ready-made tool to do so but you can read the Cython source code of the base <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.py\" rel=\"nofollow\">decision tree class</a>, in particular the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L1584\" rel=\"nofollow\">predict</a> method to understand how the prediction works from the fitted parameters of the Decision Tree model. The random forest prediction treats individual tree predictions as binary probabilities (0 or 1), average them and normalize them as <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py#L484\" rel=\"nofollow\">written here</a>.</p>\n\n<p>Turning that into PL/SQL might not be trivial though. Apparently Oracle Data Mining has some support for <a href=\"http://docs.oracle.com/cd/E11882_01/datamine.112/e16807/schemaobjs.htm#DMADM010\" rel=\"nofollow\">PMML Import/Export of decision tree models</a> among other models. Unfortunately I am not aware of any implementation of a <a href=\"http://en.wikipedia.org/wiki/Predictive_Model_Markup_Language\" rel=\"nofollow\">PMML</a> exporter for scikit-learn decision tree either (although it could be easier to write by taking <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\" rel=\"nofollow\">source code</a> of the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\" rel=\"nofollow\">graphviz tree exporter</a> as an example for instance).</p>\n\n<p>Also note that under PostgreSQL on the other hand you could directly use scikit-learn in a DB function written using <a href=\"http://www.postgresql.org/docs/9.3/static/plpython.html\" rel=\"nofollow\">PL/Python</a>.</p>\n\n\n<p>Here is how you would do it using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre><code>from skompiler import skompile\nexpr = skompile(gbr.predict)\n\nskompile(rf.predict_proba).to('sqlalchemy/oracle')\n</code></pre>\n\n<p>It might not be the most efficient way for evaluating a RF classifier, of course - for large forests, the generated query may easily reach megabytes in size. </p>\n\n<p>NB: If your forest has more than a hundred estimators, you may also need to increase the system recursion limit to compile it:</p>\n\n<pre><code>import sys\nsys.setrecursionlimit(10000)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>I was in a situation where I had to run a random forest model on an Oracle database. It's possible to generate a PL/SQL package that performs the same functions as the Python Sk-learn RF model.</p>\n\n<p>This is pretty trivial to do once you have something like Daniele's answer from <a href=\"https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree\">this SO</a></p>\n\n<p>First you have this file: <strong>rforest_to_plsql.py</strong></p>\n\n<pre><code>def t(n):\n    return \" \" * 4 * n\n\ndef get_est_code(tree, feature_names):\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    value = tree.tree_.value\n    def recurse(left, right, threshold, features, node, depth, code):\n        if (threshold[node] != -2):\n            code += t(depth) + \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) then\\n\"\n            depth += 1\n            if left[node] != -1:\n                code = recurse (left, right, threshold, features,left[node], depth, code)                 \n            code += t(depth - 1) + \"else\\n\"\n            if right[node] != -1:\n                code = recurse (left, right, threshold, features,right[node], depth, code)\n            code += t(depth - 1) + \"end if;\\n\"\n            depth -= 1\n        else:\n            code +=  t(depth) + \"return two_values(\" + str(value[node][0][0]) + \", \" + str(value[node][0][1]) + \");\\n\"\n        return code\n    return recurse(left, right, threshold, features, 0, 2, \"\")\n\n\ndef get_pkg_header_code(clf, feature_names):\n    pkg_h_code = \"\"\"create or replace package pkg_rforest_model as\n    function predict_proba (\\n\"\"\"\n    for feat in feature_names:\n        pkg_h_code += t(2) + feat + \"   number,\\n\"\n    pkg_h_code = pkg_h_code[:-2] + \")  return number;\\n\"\n    pkg_h_code += \"end pkg_rforest_model;\"\n    return pkg_h_code\n\ndef get_pkg_body_code(clf, feature_names):\n    pkg_b_code = \"create or replace package body pkg_rforest_model as\\n\"        \n    #code for each estimator\n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(1) + \"function \" + func_name + \" (\\n\"\n        for feat in feature_names:\n            pkg_b_code += t(2) + feat + \"   number,\\n\"\n        pkg_b_code = pkg_b_code[:-2] + \") return two_values as\\n    begin\\n\"\n        pkg_b_code += get_est_code(clf.estimators_[index], [\"f\" + str(i) for i in range(7)])\n        pkg_b_code += \"    end \" + func_name + \";\\n\"\n    #this function calls all each estimator function and returns a weighted probability\n    pkg_b_code += \"    function predict_proba (\\n\"\n    for feat in feature_names:\n        pkg_b_code += t(2) + feat + \"   number,\\n\"\n    pkg_b_code = pkg_b_code[:-2] + \")  return number as\\n    v_prob    number;\\n\"    \n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_a number;\\n\"\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_b number;\\n\"\n        pkg_b_code += t(2) + \"pr_est_\" + str(index).zfill(3) + \" number;\\n\"\n\n    pkg_b_code += t(1) + \"begin\\n\"    \n    for index, estimator in enumerate(clf.estimators_):\n        func_name = \"f_est_\" + str(index).zfill(3)\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_a := \" + func_name+ \"(\" + \", \".join(feature_names) + \").a;\\n\"\n        pkg_b_code += t(2) + \"v_\" + func_name + \"_b := \" + func_name+ \"(\" + \", \".join(feature_names) + \").b;\\n\"\n        pkg_b_code += t(2) + \"pr_est_\" + str(index).zfill(3) + \" := v_\" + func_name + \"_a / ( v_\" + \\\n                      func_name + \"_a + v_\" + func_name + \"_b);\\n\"\n    pkg_b_code += t(2) + \"return  (\"\n    for index, estimator in enumerate(clf.estimators_):\n        pkg_b_code += \"pr_est_\" + str(index).zfill(3) + \" + \"\n    pkg_b_code = pkg_b_code[:-2] + \") / \" + str(len(clf.estimators_)) + \";\\n\"\n    pkg_b_code += t(1) + \"end predict_proba;\\n\"   \n    pkg_b_code += \"end pkg_rforest_model;\"\n    return pkg_b_code\n</code></pre>\n\n<p>Then you train your model, and get the PL/SQL code back the file's functions:</p>\n\n<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nimport rforest_to_plsql\nn_features = 4\nX, y = make_classification(n_samples=1000, n_features=n_features,\n                            n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\nfeatures = [\"f\" + str(i) for i in range(n_features)]\npkg_h_code = rforest_to_plsql.get_pkg_header_code(clf, features)\npkg_b_code = rforest_to_plsql.get_pkg_body_code(clf, features)\nprint pkg_h_code\nprint pkg_b_code\n</code></pre>\n\n<p>Once you've created that package on the database you can do something like:</p>\n\n<pre><code>select pkg_rforest_model.predict_proba(0.513889 , 0.511111 , 0.491667 ,  0)\nfrom   dual;\n</code></pre>\n\n<p>This is pure PL/SQL and should run very quickly. If you have a very big RF, then you could compile the package natively for more performance. Be warned - the package could be 10s of 1000s of LOC. </p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>AFAIK there is no ready-made tool to do so but you can read the Cython source code of the base <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.py\" rel=\"nofollow\">decision tree class</a>, in particular the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L1584\" rel=\"nofollow\">predict</a> method to understand how the prediction works from the fitted parameters of the Decision Tree model. The random forest prediction treats individual tree predictions as binary probabilities (0 or 1), average them and normalize them as <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py#L484\" rel=\"nofollow\">written here</a>.</p>\n\n<p>Turning that into PL/SQL might not be trivial though. Apparently Oracle Data Mining has some support for <a href=\"http://docs.oracle.com/cd/E11882_01/datamine.112/e16807/schemaobjs.htm#DMADM010\" rel=\"nofollow\">PMML Import/Export of decision tree models</a> among other models. Unfortunately I am not aware of any implementation of a <a href=\"http://en.wikipedia.org/wiki/Predictive_Model_Markup_Language\" rel=\"nofollow\">PMML</a> exporter for scikit-learn decision tree either (although it could be easier to write by taking <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\" rel=\"nofollow\">source code</a> of the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\" rel=\"nofollow\">graphviz tree exporter</a> as an example for instance).</p>\n\n<p>Also note that under PostgreSQL on the other hand you could directly use scikit-learn in a DB function written using <a href=\"http://www.postgresql.org/docs/9.3/static/plpython.html\" rel=\"nofollow\">PL/Python</a>.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Here is how you would do it using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre><code>from skompiler import skompile\nexpr = skompile(gbr.predict)\n\nskompile(rf.predict_proba).to('sqlalchemy/oracle')\n</code></pre>\n\n<p>It might not be the most efficient way for evaluating a RF classifier, of course - for large forests, the generated query may easily reach megabytes in size. </p>\n\n<p>NB: If your forest has more than a hundred estimators, you may also need to increase the system recursion limit to compile it:</p>\n\n<pre><code>import sys\nsys.setrecursionlimit(10000)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>I have run a RandomForestClassifier model in Python using the sklearn module. I saved the model in a pickle file. I then extract data from Oracle, save it as a .csv file, send this .csv file to a machine that can open the model's pickle file in Python, and score the data. Once the data is scored I send the results back to Oracle.</p>\n\n<p>Is it possible to extract the scoring coefficients from the RandomForestClassifier(.predict_proba) function so I can load that data into Oracle and score the data solely inside of Oracle? </p>\n\n<p>After reading the documentation, it appears the scoring algorithm is too complex to perform the above suggestion given that it has to push each new record through each tree before it can arrive at a final scored probability. Is this correct?</p>\n\n<p>I appreciate your help in advance.</p>\n\n<p>Matt</p>\n", "answer_wo_code": "<p>I was in a situation where I had to run a random forest model on an Oracle database. It's possible to generate a PL/SQL package that performs the same functions as the Python Sk-learn RF model.</p>\n\n<p>This is pretty trivial to do once you have something like Daniele's answer from <a href=\"https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree\">this SO</a></p>\n\n<p>First you have this file: <strong>rforest_to_plsql.py</strong></p>\n\n<pre> </pre>\n\n<p>Then you train your model, and get the PL/SQL code back the file's functions:</p>\n\n<pre> </pre>\n\n<p>Once you've created that package on the database you can do something like:</p>\n\n<pre> </pre>\n\n<p>This is pure PL/SQL and should run very quickly. If you have a very big RF, then you could compile the package natively for more performance. Be warned - the package could be 10s of 1000s of LOC. </p>\n\n\n<p>AFAIK there is no ready-made tool to do so but you can read the Cython source code of the base <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.py\" rel=\"nofollow\">decision tree class</a>, in particular the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L1584\" rel=\"nofollow\">predict</a> method to understand how the prediction works from the fitted parameters of the Decision Tree model. The random forest prediction treats individual tree predictions as binary probabilities (0 or 1), average them and normalize them as <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py#L484\" rel=\"nofollow\">written here</a>.</p>\n\n<p>Turning that into PL/SQL might not be trivial though. Apparently Oracle Data Mining has some support for <a href=\"http://docs.oracle.com/cd/E11882_01/datamine.112/e16807/schemaobjs.htm#DMADM010\" rel=\"nofollow\">PMML Import/Export of decision tree models</a> among other models. Unfortunately I am not aware of any implementation of a <a href=\"http://en.wikipedia.org/wiki/Predictive_Model_Markup_Language\" rel=\"nofollow\">PMML</a> exporter for scikit-learn decision tree either (although it could be easier to write by taking <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\" rel=\"nofollow\">source code</a> of the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\" rel=\"nofollow\">graphviz tree exporter</a> as an example for instance).</p>\n\n<p>Also note that under PostgreSQL on the other hand you could directly use scikit-learn in a DB function written using <a href=\"http://www.postgresql.org/docs/9.3/static/plpython.html\" rel=\"nofollow\">PL/Python</a>.</p>\n\n\n<p>Here is how you would do it using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre> </pre>\n\n<p>It might not be the most efficient way for evaluating a RF classifier, of course - for large forests, the generated query may easily reach megabytes in size. </p>\n\n<p>NB: If your forest has more than a hundred estimators, you may also need to increase the system recursion limit to compile it:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.types.Unicode"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.types.Unicode"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "A variable length Unicode string type.\n\n    The :class:`.Unicode` type is a :class:`.String` subclass\n    that assumes input and output as Python ``unicode`` data,\n    and in that regard is equivalent to the usage of the\n    ``convert_unicode`` flag with the :class:`.String` type.\n    However, unlike plain :class:`.String`, it also implies an\n    underlying column type that is explicitly supporting of non-ASCII\n    data, such as ``NVARCHAR`` on Oracle and SQL Server.\n    This can impact the output of ``CREATE TABLE`` statements\n    and ``CAST`` functions at the dialect level, and can\n    also affect the handling of bound parameters in some\n    specific DBAPI scenarios.\n\n    The encoding used by the :class:`.Unicode` type is usually\n    determined by the DBAPI itself; most modern DBAPIs\n    feature support for Python ``unicode`` objects as bound\n    values and result set values, and the encoding should\n    be configured as detailed in the notes for the target\n    DBAPI in the :ref:`dialect_toplevel` section.\n\n    For those DBAPIs which do not support, or are not configured\n    to accommodate Python ``unicode`` objects\n    directly, SQLAlchemy does the encoding and decoding\n    outside of the DBAPI.   The encoding in this scenario\n    is determined by the ``encoding`` flag passed to\n    :func:`.create_engine`.\n\n    When using the :class:`.Unicode` type, it is only appropriate\n    to pass Python ``unicode`` objects, and not plain ``str``.\n    If a plain ``str`` is passed under Python 2, a warning\n    is emitted.  If you notice your application emitting these warnings but\n    you're not sure of the source of them, the Python\n    ``warnings`` filter, documented at\n    http://docs.python.org/library/warnings.html,\n    can be used to turn these warnings into exceptions\n    which will illustrate a stack trace::\n\n      import warnings\n      warnings.simplefilter('error')\n\n    For an application that wishes to pass plain bytestrings\n    and Python ``unicode`` objects to the ``Unicode`` type\n    equally, the bytestrings must first be decoded into\n    unicode.  The recipe at :ref:`coerce_to_unicode` illustrates\n    how this is done.\n\n    .. seealso::\n\n        :class:`.UnicodeText` - unlengthed textual counterpart\n        to :class:`.Unicode`.\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37878257"}, "title": {"type": "literal", "value": "Can't get SQLAlchemy func.lower to work with MySQL"}, "content": {"type": "literal", "value": "<p>I am doing some silly mistake with the syntax but cant figure out what.</p>\n\n<p>I am trying to use <strong>sqlalchemy</strong>    <code>func</code> to compare a DB field and a variable by turning them both in lower cases, but the output comes null as if the condition isn\u2019t satisfying. </p>\n\n<p>The same thing works if I don\u2019t use <code>func</code> and pass a static variable.</p>\n\n<p>The Code:</p>\n\n<pre><code>question = \"Hey\"\nq1 = QuestionsAndAnswers.query.filter(func.lower(QuestionsAndAnswers.question) == func.lower(question)).all()\nq2 = QuestionsAndAnswers.query.filter(QuestionsAndAnswers.question == \"Hey\").all()\nprint \"q1\", q1\nprint \"q2\", q2\n</code></pre>\n\n<p>The Output:</p>\n\n<pre><code>q1 []\nq2 [&lt;intercom_bot.models.QuestionsAndAnswers object at 0x7f1e2c7add50&gt;]\n</code></pre>\n\n<p>The DB:</p>\n\n<pre><code>+----+-----------------+--------------------------------------------------+------------+\n| id | question        | answer                                           | created_at |\n+----+-----------------+--------------------------------------------------+------------+\n|  1 | Hi              | Hey, Here I am and here you are. How can I help? | NULL       |\n|  2 | Hello           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  3 | Helo            | Hey, Here I am and here you are. How can I help? | NULL       |\n|  4 | Heelo           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  5 | Hell            | Hey, Here I am and here you are. How can I help? | NULL       |\n|  6 | Hallo           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  7 | Hey             | Hey, Here I am and here you are. How can I help? | NULL       |\n|  8 | He              | Hey, Here I am and here you are. How can I help? | NULL       |\n|  9 | Ho              | Hey, Here I am and here you are. How can I help? | NULL       |\n| 10 | I need help     | Hey, Here I am and here you are. How can I help? | NULL       |\n| 11 | Help            | Hey, Here I am and here you are. How can I help? | NULL       |\n| 12 | can you help me | Hey, Here I am and here you are. How can I help? | NULL       |\n| 13 | Greetings       | Hey, Here I am and here you are. How can I help? | NULL       |\n+----+-----------------+--------------------------------------------------+------------+\n</code></pre>\n\n<p>PS: </p>\n\n<pre><code>print QuestionsAndAnswers.query.filter(func.lower(QuestionsAndAnswers.question) == func.lower(question))\n</code></pre>\n\n<p>gives this:</p>\n\n<pre><code> SELECT questions_and_answers.id AS questions_and_answers_id, questions_and_answers.question AS questions_and_answers_question, questions_and_answers.answer AS questions_and_answers_answer, questions_and_answers.created_at AS questions_and_answers_created_at \nFROM questions_and_answers \nWHERE lower(questions_and_answers.question) = lower(:lower_1)\n</code></pre>\n\n<p>PPS: Here's the model.</p>\n\n<pre><code>class QuestionsAndAnswers(Base):\n    \"\"\"docstring for QuestionsAndAnswers\"\"\"\n    __tablename__ = 'questions_and_answers'\n    id = Column(BIGINT, primary_key=True)\n    question = Column(MEDIUMBLOB, nullable=False)\n    answer = Column(MEDIUMBLOB, nullable=False)\n    created_at = Column(DATETIME, nullable=True,\n                        default=datetime.datetime.now())\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>The issue is that the MySQL functions <code>LOWER()</code> and <code>UPPER()</code> <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre><code>question = Column(MEDIUMBLOB, nullable=False)\n</code></pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre><code>func.lower(QuestionsAndAnswers.question)\n</code></pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"><code>Unicode</code></a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"><code>UnicodeText</code></a>, but a quick solution would be to add a cast:</p>\n\n<pre><code>from sqlalchemy import Unicode, cast\n\nfunc.lower(cast(QuestionsAndAnswers.question, Unicode))\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>The issue is that the MySQL functions <code>LOWER()</code> and <code>UPPER()</code> <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre><code>question = Column(MEDIUMBLOB, nullable=False)\n</code></pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre><code>func.lower(QuestionsAndAnswers.question)\n</code></pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"><code>Unicode</code></a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"><code>UnicodeText</code></a>, but a quick solution would be to add a cast:</p>\n\n<pre><code>from sqlalchemy import Unicode, cast\n\nfunc.lower(cast(QuestionsAndAnswers.question, Unicode))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am doing some silly mistake with the syntax but cant figure out what.</p>\n\n<p>I am trying to use <strong>sqlalchemy</strong>      to compare a DB field and a variable by turning them both in lower cases, but the output comes null as if the condition isn\u2019t satisfying. </p>\n\n<p>The same thing works if I don\u2019t use   and pass a static variable.</p>\n\n<p>The Code:</p>\n\n<pre> </pre>\n\n<p>The Output:</p>\n\n<pre> </pre>\n\n<p>The DB:</p>\n\n<pre> </pre>\n\n<p>PS: </p>\n\n<pre> </pre>\n\n<p>gives this:</p>\n\n<pre> </pre>\n\n<p>PPS: Here's the model.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>The issue is that the MySQL functions   and   <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre> </pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre> </pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"> </a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"> </a>, but a quick solution would be to add a cast:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.types.UnicodeText"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.types.UnicodeText"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "An unbounded-length Unicode string type.\n\n    See :class:`.Unicode` for details on the unicode\n    behavior of this object.\n\n    Like :class:`.Unicode`, usage the :class:`.UnicodeText` type implies a\n    unicode-capable type being used on the backend, such as\n    ``NCLOB``, ``NTEXT``.\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37878257"}, "title": {"type": "literal", "value": "Can't get SQLAlchemy func.lower to work with MySQL"}, "content": {"type": "literal", "value": "<p>I am doing some silly mistake with the syntax but cant figure out what.</p>\n\n<p>I am trying to use <strong>sqlalchemy</strong>    <code>func</code> to compare a DB field and a variable by turning them both in lower cases, but the output comes null as if the condition isn\u2019t satisfying. </p>\n\n<p>The same thing works if I don\u2019t use <code>func</code> and pass a static variable.</p>\n\n<p>The Code:</p>\n\n<pre><code>question = \"Hey\"\nq1 = QuestionsAndAnswers.query.filter(func.lower(QuestionsAndAnswers.question) == func.lower(question)).all()\nq2 = QuestionsAndAnswers.query.filter(QuestionsAndAnswers.question == \"Hey\").all()\nprint \"q1\", q1\nprint \"q2\", q2\n</code></pre>\n\n<p>The Output:</p>\n\n<pre><code>q1 []\nq2 [&lt;intercom_bot.models.QuestionsAndAnswers object at 0x7f1e2c7add50&gt;]\n</code></pre>\n\n<p>The DB:</p>\n\n<pre><code>+----+-----------------+--------------------------------------------------+------------+\n| id | question        | answer                                           | created_at |\n+----+-----------------+--------------------------------------------------+------------+\n|  1 | Hi              | Hey, Here I am and here you are. How can I help? | NULL       |\n|  2 | Hello           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  3 | Helo            | Hey, Here I am and here you are. How can I help? | NULL       |\n|  4 | Heelo           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  5 | Hell            | Hey, Here I am and here you are. How can I help? | NULL       |\n|  6 | Hallo           | Hey, Here I am and here you are. How can I help? | NULL       |\n|  7 | Hey             | Hey, Here I am and here you are. How can I help? | NULL       |\n|  8 | He              | Hey, Here I am and here you are. How can I help? | NULL       |\n|  9 | Ho              | Hey, Here I am and here you are. How can I help? | NULL       |\n| 10 | I need help     | Hey, Here I am and here you are. How can I help? | NULL       |\n| 11 | Help            | Hey, Here I am and here you are. How can I help? | NULL       |\n| 12 | can you help me | Hey, Here I am and here you are. How can I help? | NULL       |\n| 13 | Greetings       | Hey, Here I am and here you are. How can I help? | NULL       |\n+----+-----------------+--------------------------------------------------+------------+\n</code></pre>\n\n<p>PS: </p>\n\n<pre><code>print QuestionsAndAnswers.query.filter(func.lower(QuestionsAndAnswers.question) == func.lower(question))\n</code></pre>\n\n<p>gives this:</p>\n\n<pre><code> SELECT questions_and_answers.id AS questions_and_answers_id, questions_and_answers.question AS questions_and_answers_question, questions_and_answers.answer AS questions_and_answers_answer, questions_and_answers.created_at AS questions_and_answers_created_at \nFROM questions_and_answers \nWHERE lower(questions_and_answers.question) = lower(:lower_1)\n</code></pre>\n\n<p>PPS: Here's the model.</p>\n\n<pre><code>class QuestionsAndAnswers(Base):\n    \"\"\"docstring for QuestionsAndAnswers\"\"\"\n    __tablename__ = 'questions_and_answers'\n    id = Column(BIGINT, primary_key=True)\n    question = Column(MEDIUMBLOB, nullable=False)\n    answer = Column(MEDIUMBLOB, nullable=False)\n    created_at = Column(DATETIME, nullable=True,\n                        default=datetime.datetime.now())\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>The issue is that the MySQL functions <code>LOWER()</code> and <code>UPPER()</code> <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre><code>question = Column(MEDIUMBLOB, nullable=False)\n</code></pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre><code>func.lower(QuestionsAndAnswers.question)\n</code></pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"><code>Unicode</code></a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"><code>UnicodeText</code></a>, but a quick solution would be to add a cast:</p>\n\n<pre><code>from sqlalchemy import Unicode, cast\n\nfunc.lower(cast(QuestionsAndAnswers.question, Unicode))\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>The issue is that the MySQL functions <code>LOWER()</code> and <code>UPPER()</code> <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre><code>question = Column(MEDIUMBLOB, nullable=False)\n</code></pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre><code>func.lower(QuestionsAndAnswers.question)\n</code></pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"><code>Unicode</code></a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"><code>UnicodeText</code></a>, but a quick solution would be to add a cast:</p>\n\n<pre><code>from sqlalchemy import Unicode, cast\n\nfunc.lower(cast(QuestionsAndAnswers.question, Unicode))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I am doing some silly mistake with the syntax but cant figure out what.</p>\n\n<p>I am trying to use <strong>sqlalchemy</strong>      to compare a DB field and a variable by turning them both in lower cases, but the output comes null as if the condition isn\u2019t satisfying. </p>\n\n<p>The same thing works if I don\u2019t use   and pass a static variable.</p>\n\n<p>The Code:</p>\n\n<pre> </pre>\n\n<p>The Output:</p>\n\n<pre> </pre>\n\n<p>The DB:</p>\n\n<pre> </pre>\n\n<p>PS: </p>\n\n<pre> </pre>\n\n<p>gives this:</p>\n\n<pre> </pre>\n\n<p>PPS: Here's the model.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>The issue is that the MySQL functions   and   <a href=\"http://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lower\" rel=\"nofollow\">cannot handle binary string types</a> and just return the binary string unmodified.</p>\n\n<p>As the column in question</p>\n\n<pre> </pre>\n\n<p>is a binary string type, the function application</p>\n\n<pre> </pre>\n\n<p>is a no-operation and will return the binary string as is. This'll mean that the comparison will be between \"Hey\" and \"hey\" and the predicate will not match.</p>\n\n<p>The proper fix would be to alter the model, table and the data to use proper text types, such as <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.Unicode\" rel=\"nofollow\"> </a> and <a href=\"http://docs.sqlalchemy.org/en/latest/core/type_basics.html#sqlalchemy.types.UnicodeText\" rel=\"nofollow\"> </a>, but a quick solution would be to add a cast:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pickle.dump"}, "class_func_label": {"type": "literal", "value": "pickle.dump"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "Write a pickled representation of obj to the open file object file.\n\nThis is equivalent to ``Pickler(file, protocol).dump(obj)``, but may\nbe more efficient.\n\nThe optional *protocol* argument tells the pickler to use the given\nprotocol supported protocols are 0, 1, 2, 3 and 4.  The default\nprotocol is 3; a backward-incompatible protocol designed for Python 3.\n\nSpecifying a negative protocol version selects the highest protocol\nversion supported.  The higher the protocol used, the more recent the\nversion of Python needed to read the pickle produced.\n\nThe *file* argument must have a write() method that accepts a single\nbytes argument.  It can thus be a file object opened for binary\nwriting, an io.BytesIO instance, or any other custom object that meets\nthis interface.\n\nIf *fix_imports* is True and protocol is less than 3, pickle will try\nto map the new Python 3 names to the old module names used in Python\n2, so that the pickle data stream is readable with Python 2."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/2047814"}, "title": {"type": "literal", "value": "Is it possible to store Python class objects in SQLite?"}, "content": {"type": "literal", "value": "<p>I would like to store Python objects into a <a href=\"http://en.wikipedia.org/wiki/SQLite\" rel=\"noreferrer\">SQLite</a> database. Is that possible?</p>\n\n<p>If so what would be some links / examples for it?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n\n\n<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre><code>import pickle\n\nclass Foo:\n    attr = 'a class attr'\n\npicklestring = pickle.dumps(Foo)\n</code></pre>\n\n\n<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre><code>f=open('object.dump', 'rw')\npickle.dump(obj, f)\n</code></pre>\n\n<p>Now read <code>object.dump</code> from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n\n\n<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n\n\n<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre><code>import sqlite3\nimport pickle\n\ndef adapt_tuple(tuple):\n    return pickle.dumps(tuple)    \n\nsqlite3.register_adapter(tuple, adapt_tuple)    #cannot use pickle.dumps directly because of inadequate argument signature \nsqlite3.register_converter(\"tuple\", pickle.loads)\n\ndef collate_tuple(string1, string2):\n    return cmp(pickle.loads(string1), pickle.loads(string2))\n\n# 1) Using declared types\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES)\n\ncon.create_collation(\"cmptuple\", collate_tuple)\n\ncur = con.cursor()\ncur.execute(\"create table test(p tuple unique collate cmptuple) \")\ncur.execute(\"create index tuple_collated_index on test(p collate cmptuple)\")\n\n\n######################### Test ########################\n\ncur.execute(\"select name, type  from sqlite_master\") # where type = 'table'\")\nprint(cur.fetchall())\n\np = (1,2,3)\np1 = (1,2)\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute(\"insert into test(p) values (?)\", (p1,))\ncur.execute(\"insert into test(p) values (?)\", ((10, 1),))\ncur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\ncur.execute(\"insert into test(p) values (?)\", (((9, 5), 33) ,))\n\ntry:\n    cur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\nexcept Exception as e:\n    print e\n\ncur.execute(\"select p from test order by p\")\nprint \"\\nwith declared types and default collate on column:\"\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test order by p collate cmptuple\")\nprint \"\\nwith declared types collate:\"\nfor raw in cur:\n    print raw\n\ncon.create_function('pycmp', 2, cmp)\n\nprint \"\\nselect grater than using cmp function:\"\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((10, ),) )\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((3,)))\nfor raw in cur:\n    print raw \n\nprint \"\\nselect grater than using collate:\"\ncur.execute(\"select p from test where p &gt; ?\", ((10,),) )\nfor raw in cur:\n    print raw  \n\ncur.execute(\"explain query plan select p from test where p &gt; ?\", ((3,)))\nfor raw in cur:\n    print raw\n\ncur.close()\ncon.close()\n</code></pre>\n\n\n<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic <code>JOIN</code>s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre><code>from sqlalchemy import Column, Integer, Unicode, UnicodeText, String\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom random import choice\nfrom string import letters\n\nengine = create_engine('sqlite:////tmp/teste.db', echo=True)\nBase = declarative_base(bind=engine)\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    name = Column(Unicode(40))\n    address = Column(UnicodeText, nullable=True)\n    password = Column(String(20))\n\n    def __init__(self, name, address=None, password=None):\n        self.name = name\n        self.address = address\n        if password is None:\n            password = ''.join(choice(letters) for n in xrange(10))\n        self.password = password\n\nBase.metadata.create_all()\n\nSession = sessionmaker(bind=engine)\ns = Session()\n</code></pre>\n\n<p>Then I can use it like this:</p>\n\n<pre><code># create instances of my user object\nu = User('nosklo')\nu.address = '66 Some Street #500'\n\nu2 = User('lakshmipathi')\nu2.password = 'ihtapimhskal'\n\n# testing\ns.add_all([u, u2])\ns.commit()\n</code></pre>\n\n<p>That would run <code>INSERT</code> statements against the database.</p>\n\n<pre><code># When you query the data back it returns instances of your class:\n\nfor user in s.query(User):\n    print type(user), user.name, user.password\n</code></pre>\n\n<p>That query would run <code>SELECT users.id AS users_id, users.name AS users_name, users.address AS users_address, users.password AS users_password</code>.</p>\n\n<p>The printed result would be:</p>\n\n<pre><code>&lt;class '__main__.User'&gt; nosklo aBPDXlTPJs\n&lt;class '__main__.User'&gt; lakshmipathi ihtapimhskal\n</code></pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "answer_3": {"type": "literal", "value": "<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre><code>import pickle\n\nclass Foo:\n    attr = 'a class attr'\n\npicklestring = pickle.dumps(Foo)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "8"}, "answer_4": {"type": "literal", "value": "<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n"}, "answer_4_votes": {"type": "literal", "value": "15"}, "answer_5": {"type": "literal", "value": "<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre><code>f=open('object.dump', 'rw')\npickle.dump(obj, f)\n</code></pre>\n\n<p>Now read <code>object.dump</code> from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "2"}, "answer_6": {"type": "literal", "value": "<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "1"}, "answer_7": {"type": "literal", "value": "<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre><code>import sqlite3\nimport pickle\n\ndef adapt_tuple(tuple):\n    return pickle.dumps(tuple)    \n\nsqlite3.register_adapter(tuple, adapt_tuple)    #cannot use pickle.dumps directly because of inadequate argument signature \nsqlite3.register_converter(\"tuple\", pickle.loads)\n\ndef collate_tuple(string1, string2):\n    return cmp(pickle.loads(string1), pickle.loads(string2))\n\n# 1) Using declared types\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES)\n\ncon.create_collation(\"cmptuple\", collate_tuple)\n\ncur = con.cursor()\ncur.execute(\"create table test(p tuple unique collate cmptuple) \")\ncur.execute(\"create index tuple_collated_index on test(p collate cmptuple)\")\n\n\n######################### Test ########################\n\ncur.execute(\"select name, type  from sqlite_master\") # where type = 'table'\")\nprint(cur.fetchall())\n\np = (1,2,3)\np1 = (1,2)\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute(\"insert into test(p) values (?)\", (p1,))\ncur.execute(\"insert into test(p) values (?)\", ((10, 1),))\ncur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\ncur.execute(\"insert into test(p) values (?)\", (((9, 5), 33) ,))\n\ntry:\n    cur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\nexcept Exception as e:\n    print e\n\ncur.execute(\"select p from test order by p\")\nprint \"\\nwith declared types and default collate on column:\"\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test order by p collate cmptuple\")\nprint \"\\nwith declared types collate:\"\nfor raw in cur:\n    print raw\n\ncon.create_function('pycmp', 2, cmp)\n\nprint \"\\nselect grater than using cmp function:\"\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((10, ),) )\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((3,)))\nfor raw in cur:\n    print raw \n\nprint \"\\nselect grater than using collate:\"\ncur.execute(\"select p from test where p &gt; ?\", ((10,),) )\nfor raw in cur:\n    print raw  \n\ncur.execute(\"explain query plan select p from test where p &gt; ?\", ((3,)))\nfor raw in cur:\n    print raw\n\ncur.close()\ncon.close()\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": ""}, "answer_8": {"type": "literal", "value": "<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic <code>JOIN</code>s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre><code>from sqlalchemy import Column, Integer, Unicode, UnicodeText, String\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom random import choice\nfrom string import letters\n\nengine = create_engine('sqlite:////tmp/teste.db', echo=True)\nBase = declarative_base(bind=engine)\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    name = Column(Unicode(40))\n    address = Column(UnicodeText, nullable=True)\n    password = Column(String(20))\n\n    def __init__(self, name, address=None, password=None):\n        self.name = name\n        self.address = address\n        if password is None:\n            password = ''.join(choice(letters) for n in xrange(10))\n        self.password = password\n\nBase.metadata.create_all()\n\nSession = sessionmaker(bind=engine)\ns = Session()\n</code></pre>\n\n<p>Then I can use it like this:</p>\n\n<pre><code># create instances of my user object\nu = User('nosklo')\nu.address = '66 Some Street #500'\n\nu2 = User('lakshmipathi')\nu2.password = 'ihtapimhskal'\n\n# testing\ns.add_all([u, u2])\ns.commit()\n</code></pre>\n\n<p>That would run <code>INSERT</code> statements against the database.</p>\n\n<pre><code># When you query the data back it returns instances of your class:\n\nfor user in s.query(User):\n    print type(user), user.name, user.password\n</code></pre>\n\n<p>That query would run <code>SELECT users.id AS users_id, users.name AS users_name, users.address AS users_address, users.password AS users_password</code>.</p>\n\n<p>The printed result would be:</p>\n\n<pre><code>&lt;class '__main__.User'&gt; nosklo aBPDXlTPJs\n&lt;class '__main__.User'&gt; lakshmipathi ihtapimhskal\n</code></pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, "answer_8_votes": {"type": "literal", "value": "59"}, "content_wo_code": "<p>I would like to store Python objects into a <a href=\"http://en.wikipedia.org/wiki/SQLite\" rel=\"noreferrer\">SQLite</a> database. Is that possible?</p>\n\n<p>If so what would be some links / examples for it?</p>\n", "answer_wo_code": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n\n\n<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre> </pre>\n\n<p>Now read   from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n\n\n<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n\n\n<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre> </pre>\n\n\n<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic  s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre> </pre>\n\n<p>Then I can use it like this:</p>\n\n<pre> </pre>\n\n<p>That would run   statements against the database.</p>\n\n<pre> </pre>\n\n<p>That query would run  .</p>\n\n<p>The printed result would be:</p>\n\n<pre> </pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pickle.dumps"}, "class_func_label": {"type": "literal", "value": "pickle.dumps"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "Return the pickled representation of the object as a bytes object.\n\nThe optional *protocol* argument tells the pickler to use the given\nprotocol; supported protocols are 0, 1, 2, 3 and 4.  The default\nprotocol is 3; a backward-incompatible protocol designed for Python 3.\n\nSpecifying a negative protocol version selects the highest protocol\nversion supported.  The higher the protocol used, the more recent the\nversion of Python needed to read the pickle produced.\n\nIf *fix_imports* is True and *protocol* is less than 3, pickle will\ntry to map the new Python 3 names to the old module names used in\nPython 2, so that the pickle data stream is readable with Python 2."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/2047814"}, "title": {"type": "literal", "value": "Is it possible to store Python class objects in SQLite?"}, "content": {"type": "literal", "value": "<p>I would like to store Python objects into a <a href=\"http://en.wikipedia.org/wiki/SQLite\" rel=\"noreferrer\">SQLite</a> database. Is that possible?</p>\n\n<p>If so what would be some links / examples for it?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n\n\n<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre><code>import pickle\n\nclass Foo:\n    attr = 'a class attr'\n\npicklestring = pickle.dumps(Foo)\n</code></pre>\n\n\n<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre><code>f=open('object.dump', 'rw')\npickle.dump(obj, f)\n</code></pre>\n\n<p>Now read <code>object.dump</code> from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n\n\n<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n\n\n<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre><code>import sqlite3\nimport pickle\n\ndef adapt_tuple(tuple):\n    return pickle.dumps(tuple)    \n\nsqlite3.register_adapter(tuple, adapt_tuple)    #cannot use pickle.dumps directly because of inadequate argument signature \nsqlite3.register_converter(\"tuple\", pickle.loads)\n\ndef collate_tuple(string1, string2):\n    return cmp(pickle.loads(string1), pickle.loads(string2))\n\n# 1) Using declared types\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES)\n\ncon.create_collation(\"cmptuple\", collate_tuple)\n\ncur = con.cursor()\ncur.execute(\"create table test(p tuple unique collate cmptuple) \")\ncur.execute(\"create index tuple_collated_index on test(p collate cmptuple)\")\n\n\n######################### Test ########################\n\ncur.execute(\"select name, type  from sqlite_master\") # where type = 'table'\")\nprint(cur.fetchall())\n\np = (1,2,3)\np1 = (1,2)\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute(\"insert into test(p) values (?)\", (p1,))\ncur.execute(\"insert into test(p) values (?)\", ((10, 1),))\ncur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\ncur.execute(\"insert into test(p) values (?)\", (((9, 5), 33) ,))\n\ntry:\n    cur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\nexcept Exception as e:\n    print e\n\ncur.execute(\"select p from test order by p\")\nprint \"\\nwith declared types and default collate on column:\"\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test order by p collate cmptuple\")\nprint \"\\nwith declared types collate:\"\nfor raw in cur:\n    print raw\n\ncon.create_function('pycmp', 2, cmp)\n\nprint \"\\nselect grater than using cmp function:\"\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((10, ),) )\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((3,)))\nfor raw in cur:\n    print raw \n\nprint \"\\nselect grater than using collate:\"\ncur.execute(\"select p from test where p &gt; ?\", ((10,),) )\nfor raw in cur:\n    print raw  \n\ncur.execute(\"explain query plan select p from test where p &gt; ?\", ((3,)))\nfor raw in cur:\n    print raw\n\ncur.close()\ncon.close()\n</code></pre>\n\n\n<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic <code>JOIN</code>s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre><code>from sqlalchemy import Column, Integer, Unicode, UnicodeText, String\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom random import choice\nfrom string import letters\n\nengine = create_engine('sqlite:////tmp/teste.db', echo=True)\nBase = declarative_base(bind=engine)\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    name = Column(Unicode(40))\n    address = Column(UnicodeText, nullable=True)\n    password = Column(String(20))\n\n    def __init__(self, name, address=None, password=None):\n        self.name = name\n        self.address = address\n        if password is None:\n            password = ''.join(choice(letters) for n in xrange(10))\n        self.password = password\n\nBase.metadata.create_all()\n\nSession = sessionmaker(bind=engine)\ns = Session()\n</code></pre>\n\n<p>Then I can use it like this:</p>\n\n<pre><code># create instances of my user object\nu = User('nosklo')\nu.address = '66 Some Street #500'\n\nu2 = User('lakshmipathi')\nu2.password = 'ihtapimhskal'\n\n# testing\ns.add_all([u, u2])\ns.commit()\n</code></pre>\n\n<p>That would run <code>INSERT</code> statements against the database.</p>\n\n<pre><code># When you query the data back it returns instances of your class:\n\nfor user in s.query(User):\n    print type(user), user.name, user.password\n</code></pre>\n\n<p>That query would run <code>SELECT users.id AS users_id, users.name AS users_name, users.address AS users_address, users.password AS users_password</code>.</p>\n\n<p>The printed result would be:</p>\n\n<pre><code>&lt;class '__main__.User'&gt; nosklo aBPDXlTPJs\n&lt;class '__main__.User'&gt; lakshmipathi ihtapimhskal\n</code></pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "answer_3": {"type": "literal", "value": "<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre><code>import pickle\n\nclass Foo:\n    attr = 'a class attr'\n\npicklestring = pickle.dumps(Foo)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "8"}, "answer_4": {"type": "literal", "value": "<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n"}, "answer_4_votes": {"type": "literal", "value": "15"}, "answer_5": {"type": "literal", "value": "<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre><code>f=open('object.dump', 'rw')\npickle.dump(obj, f)\n</code></pre>\n\n<p>Now read <code>object.dump</code> from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "2"}, "answer_6": {"type": "literal", "value": "<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "1"}, "answer_7": {"type": "literal", "value": "<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre><code>import sqlite3\nimport pickle\n\ndef adapt_tuple(tuple):\n    return pickle.dumps(tuple)    \n\nsqlite3.register_adapter(tuple, adapt_tuple)    #cannot use pickle.dumps directly because of inadequate argument signature \nsqlite3.register_converter(\"tuple\", pickle.loads)\n\ndef collate_tuple(string1, string2):\n    return cmp(pickle.loads(string1), pickle.loads(string2))\n\n# 1) Using declared types\ncon = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES)\n\ncon.create_collation(\"cmptuple\", collate_tuple)\n\ncur = con.cursor()\ncur.execute(\"create table test(p tuple unique collate cmptuple) \")\ncur.execute(\"create index tuple_collated_index on test(p collate cmptuple)\")\n\n\n######################### Test ########################\n\ncur.execute(\"select name, type  from sqlite_master\") # where type = 'table'\")\nprint(cur.fetchall())\n\np = (1,2,3)\np1 = (1,2)\n\ncur.execute(\"insert into test(p) values (?)\", (p,))\ncur.execute(\"insert into test(p) values (?)\", (p1,))\ncur.execute(\"insert into test(p) values (?)\", ((10, 1),))\ncur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\ncur.execute(\"insert into test(p) values (?)\", (((9, 5), 33) ,))\n\ntry:\n    cur.execute(\"insert into test(p) values (?)\", (tuple((9, 33)) ,))\nexcept Exception as e:\n    print e\n\ncur.execute(\"select p from test order by p\")\nprint \"\\nwith declared types and default collate on column:\"\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test order by p collate cmptuple\")\nprint \"\\nwith declared types collate:\"\nfor raw in cur:\n    print raw\n\ncon.create_function('pycmp', 2, cmp)\n\nprint \"\\nselect grater than using cmp function:\"\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((10, ),) )\nfor raw in cur:\n    print raw\n\ncur.execute(\"select p from test where pycmp(p,?) &gt;= 0\", ((3,)))\nfor raw in cur:\n    print raw \n\nprint \"\\nselect grater than using collate:\"\ncur.execute(\"select p from test where p &gt; ?\", ((10,),) )\nfor raw in cur:\n    print raw  \n\ncur.execute(\"explain query plan select p from test where p &gt; ?\", ((3,)))\nfor raw in cur:\n    print raw\n\ncur.close()\ncon.close()\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": ""}, "answer_8": {"type": "literal", "value": "<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic <code>JOIN</code>s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre><code>from sqlalchemy import Column, Integer, Unicode, UnicodeText, String\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom random import choice\nfrom string import letters\n\nengine = create_engine('sqlite:////tmp/teste.db', echo=True)\nBase = declarative_base(bind=engine)\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    name = Column(Unicode(40))\n    address = Column(UnicodeText, nullable=True)\n    password = Column(String(20))\n\n    def __init__(self, name, address=None, password=None):\n        self.name = name\n        self.address = address\n        if password is None:\n            password = ''.join(choice(letters) for n in xrange(10))\n        self.password = password\n\nBase.metadata.create_all()\n\nSession = sessionmaker(bind=engine)\ns = Session()\n</code></pre>\n\n<p>Then I can use it like this:</p>\n\n<pre><code># create instances of my user object\nu = User('nosklo')\nu.address = '66 Some Street #500'\n\nu2 = User('lakshmipathi')\nu2.password = 'ihtapimhskal'\n\n# testing\ns.add_all([u, u2])\ns.commit()\n</code></pre>\n\n<p>That would run <code>INSERT</code> statements against the database.</p>\n\n<pre><code># When you query the data back it returns instances of your class:\n\nfor user in s.query(User):\n    print type(user), user.name, user.password\n</code></pre>\n\n<p>That query would run <code>SELECT users.id AS users_id, users.name AS users_name, users.address AS users_address, users.password AS users_password</code>.</p>\n\n<p>The printed result would be:</p>\n\n<pre><code>&lt;class '__main__.User'&gt; nosklo aBPDXlTPJs\n&lt;class '__main__.User'&gt; lakshmipathi ihtapimhskal\n</code></pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, "answer_8_votes": {"type": "literal", "value": "59"}, "content_wo_code": "<p>I would like to store Python objects into a <a href=\"http://en.wikipedia.org/wiki/SQLite\" rel=\"noreferrer\">SQLite</a> database. Is that possible?</p>\n\n<p>If so what would be some links / examples for it?</p>\n", "answer_wo_code": "<p>You other choice instead of pickling is to use an <strong>ORM</strong>. This lets you map rows in a database to an object. See <a href=\"http://wiki.python.org/moin/HigherLevelDatabaseProgramming\" rel=\"nofollow noreferrer\">http://wiki.python.org/moin/HigherLevelDatabaseProgramming</a> for a starting point. I'd recommend <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">SQLAlchemy</a> or <a href=\"http://www.sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject</a>.</p>\n\n\n<p>One option is to use an O/R mapper like <a href=\"http://sqlobject.org/\" rel=\"nofollow noreferrer\">SQLObject.</a>  It will do most of the plumbing to persist the Python object to a database, and it supports SQLite.  As mentioned elsewhere you can also serialise the object using a method such as pickle, which dumps out a representation of the object that it can reconstruct by reading back in and parsing.</p>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html#what-can-be-pickled-and-unpickled\" rel=\"noreferrer\">pickle.dumps</a>, its return pickable objects as strings, you would not need to write it to temporary files.</p>\n\n<blockquote>\n  <p>Return the pickled representation of\n  the object as a <strong>string</strong>, instead of\n  writing it to a file.</p>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Yes it's possible but there are different approaches and which one is the suitable one, will depend on your requirements.</p>\n\n<ul>\n<li><p><strong>Pickling</strong></p>\n\n<p>You can use the <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> module to serialize objects, then store these objects in a   blob in sqlite3 (or a textfield, if the dump is e.g. base64 encoded). Be aware of some possible problems: <a href=\"https://stackoverflow.com/questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field\">questions/198692/can-i-pickle-a-python-dictionary-into-a-sqlite3-text-field</a></p></li>\n<li><p><strong>Object-Relational-Mapping</strong></p>\n\n<p>You can use object relational mapping. This creates, in effect, a \"virtual object database\" that can be used from within the programming language (<a href=\"http://en.wikipedia.org/wiki/Object-relational_mapping\" rel=\"nofollow noreferrer\">Wikipedia</a>). For python, there is a nice toolkit for that: <a href=\"http://www.sqlalchemy.org/\" rel=\"nofollow noreferrer\">sqlalchemy</a>.</p></li>\n</ul>\n\n\n<p>You can use <a href=\"http://docs.python.org/library/pickle.html\" rel=\"nofollow noreferrer\">pickle</a> to serialize the object. The serialized object can be inserted to the sqlite DB as a bytearray field.</p>\n\n<pre> </pre>\n\n<p>Now read   from the file, and write it to the sqlite DB. You might want to write it as a binary data type; read about storing binary data and blob in SQLite <a href=\"http://code.activestate.com/recipes/252531/\" rel=\"nofollow noreferrer\">here</a>. Note that according to <a href=\"http://effbot.org/zone/sqlite-blob.htm\" rel=\"nofollow noreferrer\">this source</a>, SQLite limits the size of such datafield to 1Mb.</p>\n\n<p>I think that a better option would be serializing your object into a file, and keeping the file <em>name</em>, not contents, in the database.</p>\n\n\n<p>Depending on your exact needs, it could be worth looking into Django (www.djangoproject.com) for this task. Django is actually a web framework, but one of the tasks it handles is to allow you to define Models as python objects (inheriting from a base class provided by the framework). It will then automatically create the database tables required to store those objects, and sqlite is among the supported backends. It also provides handy functions to query the database and return one or more matching objects. See for example the documentation about Models in django: </p>\n\n<p><a href=\"http://docs.djangoproject.com/en/1.9/topics/db/models/\" rel=\"nofollow\">http://docs.djangoproject.com/en/1.9/topics/db/models/</a></p>\n\n<p>The drawback is of course that you have to install a full web framework, and (as far as I remember) you can only store objects whose attributes are supported by django. Also, it's made for storing many instances of predefined objects, not for storing one instance each of many different objects. Depending on your needs, this may or may not be impractical.</p>\n\n\n<p>There is relatively simple way to store and compare objects, eaven to index those objects right way and  to restrict (with ubique) columns containing objects. And all of that without using ORM engines. Objects mast be stored using pickle dump (so performance might be a issue) Here is example for storing python tuples, indexing restricting and comparing. This method can be easily applied to any other python class. All that is needed is explained in python sqlite3 documentation (somebody already posted the link). Anyway here it is all put together in the following example:</p>\n\n<pre> </pre>\n\n\n<p>You can't store the object itself in the DB. What you do is to store the data from the object and reconstruct it later.</p>\n\n<p>A good way is to use the excellent <a href=\"http://www.sqlalchemy.org\" rel=\"noreferrer\">SQLAlchemy</a> library. It lets you map your defined class to a table in the database. Every mapped attribute will be stored, and can be used to reconstruct the object. Querying the database returns instances of your class.</p>\n\n<p>With it you can use not only sqlite, but most databases - It currently also supports Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase, Informix and IBM DB2. And you can have your user choose which one she wants to use, because you can basically switch between those databases without changing the code at all.</p>\n\n<p>There are also a lot of cool features - like automatic  s, polymorphing...</p>\n\n<p>A quick, simple example you can run:</p>\n\n<pre> </pre>\n\n<p>Then I can use it like this:</p>\n\n<pre> </pre>\n\n<p>That would run   statements against the database.</p>\n\n<pre> </pre>\n\n<p>That query would run  .</p>\n\n<p>The printed result would be:</p>\n\n<pre> </pre>\n\n<p>So you're effectively storing your object into the database, the best way.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.sql.expression.ClauseElement"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.sql.expression.ClauseElement"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Base class for elements of a programmatically constructed SQL\n    expression.\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/50972149"}, "title": {"type": "literal", "value": "Test where clause in sqlalchemy select object"}, "content": {"type": "literal", "value": "<p>I am trying to write some functions to build sqlalchemy select statements. For instance:</p>\n\n<pre><code>import sqlalchemy as sa\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\nmetadata = Base.metadata\n\nt_test = sa.Table(\n    'test', metadata,\n    sa.Column('col_1', sa.Text),\n    sa.Column('col_2', sa.Float)\n)\n\ndef create_test_select():\n    sa_select = sa.select([t_test.c.col_1, t_test.c.col_2])\n    return sa_select\n\ndef add_test_col1_where_clause(sa_select, x ):\n    sa_select = sa_select.where(t_test.c.col_1 == x)\n    return sa_select\n</code></pre>\n\n<p>I would like to test these functions. </p>\n\n<p>To test <code>create_test_select</code> I would write something like </p>\n\n<pre><code>class Test(unittest.TestCase):    \n    def test(self):\n         self.assertIn('col1', create_test_select().columns)\n         self.assertIn('col2', create_test_select().columns)\n</code></pre>\n\n<p>How can I test that the function <code>add_test_col1_where_clause</code>? I would like to know that it adds the correct where clause to the select. My initial thought was to examine the where clause in sqlachemy select object but I haven't been able to get this to work.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>SQLAlchemy doesn't directly expose the 'where' clause part of a select; not all SELECT statements have one, after all. In addition, the clause can become rather complex. Personally, I'd only test the expressions in an integration test, and only to make sure the correct data is returned.</p>\n\n<p>SQLAlchemy does give you tools to visit the object tree, albeit somewhat underdocumented. You could use this to extract any comparisons from the tree, so <code>&lt;left&gt; &lt;op&gt; &lt;right&gt;</code> expressions where <code>left</code> and <code>right</code> are <em>columns elements</em>:</p>\n\n<pre><code>from sqlalchemy.sql import visitors, ColumnElement\n\ndef comparison_visitor(expr, callback):\n    \"\"\"Finds all binary operators, and calls callback(op, left, right)\"\"\"\n    def visit_binary(op):\n        callback(op, op.left, op.right)\n    # visit each expr element, but for select clauses, ignore the column collection\n    visitors.traverse(expr, {'column_collections': False}, {'binary': visit_binary})\n</code></pre>\n\n<p><code>visitors.traverse()</code> traverses any SQLAlchemy expression (by repeatedly calling the <a href=\"http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.ClauseElement.get_children\" rel=\"nofollow noreferrer\"><code>ColumnClause.get_children()</code> method</a> of the objects, passing in the mapping given as the second argument to <code>traverse()</code>), and calls the function that matches the <code>__visit_name__</code> attribute of each object. <code>BinaryClause</code> objects have <code>binary</code> as the visit name.</p>\n\n<p>You can then use this to test if specific conditions are present:</p>\n\n<pre><code>from sqlalchemy.sql import operators\nfrom sqlalchemy.sql.expressions import BindParameter\nfrom sqlalchemy.sql.schema import Column\n\ndef test_where(self):\n    where = add_test_col1_where_clause(create_test_select(), 'foo bar')\n\n    def test_comparison(op, left, right):\n        self.assertIs(op.operator, operators.eq)    # == test\n\n        self.assertIsInstance(left, Column)         # between the col_1 column\n        self.assertEq(left.name, 'col_1')\n\n        self.assertIsInstance(right, BindParameter) # and a parameter\n        self.assertEq(right.value, 'foo bar')       # with value 'foo bar'\n\n    comparison_visitor(where, test_comparison)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>SQLAlchemy doesn't directly expose the 'where' clause part of a select; not all SELECT statements have one, after all. In addition, the clause can become rather complex. Personally, I'd only test the expressions in an integration test, and only to make sure the correct data is returned.</p>\n\n<p>SQLAlchemy does give you tools to visit the object tree, albeit somewhat underdocumented. You could use this to extract any comparisons from the tree, so <code>&lt;left&gt; &lt;op&gt; &lt;right&gt;</code> expressions where <code>left</code> and <code>right</code> are <em>columns elements</em>:</p>\n\n<pre><code>from sqlalchemy.sql import visitors, ColumnElement\n\ndef comparison_visitor(expr, callback):\n    \"\"\"Finds all binary operators, and calls callback(op, left, right)\"\"\"\n    def visit_binary(op):\n        callback(op, op.left, op.right)\n    # visit each expr element, but for select clauses, ignore the column collection\n    visitors.traverse(expr, {'column_collections': False}, {'binary': visit_binary})\n</code></pre>\n\n<p><code>visitors.traverse()</code> traverses any SQLAlchemy expression (by repeatedly calling the <a href=\"http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.ClauseElement.get_children\" rel=\"nofollow noreferrer\"><code>ColumnClause.get_children()</code> method</a> of the objects, passing in the mapping given as the second argument to <code>traverse()</code>), and calls the function that matches the <code>__visit_name__</code> attribute of each object. <code>BinaryClause</code> objects have <code>binary</code> as the visit name.</p>\n\n<p>You can then use this to test if specific conditions are present:</p>\n\n<pre><code>from sqlalchemy.sql import operators\nfrom sqlalchemy.sql.expressions import BindParameter\nfrom sqlalchemy.sql.schema import Column\n\ndef test_where(self):\n    where = add_test_col1_where_clause(create_test_select(), 'foo bar')\n\n    def test_comparison(op, left, right):\n        self.assertIs(op.operator, operators.eq)    # == test\n\n        self.assertIsInstance(left, Column)         # between the col_1 column\n        self.assertEq(left.name, 'col_1')\n\n        self.assertIsInstance(right, BindParameter) # and a parameter\n        self.assertEq(right.value, 'foo bar')       # with value 'foo bar'\n\n    comparison_visitor(where, test_comparison)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "content_wo_code": "<p>I am trying to write some functions to build sqlalchemy select statements. For instance:</p>\n\n<pre> </pre>\n\n<p>I would like to test these functions. </p>\n\n<p>To test   I would write something like </p>\n\n<pre> </pre>\n\n<p>How can I test that the function  ? I would like to know that it adds the correct where clause to the select. My initial thought was to examine the where clause in sqlachemy select object but I haven't been able to get this to work.</p>\n", "answer_wo_code": "<p>SQLAlchemy doesn't directly expose the 'where' clause part of a select; not all SELECT statements have one, after all. In addition, the clause can become rather complex. Personally, I'd only test the expressions in an integration test, and only to make sure the correct data is returned.</p>\n\n<p>SQLAlchemy does give you tools to visit the object tree, albeit somewhat underdocumented. You could use this to extract any comparisons from the tree, so   expressions where   and   are <em>columns elements</em>:</p>\n\n<pre> </pre>\n\n<p>  traverses any SQLAlchemy expression (by repeatedly calling the <a href=\"http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.ClauseElement.get_children\" rel=\"nofollow noreferrer\">  method</a> of the objects, passing in the mapping given as the second argument to  ), and calls the function that matches the   attribute of each object.   objects have   as the visit name.</p>\n\n<p>You can then use this to test if specific conditions are present:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/renames"}, "class_func_label": {"type": "literal", "value": "renames"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nrenames(old, new)\n\nSuper-rename; create directories as necessary and delete any left\nempty.  Works like rename, except creation of any intermediate\ndirectories needed to make the new pathname good is attempted\nfirst.  After the rename, directories corresponding to rightmost\npath segments of the old name will be pruned until either the\nwhole path is consumed or a nonempty directory is found.\n\nNote: this function can fail with the new directory structure made\nif you lack permissions needed to unlink the leaf directory or\nfile."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/27165683"}, "title": {"type": "literal", "value": "DocTest fails when creating an object"}, "content": {"type": "literal", "value": "<p>I'm having a problem with a doctest because I'm trying to rename the IDs of a content type object in an IObjectAddedEvent handler. My requirement is to have IDs that are sequential and context specific eg CAM-001, CAM-002, BLK-001, BLK-002, etc</p>\n\n<p>When I add a object manually in the browser the event handler renames the id correctly  but when I try create it in a doctest it fails soon after it is added to it's container. plone.dexterity addContentToContainer calls _setObject with the original id, then the event handler kicks in and renames the id, and then when _getObject uses original id it obviously can't find the object so it bomb with an attribute error.</p>\n\n<p>I created a product to illustrate this here <a href=\"https://github.com/mikejmets/wt.testrig\" rel=\"nofollow\">https://github.com/mikejmets/wt.testrig</a>. </p>\n\n<p>I also tried using plone.api in the doctest but that also fails.</p>\n\n<p>All ideas welcome.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Since you are using dexterity, the best solution would be to write your own <code>NameGenerator</code> behavior.</p>\n\n<p>I guess your DX content has the following behavior activated:\n<code>&lt;element value=\"plone.app.content.interfaces.INameFromTitle\" /&gt;</code></p>\n\n<p>This bahavior is responsible to rename the item after creation.\nYou should remove this and add your own.  </p>\n\n<p>Example:</p>\n\n<p>Register behavior with zcml.</p>\n\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;plone:behavior\n    title=\"Special name(id) generator\"\n    description=\"\"\n    provides=\"dotted.name.to.your.INameGenerator\"\n    factory=\"dotted.name.to.your.name_generator.NameGenerator\"\n    for=\"dotted.name.to.content.interface\"\n    /&gt;\n</code></pre>\n\n<p>Corresponding python code.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from plone.app.content.interfaces import INameFromTitle\nfrom zope.component import getUtility\nfrom zope.interface import implements\n\n\nclass INameGenerator(INameFromTitle):\n    \"\"\"Behavior interface.\n    \"\"\"\n\n\nclass NameGenerator(object):\n    \"\"\"Customized name from title behavior.\"\n    \"\"\"\n\n    implements(INameGenerator)\n\n\n    def __init__(self, context):\n        self.context = context\n\n    @property\n    def title(self):\n\n        # YOUR IMPLEMENTATION\n        title = ...\n\n        return title\n</code></pre>\n\n<p>IMPORTANT:\nInherit your interface from <code>INameFromTitle</code></p>\n\n<p>Now add <code>&lt;element value=\"dotted.name.to.your.INameGenerator\" /&gt;</code> to your contents behaviors.</p>\n\n<p>Probably removing the <code>INameFromTitle</code> behavior from your content type could be enough, but implement explicitly your own behavior would be better. </p>\n\n\n<p>Since you are using dexterity, the best solution would be to write your own <code>NameGenerator</code> behavior.</p>\n\n<p>I guess your DX content has the following behavior activated:\n<code>&lt;element value=\"plone.app.content.interfaces.INameFromTitle\" /&gt;</code></p>\n\n<p>This bahavior is responsible to rename the item after creation.\nYou should remove this and add your own.  </p>\n\n<p>Example:</p>\n\n<p>Register behavior with zcml.</p>\n\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;plone:behavior\n    title=\"Special name(id) generator\"\n    description=\"\"\n    provides=\"dotted.name.to.your.INameGenerator\"\n    factory=\"dotted.name.to.your.name_generator.NameGenerator\"\n    for=\"dotted.name.to.content.interface\"\n    /&gt;\n</code></pre>\n\n<p>Corresponding python code.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from plone.app.content.interfaces import INameFromTitle\nfrom zope.component import getUtility\nfrom zope.interface import implements\n\n\nclass INameGenerator(INameFromTitle):\n    \"\"\"Behavior interface.\n    \"\"\"\n\n\nclass NameGenerator(object):\n    \"\"\"Customized name from title behavior.\"\n    \"\"\"\n\n    implements(INameGenerator)\n\n\n    def __init__(self, context):\n        self.context = context\n\n    @property\n    def title(self):\n\n        # YOUR IMPLEMENTATION\n        title = ...\n\n        return title\n</code></pre>\n\n<p>IMPORTANT:\nInherit your interface from <code>INameFromTitle</code></p>\n\n<p>Now add <code>&lt;element value=\"dotted.name.to.your.INameGenerator\" /&gt;</code> to your contents behaviors.</p>\n\n<p>Probably removing the <code>INameFromTitle</code> behavior from your content type could be enough, but implement explicitly your own behavior would be better. </p>\n"}, "answer_1": {"type": "literal", "value": "<p>Since you are using dexterity, the best solution would be to write your own <code>NameGenerator</code> behavior.</p>\n\n<p>I guess your DX content has the following behavior activated:\n<code>&lt;element value=\"plone.app.content.interfaces.INameFromTitle\" /&gt;</code></p>\n\n<p>This bahavior is responsible to rename the item after creation.\nYou should remove this and add your own.  </p>\n\n<p>Example:</p>\n\n<p>Register behavior with zcml.</p>\n\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;plone:behavior\n    title=\"Special name(id) generator\"\n    description=\"\"\n    provides=\"dotted.name.to.your.INameGenerator\"\n    factory=\"dotted.name.to.your.name_generator.NameGenerator\"\n    for=\"dotted.name.to.content.interface\"\n    /&gt;\n</code></pre>\n\n<p>Corresponding python code.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from plone.app.content.interfaces import INameFromTitle\nfrom zope.component import getUtility\nfrom zope.interface import implements\n\n\nclass INameGenerator(INameFromTitle):\n    \"\"\"Behavior interface.\n    \"\"\"\n\n\nclass NameGenerator(object):\n    \"\"\"Customized name from title behavior.\"\n    \"\"\"\n\n    implements(INameGenerator)\n\n\n    def __init__(self, context):\n        self.context = context\n\n    @property\n    def title(self):\n\n        # YOUR IMPLEMENTATION\n        title = ...\n\n        return title\n</code></pre>\n\n<p>IMPORTANT:\nInherit your interface from <code>INameFromTitle</code></p>\n\n<p>Now add <code>&lt;element value=\"dotted.name.to.your.INameGenerator\" /&gt;</code> to your contents behaviors.</p>\n\n<p>Probably removing the <code>INameFromTitle</code> behavior from your content type could be enough, but implement explicitly your own behavior would be better. </p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I'm having a problem with a doctest because I'm trying to rename the IDs of a content type object in an IObjectAddedEvent handler. My requirement is to have IDs that are sequential and context specific eg CAM-001, CAM-002, BLK-001, BLK-002, etc</p>\n\n<p>When I add a object manually in the browser the event handler renames the id correctly  but when I try create it in a doctest it fails soon after it is added to it's container. plone.dexterity addContentToContainer calls _setObject with the original id, then the event handler kicks in and renames the id, and then when _getObject uses original id it obviously can't find the object so it bomb with an attribute error.</p>\n\n<p>I created a product to illustrate this here <a href=\"https://github.com/mikejmets/wt.testrig\" rel=\"nofollow\">https://github.com/mikejmets/wt.testrig</a>. </p>\n\n<p>I also tried using plone.api in the doctest but that also fails.</p>\n\n<p>All ideas welcome.</p>\n", "answer_wo_code": "<p>Since you are using dexterity, the best solution would be to write your own   behavior.</p>\n\n<p>I guess your DX content has the following behavior activated:\n </p>\n\n<p>This bahavior is responsible to rename the item after creation.\nYou should remove this and add your own.  </p>\n\n<p>Example:</p>\n\n<p>Register behavior with zcml.</p>\n\n<pre class=\"lang-xml prettyprint-override\"> </pre>\n\n<p>Corresponding python code.</p>\n\n<pre class=\"lang-python prettyprint-override\"> </pre>\n\n<p>IMPORTANT:\nInherit your interface from  </p>\n\n<p>Now add   to your contents behaviors.</p>\n\n<p>Probably removing the   behavior from your content type could be enough, but implement explicitly your own behavior would be better. </p>\n\n\n<p>Since you are using dexterity, the best solution would be to write your own   behavior.</p>\n\n<p>I guess your DX content has the following behavior activated:\n </p>\n\n<p>This bahavior is responsible to rename the item after creation.\nYou should remove this and add your own.  </p>\n\n<p>Example:</p>\n\n<p>Register behavior with zcml.</p>\n\n<pre class=\"lang-xml prettyprint-override\"> </pre>\n\n<p>Corresponding python code.</p>\n\n<pre class=\"lang-python prettyprint-override\"> </pre>\n\n<p>IMPORTANT:\nInherit your interface from  </p>\n\n<p>Now add   to your contents behaviors.</p>\n\n<p>Probably removing the   behavior from your content type could be enough, but implement explicitly your own behavior would be better. </p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/gensim.models.phrases.Phraser"}, "class_func_label": {"type": "literal", "value": "gensim.models.phrases.Phraser"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Minimal state & functionality exported from :class:`~gensim.models.phrases.Phrases`.\n\n    The goal of this class is to cut down memory consumption of `Phrases`, by discarding model state\n    not strictly needed for the bigram detection task.\n\n    Use this instead of `Phrases` if you do not need to update the bigram statistics with new documents any more.\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/47604717"}, "title": {"type": "literal", "value": "How do I save a Gensim model while ensuring forwards compatibility?"}, "content": {"type": "literal", "value": "<p>I'm using the <a href=\"https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser.save\" rel=\"nofollow noreferrer\">save method</a> on the Gensim Phrases class to store a model for future use but if I update my version of Gensim, I have problems loading that model back in. For example, I get the following error when loading a model in Gensim 2.3.0 that was made in 2.2.0:</p>\n\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;timed exec&gt; in &lt;module&gt;()\n\n~/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/gensim/models/phrases.py in __init__(self, phrases_model)\n    395         self.min_count = phrases_model.min_count\n    396         self.delimiter = phrases_model.delimiter\n--&gt; 397         self.scoring = phrases_model.scoring\n    398         self.phrasegrams = {}\n    399         corpus = pseudocorpus(phrases_model.vocab, phrases_model.delimiter)\n\nAttributeError: 'Phrases' object has no attribute 'scoring'\n</code></pre>\n\n<p>Is there a better way to ensure forwards compatibility?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I've used <code>gensim</code> only a couple times and is a newbie, but judging by the <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md#230-2017-07-25\" rel=\"nofollow noreferrer\">Change Log</a>, the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1464/files#diff-cb673cf37d57cdbaaa92b62f019a380dR171\" rel=\"nofollow noreferrer\"><code>scoring</code> attribute was introduced on a <code>Phrases</code>  class in 2.3.0</a>.</p>\n\n<p>Now, from what I noticed in the github issues, when it comes to saving and loading the models, backwards compatibility is something that maintainers are trying to keep. It looks like the \"missing scoring\" attribute problem was <a href=\"https://github.com/RaRe-Technologies/gensim/commit/a5872fabb069925c89dcdeaff5ff569b28813ef6\" rel=\"nofollow noreferrer\">addressed in 3.1.0</a> - see the \"backwards scoring compatibility when loading a Phrases class\" comment and the related discussion in the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1573\" rel=\"nofollow noreferrer\">pull request</a>. The idea of the fix was basically to improve the <code>load()</code> method to <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/phrases.py#L485-L496\" rel=\"nofollow noreferrer\">handle missing attributes and implicitly replacing them with the defaults</a> to avoid loading failures.</p>\n\n<p>I think in 2.3.0 <code>gensim</code> had this <a href=\"https://github.com/RaRe-Technologies/gensim/blob/2.3.0/gensim/utils.py#L249\" rel=\"nofollow noreferrer\">generic <code>SaveLoad</code> class for pickling/unpickling models</a> - as you can see, it is pretty much straightforward, no model-specific logic here.</p>\n\n<p>I am though not sure if and how you can keep the models compatible between 2.2.0 and 2.3.0. I would open a new issue at the <a href=\"https://github.com/RaRe-Technologies/gensim/issues\" rel=\"nofollow noreferrer\"><code>gensim</code> issue tracker</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I've used <code>gensim</code> only a couple times and is a newbie, but judging by the <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md#230-2017-07-25\" rel=\"nofollow noreferrer\">Change Log</a>, the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1464/files#diff-cb673cf37d57cdbaaa92b62f019a380dR171\" rel=\"nofollow noreferrer\"><code>scoring</code> attribute was introduced on a <code>Phrases</code>  class in 2.3.0</a>.</p>\n\n<p>Now, from what I noticed in the github issues, when it comes to saving and loading the models, backwards compatibility is something that maintainers are trying to keep. It looks like the \"missing scoring\" attribute problem was <a href=\"https://github.com/RaRe-Technologies/gensim/commit/a5872fabb069925c89dcdeaff5ff569b28813ef6\" rel=\"nofollow noreferrer\">addressed in 3.1.0</a> - see the \"backwards scoring compatibility when loading a Phrases class\" comment and the related discussion in the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1573\" rel=\"nofollow noreferrer\">pull request</a>. The idea of the fix was basically to improve the <code>load()</code> method to <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/phrases.py#L485-L496\" rel=\"nofollow noreferrer\">handle missing attributes and implicitly replacing them with the defaults</a> to avoid loading failures.</p>\n\n<p>I think in 2.3.0 <code>gensim</code> had this <a href=\"https://github.com/RaRe-Technologies/gensim/blob/2.3.0/gensim/utils.py#L249\" rel=\"nofollow noreferrer\">generic <code>SaveLoad</code> class for pickling/unpickling models</a> - as you can see, it is pretty much straightforward, no model-specific logic here.</p>\n\n<p>I am though not sure if and how you can keep the models compatible between 2.2.0 and 2.3.0. I would open a new issue at the <a href=\"https://github.com/RaRe-Technologies/gensim/issues\" rel=\"nofollow noreferrer\"><code>gensim</code> issue tracker</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I'm using the <a href=\"https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser.save\" rel=\"nofollow noreferrer\">save method</a> on the Gensim Phrases class to store a model for future use but if I update my version of Gensim, I have problems loading that model back in. For example, I get the following error when loading a model in Gensim 2.3.0 that was made in 2.2.0:</p>\n\n<pre> </pre>\n\n<p>Is there a better way to ensure forwards compatibility?</p>\n", "answer_wo_code": "<p>I've used   only a couple times and is a newbie, but judging by the <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/CHANGELOG.md#230-2017-07-25\" rel=\"nofollow noreferrer\">Change Log</a>, the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1464/files#diff-cb673cf37d57cdbaaa92b62f019a380dR171\" rel=\"nofollow noreferrer\">  attribute was introduced on a    class in 2.3.0</a>.</p>\n\n<p>Now, from what I noticed in the github issues, when it comes to saving and loading the models, backwards compatibility is something that maintainers are trying to keep. It looks like the \"missing scoring\" attribute problem was <a href=\"https://github.com/RaRe-Technologies/gensim/commit/a5872fabb069925c89dcdeaff5ff569b28813ef6\" rel=\"nofollow noreferrer\">addressed in 3.1.0</a> - see the \"backwards scoring compatibility when loading a Phrases class\" comment and the related discussion in the <a href=\"https://github.com/RaRe-Technologies/gensim/pull/1573\" rel=\"nofollow noreferrer\">pull request</a>. The idea of the fix was basically to improve the   method to <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/phrases.py#L485-L496\" rel=\"nofollow noreferrer\">handle missing attributes and implicitly replacing them with the defaults</a> to avoid loading failures.</p>\n\n<p>I think in 2.3.0   had this <a href=\"https://github.com/RaRe-Technologies/gensim/blob/2.3.0/gensim/utils.py#L249\" rel=\"nofollow noreferrer\">generic   class for pickling/unpickling models</a> - as you can see, it is pretty much straightforward, no model-specific logic here.</p>\n\n<p>I am though not sure if and how you can keep the models compatible between 2.2.0 and 2.3.0. I would open a new issue at the <a href=\"https://github.com/RaRe-Technologies/gensim/issues\" rel=\"nofollow noreferrer\">  issue tracker</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/version"}, "class_func_label": {"type": "literal", "value": "version"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nReturns a string specifying the bundled version of pip."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/31469213"}, "title": {"type": "literal", "value": "pyasn1 and strange mismatch when setting component"}, "content": {"type": "literal", "value": "<p>I'm running into a strange issue when setting a component using pyasn1. I construct and empty certificate and put a certificate to be signed in it:</p>\n\n<pre><code>empty = rfc2459.Certificate()\nempty['tbsCertificate'] = rfc2459.TBSCertificate()\n</code></pre>\n\n<p>Now I want to set a version, which fails with an actual version object, but works by automatically creating a type:</p>\n\n<pre><code>empty['tbsCertificate']['version'] = rfc2459.Version('v3')\n# PyAsn1Error: Component type error Version('v1') vs Version('v3')\n\nempty['tbsCertificate']['version'] = 'v3'\n# works\n</code></pre>\n\n<p>Which is strange given those two compare equal:</p>\n\n<pre><code>empty['tbsCertificate']['version'] == rfc2459.Version('v3')\n# True\n</code></pre>\n\n<p>So why doesn't the first way work?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Those Version's formally belong to different types and they have different BER tags. The rfc2459.Version is plain INTEGER:</p>\n\n<pre><code>class Version(univ.Integer):\n    namedValues = namedval.NamedValues(('v1', 0), ('v2', 1), ('v3', 2))\n</code></pre>\n\n<p>while 'version' field of rfc2459.TBSCertificate SEQUENCE contains a subclass of Version which is defined by means of additional tagging:</p>\n\n<pre><code>class TBSCertificate(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.DefaultedNamedType('version',Version('v1').subtype(\n            explicitTag=tag.Tag(\n                tag.tagClassContext, tag.tagFormatSimple, 0)\n            )\n        )...\n</code></pre>\n\n<p>That's why you can't put Version object into TBSCertificate['version']. If you could that would formally change TBSCertificate data type and its BER representation. </p>\n\n<p>In the same time TBSCertificate['version'] = 'v1' works due to automatic coercion of Python string 'v1' into Version subtype through its named value (e.g. 'v1').</p>\n\n<p>Their payloads (e.g. 0) indeed compare equal, which is sometimes possible even for different types. Consider:</p>\n\n<pre><code>&gt;&gt;&gt; float(0) == int(0)\nTrue\n&gt;&gt;&gt; float == int\nFalse\n</code></pre>\n\n<p>for example.</p>\n\n<p>To answer you question: I believe the right way is to rely on \"coercion\". That will guarantee tags/types correctness and will validate your initialiser against destination type constraints (e.g. value range, size etc). Thus you end up with fully compliant instance of ASN.1 structure.</p>\n\n\n<p>Those Version's formally belong to different types and they have different BER tags. The rfc2459.Version is plain INTEGER:</p>\n\n<pre><code>class Version(univ.Integer):\n    namedValues = namedval.NamedValues(('v1', 0), ('v2', 1), ('v3', 2))\n</code></pre>\n\n<p>while 'version' field of rfc2459.TBSCertificate SEQUENCE contains a subclass of Version which is defined by means of additional tagging:</p>\n\n<pre><code>class TBSCertificate(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.DefaultedNamedType('version',Version('v1').subtype(\n            explicitTag=tag.Tag(\n                tag.tagClassContext, tag.tagFormatSimple, 0)\n            )\n        )...\n</code></pre>\n\n<p>That's why you can't put Version object into TBSCertificate['version']. If you could that would formally change TBSCertificate data type and its BER representation. </p>\n\n<p>In the same time TBSCertificate['version'] = 'v1' works due to automatic coercion of Python string 'v1' into Version subtype through its named value (e.g. 'v1').</p>\n\n<p>Their payloads (e.g. 0) indeed compare equal, which is sometimes possible even for different types. Consider:</p>\n\n<pre><code>&gt;&gt;&gt; float(0) == int(0)\nTrue\n&gt;&gt;&gt; float == int\nFalse\n</code></pre>\n\n<p>for example.</p>\n\n<p>To answer you question: I believe the right way is to rely on \"coercion\". That will guarantee tags/types correctness and will validate your initialiser against destination type constraints (e.g. value range, size etc). Thus you end up with fully compliant instance of ASN.1 structure.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Those Version's formally belong to different types and they have different BER tags. The rfc2459.Version is plain INTEGER:</p>\n\n<pre><code>class Version(univ.Integer):\n    namedValues = namedval.NamedValues(('v1', 0), ('v2', 1), ('v3', 2))\n</code></pre>\n\n<p>while 'version' field of rfc2459.TBSCertificate SEQUENCE contains a subclass of Version which is defined by means of additional tagging:</p>\n\n<pre><code>class TBSCertificate(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.DefaultedNamedType('version',Version('v1').subtype(\n            explicitTag=tag.Tag(\n                tag.tagClassContext, tag.tagFormatSimple, 0)\n            )\n        )...\n</code></pre>\n\n<p>That's why you can't put Version object into TBSCertificate['version']. If you could that would formally change TBSCertificate data type and its BER representation. </p>\n\n<p>In the same time TBSCertificate['version'] = 'v1' works due to automatic coercion of Python string 'v1' into Version subtype through its named value (e.g. 'v1').</p>\n\n<p>Their payloads (e.g. 0) indeed compare equal, which is sometimes possible even for different types. Consider:</p>\n\n<pre><code>&gt;&gt;&gt; float(0) == int(0)\nTrue\n&gt;&gt;&gt; float == int\nFalse\n</code></pre>\n\n<p>for example.</p>\n\n<p>To answer you question: I believe the right way is to rely on \"coercion\". That will guarantee tags/types correctness and will validate your initialiser against destination type constraints (e.g. value range, size etc). Thus you end up with fully compliant instance of ASN.1 structure.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I'm running into a strange issue when setting a component using pyasn1. I construct and empty certificate and put a certificate to be signed in it:</p>\n\n<pre> </pre>\n\n<p>Now I want to set a version, which fails with an actual version object, but works by automatically creating a type:</p>\n\n<pre> </pre>\n\n<p>Which is strange given those two compare equal:</p>\n\n<pre> </pre>\n\n<p>So why doesn't the first way work?</p>\n", "answer_wo_code": "<p>Those Version's formally belong to different types and they have different BER tags. The rfc2459.Version is plain INTEGER:</p>\n\n<pre> </pre>\n\n<p>while 'version' field of rfc2459.TBSCertificate SEQUENCE contains a subclass of Version which is defined by means of additional tagging:</p>\n\n<pre> </pre>\n\n<p>That's why you can't put Version object into TBSCertificate['version']. If you could that would formally change TBSCertificate data type and its BER representation. </p>\n\n<p>In the same time TBSCertificate['version'] = 'v1' works due to automatic coercion of Python string 'v1' into Version subtype through its named value (e.g. 'v1').</p>\n\n<p>Their payloads (e.g. 0) indeed compare equal, which is sometimes possible even for different types. Consider:</p>\n\n<pre> </pre>\n\n<p>for example.</p>\n\n<p>To answer you question: I believe the right way is to rely on \"coercion\". That will guarantee tags/types correctness and will validate your initialiser against destination type constraints (e.g. value range, size etc). Thus you end up with fully compliant instance of ASN.1 structure.</p>\n\n\n<p>Those Version's formally belong to different types and they have different BER tags. The rfc2459.Version is plain INTEGER:</p>\n\n<pre> </pre>\n\n<p>while 'version' field of rfc2459.TBSCertificate SEQUENCE contains a subclass of Version which is defined by means of additional tagging:</p>\n\n<pre> </pre>\n\n<p>That's why you can't put Version object into TBSCertificate['version']. If you could that would formally change TBSCertificate data type and its BER representation. </p>\n\n<p>In the same time TBSCertificate['version'] = 'v1' works due to automatic coercion of Python string 'v1' into Version subtype through its named value (e.g. 'v1').</p>\n\n<p>Their payloads (e.g. 0) indeed compare equal, which is sometimes possible even for different types. Consider:</p>\n\n<pre> </pre>\n\n<p>for example.</p>\n\n<p>To answer you question: I believe the right way is to rely on \"coercion\". That will guarantee tags/types correctness and will validate your initialiser against destination type constraints (e.g. value range, size etc). Thus you end up with fully compliant instance of ASN.1 structure.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/bytearray"}, "class_func_label": {"type": "literal", "value": "bytearray"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "bytearray(iterable_of_ints) -> bytearray\nbytearray(string, encoding[, errors]) -> bytearray\nbytearray(bytes_or_buffer) -> mutable copy of bytes_or_buffer\nbytearray(int) -> bytes array of size given by the parameter initialized with null bytes\nbytearray() -> empty bytes array\n\nConstruct a mutable bytearray object from:\n  - an iterable yielding integers in range(256)\n  - a text string encoded using the specified encoding\n  - a bytes or a buffer object\n  - any object implementing the buffer API.\n  - an integer"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/13683953"}, "title": {"type": "literal", "value": "How to decode an ASN1 encoded data(nested structured data) using pyasn1?"}, "content": {"type": "literal", "value": "<p>My data should be able to fetch like the following way..</p>\n\n<p><strong>Example Structure:-</strong></p>\n\n<pre><code>    listOfVolumes: -&gt; SequenceOf\n         ChangeOfCharCondition -&gt; Sequence\n                dataUplink: 9612742 -&gt; Integer   \n                dataDownlink: 216449 -&gt; Integer\n                changeCondition: qoSChange (0) -&gt; Enumerated\n                Time: 1206202320082b0530 -&gt; OctetString\n\n         ChangeOfCharCondition -&gt; Sequence\n                qosNegotiated: 0223921f9396979774f9ffff -&gt; OctetString\n                dataUplink: 57664480 -&gt; Integer\n                dataDownlink: 1460443 -&gt; Integer\n                changeCondition: recordClosure (2) -&gt; Enumerated\n                Time: 1206210017072b0530 -&gt; OctetString\n</code></pre>\n\n<p>How do i decode a data encoded(bytearray) in this particular format?</p>\n\n<p>I could decode it , if it is only a single SEQUENCE within the SEQUENCEOF structure,\nbut it is quite hard to me to loop over the data more than one time , can any one please\nsuggest me a better method to sort out this issue? Any advise is valuable for me..\nThaks in advance ..<br>\n<strong>Sample code :</strong></p>\n\n<pre><code>class ChangeCondition(univ.Enumerated):\n\n     namedValues = namedval.NamedValues(\n        ('qoS', 0),\n        ('Time', 1),\n        ('Closure', 2),\n        ('ContinueOngoing', 3),\n        ('RetryandTerminateOngoing', 4),\n        ('TerminateOngoing', 5),\n        ('cGI', 6),\n        ('rAI', 7),\n        ('dT', 8),\n        ('dT-Removal', 9))\n        subtypeSpec = univ.Enumerated.subtypeSpec + \\\n                constraint.SingleValueConstraint(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n\nclass ChangeOfCharCondition(univ.Sequence):\n\n    componentType = namedtype.NamedTypes(\n    namedtype.OptionalNamedType('Negotiated', univ.OctetString().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 2))),\n    namedtype.OptionalNamedType('dataUplink', univ.Integer().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 3))),\n    namedtype.OptionalNamedType('dataDownlink', univ.Integer().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 4))),\n    namedtype.NamedType('changeCondition', ChangeCondition().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 5))),\n    namedtype.OptionalNamedType('Time', univ.OctetString().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 6)))\n       )\n\nclass ListOfVolumes(univ.SequenceOf):\n\n    tagSet = baseTagSet = tag.initTagSet(tag.Tag(tag.tagClassContext,tag.tagFormatSimple, 12),)\n    componentType = ChangeOfCharCondition()\n\nclass MyCdr(univ.Set):\n\n     tagSet = baseTagSet = tag.initTagSet(tag.Tag(tag.tagClassContext,tag.tagFormatSimple, 21))\n    componentType = namedtype.NamedTypes(\n    namedtype.OptionalNamedType('listOfVolumes', ListOfVolumes()))                                                    \n</code></pre>\n\n<p>My data is something like as follows,</p>\n\n<pre><code>bytearray(b'\\xb5\\x81\\x2a\\xac(0&amp;\\xa2\\x0e\\x81\\x0c\\x01#Q\\x1f\\x93\\x96HHt\\xf9\\xff\\xff\\x83\\x02\\x06x\\x84\\x02\\x13m\\x85\\x01\\x02\\x86\\t6\\x05\"#\\x12E+\\x050')\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>If you have the formal ASN.1 grammar for all these data structures, it would be helpful if you publish it here. </p>\n\n<p>Your general approach looks correct, however the way you do ASN.1 tagging is suspicious.</p>\n\n<p>From your initial description of the grammar, it looks like you should be able to decode your bytearray by calling pyasn1:</p>\n\n<pre><code>decoder.decode(mybytearray, asn1Spec=ListOfVolumes())\n</code></pre>\n\n<p>e.g. passing ListOfVolumes() class instance as a top-level prototype object to the decoder. If it fails it may be due to incorrect tagging.</p>\n\n<p>To dig deeper it may help to enable pyasn1 debugging:</p>\n\n<pre><code>from pyasn1 import debug\ndebug.setLogger(debug.Debug('all')\n</code></pre>\n\n<p>and see what tags are read from bytearray and what objects from your specification match them.</p>\n\n\n<p>If you have the formal ASN.1 grammar for all these data structures, it would be helpful if you publish it here. </p>\n\n<p>Your general approach looks correct, however the way you do ASN.1 tagging is suspicious.</p>\n\n<p>From your initial description of the grammar, it looks like you should be able to decode your bytearray by calling pyasn1:</p>\n\n<pre><code>decoder.decode(mybytearray, asn1Spec=ListOfVolumes())\n</code></pre>\n\n<p>e.g. passing ListOfVolumes() class instance as a top-level prototype object to the decoder. If it fails it may be due to incorrect tagging.</p>\n\n<p>To dig deeper it may help to enable pyasn1 debugging:</p>\n\n<pre><code>from pyasn1 import debug\ndebug.setLogger(debug.Debug('all')\n</code></pre>\n\n<p>and see what tags are read from bytearray and what objects from your specification match them.</p>\n\n\n<p>If your encoding contains a valid SEQUENCE OF items and you defined your pyasn1 data structures accordingly, pyasn1 decoder should be able to loop over all SEQUENCE OF instances  all by itself.</p>\n\n<p>Could you post your pyasn1 specification of your data structure?</p>\n\n\n<p>If your encoding contains a valid SEQUENCE OF items and you defined your pyasn1 data structures accordingly, pyasn1 decoder should be able to loop over all SEQUENCE OF instances  all by itself.</p>\n\n<p>Could you post your pyasn1 specification of your data structure?</p>\n"}, "answer_1": {"type": "literal", "value": "<p>If you have the formal ASN.1 grammar for all these data structures, it would be helpful if you publish it here. </p>\n\n<p>Your general approach looks correct, however the way you do ASN.1 tagging is suspicious.</p>\n\n<p>From your initial description of the grammar, it looks like you should be able to decode your bytearray by calling pyasn1:</p>\n\n<pre><code>decoder.decode(mybytearray, asn1Spec=ListOfVolumes())\n</code></pre>\n\n<p>e.g. passing ListOfVolumes() class instance as a top-level prototype object to the decoder. If it fails it may be due to incorrect tagging.</p>\n\n<p>To dig deeper it may help to enable pyasn1 debugging:</p>\n\n<pre><code>from pyasn1 import debug\ndebug.setLogger(debug.Debug('all')\n</code></pre>\n\n<p>and see what tags are read from bytearray and what objects from your specification match them.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>If your encoding contains a valid SEQUENCE OF items and you defined your pyasn1 data structures accordingly, pyasn1 decoder should be able to loop over all SEQUENCE OF instances  all by itself.</p>\n\n<p>Could you post your pyasn1 specification of your data structure?</p>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>My data should be able to fetch like the following way..</p>\n\n<p><strong>Example Structure:-</strong></p>\n\n<pre> </pre>\n\n<p>How do i decode a data encoded(bytearray) in this particular format?</p>\n\n<p>I could decode it , if it is only a single SEQUENCE within the SEQUENCEOF structure,\nbut it is quite hard to me to loop over the data more than one time , can any one please\nsuggest me a better method to sort out this issue? Any advise is valuable for me..\nThaks in advance ..<br>\n<strong>Sample code :</strong></p>\n\n<pre> </pre>\n\n<p>My data is something like as follows,</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>If you have the formal ASN.1 grammar for all these data structures, it would be helpful if you publish it here. </p>\n\n<p>Your general approach looks correct, however the way you do ASN.1 tagging is suspicious.</p>\n\n<p>From your initial description of the grammar, it looks like you should be able to decode your bytearray by calling pyasn1:</p>\n\n<pre> </pre>\n\n<p>e.g. passing ListOfVolumes() class instance as a top-level prototype object to the decoder. If it fails it may be due to incorrect tagging.</p>\n\n<p>To dig deeper it may help to enable pyasn1 debugging:</p>\n\n<pre> </pre>\n\n<p>and see what tags are read from bytearray and what objects from your specification match them.</p>\n\n\n<p>If you have the formal ASN.1 grammar for all these data structures, it would be helpful if you publish it here. </p>\n\n<p>Your general approach looks correct, however the way you do ASN.1 tagging is suspicious.</p>\n\n<p>From your initial description of the grammar, it looks like you should be able to decode your bytearray by calling pyasn1:</p>\n\n<pre> </pre>\n\n<p>e.g. passing ListOfVolumes() class instance as a top-level prototype object to the decoder. If it fails it may be due to incorrect tagging.</p>\n\n<p>To dig deeper it may help to enable pyasn1 debugging:</p>\n\n<pre> </pre>\n\n<p>and see what tags are read from bytearray and what objects from your specification match them.</p>\n\n\n<p>If your encoding contains a valid SEQUENCE OF items and you defined your pyasn1 data structures accordingly, pyasn1 decoder should be able to loop over all SEQUENCE OF instances  all by itself.</p>\n\n<p>Could you post your pyasn1 specification of your data structure?</p>\n\n\n<p>If your encoding contains a valid SEQUENCE OF items and you defined your pyasn1 data structures accordingly, pyasn1 decoder should be able to loop over all SEQUENCE OF instances  all by itself.</p>\n\n<p>Could you post your pyasn1 specification of your data structure?</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/OpenSSL.crypto.X509"}, "class_func_label": {"type": "literal", "value": "OpenSSL.crypto.X509"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    An X.509 certificate.\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/5519958"}, "title": {"type": "literal", "value": "How do I parse subjectAltName extension data using pyasn1?"}, "content": {"type": "literal", "value": "<p>I have some data that pyOpenSSL gave me, <code>'0\\r\\x82\\x0bexample.com'</code>.  This should be the value of a subjectAltName X509 extension.  I tried to encode the necessary parts of the ASN1 specification for this extension using pyasn1 (and based on one of the pyasn1 examples):</p>\n\n<pre><code>from pyasn1.type import univ, constraint, char, namedtype\n\nfrom pyasn1.codec.der.decoder import decode\n\nMAX = 64\n\nclass DirectoryString(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType(\n            'teletexString', char.TeletexString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'printableString', char.PrintableString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'universalString', char.UniversalString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'utf8String', char.UTF8String().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'bmpString', char.BMPString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'ia5String', char.IA5String().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        )\n\n\nclass AttributeValue(DirectoryString):\n    pass\n\n\nclass AttributeType(univ.ObjectIdentifier):\n    pass\n\n\nclass AttributeTypeAndValue(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('type', AttributeType()),\n        namedtype.NamedType('value', AttributeValue()),\n        )\n\n\nclass RelativeDistinguishedName(univ.SetOf):\n    componentType = AttributeTypeAndValue()\n\nclass RDNSequence(univ.SequenceOf):\n    componentType = RelativeDistinguishedName()\n\n\nclass Name(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('', RDNSequence()),\n        )\n\n\nclass Extension(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('extnID', univ.ObjectIdentifier()),\n        namedtype.DefaultedNamedType('critical', univ.Boolean('False')),\n        namedtype.NamedType('extnValue', univ.OctetString()),\n        )\n\n\nclass Extensions(univ.SequenceOf):\n    componentType = Extension()\n    sizeSpec = univ.SequenceOf.sizeSpec + constraint.ValueSizeConstraint(1, MAX)\n\n\nclass GeneralName(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        # namedtype.NamedType('otherName', AnotherName()),\n        namedtype.NamedType('rfc822Name', char.IA5String()),\n        namedtype.NamedType('dNSName', char.IA5String()),\n        # namedtype.NamedType('x400Address', ORAddress()),\n        namedtype.NamedType('directoryName', Name()),\n        # namedtype.NamedType('ediPartyName', EDIPartyName()),\n        namedtype.NamedType('uniformResourceIdentifier', char.IA5String()),\n        namedtype.NamedType('iPAddress', univ.OctetString()),\n        namedtype.NamedType('registeredID', univ.ObjectIdentifier()),\n        )\n\n\nclass GeneralNames(univ.SequenceOf):\n    componentType = GeneralName()\n    sizeSpec = univ.SequenceOf.sizeSpec + constraint.ValueSizeConstraint(1, MAX)\n\n\nclass SubjectAltName(GeneralNames):\n    pass\n\nprint decode('0\\r\\x82\\x0bexample.com', asn1Spec=GeneralNames())\n</code></pre>\n\n<p>Clearly I got a little bored near the end and didn't fully specify the <code>GeneralName</code> type.  However, the test string should contain a <code>dNSName</code>, not one of the skipped values, so I hope it doesn't matter.</p>\n\n<p>When the program is run, it fails with an error I'm not able to interpret:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"x509.py\", line 94, in &lt;module&gt;\n    print decode('0\\r\\x82\\x0bexample.com', asn1Spec=GeneralNames())\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 493, in __call__\n    length, stGetValueDecoder, decodeFun\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 202, in valueDecoder\n    substrate, asn1Spec\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 453, in __call__\n    __chosenSpec.getTypeMap().has_key(tagSet):\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/univ.py\", line 608, in getTypeMap\n    return Set.getComponentTypeMap(self)\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/univ.py\", line 535, in getComponentTypeMap\n    def getComponentTypeMap(self): return self._componentType.getTypeMap(1)\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/namedtype.py\", line 126, in getTypeMap\n    'Duplicate type %s in map %s'%(k,self.__typeMap)\npyasn1.error.PyAsn1Error: Duplicate type TagSet(Tag(tagClass=0, tagFormat=0, tagId=22)) in map {TagSet(Tag(tagClass=0, tagFormat=0, tagId=22)): IA5String()}\n</code></pre>\n\n<p>Any tips on where I went wrong and how to successfully parse this extension type with pyasn1 would be much appreciated.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run <code>pip install pyasn1</code>, <code>pip install pyasn1-modules</code> and <code>pip install pyopenssl</code> otherwise you'll get import errors.</p>\n\n<pre><code># Import pyasn and the proper decode function\nimport pyasn1\nfrom pyasn1.codec.der.decoder import decode as asn1_decoder\n\n# Import SubjectAltName from rfc2459 module\nfrom pyasn1_modules.rfc2459 import SubjectAltName\n\n# Import native Python type encoder\nfrom pyasn1.codec.native.encoder import encode as nat_encoder\n\n# Import OpenSSL tools for working with certs.\nfrom OpenSSL import crypto\n# Read raw certificate file\nwith open('PATH/TO/CERTIFICATE.crt', 'r') as cert_f:\n    raw_cert = cert_f.read()\n\ncert = crypto.load_certificate(crypto.FILETYPE_PEM, raw_cert)\n\n# Note this example assumes SubjectAltName is the only Extension for this cert. \nraw_alt_names = cert.get_extension(0).get_data()\n\ndecoded_alt_names, _ = asn1_decoder(raw_alt_names, asn1Spec=SubjectAltName())\n\n# Unless a raw string of ASN.1 is what you need encode back to native Python types\npy_alt_names = nat_encoder(decoded_alt_names)\n\n# And Finally a plain Python list of UTF-8 encoded strings representing the SubjectAltNames\nsubject_alt_names = [ x['dNSName'].decode('utf-8') for x in py_alt_names]\n</code></pre>\n\n<p>The output of this will be something like</p>\n\n<pre><code>['cdn1.example.com', 'cdn2.example.com']\n</code></pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n\n\n<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each <code>NamedType</code> in <code>GeneralName.componentType</code> needs to be given tag information.  This is done with the <code>subtype</code> method.  For example, instead of:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String()),\n</code></pre>\n\n<p>the definition should be:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext,\n                            tag.tagFormatSimple, 1))),\n</code></pre>\n\n<p>where <code>1</code> comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre><code>GeneralName ::= CHOICE {\n   otherName                       [0]     OtherName,\n   rfc822Name                      [1]     IA5String,\n   dNSName                         [2]     IA5String,\n   x400Address                     [3]     ORAddress,\n   directoryName                   [4]     Name,\n   ediPartyName                    [5]     EDIPartyName,\n   uniformResourceIdentifier       [6]     IA5String,\n   iPAddress                       [7]     OCTET STRING,\n   registeredID                    [8]     OBJECT IDENTIFIER\n}\n</code></pre>\n\n<p>After defining a tag for each of these fields of the <code>componentType</code>, parsing succeeds:</p>\n\n<pre><code>(GeneralNames().setComponentByPosition(\n    0, GeneralName().setComponentByPosition(1, IA5String('example.com'))), '')\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run <code>pip install pyasn1</code>, <code>pip install pyasn1-modules</code> and <code>pip install pyopenssl</code> otherwise you'll get import errors.</p>\n\n<pre><code># Import pyasn and the proper decode function\nimport pyasn1\nfrom pyasn1.codec.der.decoder import decode as asn1_decoder\n\n# Import SubjectAltName from rfc2459 module\nfrom pyasn1_modules.rfc2459 import SubjectAltName\n\n# Import native Python type encoder\nfrom pyasn1.codec.native.encoder import encode as nat_encoder\n\n# Import OpenSSL tools for working with certs.\nfrom OpenSSL import crypto\n# Read raw certificate file\nwith open('PATH/TO/CERTIFICATE.crt', 'r') as cert_f:\n    raw_cert = cert_f.read()\n\ncert = crypto.load_certificate(crypto.FILETYPE_PEM, raw_cert)\n\n# Note this example assumes SubjectAltName is the only Extension for this cert. \nraw_alt_names = cert.get_extension(0).get_data()\n\ndecoded_alt_names, _ = asn1_decoder(raw_alt_names, asn1Spec=SubjectAltName())\n\n# Unless a raw string of ASN.1 is what you need encode back to native Python types\npy_alt_names = nat_encoder(decoded_alt_names)\n\n# And Finally a plain Python list of UTF-8 encoded strings representing the SubjectAltNames\nsubject_alt_names = [ x['dNSName'].decode('utf-8') for x in py_alt_names]\n</code></pre>\n\n<p>The output of this will be something like</p>\n\n<pre><code>['cdn1.example.com', 'cdn2.example.com']\n</code></pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each <code>NamedType</code> in <code>GeneralName.componentType</code> needs to be given tag information.  This is done with the <code>subtype</code> method.  For example, instead of:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String()),\n</code></pre>\n\n<p>the definition should be:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext,\n                            tag.tagFormatSimple, 1))),\n</code></pre>\n\n<p>where <code>1</code> comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre><code>GeneralName ::= CHOICE {\n   otherName                       [0]     OtherName,\n   rfc822Name                      [1]     IA5String,\n   dNSName                         [2]     IA5String,\n   x400Address                     [3]     ORAddress,\n   directoryName                   [4]     Name,\n   ediPartyName                    [5]     EDIPartyName,\n   uniformResourceIdentifier       [6]     IA5String,\n   iPAddress                       [7]     OCTET STRING,\n   registeredID                    [8]     OBJECT IDENTIFIER\n}\n</code></pre>\n\n<p>After defining a tag for each of these fields of the <code>componentType</code>, parsing succeeds:</p>\n\n<pre><code>(GeneralNames().setComponentByPosition(\n    0, GeneralName().setComponentByPosition(1, IA5String('example.com'))), '')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "16"}, "content_wo_code": "<p>I have some data that pyOpenSSL gave me,  .  This should be the value of a subjectAltName X509 extension.  I tried to encode the necessary parts of the ASN1 specification for this extension using pyasn1 (and based on one of the pyasn1 examples):</p>\n\n<pre> </pre>\n\n<p>Clearly I got a little bored near the end and didn't fully specify the   type.  However, the test string should contain a  , not one of the skipped values, so I hope it doesn't matter.</p>\n\n<p>When the program is run, it fails with an error I'm not able to interpret:</p>\n\n<pre> </pre>\n\n<p>Any tips on where I went wrong and how to successfully parse this extension type with pyasn1 would be much appreciated.</p>\n", "answer_wo_code": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run  ,   and   otherwise you'll get import errors.</p>\n\n<pre> </pre>\n\n<p>The output of this will be something like</p>\n\n<pre> </pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n\n\n<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each   in   needs to be given tag information.  This is done with the   method.  For example, instead of:</p>\n\n<pre> </pre>\n\n<p>the definition should be:</p>\n\n<pre> </pre>\n\n<p>where   comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre> </pre>\n\n<p>After defining a tag for each of these fields of the  , parsing succeeds:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/OpenSSL.crypto.X509Extension"}, "class_func_label": {"type": "literal", "value": "OpenSSL.crypto.X509Extension"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    An X.509 v3 certificate extension.\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/5519958"}, "title": {"type": "literal", "value": "How do I parse subjectAltName extension data using pyasn1?"}, "content": {"type": "literal", "value": "<p>I have some data that pyOpenSSL gave me, <code>'0\\r\\x82\\x0bexample.com'</code>.  This should be the value of a subjectAltName X509 extension.  I tried to encode the necessary parts of the ASN1 specification for this extension using pyasn1 (and based on one of the pyasn1 examples):</p>\n\n<pre><code>from pyasn1.type import univ, constraint, char, namedtype\n\nfrom pyasn1.codec.der.decoder import decode\n\nMAX = 64\n\nclass DirectoryString(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType(\n            'teletexString', char.TeletexString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'printableString', char.PrintableString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'universalString', char.UniversalString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'utf8String', char.UTF8String().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'bmpString', char.BMPString().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        namedtype.NamedType(\n            'ia5String', char.IA5String().subtype(\n                subtypeSpec=constraint.ValueSizeConstraint(1, MAX))),\n        )\n\n\nclass AttributeValue(DirectoryString):\n    pass\n\n\nclass AttributeType(univ.ObjectIdentifier):\n    pass\n\n\nclass AttributeTypeAndValue(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('type', AttributeType()),\n        namedtype.NamedType('value', AttributeValue()),\n        )\n\n\nclass RelativeDistinguishedName(univ.SetOf):\n    componentType = AttributeTypeAndValue()\n\nclass RDNSequence(univ.SequenceOf):\n    componentType = RelativeDistinguishedName()\n\n\nclass Name(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('', RDNSequence()),\n        )\n\n\nclass Extension(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('extnID', univ.ObjectIdentifier()),\n        namedtype.DefaultedNamedType('critical', univ.Boolean('False')),\n        namedtype.NamedType('extnValue', univ.OctetString()),\n        )\n\n\nclass Extensions(univ.SequenceOf):\n    componentType = Extension()\n    sizeSpec = univ.SequenceOf.sizeSpec + constraint.ValueSizeConstraint(1, MAX)\n\n\nclass GeneralName(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        # namedtype.NamedType('otherName', AnotherName()),\n        namedtype.NamedType('rfc822Name', char.IA5String()),\n        namedtype.NamedType('dNSName', char.IA5String()),\n        # namedtype.NamedType('x400Address', ORAddress()),\n        namedtype.NamedType('directoryName', Name()),\n        # namedtype.NamedType('ediPartyName', EDIPartyName()),\n        namedtype.NamedType('uniformResourceIdentifier', char.IA5String()),\n        namedtype.NamedType('iPAddress', univ.OctetString()),\n        namedtype.NamedType('registeredID', univ.ObjectIdentifier()),\n        )\n\n\nclass GeneralNames(univ.SequenceOf):\n    componentType = GeneralName()\n    sizeSpec = univ.SequenceOf.sizeSpec + constraint.ValueSizeConstraint(1, MAX)\n\n\nclass SubjectAltName(GeneralNames):\n    pass\n\nprint decode('0\\r\\x82\\x0bexample.com', asn1Spec=GeneralNames())\n</code></pre>\n\n<p>Clearly I got a little bored near the end and didn't fully specify the <code>GeneralName</code> type.  However, the test string should contain a <code>dNSName</code>, not one of the skipped values, so I hope it doesn't matter.</p>\n\n<p>When the program is run, it fails with an error I'm not able to interpret:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"x509.py\", line 94, in &lt;module&gt;\n    print decode('0\\r\\x82\\x0bexample.com', asn1Spec=GeneralNames())\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 493, in __call__\n    length, stGetValueDecoder, decodeFun\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 202, in valueDecoder\n    substrate, asn1Spec\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/codec/ber/decoder.py\", line 453, in __call__\n    __chosenSpec.getTypeMap().has_key(tagSet):\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/univ.py\", line 608, in getTypeMap\n    return Set.getComponentTypeMap(self)\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/univ.py\", line 535, in getComponentTypeMap\n    def getComponentTypeMap(self): return self._componentType.getTypeMap(1)\n  File \"/usr/lib/pymodules/python2.6/pyasn1/v1/type/namedtype.py\", line 126, in getTypeMap\n    'Duplicate type %s in map %s'%(k,self.__typeMap)\npyasn1.error.PyAsn1Error: Duplicate type TagSet(Tag(tagClass=0, tagFormat=0, tagId=22)) in map {TagSet(Tag(tagClass=0, tagFormat=0, tagId=22)): IA5String()}\n</code></pre>\n\n<p>Any tips on where I went wrong and how to successfully parse this extension type with pyasn1 would be much appreciated.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run <code>pip install pyasn1</code>, <code>pip install pyasn1-modules</code> and <code>pip install pyopenssl</code> otherwise you'll get import errors.</p>\n\n<pre><code># Import pyasn and the proper decode function\nimport pyasn1\nfrom pyasn1.codec.der.decoder import decode as asn1_decoder\n\n# Import SubjectAltName from rfc2459 module\nfrom pyasn1_modules.rfc2459 import SubjectAltName\n\n# Import native Python type encoder\nfrom pyasn1.codec.native.encoder import encode as nat_encoder\n\n# Import OpenSSL tools for working with certs.\nfrom OpenSSL import crypto\n# Read raw certificate file\nwith open('PATH/TO/CERTIFICATE.crt', 'r') as cert_f:\n    raw_cert = cert_f.read()\n\ncert = crypto.load_certificate(crypto.FILETYPE_PEM, raw_cert)\n\n# Note this example assumes SubjectAltName is the only Extension for this cert. \nraw_alt_names = cert.get_extension(0).get_data()\n\ndecoded_alt_names, _ = asn1_decoder(raw_alt_names, asn1Spec=SubjectAltName())\n\n# Unless a raw string of ASN.1 is what you need encode back to native Python types\npy_alt_names = nat_encoder(decoded_alt_names)\n\n# And Finally a plain Python list of UTF-8 encoded strings representing the SubjectAltNames\nsubject_alt_names = [ x['dNSName'].decode('utf-8') for x in py_alt_names]\n</code></pre>\n\n<p>The output of this will be something like</p>\n\n<pre><code>['cdn1.example.com', 'cdn2.example.com']\n</code></pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n\n\n<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each <code>NamedType</code> in <code>GeneralName.componentType</code> needs to be given tag information.  This is done with the <code>subtype</code> method.  For example, instead of:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String()),\n</code></pre>\n\n<p>the definition should be:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext,\n                            tag.tagFormatSimple, 1))),\n</code></pre>\n\n<p>where <code>1</code> comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre><code>GeneralName ::= CHOICE {\n   otherName                       [0]     OtherName,\n   rfc822Name                      [1]     IA5String,\n   dNSName                         [2]     IA5String,\n   x400Address                     [3]     ORAddress,\n   directoryName                   [4]     Name,\n   ediPartyName                    [5]     EDIPartyName,\n   uniformResourceIdentifier       [6]     IA5String,\n   iPAddress                       [7]     OCTET STRING,\n   registeredID                    [8]     OBJECT IDENTIFIER\n}\n</code></pre>\n\n<p>After defining a tag for each of these fields of the <code>componentType</code>, parsing succeeds:</p>\n\n<pre><code>(GeneralNames().setComponentByPosition(\n    0, GeneralName().setComponentByPosition(1, IA5String('example.com'))), '')\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run <code>pip install pyasn1</code>, <code>pip install pyasn1-modules</code> and <code>pip install pyopenssl</code> otherwise you'll get import errors.</p>\n\n<pre><code># Import pyasn and the proper decode function\nimport pyasn1\nfrom pyasn1.codec.der.decoder import decode as asn1_decoder\n\n# Import SubjectAltName from rfc2459 module\nfrom pyasn1_modules.rfc2459 import SubjectAltName\n\n# Import native Python type encoder\nfrom pyasn1.codec.native.encoder import encode as nat_encoder\n\n# Import OpenSSL tools for working with certs.\nfrom OpenSSL import crypto\n# Read raw certificate file\nwith open('PATH/TO/CERTIFICATE.crt', 'r') as cert_f:\n    raw_cert = cert_f.read()\n\ncert = crypto.load_certificate(crypto.FILETYPE_PEM, raw_cert)\n\n# Note this example assumes SubjectAltName is the only Extension for this cert. \nraw_alt_names = cert.get_extension(0).get_data()\n\ndecoded_alt_names, _ = asn1_decoder(raw_alt_names, asn1Spec=SubjectAltName())\n\n# Unless a raw string of ASN.1 is what you need encode back to native Python types\npy_alt_names = nat_encoder(decoded_alt_names)\n\n# And Finally a plain Python list of UTF-8 encoded strings representing the SubjectAltNames\nsubject_alt_names = [ x['dNSName'].decode('utf-8') for x in py_alt_names]\n</code></pre>\n\n<p>The output of this will be something like</p>\n\n<pre><code>['cdn1.example.com', 'cdn2.example.com']\n</code></pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each <code>NamedType</code> in <code>GeneralName.componentType</code> needs to be given tag information.  This is done with the <code>subtype</code> method.  For example, instead of:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String()),\n</code></pre>\n\n<p>the definition should be:</p>\n\n<pre><code>namedtype.NamedType('rfc822Name', char.IA5String().subtype(\n        implicitTag=tag.Tag(tag.tagClassContext,\n                            tag.tagFormatSimple, 1))),\n</code></pre>\n\n<p>where <code>1</code> comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre><code>GeneralName ::= CHOICE {\n   otherName                       [0]     OtherName,\n   rfc822Name                      [1]     IA5String,\n   dNSName                         [2]     IA5String,\n   x400Address                     [3]     ORAddress,\n   directoryName                   [4]     Name,\n   ediPartyName                    [5]     EDIPartyName,\n   uniformResourceIdentifier       [6]     IA5String,\n   iPAddress                       [7]     OCTET STRING,\n   registeredID                    [8]     OBJECT IDENTIFIER\n}\n</code></pre>\n\n<p>After defining a tag for each of these fields of the <code>componentType</code>, parsing succeeds:</p>\n\n<pre><code>(GeneralNames().setComponentByPosition(\n    0, GeneralName().setComponentByPosition(1, IA5String('example.com'))), '')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "16"}, "content_wo_code": "<p>I have some data that pyOpenSSL gave me,  .  This should be the value of a subjectAltName X509 extension.  I tried to encode the necessary parts of the ASN1 specification for this extension using pyasn1 (and based on one of the pyasn1 examples):</p>\n\n<pre> </pre>\n\n<p>Clearly I got a little bored near the end and didn't fully specify the   type.  However, the test string should contain a  , not one of the skipped values, so I hope it doesn't matter.</p>\n\n<p>When the program is run, it fails with an error I'm not able to interpret:</p>\n\n<pre> </pre>\n\n<p>Any tips on where I went wrong and how to successfully parse this extension type with pyasn1 would be much appreciated.</p>\n", "answer_wo_code": "<p>Coming in way late with this answer but instead of writing the ASN.1 Schema by hand you can also use the RF2459 module provided in <a href=\"https://github.com/etingof/pyasn1-modules\" rel=\"nofollow noreferrer\">pyasn1-modules</a> (also authored by Ilya Etingof)</p>\n\n<p>Minimally this code should work and will hopefully be enough to get you started on more complex ANS.1 constructs. Make sure you have run  ,   and   otherwise you'll get import errors.</p>\n\n<pre> </pre>\n\n<p>The output of this will be something like</p>\n\n<pre> </pre>\n\n<p>If the cert you are working on has multiple extensions you will need to use <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509.get_extension_count\" rel=\"nofollow noreferrer\">get_extension_count</a> from the X509 object and <a href=\"https://pyopenssl.org/en/stable/api/crypto.html#OpenSSL.crypto.X509Extension.get_short_name\" rel=\"nofollow noreferrer\">get_short_name</a> from the X509Extension object provided in pyopenssl.</p>\n\n\n<p>I posted this question on the pyasn1-users list and Ilya Etingof (the author of pyasn1) pointed out my mistake.  In brief, each   in   needs to be given tag information.  This is done with the   method.  For example, instead of:</p>\n\n<pre> </pre>\n\n<p>the definition should be:</p>\n\n<pre> </pre>\n\n<p>where   comes from the ASN.1 definition of <em>GeneralName</em>:</p>\n\n<pre> </pre>\n\n<p>After defining a tag for each of these fields of the  , parsing succeeds:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/choice"}, "class_func_label": {"type": "literal", "value": "choice"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nChoose a random element from a non-empty sequence."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/39348029"}, "title": {"type": "literal", "value": "[Pyasn1]: raise error.PyAsn1Error('Component type error %r vs %r' % (t, value))"}, "content": {"type": "literal", "value": "<p>I have only one choice and within that choice I want to pass the object of the class with only one field. </p>\n\n<p>Here is my code snippet:-</p>\n\n<pre><code>from pyasn1.type import univ, namedtype, tag, char, namedval, useful\nfrom pyasn1.codec.ber import encoder\n\nclass MiepPullWtdr(univ.Sequence):\n    componentType = namedtype.NamedTypes(namedtype.NamedType('wtdrId', univ.Integer().subtype(implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0))))\n\nclass ChoiceData(univ.Choice):\n    componentType = namedtype.NamedTypes(namedtype.NamedType('miepPullWtdr', MiepPullWtdr().subtype(implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0))))\n\nseqObj = MiepPullWtdr()\nseqObj.setComponentByName('wtdrId', 6555)\nchoiceObj = ChoiceData()\nchoiceObj.setComponentByName('miepPullWtdr', seqObj)\n</code></pre>\n\n<p>When I run my script test.py, it throws this error:-</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 18, in &lt;module&gt;\n  choiceObj.setComponentByName('miepPullWtdr', seqObj)\n  File \"/data/aman/cdr/lib/pyasn1-0.1.7/pyasn1/type/univ.py\", line 760, in setComponentByName  verifyConstraints\n  File \"/data/aman/cdr/lib/pyasn1-0.1.7/pyasn1/type/univ.py\", line 979, in setComponentByPosition\nself._verifyComponent(idx, value)\nFile \"/data/aman/cdr/lib/pyasn1-0.1.7/pyasn1/type/univ.py\", line 751, in _verifyComponent\nraise error.PyAsn1Error('Component type error %r vs %r' % (t, value))\npyasn1.error.PyAsn1Error: Component type error MiepPullWtdr() vs MiepPullWtdr().setComponentByPosition(0, Integer(6555))\n</code></pre>\n\n<p>Any help? Thanks.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>There is an inconsistency in how <code>MiepPullWtdr</code> type is ASN.1 tagged in its stand-alone definition versus as a <code>ChoiceData</code> component. I am not sure what exactly your intention is, here is one of possibly many consistent versions:</p>\n\n<pre><code>from pyasn1.type import univ, namedtype, tag\n\nclass MiepPullWtdr(univ.Sequence):\n    tagSet = univ.Sequence.tagSet.tagImplicitly(\n        tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)\n    )\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('wtdrId', univ.Integer().subtype(implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)))\n    )\n\nclass ChoiceData(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('miepPullWtdr', MiepPullWtdr())\n    )\n\nseqObj = MiepPullWtdr()\nseqObj['wtdrId'] = 6555\nchoiceObj = ChoiceData()\nchoiceObj['miepPullWtdr'] = seqObj\n\nprint(choiceObj.prettyPrint())\n</code></pre>\n\n\n<p>There is an inconsistency in how <code>MiepPullWtdr</code> type is ASN.1 tagged in its stand-alone definition versus as a <code>ChoiceData</code> component. I am not sure what exactly your intention is, here is one of possibly many consistent versions:</p>\n\n<pre><code>from pyasn1.type import univ, namedtype, tag\n\nclass MiepPullWtdr(univ.Sequence):\n    tagSet = univ.Sequence.tagSet.tagImplicitly(\n        tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)\n    )\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('wtdrId', univ.Integer().subtype(implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)))\n    )\n\nclass ChoiceData(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('miepPullWtdr', MiepPullWtdr())\n    )\n\nseqObj = MiepPullWtdr()\nseqObj['wtdrId'] = 6555\nchoiceObj = ChoiceData()\nchoiceObj['miepPullWtdr'] = seqObj\n\nprint(choiceObj.prettyPrint())\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>There is an inconsistency in how <code>MiepPullWtdr</code> type is ASN.1 tagged in its stand-alone definition versus as a <code>ChoiceData</code> component. I am not sure what exactly your intention is, here is one of possibly many consistent versions:</p>\n\n<pre><code>from pyasn1.type import univ, namedtype, tag\n\nclass MiepPullWtdr(univ.Sequence):\n    tagSet = univ.Sequence.tagSet.tagImplicitly(\n        tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)\n    )\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('wtdrId', univ.Integer().subtype(implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0)))\n    )\n\nclass ChoiceData(univ.Choice):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('miepPullWtdr', MiepPullWtdr())\n    )\n\nseqObj = MiepPullWtdr()\nseqObj['wtdrId'] = 6555\nchoiceObj = ChoiceData()\nchoiceObj['miepPullWtdr'] = seqObj\n\nprint(choiceObj.prettyPrint())\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have only one choice and within that choice I want to pass the object of the class with only one field. </p>\n\n<p>Here is my code snippet:-</p>\n\n<pre> </pre>\n\n<p>When I run my script test.py, it throws this error:-</p>\n\n<pre> </pre>\n\n<p>Any help? Thanks.</p>\n", "answer_wo_code": "<p>There is an inconsistency in how   type is ASN.1 tagged in its stand-alone definition versus as a   component. I am not sure what exactly your intention is, here is one of possibly many consistent versions:</p>\n\n<pre> </pre>\n\n\n<p>There is an inconsistency in how   type is ASN.1 tagged in its stand-alone definition versus as a   component. I am not sure what exactly your intention is, here is one of possibly many consistent versions:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sqlalchemy.types.Text"}, "class_func_label": {"type": "literal", "value": "sqlalchemy.types.Text"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "A variably sized string type.\n\n    In SQL, usually corresponds to CLOB or TEXT. Can also take Python\n    unicode objects and encode to the database's encoding in bind\n    params (and the reverse for result sets.)  In general, TEXT objects\n    do not have a length; while some databases will accept a length\n    argument here, it will be rejected by others.\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/29349189"}, "title": {"type": "literal", "value": "SQLALCHEMY/PANDAS - SQLAlchemy reading column as CLOB for Pandas to_sql"}, "content": {"type": "literal", "value": "<p>I have written a dataset to a data-frame.  </p>\n\n<pre><code>inv.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 43839 entries, 0 to 43838\nData columns (total 16 columns):\nMST_CO                 43839 non-null object\nLOAD_DATE              43839 non-null object\nWHSE_CODE              43839 non-null object\nITEM_NO                43839 non-null object\nITEM_ID                43839 non-null int64\nLOCATION               43839 non-null object\nLOT_NO                 43839 non-null object\nLOT_STATUS             43833 non-null object\nLOT_CREATED_DATE       43839 non-null datetime64[ns]\nLOT_EXPIRATION_DATE    43839 non-null object\nDATE_RECEIVED          43839 non-null datetime64[ns]\nONHAND_QTY1            43839 non-null float64\nUOM1                   43839 non-null object\nONHAND_QTY2            43418 non-null float64\nUOM2                   43408 non-null object\nSOURCE                 43839 non-null object\ndtypes: datetime64[ns](2), float64(2), int64(1), object(11)\n</code></pre>\n\n<p>When I attempt to write the dataframe to SQL, I get the errors below. </p>\n\n<pre><code>inv.to_sql('inventory', db2, 'MST', if_exists='append', index=False, chunksize=3000)\n2015-03-30 09:33:10,656 INFO sqlalchemy.engine.base.Engine SELECT \"SYSCAT\".\"TABLES\".\"TABNAME\" \nFROM \"SYSCAT\".\"TABLES\" \nWHERE \"SYSCAT\".\"TABLES\".\"TABSCHEMA\" = ? AND \"SYSCAT\".\"TABLES\".\"TABNAME\" = ?\n2015-03-30 09:33:10,656 INFO sqlalchemy.engine.base.Engine ('DWETL', 'INVENTORY')\n2015-03-30 09:33:10,731 INFO sqlalchemy.engine.base.Engine \nCREATE TABLE inventory (\n\"MST_CO\" CLOB, \n\"LOAD_DATE\" DATE, \n\"WHSE_CODE\" CLOB, \n\"ITEM_NO\" CLOB, \n\"ITEM_ID\" BIGINT, \n\"LOCATION\" CLOB, \n\"LOT_NO\" CLOB, \n\"LOT_STATUS\" CLOB, \n\"LOT_CREATED_DATE\" TIMESTAMP, \n\"LOT_EXPIRATION_DATE\" DATE, \n\"DATE_RECEIVED\" TIMESTAMP, \n\"ONHAND_QTY1\" FLOAT(53), \n\"UOM1\" CLOB, \n\"ONHAND_QTY2\" FLOAT(53), \n\"UOM2\" CLOB, \n\"SOURCE\" CLOB\n)\n2015-03-30 09:33:10,731 INFO sqlalchemy.engine.base.Engine ()\n2015-03-30 09:33:10,755 INFO sqlalchemy.engine.base.Engine ROLLBACK\nTraceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py\", line 977, in to_sql\ndtype=dtype)\nFile \"/usr/local/lib/python2.7/dist-packages/pandas/io/sql.py\", line 538, in to_sql\nchunksize=chunksize, dtype=dtype)\nFile \"/usr/local/lib/python2.7/dist-packages/pandas/io/sql.py\", line 1176, in to_sql\ntable.create()\nFile \"/usr/local/lib/python2.7/dist-packages/pandas/io/sql.py\", line 649, in create\nself._execute_create()\nFile \"/usr/local/lib/python2.7/dist-packages/pandas/io/sql.py\", line 634, in _execute_create\nself.table.create()\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/schema.py\", line 707, in create\ncheckfirst=checkfirst)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1728, in _run_visitor\nconn._run_visitor(visitorcallable, element, **kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1357, in _run_visitor\n**kwargs).traverse_single(element)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/visitors.py\", line 120, in traverse_single\nreturn meth(obj, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/ddl.py\", line 732, in visit_table\nself.connection.execute(CreateTable(table))\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 841, in execute\nreturn meth(self, multiparams, params)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/ddl.py\", line 69, in _execute_on_connection\nreturn connection._execute_ddl(self, multiparams, params)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 895, in _execute_ddl\ncompiled\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1070, in _execute_context\ncontext)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1271, in _handle_dbapi_exception\nexc_info\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/compat.py\", line 199, in raise_from_cause\nreraise(type(exception), exception, tb=exc_tb)\nFile \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1063, in _execute_context\ncontext)\nFile \"/usr/local/lib/python2.7/dist-packages/ibm_db_sa/ibm_db.py\", line 106, in do_execute\ncursor.execute(statement, parameters)\nFile \"/usr/local/lib/python2.7/dist-packages/ibm_db_dbi.py\", line 1335, in execute\nself._execute_helper(parameters)\nFile \"/usr/local/lib/python2.7/dist-packages/ibm_db_dbi.py\", line 1247, in _execute_helper\nraise self.messages[len(self.messages) - 1]\nsqlalchemy.exc.ProgrammingError: (ProgrammingError) ibm_db_dbi::ProgrammingError: Statement Execute Failed: [IBM][CLI Driver][DB2/LINUXX8664] SQL1666N  The table definition statement failed because some functionality was specified in the table definition that is not supported with the table type.  Unsupported functionality: \"CLOB\".  SQLSTATE=42613 SQLCODE=-1666 '\\nCREATE TABLE inventory (\\n\\t\"MST_CO\" CLOB, \\n\\t\"LOAD_DATE\" DATE, \\n\\t\"WHSE_CODE\" CLOB, \\n\\t\"ITEM_NO\" CLOB, \\n\\t\"ITEM_ID\" BIGINT, \\n\\t\"LOCATION\" CLOB, \\n\\t\"LOT_NO\" CLOB, \\n\\t\"LOT_STATUS\" CLOB, \\n\\t\"LOT_CREATED_DATE\" TIMESTAMP, \\n\\t\"LOT_EXPIRATION_DATE\" DATE, \\n\\t\"DATE_RECEIVED\" TIMESTAMP, \\n\\t\"ONHAND_QTY1\" FLOAT(53), \\n\\t\"UOM1\" CLOB, \\n\\t\"ONHAND_QTY2\" FLOAT(53), \\n\\t\"UOM2\" CLOB, \\n\\t\"SOURCE\" CLOB\\n)\\n\\n' ()\n</code></pre>\n\n<p>How can I get SQLAlchemy and Pandas to play nicely? I just need to convert the CLOB to be read as STR. Thanks!</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can specify the SQL type to use for a certain column with the <code>dtype</code> keyword argument (see <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#sql-data-types\" rel=\"nofollow\">docs</a>):</p>\n\n<pre><code>from sqlalchemy.types import String\ninv.to_sql('inventory', db2, dtype={'col_name': String})\n</code></pre>\n\n<p>By default, pandas uses <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/type_basics.html#sqlalchemy.types.Text\" rel=\"nofollow\"><code>TEXT</code> type</a> for object/string columns, which sqlalchemy maps to <code>CLOB</code> or <code>TEXT</code>, but apparantly your database does not know this type. So with the above, you can manually specify a type that your database does know.</p>\n\n<p>You seem to have multiple columns. So to make this mapping automatic for all object dtyped columns. You can select the columns with dtype object:</p>\n\n<pre><code>cols = df.dtypes[df.dtypes=='object'].index\ntype_mapping = {col : String for col in cols }\ninv.to_sql('inventory', db2, dtype=type_mapping)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>You can specify the SQL type to use for a certain column with the <code>dtype</code> keyword argument (see <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#sql-data-types\" rel=\"nofollow\">docs</a>):</p>\n\n<pre><code>from sqlalchemy.types import String\ninv.to_sql('inventory', db2, dtype={'col_name': String})\n</code></pre>\n\n<p>By default, pandas uses <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/type_basics.html#sqlalchemy.types.Text\" rel=\"nofollow\"><code>TEXT</code> type</a> for object/string columns, which sqlalchemy maps to <code>CLOB</code> or <code>TEXT</code>, but apparantly your database does not know this type. So with the above, you can manually specify a type that your database does know.</p>\n\n<p>You seem to have multiple columns. So to make this mapping automatic for all object dtyped columns. You can select the columns with dtype object:</p>\n\n<pre><code>cols = df.dtypes[df.dtypes=='object'].index\ntype_mapping = {col : String for col in cols }\ninv.to_sql('inventory', db2, dtype=type_mapping)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I have written a dataset to a data-frame.  </p>\n\n<pre> </pre>\n\n<p>When I attempt to write the dataframe to SQL, I get the errors below. </p>\n\n<pre> </pre>\n\n<p>How can I get SQLAlchemy and Pandas to play nicely? I just need to convert the CLOB to be read as STR. Thanks!</p>\n", "answer_wo_code": "<p>You can specify the SQL type to use for a certain column with the   keyword argument (see <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#sql-data-types\" rel=\"nofollow\">docs</a>):</p>\n\n<pre> </pre>\n\n<p>By default, pandas uses <a href=\"http://docs.sqlalchemy.org/en/rel_0_9/core/type_basics.html#sqlalchemy.types.Text\" rel=\"nofollow\">  type</a> for object/string columns, which sqlalchemy maps to   or  , but apparantly your database does not know this type. So with the above, you can manually specify a type that your database does know.</p>\n\n<p>You seem to have multiple columns. So to make this mapping automatic for all object dtyped columns. You can select the columns with dtype object:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/UnicodeEncodeError"}, "class_func_label": {"type": "literal", "value": "UnicodeEncodeError"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "Unicode encoding error."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/33442332"}, "title": {"type": "literal", "value": "Another UnicodeEncodeError when using pandas method to_sql with MySQL"}, "content": {"type": "literal", "value": "<p>I posted on stack overflow a few days ago with a <a href=\"https://stackoverflow.com/questions/33337798/unicodeencodeerror-when-using-pandas-method-to-sql-on-a-dataframe-with-unicode-c\">similar problem</a> (which was solved), and I'm not sure what the proper etiquette is here, but I'm making a new post.</p>\n\n<p>Basically, I am getting a UnicodeEncodeError when I try to write a pandas DataFrame to a MySQL database. I can reproduce the error with the following code:</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql://root:@localhost/testdb')\ndf = pd.DataFrame([[u'\\u2013',2],['e',4]], index = ['a','b'], columns = ['c','d'])\ndf.to_sql('data', engine, if_exists = 'replace', index = False)\n</code></pre>\n\n<p>Here is the error:</p>\n\n<pre><code>UnicodeEncodeError: 'latin-1' codec can't encode character u'\\u2013' in position 0: ordinal not in range(256)\n</code></pre>\n\n<p>And this is the last relevant line of the traceback:</p>\n\n<pre><code>C:\\Anaconda\\lib\\site-packages\\sqlalchemy\\dialects\\mysql\\mysqldb.pyc in do_executemany(self, cursor, statement, parameters, context)\n     93 \n     94     def do_executemany(self, cursor, statement, parameters, context=None):\n---&gt; 95         rowcount = cursor.executemany(statement, parameters)\n     96         if context is not None:\n     97             context._rowcount = rowcount\n</code></pre>\n\n<p>When I was having this issue before, it was due to a bug in pandas.io.sql, and the fix was to change <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow noreferrer\">a few lines of code</a>. This worked fine until I encountered characters outside the range of the latin-1 codec.</p>\n\n<p>Do you guys have any suggestions?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Well, within an hour of posting my question, I already figured it out. Maybe I should have done a bit more research before posting.</p>\n\n<p>The problem is that sqlalchemy needs to be configured to use utf-8 encoding. The solution in the above code would be to change line 3 to:</p>\n\n<pre><code>engine = create_engine('mysql://root:@localhost/testdb?charset=utf8', encoding = 'utf-8')\n</code></pre>\n\n\n<p>Well, within an hour of posting my question, I already figured it out. Maybe I should have done a bit more research before posting.</p>\n\n<p>The problem is that sqlalchemy needs to be configured to use utf-8 encoding. The solution in the above code would be to change line 3 to:</p>\n\n<pre><code>engine = create_engine('mysql://root:@localhost/testdb?charset=utf8', encoding = 'utf-8')\n</code></pre>\n\n\n<p><code>\\u2013</code> is an \"en dash\".  Perhaps some word processor is creating that?  Perhaps you would be happy enough with a simple <code>-</code>?</p>\n\n<p>See <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode</a></p>\n\n\n<p><code>\\u2013</code> is an \"en dash\".  Perhaps some word processor is creating that?  Perhaps you would be happy enough with a simple <code>-</code>?</p>\n\n<p>See <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode</a></p>\n"}, "answer_1": {"type": "literal", "value": "<p>Well, within an hour of posting my question, I already figured it out. Maybe I should have done a bit more research before posting.</p>\n\n<p>The problem is that sqlalchemy needs to be configured to use utf-8 encoding. The solution in the above code would be to change line 3 to:</p>\n\n<pre><code>engine = create_engine('mysql://root:@localhost/testdb?charset=utf8', encoding = 'utf-8')\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "8"}, "answer_2": {"type": "literal", "value": "<p><code>\\u2013</code> is an \"en dash\".  Perhaps some word processor is creating that?  Perhaps you would be happy enough with a simple <code>-</code>?</p>\n\n<p>See <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode</a></p>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>I posted on stack overflow a few days ago with a <a href=\"https://stackoverflow.com/questions/33337798/unicodeencodeerror-when-using-pandas-method-to-sql-on-a-dataframe-with-unicode-c\">similar problem</a> (which was solved), and I'm not sure what the proper etiquette is here, but I'm making a new post.</p>\n\n<p>Basically, I am getting a UnicodeEncodeError when I try to write a pandas DataFrame to a MySQL database. I can reproduce the error with the following code:</p>\n\n<pre> </pre>\n\n<p>Here is the error:</p>\n\n<pre> </pre>\n\n<p>And this is the last relevant line of the traceback:</p>\n\n<pre> </pre>\n\n<p>When I was having this issue before, it was due to a bug in pandas.io.sql, and the fix was to change <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow noreferrer\">a few lines of code</a>. This worked fine until I encountered characters outside the range of the latin-1 codec.</p>\n\n<p>Do you guys have any suggestions?</p>\n", "answer_wo_code": "<p>Well, within an hour of posting my question, I already figured it out. Maybe I should have done a bit more research before posting.</p>\n\n<p>The problem is that sqlalchemy needs to be configured to use utf-8 encoding. The solution in the above code would be to change line 3 to:</p>\n\n<pre> </pre>\n\n\n<p>Well, within an hour of posting my question, I already figured it out. Maybe I should have done a bit more research before posting.</p>\n\n<p>The problem is that sqlalchemy needs to be configured to use utf-8 encoding. The solution in the above code would be to change line 3 to:</p>\n\n<pre> </pre>\n\n\n<p>  is an \"en dash\".  Perhaps some word processor is creating that?  Perhaps you would be happy enough with a simple  ?</p>\n\n<p>See <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode</a></p>\n\n\n<p>  is an \"en dash\".  Perhaps some word processor is creating that?  Perhaps you would be happy enough with a simple  ?</p>\n\n<p>See <a href=\"https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode\" rel=\"nofollow noreferrer\">https://docs.sqlalchemy.org/en/latest/dialects/mysql.html#mysql-unicode</a></p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37307142"}, "title": {"type": "literal", "value": "Replacing Bad Data in Pandas Data Frame"}, "content": {"type": "literal", "value": "<p>In Python 2.7, I'm connecting to an external data source using the following:</p>\n\n<pre><code>import pypyodbc\nimport pandas as pd\nimport datetime\nimport csv\nimport boto3\nimport os\n\n# Connect to the DataSource\nconn = pypyodbc.connect(\"DSN = FAKE DATA SOURCE; UID=FAKEID; PWD=FAKEPASSWORD\")\n\n# Specify the query we're going to run on it\nscript = (\"SELECT * FROM table\")\n\n# Create a dataframe from the above query\ndf = pd.read_sql_query(script, conn)\n</code></pre>\n\n<p>I get the following error:</p>\n\n<pre><code>C:\\Python27\\python.exe \"C:/Thing.py\"\nTraceback (most recent call last):\n  File \"C:/Thing.py\", line 30, in &lt;module&gt;\n   df = pd.read_sql_query(script,conn)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 431, in read_sql_query\n   parse_dates=parse_dates, chunksize=chunksize)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 1608, in read_query\n   data = self._fetchall_as_list(cursor)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 1617, in _fetchall_as_list\n   result = cur.fetchall()\n  File \"build\\bdist.win32\\egg\\pypyodbc.py\", line 1819, in fetchall\n  File \"build\\bdist.win32\\egg\\pypyodbc.py\", line 1871, in fetchone\nValueError: could not convert string to float: ?\n</code></pre>\n\n<p>It's seems to me that in one of the float columns, there is a '?' symbol for some reason. I've reached out to the owner of the data source, but they cannot change the underlying table.</p>\n\n<p>Is there a way to replace incorrect data like this using pandas? I've tried using replace after the <code>read_sql_query</code> statement, but I get the same error.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Hard to know for certain without having your data obviously, but you could try setting <code>coerce_float</code> to <code>False</code>, i.e. replace your last line with </p>\n\n<pre><code>df = pd.read_sql_query(script, conn, coerce_float=False)\n</code></pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of <code>read_sql_query</code></a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Hard to know for certain without having your data obviously, but you could try setting <code>coerce_float</code> to <code>False</code>, i.e. replace your last line with </p>\n\n<pre><code>df = pd.read_sql_query(script, conn, coerce_float=False)\n</code></pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of <code>read_sql_query</code></a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>In Python 2.7, I'm connecting to an external data source using the following:</p>\n\n<pre> </pre>\n\n<p>I get the following error:</p>\n\n<pre> </pre>\n\n<p>It's seems to me that in one of the float columns, there is a '?' symbol for some reason. I've reached out to the owner of the data source, but they cannot change the underlying table.</p>\n\n<p>Is there a way to replace incorrect data like this using pandas? I've tried using replace after the   statement, but I get the same error.</p>\n", "answer_wo_code": "<p>Hard to know for certain without having your data obviously, but you could try setting   to  , i.e. replace your last line with </p>\n\n<pre> </pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of  </a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql_query"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql_query"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query into a DataFrame.\n\nReturns a DataFrame corresponding to the result set of the query\nstring. Optionally provide an `index_col` parameter to use one of the\ncolumns as the index, otherwise default integer index will be used.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/37307142"}, "title": {"type": "literal", "value": "Replacing Bad Data in Pandas Data Frame"}, "content": {"type": "literal", "value": "<p>In Python 2.7, I'm connecting to an external data source using the following:</p>\n\n<pre><code>import pypyodbc\nimport pandas as pd\nimport datetime\nimport csv\nimport boto3\nimport os\n\n# Connect to the DataSource\nconn = pypyodbc.connect(\"DSN = FAKE DATA SOURCE; UID=FAKEID; PWD=FAKEPASSWORD\")\n\n# Specify the query we're going to run on it\nscript = (\"SELECT * FROM table\")\n\n# Create a dataframe from the above query\ndf = pd.read_sql_query(script, conn)\n</code></pre>\n\n<p>I get the following error:</p>\n\n<pre><code>C:\\Python27\\python.exe \"C:/Thing.py\"\nTraceback (most recent call last):\n  File \"C:/Thing.py\", line 30, in &lt;module&gt;\n   df = pd.read_sql_query(script,conn)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 431, in read_sql_query\n   parse_dates=parse_dates, chunksize=chunksize)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 1608, in read_query\n   data = self._fetchall_as_list(cursor)\n  File \"C:\\Python27\\lib\\site-packages\\pandas-0.18.1-py2.7-win32.egg\\pandas\\io\\sql.py\", line 1617, in _fetchall_as_list\n   result = cur.fetchall()\n  File \"build\\bdist.win32\\egg\\pypyodbc.py\", line 1819, in fetchall\n  File \"build\\bdist.win32\\egg\\pypyodbc.py\", line 1871, in fetchone\nValueError: could not convert string to float: ?\n</code></pre>\n\n<p>It's seems to me that in one of the float columns, there is a '?' symbol for some reason. I've reached out to the owner of the data source, but they cannot change the underlying table.</p>\n\n<p>Is there a way to replace incorrect data like this using pandas? I've tried using replace after the <code>read_sql_query</code> statement, but I get the same error.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Hard to know for certain without having your data obviously, but you could try setting <code>coerce_float</code> to <code>False</code>, i.e. replace your last line with </p>\n\n<pre><code>df = pd.read_sql_query(script, conn, coerce_float=False)\n</code></pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of <code>read_sql_query</code></a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Hard to know for certain without having your data obviously, but you could try setting <code>coerce_float</code> to <code>False</code>, i.e. replace your last line with </p>\n\n<pre><code>df = pd.read_sql_query(script, conn, coerce_float=False)\n</code></pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of <code>read_sql_query</code></a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>In Python 2.7, I'm connecting to an external data source using the following:</p>\n\n<pre> </pre>\n\n<p>I get the following error:</p>\n\n<pre> </pre>\n\n<p>It's seems to me that in one of the float columns, there is a '?' symbol for some reason. I've reached out to the owner of the data source, but they cannot change the underlying table.</p>\n\n<p>Is there a way to replace incorrect data like this using pandas? I've tried using replace after the   statement, but I get the same error.</p>\n", "answer_wo_code": "<p>Hard to know for certain without having your data obviously, but you could try setting   to  , i.e. replace your last line with </p>\n\n<pre> </pre>\n\n<p>See the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"nofollow\">documentation of  </a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.io.excel.read_excel"}, "class_func_label": {"type": "literal", "value": "pandas.io.excel.read_excel"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead an Excel file into a pandas DataFrame.\n\nSupport both `xls` and `xlsx` file extensions from a local filesystem or URL.\nSupport an option to read a single sheet or a list of sheets.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/20486453"}, "title": {"type": "literal", "value": "Pandas dataframe from excel named range"}, "content": {"type": "literal", "value": "<p>How do you read from a named range in excel into a pandas dataframe?  The read_excel is designed to read entire sheets within a workbook.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can do this in a round about way using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html\" rel=\"nofollow\">read_excel</a>, it offers:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>skiprows : list-like\n    Rows to skip at the beginning (0-indexed)\n\nskip_footer : int, default 0\n    Rows at the end to skip (0-indexed)\n\nparse_cols : int or list, default None\n        If None then parse all columns,\n        If int then indicates last column to be parsed\n        If list of ints then indicates list of column numbers to be parsed\n        If string then indicates comma separated list of column names and column ranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d)\n</code></pre>\n\n<p>This means if you know the column names and the row numbers (presumably what you mean  by \"named range\"?) you can select just that section to make the DataFrame.</p>\n\n\n<p>Here is the way I use openpyxl to copy a range in a [[]] :</p>\n\n<pre><code>wb = load_workbook(filename=xlPath)\nws, range= next(wb.defined_names[\"rangename\"].destinations)\nmaterials = [[cell.value for cell in row] for row in wb[ws][range]]\n</code></pre>\n\n\n<p>Maybe someday pandas will support this natively. Until then, I use a helper function:</p>\n\n<pre><code>import pandas as pd\nimport openpyxl\n\ndef data_frame_from_xlsx(xlsx_file, range_name):\n    \"\"\" Get a single rectangular region from the specified file.\n    range_name can be a standard Excel reference ('Sheet1!A2:B7') or \n    refer to a named region ('my_cells').\"\"\"\n    wb = openpyxl.load_workbook(xlsx_file, data_only=True, read_only=True)\n    if '!' in range_name:\n        # passed a worksheet!cell reference\n        ws_name, reg = range_name.split('!')\n        if ws_name.startswith(\"'\") and ws_name.endswith(\"'\"):\n            # optionally strip single quotes around sheet name\n            ws_name = ws_name[1:-1]\n        region = wb[ws_name][reg]\n    else:\n        # passed a named range; find the cells in the workbook\n        full_range = wb.get_named_range(range_name)\n        if full_range is None:\n            raise ValueError(\n                'Range \"{}\" not found in workbook \"{}\".'.format(range_name, xlsx_file)\n            )\n        # convert to list (openpyxl 2.3 returns a list but 2.4+ returns a generator)\n        destinations = list(full_range.destinations) \n        if len(destinations) &gt; 1:\n            raise ValueError(\n                'Range \"{}\" in workbook \"{}\" contains more than one region.'\n                .format(range_name, xlsx_file)\n            )\n        ws, reg = destinations[0]\n        # convert to worksheet object (openpyxl 2.3 returns a worksheet object \n        # but 2.4+ returns the name of a worksheet)\n        if isinstance(ws, str):\n            ws = wb[ws]\n        region = ws[reg]\n    df = pd.DataFrame([cell.value for cell in row] for row in region)\n    return df\n</code></pre>\n\n\n<p>To quote the <a href=\"http://office.microsoft.com/en-us/excel-help/define-and-use-names-in-formulas-HA010147120.aspx\" rel=\"nofollow\">Microsoft Office help pages</a>!: </p>\n\n<blockquote>\n  <p>A [named range] is a meaningful shorthand that makes it easier to understand the purpose of a cell reference, constant, formula, or table, each of which may be difficult to comprehend at first glance.\"</p>\n</blockquote>\n\n<p>Named ranges are furthermore frequently used in spreadsheets to easier access data through ODBC and are particularly useful when there are several data ranges within the same worksheet. To connect via ODBC to Excel, simply choose the appropriate <a href=\"http://www.connectionstrings.com/excel-2007-odbc/\" rel=\"nofollow\">Excel driver</a> and send an SQL statement such as e.g.:</p>\n\n<pre><code>SELECT * \nFROM namedRange\n</code></pre>\n\n<p>The useful command in Pandas would probably be read_sql.</p>\n\n<p>In Windows, this solution requires however that you align/streamline the installed software versions (32-bit or 64-bit) of Excel, the ODBC driver and the software package from which you open the ODBC connection. As an example, an installed Excel 32-bit version will require a 32-bit ODBC driver and normally a 32-bit installation of Python. <em>Note: this latter point remains to be confirmed for the Python case (I'm a beginner to Python), but I can definitely confirm this point for ODBC connections launched from SAS, SPSS or Stata.</em></p>\n\n<p>The previous requirement is a very significant drawback and actually speaks in favor of any solution which does not involve ODBC at all. That said, it would be nice if read_Excel provided such a facility. In this context, it is interesting to note that SAS, SPSS and Stata currently do not allow direct access to named ranges in their respective Excel filters - so <em>maybe</em> there is an objective reason for this lacking feature...</p>\n\n\n<p>You can use the underlying <code>xlrd</code> package to do this. </p>\n\n<p>The <code>xlrd</code> package comes with an <code>examples</code> directory which contains <code>xlrdnameAPIdemo.py</code>, as documented <a href=\"https://secure.simplistix.co.uk/svn/xlrd/trunk/xlrd/doc/xlrd.html?p=4966\" rel=\"nofollow\">here</a>.</p>\n\n<p>In a nutshell for the named range <code>print_area</code> try:</p>\n\n<pre><code>book = xlrd.open_workbook('examples/namesdemo.xls')\nname_obj = book.name_map['print_area'][0]\nprint name_obj.__dict__\n</code></pre>\n\n<p>You'll see <code>name_obj</code> has an entry:</p>\n\n<pre><code>'result': Operand(kind=oREF, value=[Ref3D(coords=(2, 3, 0, 4, 0, 14))], text=u'Sheet3!$A$1:$N$4')\n</code></pre>\n\n<p>which you can follow the example to interpret, though it doesn't look straightforward - eg. the range may be relative or not, depending on the value <code>result.kind</code>.</p>\n\n<p>Further, when I tried to use this to read my own spreadsheet (created on a Mac), I found <code>result</code> was <code>None</code>; instead, the only ref to the range in <code>name_obj</code> was:</p>\n\n<pre><code>'formula_text': u'Sheet1!$B$6:$E$11'\n</code></pre>\n\n<p>So there may be a way to make this work in a general case, but it looks like it would take some trial and error.</p>\n\n<p>As an alternative, if you can format your spreadsheet so that instead of named ranges, your table follows in the rows immediately after a unique heading (<code>key</code>), and finishes with a blank row, here is a function which finds the right parameters to send to <code>pd.read_excel</code>:</p>\n\n<pre><code>def table_position(path, sheet_name, key):\n    \"\"\"\n    Find the start and end rows of a table in an Excel spreadsheet\n    based on the first occurence of key text on the sheet, and down\n    to the first blank line.\n\n    Returns (col, start_row, end_row, skip_footer)\n\n    where: \n        col is the column number containing the key text,\n        start_row is the row after this, \n        end_row is the row number of the next blank line,\n        skip_footer is how many rows from the end of the sheet this is.\n\n    You can then read in the table with:\n        x = pd.read_excel(path, sheet_name, skiprows=start, skip_footer=skip_footer, header=0)\n        x = x.dropna(axis=1, how='all')\n    \"\"\"\n    import xlrd\n    book = xlrd.open_workbook(path)\n    sheet = book.sheet_by_name(sheet_name)\n    # find the first occurrence of the key, and the next line break\n    (col, start, end) = (-1, -1, sheet.nrows)\n    for rownum in xrange(sheet.nrows):\n        if col&lt;0: # look for key to start the table off\n            try:\n                test_col = next(c for c in xrange(sheet.ncols) if sheet.cell(rownum, c).value==key)\n            except StopIteration:\n                pass\n            else:\n                col, start = test_col, rownum+1 # row after key text is the start\n        else: # test for blank line as end of table\n            if not [True for cell in sheet.row(rownum) if cell.value]:\n                end = rownum\n                break\n    skip_footer = sheet.nrows - end\n    return (col, start, end, skip_footer)\n</code></pre>\n\n<p>If you do follow this with a <code>pd.read_excel</code> then you are reading the data file twice, which is silly, but you get the idea.</p>\n\n\n<p>Well, it's been a while, but I would definitely recommend giving a shot to <a href=\"https://www.xlwings.org/\" rel=\"nofollow noreferrer\">xlwings</a>. </p>\n\n<p>See also <a href=\"https://stackoverflow.com/questions/43324967/xlwings-take-value-from-defined-names\">Xlwings take value from defined names\n</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You can do this in a round about way using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html\" rel=\"nofollow\">read_excel</a>, it offers:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>skiprows : list-like\n    Rows to skip at the beginning (0-indexed)\n\nskip_footer : int, default 0\n    Rows at the end to skip (0-indexed)\n\nparse_cols : int or list, default None\n        If None then parse all columns,\n        If int then indicates last column to be parsed\n        If list of ints then indicates list of column numbers to be parsed\n        If string then indicates comma separated list of column names and column ranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d)\n</code></pre>\n\n<p>This means if you know the column names and the row numbers (presumably what you mean  by \"named range\"?) you can select just that section to make the DataFrame.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>Here is the way I use openpyxl to copy a range in a [[]] :</p>\n\n<pre><code>wb = load_workbook(filename=xlPath)\nws, range= next(wb.defined_names[\"rangename\"].destinations)\nmaterials = [[cell.value for cell in row] for row in wb[ws][range]]\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "answer_3": {"type": "literal", "value": "<p>Maybe someday pandas will support this natively. Until then, I use a helper function:</p>\n\n<pre><code>import pandas as pd\nimport openpyxl\n\ndef data_frame_from_xlsx(xlsx_file, range_name):\n    \"\"\" Get a single rectangular region from the specified file.\n    range_name can be a standard Excel reference ('Sheet1!A2:B7') or \n    refer to a named region ('my_cells').\"\"\"\n    wb = openpyxl.load_workbook(xlsx_file, data_only=True, read_only=True)\n    if '!' in range_name:\n        # passed a worksheet!cell reference\n        ws_name, reg = range_name.split('!')\n        if ws_name.startswith(\"'\") and ws_name.endswith(\"'\"):\n            # optionally strip single quotes around sheet name\n            ws_name = ws_name[1:-1]\n        region = wb[ws_name][reg]\n    else:\n        # passed a named range; find the cells in the workbook\n        full_range = wb.get_named_range(range_name)\n        if full_range is None:\n            raise ValueError(\n                'Range \"{}\" not found in workbook \"{}\".'.format(range_name, xlsx_file)\n            )\n        # convert to list (openpyxl 2.3 returns a list but 2.4+ returns a generator)\n        destinations = list(full_range.destinations) \n        if len(destinations) &gt; 1:\n            raise ValueError(\n                'Range \"{}\" in workbook \"{}\" contains more than one region.'\n                .format(range_name, xlsx_file)\n            )\n        ws, reg = destinations[0]\n        # convert to worksheet object (openpyxl 2.3 returns a worksheet object \n        # but 2.4+ returns the name of a worksheet)\n        if isinstance(ws, str):\n            ws = wb[ws]\n        region = ws[reg]\n    df = pd.DataFrame([cell.value for cell in row] for row in region)\n    return df\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "3"}, "answer_4": {"type": "literal", "value": "<p>To quote the <a href=\"http://office.microsoft.com/en-us/excel-help/define-and-use-names-in-formulas-HA010147120.aspx\" rel=\"nofollow\">Microsoft Office help pages</a>!: </p>\n\n<blockquote>\n  <p>A [named range] is a meaningful shorthand that makes it easier to understand the purpose of a cell reference, constant, formula, or table, each of which may be difficult to comprehend at first glance.\"</p>\n</blockquote>\n\n<p>Named ranges are furthermore frequently used in spreadsheets to easier access data through ODBC and are particularly useful when there are several data ranges within the same worksheet. To connect via ODBC to Excel, simply choose the appropriate <a href=\"http://www.connectionstrings.com/excel-2007-odbc/\" rel=\"nofollow\">Excel driver</a> and send an SQL statement such as e.g.:</p>\n\n<pre><code>SELECT * \nFROM namedRange\n</code></pre>\n\n<p>The useful command in Pandas would probably be read_sql.</p>\n\n<p>In Windows, this solution requires however that you align/streamline the installed software versions (32-bit or 64-bit) of Excel, the ODBC driver and the software package from which you open the ODBC connection. As an example, an installed Excel 32-bit version will require a 32-bit ODBC driver and normally a 32-bit installation of Python. <em>Note: this latter point remains to be confirmed for the Python case (I'm a beginner to Python), but I can definitely confirm this point for ODBC connections launched from SAS, SPSS or Stata.</em></p>\n\n<p>The previous requirement is a very significant drawback and actually speaks in favor of any solution which does not involve ODBC at all. That said, it would be nice if read_Excel provided such a facility. In this context, it is interesting to note that SAS, SPSS and Stata currently do not allow direct access to named ranges in their respective Excel filters - so <em>maybe</em> there is an objective reason for this lacking feature...</p>\n"}, "answer_4_votes": {"type": "literal", "value": "2"}, "answer_5": {"type": "literal", "value": "<p>You can use the underlying <code>xlrd</code> package to do this. </p>\n\n<p>The <code>xlrd</code> package comes with an <code>examples</code> directory which contains <code>xlrdnameAPIdemo.py</code>, as documented <a href=\"https://secure.simplistix.co.uk/svn/xlrd/trunk/xlrd/doc/xlrd.html?p=4966\" rel=\"nofollow\">here</a>.</p>\n\n<p>In a nutshell for the named range <code>print_area</code> try:</p>\n\n<pre><code>book = xlrd.open_workbook('examples/namesdemo.xls')\nname_obj = book.name_map['print_area'][0]\nprint name_obj.__dict__\n</code></pre>\n\n<p>You'll see <code>name_obj</code> has an entry:</p>\n\n<pre><code>'result': Operand(kind=oREF, value=[Ref3D(coords=(2, 3, 0, 4, 0, 14))], text=u'Sheet3!$A$1:$N$4')\n</code></pre>\n\n<p>which you can follow the example to interpret, though it doesn't look straightforward - eg. the range may be relative or not, depending on the value <code>result.kind</code>.</p>\n\n<p>Further, when I tried to use this to read my own spreadsheet (created on a Mac), I found <code>result</code> was <code>None</code>; instead, the only ref to the range in <code>name_obj</code> was:</p>\n\n<pre><code>'formula_text': u'Sheet1!$B$6:$E$11'\n</code></pre>\n\n<p>So there may be a way to make this work in a general case, but it looks like it would take some trial and error.</p>\n\n<p>As an alternative, if you can format your spreadsheet so that instead of named ranges, your table follows in the rows immediately after a unique heading (<code>key</code>), and finishes with a blank row, here is a function which finds the right parameters to send to <code>pd.read_excel</code>:</p>\n\n<pre><code>def table_position(path, sheet_name, key):\n    \"\"\"\n    Find the start and end rows of a table in an Excel spreadsheet\n    based on the first occurence of key text on the sheet, and down\n    to the first blank line.\n\n    Returns (col, start_row, end_row, skip_footer)\n\n    where: \n        col is the column number containing the key text,\n        start_row is the row after this, \n        end_row is the row number of the next blank line,\n        skip_footer is how many rows from the end of the sheet this is.\n\n    You can then read in the table with:\n        x = pd.read_excel(path, sheet_name, skiprows=start, skip_footer=skip_footer, header=0)\n        x = x.dropna(axis=1, how='all')\n    \"\"\"\n    import xlrd\n    book = xlrd.open_workbook(path)\n    sheet = book.sheet_by_name(sheet_name)\n    # find the first occurrence of the key, and the next line break\n    (col, start, end) = (-1, -1, sheet.nrows)\n    for rownum in xrange(sheet.nrows):\n        if col&lt;0: # look for key to start the table off\n            try:\n                test_col = next(c for c in xrange(sheet.ncols) if sheet.cell(rownum, c).value==key)\n            except StopIteration:\n                pass\n            else:\n                col, start = test_col, rownum+1 # row after key text is the start\n        else: # test for blank line as end of table\n            if not [True for cell in sheet.row(rownum) if cell.value]:\n                end = rownum\n                break\n    skip_footer = sheet.nrows - end\n    return (col, start, end, skip_footer)\n</code></pre>\n\n<p>If you do follow this with a <code>pd.read_excel</code> then you are reading the data file twice, which is silly, but you get the idea.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "1"}, "answer_6": {"type": "literal", "value": "<p>Well, it's been a while, but I would definitely recommend giving a shot to <a href=\"https://www.xlwings.org/\" rel=\"nofollow noreferrer\">xlwings</a>. </p>\n\n<p>See also <a href=\"https://stackoverflow.com/questions/43324967/xlwings-take-value-from-defined-names\">Xlwings take value from defined names\n</a>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>How do you read from a named range in excel into a pandas dataframe?  The read_excel is designed to read entire sheets within a workbook.</p>\n", "answer_wo_code": "<p>You can do this in a round about way using <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html\" rel=\"nofollow\">read_excel</a>, it offers:</p>\n\n<pre class=\"lang-none prettyprint-override\"> </pre>\n\n<p>This means if you know the column names and the row numbers (presumably what you mean  by \"named range\"?) you can select just that section to make the DataFrame.</p>\n\n\n<p>Here is the way I use openpyxl to copy a range in a [[]] :</p>\n\n<pre> </pre>\n\n\n<p>Maybe someday pandas will support this natively. Until then, I use a helper function:</p>\n\n<pre> </pre>\n\n\n<p>To quote the <a href=\"http://office.microsoft.com/en-us/excel-help/define-and-use-names-in-formulas-HA010147120.aspx\" rel=\"nofollow\">Microsoft Office help pages</a>!: </p>\n\n<blockquote>\n  <p>A [named range] is a meaningful shorthand that makes it easier to understand the purpose of a cell reference, constant, formula, or table, each of which may be difficult to comprehend at first glance.\"</p>\n</blockquote>\n\n<p>Named ranges are furthermore frequently used in spreadsheets to easier access data through ODBC and are particularly useful when there are several data ranges within the same worksheet. To connect via ODBC to Excel, simply choose the appropriate <a href=\"http://www.connectionstrings.com/excel-2007-odbc/\" rel=\"nofollow\">Excel driver</a> and send an SQL statement such as e.g.:</p>\n\n<pre> </pre>\n\n<p>The useful command in Pandas would probably be read_sql.</p>\n\n<p>In Windows, this solution requires however that you align/streamline the installed software versions (32-bit or 64-bit) of Excel, the ODBC driver and the software package from which you open the ODBC connection. As an example, an installed Excel 32-bit version will require a 32-bit ODBC driver and normally a 32-bit installation of Python. <em>Note: this latter point remains to be confirmed for the Python case (I'm a beginner to Python), but I can definitely confirm this point for ODBC connections launched from SAS, SPSS or Stata.</em></p>\n\n<p>The previous requirement is a very significant drawback and actually speaks in favor of any solution which does not involve ODBC at all. That said, it would be nice if read_Excel provided such a facility. In this context, it is interesting to note that SAS, SPSS and Stata currently do not allow direct access to named ranges in their respective Excel filters - so <em>maybe</em> there is an objective reason for this lacking feature...</p>\n\n\n<p>You can use the underlying   package to do this. </p>\n\n<p>The   package comes with an   directory which contains  , as documented <a href=\"https://secure.simplistix.co.uk/svn/xlrd/trunk/xlrd/doc/xlrd.html?p=4966\" rel=\"nofollow\">here</a>.</p>\n\n<p>In a nutshell for the named range   try:</p>\n\n<pre> </pre>\n\n<p>You'll see   has an entry:</p>\n\n<pre> </pre>\n\n<p>which you can follow the example to interpret, though it doesn't look straightforward - eg. the range may be relative or not, depending on the value  .</p>\n\n<p>Further, when I tried to use this to read my own spreadsheet (created on a Mac), I found   was  ; instead, the only ref to the range in   was:</p>\n\n<pre> </pre>\n\n<p>So there may be a way to make this work in a general case, but it looks like it would take some trial and error.</p>\n\n<p>As an alternative, if you can format your spreadsheet so that instead of named ranges, your table follows in the rows immediately after a unique heading ( ), and finishes with a blank row, here is a function which finds the right parameters to send to  :</p>\n\n<pre> </pre>\n\n<p>If you do follow this with a   then you are reading the data file twice, which is silly, but you get the idea.</p>\n\n\n<p>Well, it's been a while, but I would definitely recommend giving a shot to <a href=\"https://www.xlwings.org/\" rel=\"nofollow noreferrer\">xlwings</a>. </p>\n\n<p>See also <a href=\"https://stackoverflow.com/questions/43324967/xlwings-take-value-from-defined-names\">Xlwings take value from defined names\n</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.ExcelWriter"}, "class_func_label": {"type": "literal", "value": "pandas.ExcelWriter"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Class for writing DataFrame objects into excel sheets, default is to use\n    xlwt for xls, openpyxl for xlsx.  See DataFrame.to_excel for typical usage.\n\n    Parameters\n    ----------\n    path : string\n        Path to xls or xlsx file.\n    engine : string (optional)\n        Engine to use for writing. If None, defaults to\n        ``io.excel.<extension>.writer``.  NOTE: can only be passed as a keyword\n        argument.\n    date_format : string, default None\n        Format string for dates written into Excel files (e.g. 'YYYY-MM-DD')\n    datetime_format : string, default None\n        Format string for datetime objects written into Excel files\n        (e.g. 'YYYY-MM-DD HH:MM:SS')\n    mode : {'w', 'a'}, default 'w'\n        File mode to use (write or append).\n\n        .. versionadded:: 0.24.0\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Notes\n    -----\n    None of the methods and properties are considered public.\n\n    For compatibility with CSV writers, ExcelWriter serializes lists\n    and dicts to strings before writing.\n\n    Examples\n    --------\n    Default usage:\n\n    >>> with ExcelWriter('path_to_file.xlsx') as writer:\n    ...     df.to_excel(writer)\n\n    To write to separate sheets in a single file:\n\n    >>> with ExcelWriter('path_to_file.xlsx') as writer:\n    ...     df1.to_excel(writer, sheet_name='Sheet1')\n    ...     df2.to_excel(writer, sheet_name='Sheet2')\n\n    You can set the date format or datetime format:\n\n    >>> with ExcelWriter('path_to_file.xlsx',\n                          date_format='YYYY-MM-DD',\n                          datetime_format='YYYY-MM-DD HH:MM:SS') as writer:\n    ...     df.to_excel(writer)\n\n    You can also append to an existing Excel file:\n\n    >>> with ExcelWriter('path_to_file.xlsx', mode='a') as writer:\n    ...     df.to_excel(writer, sheet_name='Sheet3')\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/17326973"}, "title": {"type": "literal", "value": "Is there a way to auto-adjust Excel column widths with pandas.ExcelWriter?"}, "content": {"type": "literal", "value": "<p>I am being asked to generate some Excel reports. I am currently using pandas quite heavily for my data, so naturally I would like to use the pandas.ExcelWriter method to generate these reports.  However the fixed column widths are a problem.   </p>\n\n<p>The code I have so far is simple enough.  Say I have a dataframe called 'df':</p>\n\n<pre><code>writer = pd.ExcelWriter(excel_file_path)\ndf.to_excel(writer, sheet_name=\"Summary\")\n</code></pre>\n\n<p>I was looking over the pandas code, and I don't really see any options to set column widths.  Is there a trick out there in the universe to make it such that the columns auto-adjust to the data? Or is there something I can do after the fact to the xlsx file to adjust the column widths? </p>\n\n<p>(I am using the OpenPyXL library, and generating .xlsx files - if that makes any difference.)</p>\n\n<p>Thank you.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I'm posting this because I just ran into the same issue and found that the official documentation for Xlsxwriter and pandas still have this functionality listed as unsupported. I hacked together a solution that solved the issue i was having. I basically just iterate through each column and use worksheet.set_column to set the column width == the max length of the contents of that column. </p>\n\n<p>One important note, however. This solution does not fit the column headers, simply the column values. That should be an easy change though if you need to fit the headers instead. Hope this helps someone :)</p>\n\n<pre><code>import pandas as pd\nimport sqlalchemy as sa\nimport urllib\n\n\nread_server = 'serverName'\nread_database = 'databaseName'\n\nread_params = urllib.quote_plus(\"DRIVER={SQL Server};SERVER=\"+read_server+\";DATABASE=\"+read_database+\";TRUSTED_CONNECTION=Yes\")\nread_engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % read_params)\n\n#Output some SQL Server data into a dataframe\nmy_sql_query = \"\"\" SELECT * FROM dbo.my_table \"\"\"\nmy_dataframe = pd.read_sql_query(my_sql_query,con=read_engine)\n\n#Set destination directory to save excel.\nxlsFilepath = r'H:\\my_project' + \"\\\\\" + 'my_file_name.xlsx'\nwriter = pd.ExcelWriter(xlsFilepath, engine='xlsxwriter')\n\n#Write excel to file using pandas to_excel\nmy_dataframe.to_excel(writer, startrow = 1, sheet_name='Sheet1', index=False)\n\n#Indicate workbook and worksheet for formatting\nworkbook = writer.book\nworksheet = writer.sheets['Sheet1']\n\n#Iterate through each column and set the width == the max length in that column. A padding length of 2 is also added.\nfor i, col in enumerate(my_dataframe.columns):\n    # find length of column i\n    column_len = my_dataframe[col].astype(str).str.len().max()\n    # Setting the length if the column header is larger\n    # than the max column value length\n    column_len = max(column_len, len(col)) + 2\n    # set the column length\n    worksheet.set_column(i, i, column_len)\nwriter.save()\n</code></pre>\n\n\n<p>Inspired by <a href=\"https://stackoverflow.com/a/36554382/95852\">user6178746's answer</a>, I have the following:</p>\n\n<pre><code># Given a dict of dataframes, for example:\n# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}\n\nwriter = pd.ExcelWriter(filename, engine='xlsxwriter')\nfor sheetname, df in dfs.items():  # loop through `dict` of dataframes\n    df.to_excel(writer, sheet_name=sheetname)  # send df to writer\n    worksheet = writer.sheets[sheetname]  # pull worksheet object\n    for idx, col in enumerate(df):  # loop through all columns\n        series = df[col]\n        max_len = max((\n            series.astype(str).map(len).max(),  # len of largest item\n            len(str(series.name))  # len of column name/header\n            )) + 1  # adding a little extra space\n        worksheet.set_column(idx, idx, max_len)  # set column width\nwriter.save()\n</code></pre>\n\n\n<pre><code>import re\nimport openpyxl\n..\nfor col in _ws.columns:\n    max_lenght = 0\n    print(col[0])\n    col_name = re.findall('\\w\\d', str(col[0]))\n    col_name = col_name[0]\n    col_name = re.findall('\\w', str(col_name))[0]\n    print(col_name)\n    for cell in col:\n        try:\n            if len(str(cell.value)) &gt; max_lenght:\n                max_lenght = len(cell.value)\n        except:\n            pass\n    adjusted_width = (max_lenght+2)\n    _ws.column_dimensions[col_name].width = adjusted_width\n</code></pre>\n\n\n<p>Easiest solution is to specify width of column in set_column method. </p>\n\n<pre><code>    for worksheet in writer.sheets.values():\n        worksheet.set_column(0,last_column_value, required_width_constant)\n</code></pre>\n\n\n<p>There is probably no automatic way to do it right now, but as you use openpyxl, the following line (adapted from another answer by user <a href=\"https://stackoverflow.com/users/443457/bufke\">Bufke</a> on <a href=\"https://stackoverflow.com/a/14450572/2375855\">how to do in manually</a>) allows you to specify a sane value (in character widths):</p>\n\n<pre><code>writer.sheets['Summary'].column_dimensions['A'].width = 15\n</code></pre>\n\n\n<p>Combining the other answers and comments and also supporting multi-indices:</p>\n\n<pre><code>def autosize_excel_columns(worksheet, df):\n  autosize_excel_columns_df(worksheet, df.index.to_frame())\n  autosize_excel_columns_df(worksheet, df, offset=df.index.nlevels)\n\ndef autosize_excel_columns_df(worksheet, df, offset=0):\n  for idx, col in enumerate(df):\n    series = df[col]\n    max_len = max((\n      series.astype(str).map(len).max(),\n      len(str(series.name))\n    )) + 1\n    worksheet.set_column(idx+offset, idx+offset, max_len)\n\nsheetname=...\ndf.to_excel(writer, sheet_name=sheetname, freeze_panes=(df.columns.nlevels, df.index.nlevels))\nworksheet = writer.sheets[sheetname]\nautosize_excel_columns(worksheet, df)\nwriter.save()\n</code></pre>\n\n\n<p>There is a nice package that I started to use recently called StyleFrame.</p>\n\n<p>it gets DataFrame and lets you to style it very easily...</p>\n\n<p>by default the columns width is auto-adjusting.</p>\n\n<p>for example:</p>\n\n<pre><code>from StyleFrame import StyleFrame\nimport pandas as pd\n\ndf = pd.DataFrame({'aaaaaaaaaaa': [1, 2, 3], \n                   'bbbbbbbbb': [1, 1, 1],\n                   'ccccccccccc': [2, 3, 4]})\nexcel_writer = StyleFrame.ExcelWriter('example.xlsx')\nsf = StyleFrame(df)\nsf.to_excel(excel_writer=excel_writer, row_to_add_filters=0,\n            columns_and_rows_to_freeze='B2')\nexcel_writer.save()\n</code></pre>\n\n<p>you can also change the columns width:</p>\n\n<pre><code>sf.set_column_width(columns=['aaaaaaaaaaa', 'bbbbbbbbb'],\n                    width=35.3)\n</code></pre>\n\n<p><br>\n<strong>UPDATE</strong></p>\n\n<p>In version 1.4 <code>best_fit</code> argument was added to <code>StyleFrame.to_excel</code>.\nSee the <a href=\"https://styleframe.readthedocs.io/en/latest/api_documentation.html?highlight=best_fit#to-excel\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>I'm posting this because I just ran into the same issue and found that the official documentation for Xlsxwriter and pandas still have this functionality listed as unsupported. I hacked together a solution that solved the issue i was having. I basically just iterate through each column and use worksheet.set_column to set the column width == the max length of the contents of that column. </p>\n\n<p>One important note, however. This solution does not fit the column headers, simply the column values. That should be an easy change though if you need to fit the headers instead. Hope this helps someone :)</p>\n\n<pre><code>import pandas as pd\nimport sqlalchemy as sa\nimport urllib\n\n\nread_server = 'serverName'\nread_database = 'databaseName'\n\nread_params = urllib.quote_plus(\"DRIVER={SQL Server};SERVER=\"+read_server+\";DATABASE=\"+read_database+\";TRUSTED_CONNECTION=Yes\")\nread_engine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % read_params)\n\n#Output some SQL Server data into a dataframe\nmy_sql_query = \"\"\" SELECT * FROM dbo.my_table \"\"\"\nmy_dataframe = pd.read_sql_query(my_sql_query,con=read_engine)\n\n#Set destination directory to save excel.\nxlsFilepath = r'H:\\my_project' + \"\\\\\" + 'my_file_name.xlsx'\nwriter = pd.ExcelWriter(xlsFilepath, engine='xlsxwriter')\n\n#Write excel to file using pandas to_excel\nmy_dataframe.to_excel(writer, startrow = 1, sheet_name='Sheet1', index=False)\n\n#Indicate workbook and worksheet for formatting\nworkbook = writer.book\nworksheet = writer.sheets['Sheet1']\n\n#Iterate through each column and set the width == the max length in that column. A padding length of 2 is also added.\nfor i, col in enumerate(my_dataframe.columns):\n    # find length of column i\n    column_len = my_dataframe[col].astype(str).str.len().max()\n    # Setting the length if the column header is larger\n    # than the max column value length\n    column_len = max(column_len, len(col)) + 2\n    # set the column length\n    worksheet.set_column(i, i, column_len)\nwriter.save()\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "18"}, "answer_2": {"type": "literal", "value": "<p>Inspired by <a href=\"https://stackoverflow.com/a/36554382/95852\">user6178746's answer</a>, I have the following:</p>\n\n<pre><code># Given a dict of dataframes, for example:\n# dfs = {'gadgets': df_gadgets, 'widgets': df_widgets}\n\nwriter = pd.ExcelWriter(filename, engine='xlsxwriter')\nfor sheetname, df in dfs.items():  # loop through `dict` of dataframes\n    df.to_excel(writer, sheet_name=sheetname)  # send df to writer\n    worksheet = writer.sheets[sheetname]  # pull worksheet object\n    for idx, col in enumerate(df):  # loop through all columns\n        series = df[col]\n        max_len = max((\n            series.astype(str).map(len).max(),  # len of largest item\n            len(str(series.name))  # len of column name/header\n            )) + 1  # adding a little extra space\n        worksheet.set_column(idx, idx, max_len)  # set column width\nwriter.save()\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "27"}, "answer_3": {"type": "literal", "value": "<pre><code>import re\nimport openpyxl\n..\nfor col in _ws.columns:\n    max_lenght = 0\n    print(col[0])\n    col_name = re.findall('\\w\\d', str(col[0]))\n    col_name = col_name[0]\n    col_name = re.findall('\\w', str(col_name))[0]\n    print(col_name)\n    for cell in col:\n        try:\n            if len(str(cell.value)) &gt; max_lenght:\n                max_lenght = len(cell.value)\n        except:\n            pass\n    adjusted_width = (max_lenght+2)\n    _ws.column_dimensions[col_name].width = adjusted_width\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "answer_4": {"type": "literal", "value": "<p>Easiest solution is to specify width of column in set_column method. </p>\n\n<pre><code>    for worksheet in writer.sheets.values():\n        worksheet.set_column(0,last_column_value, required_width_constant)\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": ""}, "answer_5": {"type": "literal", "value": "<p>There is probably no automatic way to do it right now, but as you use openpyxl, the following line (adapted from another answer by user <a href=\"https://stackoverflow.com/users/443457/bufke\">Bufke</a> on <a href=\"https://stackoverflow.com/a/14450572/2375855\">how to do in manually</a>) allows you to specify a sane value (in character widths):</p>\n\n<pre><code>writer.sheets['Summary'].column_dimensions['A'].width = 15\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "21"}, "answer_6": {"type": "literal", "value": "<p>Combining the other answers and comments and also supporting multi-indices:</p>\n\n<pre><code>def autosize_excel_columns(worksheet, df):\n  autosize_excel_columns_df(worksheet, df.index.to_frame())\n  autosize_excel_columns_df(worksheet, df, offset=df.index.nlevels)\n\ndef autosize_excel_columns_df(worksheet, df, offset=0):\n  for idx, col in enumerate(df):\n    series = df[col]\n    max_len = max((\n      series.astype(str).map(len).max(),\n      len(str(series.name))\n    )) + 1\n    worksheet.set_column(idx+offset, idx+offset, max_len)\n\nsheetname=...\ndf.to_excel(writer, sheet_name=sheetname, freeze_panes=(df.columns.nlevels, df.index.nlevels))\nworksheet = writer.sheets[sheetname]\nautosize_excel_columns(worksheet, df)\nwriter.save()\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": ""}, "answer_7": {"type": "literal", "value": "<p>There is a nice package that I started to use recently called StyleFrame.</p>\n\n<p>it gets DataFrame and lets you to style it very easily...</p>\n\n<p>by default the columns width is auto-adjusting.</p>\n\n<p>for example:</p>\n\n<pre><code>from StyleFrame import StyleFrame\nimport pandas as pd\n\ndf = pd.DataFrame({'aaaaaaaaaaa': [1, 2, 3], \n                   'bbbbbbbbb': [1, 1, 1],\n                   'ccccccccccc': [2, 3, 4]})\nexcel_writer = StyleFrame.ExcelWriter('example.xlsx')\nsf = StyleFrame(df)\nsf.to_excel(excel_writer=excel_writer, row_to_add_filters=0,\n            columns_and_rows_to_freeze='B2')\nexcel_writer.save()\n</code></pre>\n\n<p>you can also change the columns width:</p>\n\n<pre><code>sf.set_column_width(columns=['aaaaaaaaaaa', 'bbbbbbbbb'],\n                    width=35.3)\n</code></pre>\n\n<p><br>\n<strong>UPDATE</strong></p>\n\n<p>In version 1.4 <code>best_fit</code> argument was added to <code>StyleFrame.to_excel</code>.\nSee the <a href=\"https://styleframe.readthedocs.io/en/latest/api_documentation.html?highlight=best_fit#to-excel\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "13"}, "content_wo_code": "<p>I am being asked to generate some Excel reports. I am currently using pandas quite heavily for my data, so naturally I would like to use the pandas.ExcelWriter method to generate these reports.  However the fixed column widths are a problem.   </p>\n\n<p>The code I have so far is simple enough.  Say I have a dataframe called 'df':</p>\n\n<pre> </pre>\n\n<p>I was looking over the pandas code, and I don't really see any options to set column widths.  Is there a trick out there in the universe to make it such that the columns auto-adjust to the data? Or is there something I can do after the fact to the xlsx file to adjust the column widths? </p>\n\n<p>(I am using the OpenPyXL library, and generating .xlsx files - if that makes any difference.)</p>\n\n<p>Thank you.</p>\n", "answer_wo_code": "<p>I'm posting this because I just ran into the same issue and found that the official documentation for Xlsxwriter and pandas still have this functionality listed as unsupported. I hacked together a solution that solved the issue i was having. I basically just iterate through each column and use worksheet.set_column to set the column width == the max length of the contents of that column. </p>\n\n<p>One important note, however. This solution does not fit the column headers, simply the column values. That should be an easy change though if you need to fit the headers instead. Hope this helps someone :)</p>\n\n<pre> </pre>\n\n\n<p>Inspired by <a href=\"https://stackoverflow.com/a/36554382/95852\">user6178746's answer</a>, I have the following:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n\n<p>Easiest solution is to specify width of column in set_column method. </p>\n\n<pre> </pre>\n\n\n<p>There is probably no automatic way to do it right now, but as you use openpyxl, the following line (adapted from another answer by user <a href=\"https://stackoverflow.com/users/443457/bufke\">Bufke</a> on <a href=\"https://stackoverflow.com/a/14450572/2375855\">how to do in manually</a>) allows you to specify a sane value (in character widths):</p>\n\n<pre> </pre>\n\n\n<p>Combining the other answers and comments and also supporting multi-indices:</p>\n\n<pre> </pre>\n\n\n<p>There is a nice package that I started to use recently called StyleFrame.</p>\n\n<p>it gets DataFrame and lets you to style it very easily...</p>\n\n<p>by default the columns width is auto-adjusting.</p>\n\n<p>for example:</p>\n\n<pre> </pre>\n\n<p>you can also change the columns width:</p>\n\n<pre> </pre>\n\n<p><br>\n<strong>UPDATE</strong></p>\n\n<p>In version 1.4   argument was added to  .\nSee the <a href=\"https://styleframe.readthedocs.io/en/latest/api_documentation.html?highlight=best_fit#to-excel\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/networkx.algorithms.shortest_path"}, "class_func_label": {"type": "literal", "value": "networkx.algorithms.shortest_path"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nCompute shortest paths in the graph.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/50870045"}, "title": {"type": "literal", "value": "finding relations between co-concurrent data"}, "content": {"type": "literal", "value": "<p>I have a dataframe that looks like graph database. </p>\n\n<pre><code>import pandas as pd\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n       [0, 0, 1, 1, 0, 0],\n       [0, 1, 0, 1, 0, 0],\n       [3, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 0, 4],\n       [0, 0, 0, 0, 4, 0]], columns=mycols)\n\ndf.index=mycols\n</code></pre>\n\n<p>The simplified dummy dataframe looks like this:</p>\n\n<pre><code>           china    england france  india   pakistan    taiwan\nchina          0          0      0      3          0    0\nengland        0          0      1      1          0    0\nfrance         0          1      0      1          0    0\nindia          3          1      1      0          1    0\npakistan       0          0      0      1          0    4\ntaiwan         0          0      0      0          4    0\n</code></pre>\n\n<p>Let's assume a user want to go from china to india, there is direct route.</p>\n\n<pre><code>df[df['china'] &gt; 0].index.str.contains('india')\narray([ True])\n</code></pre>\n\n<p>But no direct route to england:</p>\n\n<pre><code>df[df['china'] &gt; 0].index.str.contains('england')\narray([False])\n</code></pre>\n\n<p>In that case, I need to find the common country:</p>\n\n<pre><code>set(df[df.loc['china'] &gt; 0].index.values) &amp; set(df[df.loc['england'] &gt; 0].index.values)\n{'india'}\n</code></pre>\n\n<p>But there are cases where there is no common friend, and I need to find friend of friend to reach the destination. for e.g.</p>\n\n<pre><code>set(df[df.loc['china'] &gt; 0].index.values) &amp; set(df[df.loc['taiwan'] &gt; 0].index.values)\n</code></pre>\n\n<p>1) In this case how do I write a query that will return china - india - pakistan - taiwan ?</p>\n\n<p>2) Is there any better way to store this? Or SQL like (rows / columns) is ok? </p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre><code>import pandas as pd\nimport networkx as nx\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n   [0, 0, 1, 1, 0, 0],\n   [0, 1, 0, 1, 0, 0],\n   [3, 1, 1, 0, 1, 0],\n   [0, 0, 0, 1, 0, 4],\n   [0, 0, 0, 0, 4, 0]], columns=mycols)\n\n#Load the graph from dataframe\nG = nx.from_numpy_matrix(df.values)\n\n#set the nodes names\nG = nx.relabel_nodes(graph, dict(enumerate(mycols)))\n</code></pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre><code>print G.edges()\n#EdgeView([('pakistan', 'taiwan'), ('pakistan', 'india'), ('england', 'india'), ('england', 'france'), ('india', 'china'), ('india', 'france')])\n\nprint graph['china']\n#AtlasView({'india': {'weight': 3}})\n\nprint graph['england']\n#AtlasView({'india': {'weight': 1}, 'france': {'weight': 1}})\n</code></pre>\n\n<p>Now suppose you need to find all path from <code>china</code> to <code>india</code></p>\n\n<pre><code>for path in nx.all_simple_paths(graph, source='china', target='taiwan'):\n    print path\n#Output : ['china', 'india', 'pakistan', 'taiwan']\n</code></pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre><code>for path in nx.all_shortest_paths(graph, source='taiwan', target='india'):\n    print path\n#Output : ['taiwan', 'pakistan', 'india']\n</code></pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n\n\n<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"><code>shortest_path</code></a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre><code>G = nx.path_graph(5)\n&gt;&gt;&gt; print(nx.shortest_path(G, source=0, target=4))\n[0, 1, 2, 3, 4]\n</code></pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the <code>target</code> node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre><code>import pandas as pd\nimport networkx as nx\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n   [0, 0, 1, 1, 0, 0],\n   [0, 1, 0, 1, 0, 0],\n   [3, 1, 1, 0, 1, 0],\n   [0, 0, 0, 1, 0, 4],\n   [0, 0, 0, 0, 4, 0]], columns=mycols)\n\n#Load the graph from dataframe\nG = nx.from_numpy_matrix(df.values)\n\n#set the nodes names\nG = nx.relabel_nodes(graph, dict(enumerate(mycols)))\n</code></pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre><code>print G.edges()\n#EdgeView([('pakistan', 'taiwan'), ('pakistan', 'india'), ('england', 'india'), ('england', 'france'), ('india', 'china'), ('india', 'france')])\n\nprint graph['china']\n#AtlasView({'india': {'weight': 3}})\n\nprint graph['england']\n#AtlasView({'india': {'weight': 1}, 'france': {'weight': 1}})\n</code></pre>\n\n<p>Now suppose you need to find all path from <code>china</code> to <code>india</code></p>\n\n<pre><code>for path in nx.all_simple_paths(graph, source='china', target='taiwan'):\n    print path\n#Output : ['china', 'india', 'pakistan', 'taiwan']\n</code></pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre><code>for path in nx.all_shortest_paths(graph, source='taiwan', target='india'):\n    print path\n#Output : ['taiwan', 'pakistan', 'india']\n</code></pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "answer_2": {"type": "literal", "value": "<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"><code>shortest_path</code></a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre><code>G = nx.path_graph(5)\n&gt;&gt;&gt; print(nx.shortest_path(G, source=0, target=4))\n[0, 1, 2, 3, 4]\n</code></pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the <code>target</code> node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have a dataframe that looks like graph database. </p>\n\n<pre> </pre>\n\n<p>The simplified dummy dataframe looks like this:</p>\n\n<pre> </pre>\n\n<p>Let's assume a user want to go from china to india, there is direct route.</p>\n\n<pre> </pre>\n\n<p>But no direct route to england:</p>\n\n<pre> </pre>\n\n<p>In that case, I need to find the common country:</p>\n\n<pre> </pre>\n\n<p>But there are cases where there is no common friend, and I need to find friend of friend to reach the destination. for e.g.</p>\n\n<pre> </pre>\n\n<p>1) In this case how do I write a query that will return china - india - pakistan - taiwan ?</p>\n\n<p>2) Is there any better way to store this? Or SQL like (rows / columns) is ok? </p>\n", "answer_wo_code": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre> </pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre> </pre>\n\n<p>Now suppose you need to find all path from   to  </p>\n\n<pre> </pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre> </pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n\n\n<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"> </a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre> </pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the   node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/networkx.algorithms.shortest_paths.generic.shortest_path"}, "class_func_label": {"type": "literal", "value": "networkx.algorithms.shortest_paths.generic.shortest_path"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nCompute shortest paths in the graph.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/50870045"}, "title": {"type": "literal", "value": "finding relations between co-concurrent data"}, "content": {"type": "literal", "value": "<p>I have a dataframe that looks like graph database. </p>\n\n<pre><code>import pandas as pd\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n       [0, 0, 1, 1, 0, 0],\n       [0, 1, 0, 1, 0, 0],\n       [3, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 0, 4],\n       [0, 0, 0, 0, 4, 0]], columns=mycols)\n\ndf.index=mycols\n</code></pre>\n\n<p>The simplified dummy dataframe looks like this:</p>\n\n<pre><code>           china    england france  india   pakistan    taiwan\nchina          0          0      0      3          0    0\nengland        0          0      1      1          0    0\nfrance         0          1      0      1          0    0\nindia          3          1      1      0          1    0\npakistan       0          0      0      1          0    4\ntaiwan         0          0      0      0          4    0\n</code></pre>\n\n<p>Let's assume a user want to go from china to india, there is direct route.</p>\n\n<pre><code>df[df['china'] &gt; 0].index.str.contains('india')\narray([ True])\n</code></pre>\n\n<p>But no direct route to england:</p>\n\n<pre><code>df[df['china'] &gt; 0].index.str.contains('england')\narray([False])\n</code></pre>\n\n<p>In that case, I need to find the common country:</p>\n\n<pre><code>set(df[df.loc['china'] &gt; 0].index.values) &amp; set(df[df.loc['england'] &gt; 0].index.values)\n{'india'}\n</code></pre>\n\n<p>But there are cases where there is no common friend, and I need to find friend of friend to reach the destination. for e.g.</p>\n\n<pre><code>set(df[df.loc['china'] &gt; 0].index.values) &amp; set(df[df.loc['taiwan'] &gt; 0].index.values)\n</code></pre>\n\n<p>1) In this case how do I write a query that will return china - india - pakistan - taiwan ?</p>\n\n<p>2) Is there any better way to store this? Or SQL like (rows / columns) is ok? </p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre><code>import pandas as pd\nimport networkx as nx\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n   [0, 0, 1, 1, 0, 0],\n   [0, 1, 0, 1, 0, 0],\n   [3, 1, 1, 0, 1, 0],\n   [0, 0, 0, 1, 0, 4],\n   [0, 0, 0, 0, 4, 0]], columns=mycols)\n\n#Load the graph from dataframe\nG = nx.from_numpy_matrix(df.values)\n\n#set the nodes names\nG = nx.relabel_nodes(graph, dict(enumerate(mycols)))\n</code></pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre><code>print G.edges()\n#EdgeView([('pakistan', 'taiwan'), ('pakistan', 'india'), ('england', 'india'), ('england', 'france'), ('india', 'china'), ('india', 'france')])\n\nprint graph['china']\n#AtlasView({'india': {'weight': 3}})\n\nprint graph['england']\n#AtlasView({'india': {'weight': 1}, 'france': {'weight': 1}})\n</code></pre>\n\n<p>Now suppose you need to find all path from <code>china</code> to <code>india</code></p>\n\n<pre><code>for path in nx.all_simple_paths(graph, source='china', target='taiwan'):\n    print path\n#Output : ['china', 'india', 'pakistan', 'taiwan']\n</code></pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre><code>for path in nx.all_shortest_paths(graph, source='taiwan', target='india'):\n    print path\n#Output : ['taiwan', 'pakistan', 'india']\n</code></pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n\n\n<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"><code>shortest_path</code></a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre><code>G = nx.path_graph(5)\n&gt;&gt;&gt; print(nx.shortest_path(G, source=0, target=4))\n[0, 1, 2, 3, 4]\n</code></pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the <code>target</code> node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre><code>import pandas as pd\nimport networkx as nx\nmycols=['china', 'england', 'france', 'india', 'pakistan', 'taiwan']\n\ndf=pd.DataFrame([[0, 0, 0, 3, 0, 0],\n   [0, 0, 1, 1, 0, 0],\n   [0, 1, 0, 1, 0, 0],\n   [3, 1, 1, 0, 1, 0],\n   [0, 0, 0, 1, 0, 4],\n   [0, 0, 0, 0, 4, 0]], columns=mycols)\n\n#Load the graph from dataframe\nG = nx.from_numpy_matrix(df.values)\n\n#set the nodes names\nG = nx.relabel_nodes(graph, dict(enumerate(mycols)))\n</code></pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre><code>print G.edges()\n#EdgeView([('pakistan', 'taiwan'), ('pakistan', 'india'), ('england', 'india'), ('england', 'france'), ('india', 'china'), ('india', 'france')])\n\nprint graph['china']\n#AtlasView({'india': {'weight': 3}})\n\nprint graph['england']\n#AtlasView({'india': {'weight': 1}, 'france': {'weight': 1}})\n</code></pre>\n\n<p>Now suppose you need to find all path from <code>china</code> to <code>india</code></p>\n\n<pre><code>for path in nx.all_simple_paths(graph, source='china', target='taiwan'):\n    print path\n#Output : ['china', 'india', 'pakistan', 'taiwan']\n</code></pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre><code>for path in nx.all_shortest_paths(graph, source='taiwan', target='india'):\n    print path\n#Output : ['taiwan', 'pakistan', 'india']\n</code></pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "answer_2": {"type": "literal", "value": "<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"><code>shortest_path</code></a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre><code>G = nx.path_graph(5)\n&gt;&gt;&gt; print(nx.shortest_path(G, source=0, target=4))\n[0, 1, 2, 3, 4]\n</code></pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the <code>target</code> node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have a dataframe that looks like graph database. </p>\n\n<pre> </pre>\n\n<p>The simplified dummy dataframe looks like this:</p>\n\n<pre> </pre>\n\n<p>Let's assume a user want to go from china to india, there is direct route.</p>\n\n<pre> </pre>\n\n<p>But no direct route to england:</p>\n\n<pre> </pre>\n\n<p>In that case, I need to find the common country:</p>\n\n<pre> </pre>\n\n<p>But there are cases where there is no common friend, and I need to find friend of friend to reach the destination. for e.g.</p>\n\n<pre> </pre>\n\n<p>1) In this case how do I write a query that will return china - india - pakistan - taiwan ?</p>\n\n<p>2) Is there any better way to store this? Or SQL like (rows / columns) is ok? </p>\n", "answer_wo_code": "<p>You can do this using <a href=\"https://networkx.github.io/documentation/latest/\" rel=\"nofollow noreferrer\">Networkx</a> in the following way</p>\n\n<p><strong>Load the graph</strong></p>\n\n<pre> </pre>\n\n<p><strong>Test if the graph is correctly loaded</strong></p>\n\n<pre> </pre>\n\n<p>Now suppose you need to find all path from   to  </p>\n\n<pre> </pre>\n\n<p>If you want to find shortest paths from one node to another</p>\n\n<pre> </pre>\n\n<p>You can find multiple other algorithms for finding the shortext path, all-pair shortest path, dijsktra algorithm, etc. <a href=\"https://networkx.github.io/documentation/latest/reference/algorithms/shortest_paths.html?highlight=shortest%20path\" rel=\"nofollow noreferrer\">at their documentation</a> to suit your queries</p>\n\n<p><strong>Note</strong> there might exist a way to load the graph directly from pandas using <a href=\"https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.convert_matrix.from_pandas_dataframe.html\" rel=\"nofollow noreferrer\">from_pandas_dataframe</a>, but I was not sure if the use case was correct, as it requires a source and target</p>\n\n\n<p>Your problem (I am assuming) is basically to find the shortest path between any two given nodes in a weighted graph. Algorithmically speaking, this is called <a href=\"https://en.wikipedia.org/wiki/Shortest_path_problem#Single-source_shortest_paths\" rel=\"nofollow noreferrer\">Shortest path problem</a> (or more precisely <strong>single-pair shortest path problem</strong>). Networkx 2.1 has a function <a href=\"https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\" rel=\"nofollow noreferrer\"> </a> for exactly doing this </p>\n\n<p>From their example, </p>\n\n<pre> </pre>\n\n<blockquote>\n  <p>If the source and target are both specified, return a single list of\n  nodes in a shortest path from the source to the target.</p>\n</blockquote>\n\n<p>If you want to get the get the shortest path to all the nodes from a source, just skip the   node (essentially making it a <strong>single-source shortest path problem</strong>)</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/sklearn.tree.DecisionTreeClassifier"}, "class_func_label": {"type": "literal", "value": "sklearn.tree.DecisionTreeClassifier"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "A decision tree classifier.\n\n    Read more in the :ref:`User Guide <tree>`.\n\n    Parameters\n    ----------\n    criterion : str, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n    splitter : str, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, str or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n            - If int, then consider `max_features` features at each split.\n            - If float, then `max_features` is a fraction and\n              `int(max_features * n_features)` features are considered at each\n              split.\n            - If \"auto\", then `max_features=sqrt(n_features)`.\n            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n            - If \"log2\", then `max_features=log2(n_features)`.\n            - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=1e-7\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    presort : deprecated, default='deprecated'\n        This parameter is deprecated and will be removed in v0.24.\n\n        .. deprecated:: 0.22\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem),\n        or a list of arrays of class labels (multi-output problem).\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances. The higher, the more important the\n        feature. The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance [4]_.\n\n    max_features_ : int,\n        The inferred value of max_features.\n\n    n_classes_ : int or list\n        The number of classes (for single output problems),\n        or a list containing the number of classes for each\n        output (for multi-output problems).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    tree_ : Tree object\n        The underlying Tree object. Please refer to\n        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n        for basic usage of these attributes.\n\n    See Also\n    --------\n    DecisionTreeRegressor : A decision tree regressor.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n           Learning\", Springer, 2009.\n\n    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> clf = DecisionTreeClassifier(random_state=0)\n    >>> iris = load_iris()\n    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n    ...                             # doctest: +SKIP\n    ...\n    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/20224526"}, "title": {"type": "literal", "value": "How to extract the decision rules from scikit-learn decision-tree?"}, "content": {"type": "literal", "value": "<p>Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list?</p>\n\n<p>Something like: </p>\n\n<p><code>if A&gt;0.4 then if B&lt;0.2 then if C&gt;0.8 then class='X'</code> </p>\n\n<p>Thanks for your help.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Here is a way to translate the whole tree into a single (not necessarily too human-readable) python expression using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre><code>from skompiler import skompile\nskompile(dtree.predict).to('python/code')\n</code></pre>\n\n\n<p>Codes below is my approach under anaconda python 2.7 plus a package name \"pydot-ng\" to making a PDF file with decision rules. I hope it is helpful.</p>\n\n<pre><code>from sklearn import tree\n\nclf = tree.DecisionTreeClassifier(max_leaf_nodes=n)\nclf_ = clf.fit(X, data_y)\n\nfeature_names = X.columns\nclass_name = clf_.classes_.astype(int).astype(str)\n\ndef output_pdf(clf_, name):\n    from sklearn import tree\n    from sklearn.externals.six import StringIO\n    import pydot_ng as pydot\n    dot_data = StringIO()\n    tree.export_graphviz(clf_, out_file=dot_data,\n                         feature_names=feature_names,\n                         class_names=class_name,\n                         filled=True, rounded=True,\n                         special_characters=True,\n                          node_ids=1,)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n    graph.write_pdf(\"%s.pdf\"%name)\n\noutput_pdf(clf_, name='filename%s'%n)\n</code></pre>\n\n<p><a href=\"http://i.stack.imgur.com/OqHfU.png\" rel=\"nofollow\">a tree graphy show here</a></p>\n\n\n<p>This builds on @paulkernfeld 's answer. If you have a dataframe X with your features and a target dataframe y with your resonses and you you want to get an idea which y value ended in which node (and also ant to plot it accordingly) you can do the following:</p>\n\n<pre><code>    def tree_to_code(tree, feature_names):\n        codelines = []\n        codelines.append('def get_cat(X_tmp):\\n')\n        codelines.append('   catout = []\\n')\n        codelines.append('   for codelines in range(0,X_tmp.shape[0]):\\n')\n        codelines.append('      Xin = X_tmp.iloc[codelines]\\n')\n        tree_ = tree.tree_\n        feature_name = [\n            feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n            for i in tree_.feature\n        ]\n        #print \"def tree({}):\".format(\", \".join(feature_names))\n\n        def recurse(node, depth):\n            indent = \"      \" * depth\n            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n                name = feature_name[node]\n                threshold = tree_.threshold[node]\n                codelines.append ('{}if Xin[\"{}\"] &lt;= {}:\\n'.format(indent, name, threshold))\n                recurse(tree_.children_left[node], depth + 1)\n                codelines.append( '{}else:  # if Xin[\"{}\"] &gt; {}\\n'.format(indent, name, threshold))\n                recurse(tree_.children_right[node], depth + 1)\n            else:\n                codelines.append( '{}mycat = {}\\n'.format(indent, node))\n\n        recurse(0, 1)\n        codelines.append('      catout.append(mycat)\\n')\n        codelines.append('   return pd.DataFrame(catout,index=X_tmp.index,columns=[\"category\"])\\n')\n        codelines.append('node_ids = get_cat(X)\\n')\n        return codelines\n    mycode = tree_to_code(clf,X.columns.values)\n\n    # now execute the function and obtain the dataframe with all nodes\n    exec(''.join(mycode))\n    node_ids = [int(x[0]) for x in node_ids.values]\n    node_ids2 = pd.DataFrame(node_ids)\n\n    print('make plot')\n    import matplotlib.cm as cm\n    colors = cm.rainbow(np.linspace(0, 1, 1+max( list(set(node_ids)))))\n    #plt.figure(figsize=cm2inch(24, 21))\n    for i in list(set(node_ids)):\n        plt.plot(y[node_ids2.values==i],'o',color=colors[i], label=str(i))  \n    mytitle = ['y colored by node']\n    plt.title(mytitle ,fontsize=14)\n    plt.xlabel('my xlabel')\n    plt.ylabel(tagname)\n    plt.xticks(rotation=70)       \n    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00), shadow=True, ncol=9)\n    plt.tight_layout()\n    plt.show()\n    plt.close \n</code></pre>\n\n<p>not the most elegant version but it does the job...  </p>\n\n\n<p>I believe that this answer is more correct than the other answers here:</p>\n\n<pre><code>from sklearn.tree import _tree\n\ndef tree_to_code(tree, feature_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print \"def tree({}):\".format(\", \".join(feature_names))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print \"{}if {} &lt;= {}:\".format(indent, name, threshold)\n            recurse(tree_.children_left[node], depth + 1)\n            print \"{}else:  # if {} &gt; {}\".format(indent, name, threshold)\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print \"{}return {}\".format(indent, tree_.value[node])\n\n    recurse(0, 1)\n</code></pre>\n\n<p>This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10.</p>\n\n<pre><code>def tree(f0):\n  if f0 &lt;= 6.0:\n    if f0 &lt;= 1.5:\n      return [[ 0.]]\n    else:  # if f0 &gt; 1.5\n      if f0 &lt;= 4.5:\n        if f0 &lt;= 3.5:\n          return [[ 3.]]\n        else:  # if f0 &gt; 3.5\n          return [[ 4.]]\n      else:  # if f0 &gt; 4.5\n        return [[ 5.]]\n  else:  # if f0 &gt; 6.0\n    if f0 &lt;= 8.5:\n      if f0 &lt;= 7.5:\n        return [[ 7.]]\n      else:  # if f0 &gt; 7.5\n        return [[ 8.]]\n    else:  # if f0 &gt; 8.5\n      return [[ 9.]]\n</code></pre>\n\n<p>Here are some stumbling blocks that I see in other answers:</p>\n\n<ol>\n<li>Using <code>tree_.threshold == -2</code> to decide whether a node is a leaf isn't a good idea. What if it's a real decision node with a threshold of -2? Instead, you should look at <code>tree.feature</code> or <code>tree.children_*</code>.</li>\n<li>The line <code>features = [feature_names[i] for i in tree_.feature]</code> crashes with my version of sklearn, because some values of <code>tree.tree_.feature</code> are -2 (specifically for leaf nodes).</li>\n<li>There is no need to have multiple if statements in the recursive function, just one is fine.</li>\n</ol>\n\n\n<p>I've been going through this, but i needed the rules to be written in this format </p>\n\n<pre><code>if A&gt;0.4 then if B&lt;0.2 then if C&gt;0.8 then class='X' \n</code></pre>\n\n<p>So I adapted the answer of @paulkernfeld (thanks) that you can customize to your need</p>\n\n<pre><code>def tree_to_code(tree, feature_names, Y):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    pathto=dict()\n\n    global k\n    k = 0\n    def recurse(node, depth, parent):\n        global k\n        indent = \"  \" * depth\n\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            s= \"{} &lt;= {} \".format( name, threshold, node )\n            if node == 0:\n                pathto[node]=s\n            else:\n                pathto[node]=pathto[parent]+' &amp; ' +s\n\n            recurse(tree_.children_left[node], depth + 1, node)\n            s=\"{} &gt; {}\".format( name, threshold)\n            if node == 0:\n                pathto[node]=s\n            else:\n                pathto[node]=pathto[parent]+' &amp; ' +s\n            recurse(tree_.children_right[node], depth + 1, node)\n        else:\n            k=k+1\n            print(k,')',pathto[parent], tree_.value[node])\n    recurse(0, 1, 0)\n</code></pre>\n\n\n<p>I modified the code submitted by <a href=\"https://stackoverflow.com/users/919872/zelazny7\">Zelazny7</a> to print some pseudocode:</p>\n\n<pre><code>def get_code(tree, feature_names):\n        left      = tree.tree_.children_left\n        right     = tree.tree_.children_right\n        threshold = tree.tree_.threshold\n        features  = [feature_names[i] for i in tree.tree_.feature]\n        value = tree.tree_.value\n\n        def recurse(left, right, threshold, features, node):\n                if (threshold[node] != -2):\n                        print \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\"\n                        if left[node] != -1:\n                                recurse (left, right, threshold, features,left[node])\n                        print \"} else {\"\n                        if right[node] != -1:\n                                recurse (left, right, threshold, features,right[node])\n                        print \"}\"\n                else:\n                        print \"return \" + str(value[node])\n\n        recurse(left, right, threshold, features, 0)\n</code></pre>\n\n<p>if you call <code>get_code(dt, df.columns)</code> on the same example you will obtain:</p>\n\n<pre><code>if ( col1 &lt;= 0.5 ) {\nreturn [[ 1.  0.]]\n} else {\nif ( col2 &lt;= 4.5 ) {\nreturn [[ 0.  1.]]\n} else {\nif ( col1 &lt;= 2.5 ) {\nreturn [[ 1.  0.]]\n} else {\nreturn [[ 0.  1.]]\n}\n}\n}\n</code></pre>\n\n\n<pre><code>from StringIO import StringIO\nout = StringIO()\nout = tree.export_graphviz(clf, out_file=out)\nprint out.getvalue()\n</code></pre>\n\n<p>You can see a digraph Tree. Then, <code>clf.tree_.feature</code> and <code>clf.tree_.value</code> are array of nodes splitting feature and array of nodes values respectively. You can refer to more details from this <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\">github source</a>. </p>\n\n\n<p>There is a new <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\" rel=\"nofollow noreferrer\"><code>DecisionTreeClassifier</code></a> method, <code>decision_path</code>, in the <a href=\"http://scikit-learn.org/stable/whats_new.html#version-0-18\" rel=\"nofollow noreferrer\">0.18.0</a> release.  The developers provide an extensive (well-documented) <a href=\"http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\" rel=\"nofollow noreferrer\">walkthrough</a>.</p>\n\n<p>The first section of code in the walkthrough that prints the tree structure seems to be OK.  However, I modified the code in the second section to interrogate one sample.  My changes denoted with <code># &lt;--</code></p>\n\n<p><strong>Edit</strong> The changes marked by <code># &lt;--</code> in the code below have since been updated in walkthrough link after the errors were pointed out in pull requests <a href=\"https://github.com/scikit-learn/scikit-learn/pull/8653\" rel=\"nofollow noreferrer\">#8653</a> and <a href=\"https://github.com/scikit-learn/scikit-learn/pull/10951\" rel=\"nofollow noreferrer\">#10951</a>. It's much easier to follow along now. </p>\n\n<pre><code>sample_id = 0\nnode_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                    node_indicator.indptr[sample_id + 1]]\n\nprint('Rules used to predict sample %s: ' % sample_id)\nfor node_id in node_index:\n\n    if leave_id[sample_id] == node_id:  # &lt;-- changed != to ==\n        #continue # &lt;-- comment out\n        print(\"leaf node {} reached, no decision here\".format(leave_id[sample_id])) # &lt;--\n\n    else: # &lt; -- added else to iterate through decision nodes\n        if (X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]):\n            threshold_sign = \"&lt;=\"\n        else:\n            threshold_sign = \"&gt;\"\n\n        print(\"decision id node %s : (X[%s, %s] (= %s) %s %s)\"\n              % (node_id,\n                 sample_id,\n                 feature[node_id],\n                 X_test[sample_id, feature[node_id]], # &lt;-- changed i to sample_id\n                 threshold_sign,\n                 threshold[node_id]))\n\nRules used to predict sample 0: \ndecision id node 0 : (X[0, 3] (= 2.4) &gt; 0.800000011921)\ndecision id node 2 : (X[0, 2] (= 5.1) &gt; 4.94999980927)\nleaf node 4 reached, no decision here\n</code></pre>\n\n<p>Change the <code>sample_id</code> to see the decision paths for other samples.  I haven't asked the developers about these changes, just seemed more intuitive when working through the example.</p>\n\n\n<p>Modified Zelazny7's code to fetch SQL from the decision tree.</p>\n\n<pre><code># SQL from decision tree\n\ndef get_lineage(tree, feature_names):\n     left      = tree.tree_.children_left\n     right     = tree.tree_.children_right\n     threshold = tree.tree_.threshold\n     features  = [feature_names[i] for i in tree.tree_.feature]\n     le='&lt;='               \n     g ='&gt;'\n     # get ids of child nodes\n     idx = np.argwhere(left == -1)[:,0]     \n\n     def recurse(left, right, child, lineage=None):          \n          if lineage is None:\n               lineage = [child]\n          if child in left:\n               parent = np.where(left == child)[0].item()\n               split = 'l'\n          else:\n               parent = np.where(right == child)[0].item()\n               split = 'r'\n          lineage.append((parent, split, threshold[parent], features[parent]))\n          if parent == 0:\n               lineage.reverse()\n               return lineage\n          else:\n               return recurse(left, right, parent, lineage)\n     print 'case '\n     for j,child in enumerate(idx):\n        clause=' when '\n        for node in recurse(left, right, child):\n            if len(str(node))&lt;3:\n                continue\n            i=node\n            if i[1]=='l':  sign=le \n            else: sign=g\n            clause=clause+i[3]+sign+str(i[2])+' and '\n        clause=clause[:-4]+' then '+str(j)\n        print clause\n     print 'else 99 end as clusters'\n</code></pre>\n\n\n<p>Just use the function from <strong>sklearn.tree</strong> like this</p>\n\n<pre><code>from sklearn.tree import export_graphviz\n    export_graphviz(tree,\n                out_file = \"tree.dot\",\n                feature_names = tree.columns) //or just [\"petal length\", \"petal width\"]\n</code></pre>\n\n<p>And then look in your project folder for the file <strong>tree.dot</strong>, copy the ALL the content and paste it here <a href=\"http://www.webgraphviz.com/\" rel=\"nofollow noreferrer\">http://www.webgraphviz.com/</a> and generate your graph :)</p>\n\n\n<p>Apparently a long time ago somebody already decided to try to add the following function to the official scikit's tree export functions (which basically only supports export_graphviz)</p>\n\n<pre><code>def export_dict(tree, feature_names=None, max_depth=None) :\n    \"\"\"Export a decision tree in dict format.\n</code></pre>\n\n<p>Here is his full commit:</p>\n\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py</a></p>\n\n<p>Not exactly sure what happened to this comment. But you could also try to use that function.</p>\n\n<p>I think this warrants a serious documentation request to the good people of scikit-learn to properly document the <code>sklearn.tree.Tree</code> API which is the underlying tree structure that <code>DecisionTreeClassifier</code> exposes as its attribute <code>tree_</code>.</p>\n\n\n<p>Just because everyone was so helpful I'll just add a modification to Zelazny7 and Daniele's beautiful solutions. This one is for python 2.7, with tabs to make it more readable:</p>\n\n<pre><code>def get_code(tree, feature_names, tabdepth=0):\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    value = tree.tree_.value\n\n    def recurse(left, right, threshold, features, node, tabdepth=0):\n            if (threshold[node] != -2):\n                    print '\\t' * tabdepth,\n                    print \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\"\n                    if left[node] != -1:\n                            recurse (left, right, threshold, features,left[node], tabdepth+1)\n                    print '\\t' * tabdepth,\n                    print \"} else {\"\n                    if right[node] != -1:\n                            recurse (left, right, threshold, features,right[node], tabdepth+1)\n                    print '\\t' * tabdepth,\n                    print \"}\"\n            else:\n                    print '\\t' * tabdepth,\n                    print \"return \" + str(value[node])\n\n    recurse(left, right, threshold, features, 0)\n</code></pre>\n\n\n<p>I created my own function to extract the rules from the decision trees created by sklearn:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n# dummy data:\ndf = pd.DataFrame({'col1':[0,1,2,3],'col2':[3,4,5,6],'dv':[0,1,0,1]})\n\n# create decision tree\ndt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1)\ndt.fit(df.ix[:,:2], df.dv)\n</code></pre>\n\n<p>This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic:</p>\n\n<pre><code>def get_lineage(tree, feature_names):\n     left      = tree.tree_.children_left\n     right     = tree.tree_.children_right\n     threshold = tree.tree_.threshold\n     features  = [feature_names[i] for i in tree.tree_.feature]\n\n     # get ids of child nodes\n     idx = np.argwhere(left == -1)[:,0]     \n\n     def recurse(left, right, child, lineage=None):          \n          if lineage is None:\n               lineage = [child]\n          if child in left:\n               parent = np.where(left == child)[0].item()\n               split = 'l'\n          else:\n               parent = np.where(right == child)[0].item()\n               split = 'r'\n\n          lineage.append((parent, split, threshold[parent], features[parent]))\n\n          if parent == 0:\n               lineage.reverse()\n               return lineage\n          else:\n               return recurse(left, right, parent, lineage)\n\n     for child in idx:\n          for node in recurse(left, right, child):\n               print node\n</code></pre>\n\n<p>The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using <code>do</code> blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.</p>\n\n<pre><code>In [1]: get_lineage(dt, df.columns)\n(0, 'l', 0.5, 'col1')\n1\n(0, 'r', 0.5, 'col1')\n(2, 'l', 4.5, 'col2')\n3\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'l', 2.5, 'col1')\n5\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'r', 2.5, 'col1')\n6\n</code></pre>\n\n<p><img src=\"https://i.stack.imgur.com/SWwtO.png\" alt=\"GraphViz output of example tree\"></p>\n\n\n<p>Here is a function, printing rules of a scikit-learn decision tree under python 3 and with offsets for conditional blocks to make the structure more readable:</p>\n\n<pre><code>def print_decision_tree(tree, feature_names=None, offset_unit='    '):\n    '''Plots textual representation of rules of a decision tree\n    tree: scikit-learn representation of tree\n    feature_names: list of feature names. They are set to f1,f2,f3,... if not specified\n    offset_unit: a string of offset of the conditional block'''\n\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    value = tree.tree_.value\n    if feature_names is None:\n        features  = ['f%d'%i for i in tree.tree_.feature]\n    else:\n        features  = [feature_names[i] for i in tree.tree_.feature]        \n\n    def recurse(left, right, threshold, features, node, depth=0):\n            offset = offset_unit*depth\n            if (threshold[node] != -2):\n                    print(offset+\"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\")\n                    if left[node] != -1:\n                            recurse (left, right, threshold, features,left[node],depth+1)\n                    print(offset+\"} else {\")\n                    if right[node] != -1:\n                            recurse (left, right, threshold, features,right[node],depth+1)\n                    print(offset+\"}\")\n            else:\n                    print(offset+\"return \" + str(value[node]))\n\n    recurse(left, right, threshold, features, 0,0)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Here is a way to translate the whole tree into a single (not necessarily too human-readable) python expression using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre><code>from skompiler import skompile\nskompile(dtree.predict).to('python/code')\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>Codes below is my approach under anaconda python 2.7 plus a package name \"pydot-ng\" to making a PDF file with decision rules. I hope it is helpful.</p>\n\n<pre><code>from sklearn import tree\n\nclf = tree.DecisionTreeClassifier(max_leaf_nodes=n)\nclf_ = clf.fit(X, data_y)\n\nfeature_names = X.columns\nclass_name = clf_.classes_.astype(int).astype(str)\n\ndef output_pdf(clf_, name):\n    from sklearn import tree\n    from sklearn.externals.six import StringIO\n    import pydot_ng as pydot\n    dot_data = StringIO()\n    tree.export_graphviz(clf_, out_file=dot_data,\n                         feature_names=feature_names,\n                         class_names=class_name,\n                         filled=True, rounded=True,\n                         special_characters=True,\n                          node_ids=1,)\n    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n    graph.write_pdf(\"%s.pdf\"%name)\n\noutput_pdf(clf_, name='filename%s'%n)\n</code></pre>\n\n<p><a href=\"http://i.stack.imgur.com/OqHfU.png\" rel=\"nofollow\">a tree graphy show here</a></p>\n"}, "answer_2_votes": {"type": "literal", "value": "2"}, "answer_3": {"type": "literal", "value": "<p>This builds on @paulkernfeld 's answer. If you have a dataframe X with your features and a target dataframe y with your resonses and you you want to get an idea which y value ended in which node (and also ant to plot it accordingly) you can do the following:</p>\n\n<pre><code>    def tree_to_code(tree, feature_names):\n        codelines = []\n        codelines.append('def get_cat(X_tmp):\\n')\n        codelines.append('   catout = []\\n')\n        codelines.append('   for codelines in range(0,X_tmp.shape[0]):\\n')\n        codelines.append('      Xin = X_tmp.iloc[codelines]\\n')\n        tree_ = tree.tree_\n        feature_name = [\n            feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n            for i in tree_.feature\n        ]\n        #print \"def tree({}):\".format(\", \".join(feature_names))\n\n        def recurse(node, depth):\n            indent = \"      \" * depth\n            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n                name = feature_name[node]\n                threshold = tree_.threshold[node]\n                codelines.append ('{}if Xin[\"{}\"] &lt;= {}:\\n'.format(indent, name, threshold))\n                recurse(tree_.children_left[node], depth + 1)\n                codelines.append( '{}else:  # if Xin[\"{}\"] &gt; {}\\n'.format(indent, name, threshold))\n                recurse(tree_.children_right[node], depth + 1)\n            else:\n                codelines.append( '{}mycat = {}\\n'.format(indent, node))\n\n        recurse(0, 1)\n        codelines.append('      catout.append(mycat)\\n')\n        codelines.append('   return pd.DataFrame(catout,index=X_tmp.index,columns=[\"category\"])\\n')\n        codelines.append('node_ids = get_cat(X)\\n')\n        return codelines\n    mycode = tree_to_code(clf,X.columns.values)\n\n    # now execute the function and obtain the dataframe with all nodes\n    exec(''.join(mycode))\n    node_ids = [int(x[0]) for x in node_ids.values]\n    node_ids2 = pd.DataFrame(node_ids)\n\n    print('make plot')\n    import matplotlib.cm as cm\n    colors = cm.rainbow(np.linspace(0, 1, 1+max( list(set(node_ids)))))\n    #plt.figure(figsize=cm2inch(24, 21))\n    for i in list(set(node_ids)):\n        plt.plot(y[node_ids2.values==i],'o',color=colors[i], label=str(i))  \n    mytitle = ['y colored by node']\n    plt.title(mytitle ,fontsize=14)\n    plt.xlabel('my xlabel')\n    plt.ylabel(tagname)\n    plt.xticks(rotation=70)       \n    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00), shadow=True, ncol=9)\n    plt.tight_layout()\n    plt.show()\n    plt.close \n</code></pre>\n\n<p>not the most elegant version but it does the job...  </p>\n"}, "answer_3_votes": {"type": "literal", "value": "2"}, "answer_4": {"type": "literal", "value": "<p>I believe that this answer is more correct than the other answers here:</p>\n\n<pre><code>from sklearn.tree import _tree\n\ndef tree_to_code(tree, feature_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print \"def tree({}):\".format(\", \".join(feature_names))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print \"{}if {} &lt;= {}:\".format(indent, name, threshold)\n            recurse(tree_.children_left[node], depth + 1)\n            print \"{}else:  # if {} &gt; {}\".format(indent, name, threshold)\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print \"{}return {}\".format(indent, tree_.value[node])\n\n    recurse(0, 1)\n</code></pre>\n\n<p>This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10.</p>\n\n<pre><code>def tree(f0):\n  if f0 &lt;= 6.0:\n    if f0 &lt;= 1.5:\n      return [[ 0.]]\n    else:  # if f0 &gt; 1.5\n      if f0 &lt;= 4.5:\n        if f0 &lt;= 3.5:\n          return [[ 3.]]\n        else:  # if f0 &gt; 3.5\n          return [[ 4.]]\n      else:  # if f0 &gt; 4.5\n        return [[ 5.]]\n  else:  # if f0 &gt; 6.0\n    if f0 &lt;= 8.5:\n      if f0 &lt;= 7.5:\n        return [[ 7.]]\n      else:  # if f0 &gt; 7.5\n        return [[ 8.]]\n    else:  # if f0 &gt; 8.5\n      return [[ 9.]]\n</code></pre>\n\n<p>Here are some stumbling blocks that I see in other answers:</p>\n\n<ol>\n<li>Using <code>tree_.threshold == -2</code> to decide whether a node is a leaf isn't a good idea. What if it's a real decision node with a threshold of -2? Instead, you should look at <code>tree.feature</code> or <code>tree.children_*</code>.</li>\n<li>The line <code>features = [feature_names[i] for i in tree_.feature]</code> crashes with my version of sklearn, because some values of <code>tree.tree_.feature</code> are -2 (specifically for leaf nodes).</li>\n<li>There is no need to have multiple if statements in the recursive function, just one is fine.</li>\n</ol>\n"}, "answer_4_votes": {"type": "literal", "value": "107"}, "answer_5": {"type": "literal", "value": "<p>I've been going through this, but i needed the rules to be written in this format </p>\n\n<pre><code>if A&gt;0.4 then if B&lt;0.2 then if C&gt;0.8 then class='X' \n</code></pre>\n\n<p>So I adapted the answer of @paulkernfeld (thanks) that you can customize to your need</p>\n\n<pre><code>def tree_to_code(tree, feature_names, Y):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    pathto=dict()\n\n    global k\n    k = 0\n    def recurse(node, depth, parent):\n        global k\n        indent = \"  \" * depth\n\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            s= \"{} &lt;= {} \".format( name, threshold, node )\n            if node == 0:\n                pathto[node]=s\n            else:\n                pathto[node]=pathto[parent]+' &amp; ' +s\n\n            recurse(tree_.children_left[node], depth + 1, node)\n            s=\"{} &gt; {}\".format( name, threshold)\n            if node == 0:\n                pathto[node]=s\n            else:\n                pathto[node]=pathto[parent]+' &amp; ' +s\n            recurse(tree_.children_right[node], depth + 1, node)\n        else:\n            k=k+1\n            print(k,')',pathto[parent], tree_.value[node])\n    recurse(0, 1, 0)\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "1"}, "answer_6": {"type": "literal", "value": "<p>I modified the code submitted by <a href=\"https://stackoverflow.com/users/919872/zelazny7\">Zelazny7</a> to print some pseudocode:</p>\n\n<pre><code>def get_code(tree, feature_names):\n        left      = tree.tree_.children_left\n        right     = tree.tree_.children_right\n        threshold = tree.tree_.threshold\n        features  = [feature_names[i] for i in tree.tree_.feature]\n        value = tree.tree_.value\n\n        def recurse(left, right, threshold, features, node):\n                if (threshold[node] != -2):\n                        print \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\"\n                        if left[node] != -1:\n                                recurse (left, right, threshold, features,left[node])\n                        print \"} else {\"\n                        if right[node] != -1:\n                                recurse (left, right, threshold, features,right[node])\n                        print \"}\"\n                else:\n                        print \"return \" + str(value[node])\n\n        recurse(left, right, threshold, features, 0)\n</code></pre>\n\n<p>if you call <code>get_code(dt, df.columns)</code> on the same example you will obtain:</p>\n\n<pre><code>if ( col1 &lt;= 0.5 ) {\nreturn [[ 1.  0.]]\n} else {\nif ( col2 &lt;= 4.5 ) {\nreturn [[ 0.  1.]]\n} else {\nif ( col1 &lt;= 2.5 ) {\nreturn [[ 1.  0.]]\n} else {\nreturn [[ 0.  1.]]\n}\n}\n}\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "34"}, "answer_7": {"type": "literal", "value": "<pre><code>from StringIO import StringIO\nout = StringIO()\nout = tree.export_graphviz(clf, out_file=out)\nprint out.getvalue()\n</code></pre>\n\n<p>You can see a digraph Tree. Then, <code>clf.tree_.feature</code> and <code>clf.tree_.value</code> are array of nodes splitting feature and array of nodes values respectively. You can refer to more details from this <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\">github source</a>. </p>\n"}, "answer_7_votes": {"type": "literal", "value": "12"}, "answer_8": {"type": "literal", "value": "<p>There is a new <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\" rel=\"nofollow noreferrer\"><code>DecisionTreeClassifier</code></a> method, <code>decision_path</code>, in the <a href=\"http://scikit-learn.org/stable/whats_new.html#version-0-18\" rel=\"nofollow noreferrer\">0.18.0</a> release.  The developers provide an extensive (well-documented) <a href=\"http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\" rel=\"nofollow noreferrer\">walkthrough</a>.</p>\n\n<p>The first section of code in the walkthrough that prints the tree structure seems to be OK.  However, I modified the code in the second section to interrogate one sample.  My changes denoted with <code># &lt;--</code></p>\n\n<p><strong>Edit</strong> The changes marked by <code># &lt;--</code> in the code below have since been updated in walkthrough link after the errors were pointed out in pull requests <a href=\"https://github.com/scikit-learn/scikit-learn/pull/8653\" rel=\"nofollow noreferrer\">#8653</a> and <a href=\"https://github.com/scikit-learn/scikit-learn/pull/10951\" rel=\"nofollow noreferrer\">#10951</a>. It's much easier to follow along now. </p>\n\n<pre><code>sample_id = 0\nnode_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                    node_indicator.indptr[sample_id + 1]]\n\nprint('Rules used to predict sample %s: ' % sample_id)\nfor node_id in node_index:\n\n    if leave_id[sample_id] == node_id:  # &lt;-- changed != to ==\n        #continue # &lt;-- comment out\n        print(\"leaf node {} reached, no decision here\".format(leave_id[sample_id])) # &lt;--\n\n    else: # &lt; -- added else to iterate through decision nodes\n        if (X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]):\n            threshold_sign = \"&lt;=\"\n        else:\n            threshold_sign = \"&gt;\"\n\n        print(\"decision id node %s : (X[%s, %s] (= %s) %s %s)\"\n              % (node_id,\n                 sample_id,\n                 feature[node_id],\n                 X_test[sample_id, feature[node_id]], # &lt;-- changed i to sample_id\n                 threshold_sign,\n                 threshold[node_id]))\n\nRules used to predict sample 0: \ndecision id node 0 : (X[0, 3] (= 2.4) &gt; 0.800000011921)\ndecision id node 2 : (X[0, 2] (= 5.1) &gt; 4.94999980927)\nleaf node 4 reached, no decision here\n</code></pre>\n\n<p>Change the <code>sample_id</code> to see the decision paths for other samples.  I haven't asked the developers about these changes, just seemed more intuitive when working through the example.</p>\n"}, "answer_8_votes": {"type": "literal", "value": "12"}, "answer_9": {"type": "literal", "value": "<p>Modified Zelazny7's code to fetch SQL from the decision tree.</p>\n\n<pre><code># SQL from decision tree\n\ndef get_lineage(tree, feature_names):\n     left      = tree.tree_.children_left\n     right     = tree.tree_.children_right\n     threshold = tree.tree_.threshold\n     features  = [feature_names[i] for i in tree.tree_.feature]\n     le='&lt;='               \n     g ='&gt;'\n     # get ids of child nodes\n     idx = np.argwhere(left == -1)[:,0]     \n\n     def recurse(left, right, child, lineage=None):          \n          if lineage is None:\n               lineage = [child]\n          if child in left:\n               parent = np.where(left == child)[0].item()\n               split = 'l'\n          else:\n               parent = np.where(right == child)[0].item()\n               split = 'r'\n          lineage.append((parent, split, threshold[parent], features[parent]))\n          if parent == 0:\n               lineage.reverse()\n               return lineage\n          else:\n               return recurse(left, right, parent, lineage)\n     print 'case '\n     for j,child in enumerate(idx):\n        clause=' when '\n        for node in recurse(left, right, child):\n            if len(str(node))&lt;3:\n                continue\n            i=node\n            if i[1]=='l':  sign=le \n            else: sign=g\n            clause=clause+i[3]+sign+str(i[2])+' and '\n        clause=clause[:-4]+' then '+str(j)\n        print clause\n     print 'else 99 end as clusters'\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": ""}, "answer_10": {"type": "literal", "value": "<p>Just use the function from <strong>sklearn.tree</strong> like this</p>\n\n<pre><code>from sklearn.tree import export_graphviz\n    export_graphviz(tree,\n                out_file = \"tree.dot\",\n                feature_names = tree.columns) //or just [\"petal length\", \"petal width\"]\n</code></pre>\n\n<p>And then look in your project folder for the file <strong>tree.dot</strong>, copy the ALL the content and paste it here <a href=\"http://www.webgraphviz.com/\" rel=\"nofollow noreferrer\">http://www.webgraphviz.com/</a> and generate your graph :)</p>\n"}, "answer_10_votes": {"type": "literal", "value": ""}, "answer_11": {"type": "literal", "value": "<p>Apparently a long time ago somebody already decided to try to add the following function to the official scikit's tree export functions (which basically only supports export_graphviz)</p>\n\n<pre><code>def export_dict(tree, feature_names=None, max_depth=None) :\n    \"\"\"Export a decision tree in dict format.\n</code></pre>\n\n<p>Here is his full commit:</p>\n\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py</a></p>\n\n<p>Not exactly sure what happened to this comment. But you could also try to use that function.</p>\n\n<p>I think this warrants a serious documentation request to the good people of scikit-learn to properly document the <code>sklearn.tree.Tree</code> API which is the underlying tree structure that <code>DecisionTreeClassifier</code> exposes as its attribute <code>tree_</code>.</p>\n"}, "answer_11_votes": {"type": "literal", "value": ""}, "answer_12": {"type": "literal", "value": "<p>Just because everyone was so helpful I'll just add a modification to Zelazny7 and Daniele's beautiful solutions. This one is for python 2.7, with tabs to make it more readable:</p>\n\n<pre><code>def get_code(tree, feature_names, tabdepth=0):\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    value = tree.tree_.value\n\n    def recurse(left, right, threshold, features, node, tabdepth=0):\n            if (threshold[node] != -2):\n                    print '\\t' * tabdepth,\n                    print \"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\"\n                    if left[node] != -1:\n                            recurse (left, right, threshold, features,left[node], tabdepth+1)\n                    print '\\t' * tabdepth,\n                    print \"} else {\"\n                    if right[node] != -1:\n                            recurse (left, right, threshold, features,right[node], tabdepth+1)\n                    print '\\t' * tabdepth,\n                    print \"}\"\n            else:\n                    print '\\t' * tabdepth,\n                    print \"return \" + str(value[node])\n\n    recurse(left, right, threshold, features, 0)\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "3"}, "answer_13": {"type": "literal", "value": "<p>I created my own function to extract the rules from the decision trees created by sklearn:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n# dummy data:\ndf = pd.DataFrame({'col1':[0,1,2,3],'col2':[3,4,5,6],'dv':[0,1,0,1]})\n\n# create decision tree\ndt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1)\ndt.fit(df.ix[:,:2], df.dv)\n</code></pre>\n\n<p>This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic:</p>\n\n<pre><code>def get_lineage(tree, feature_names):\n     left      = tree.tree_.children_left\n     right     = tree.tree_.children_right\n     threshold = tree.tree_.threshold\n     features  = [feature_names[i] for i in tree.tree_.feature]\n\n     # get ids of child nodes\n     idx = np.argwhere(left == -1)[:,0]     \n\n     def recurse(left, right, child, lineage=None):          \n          if lineage is None:\n               lineage = [child]\n          if child in left:\n               parent = np.where(left == child)[0].item()\n               split = 'l'\n          else:\n               parent = np.where(right == child)[0].item()\n               split = 'r'\n\n          lineage.append((parent, split, threshold[parent], features[parent]))\n\n          if parent == 0:\n               lineage.reverse()\n               return lineage\n          else:\n               return recurse(left, right, parent, lineage)\n\n     for child in idx:\n          for node in recurse(left, right, child):\n               print node\n</code></pre>\n\n<p>The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using <code>do</code> blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.</p>\n\n<pre><code>In [1]: get_lineage(dt, df.columns)\n(0, 'l', 0.5, 'col1')\n1\n(0, 'r', 0.5, 'col1')\n(2, 'l', 4.5, 'col2')\n3\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'l', 2.5, 'col1')\n5\n(0, 'r', 0.5, 'col1')\n(2, 'r', 4.5, 'col2')\n(4, 'r', 2.5, 'col1')\n6\n</code></pre>\n\n<p><img src=\"https://i.stack.imgur.com/SWwtO.png\" alt=\"GraphViz output of example tree\"></p>\n"}, "answer_13_votes": {"type": "literal", "value": "45"}, "answer_14": {"type": "literal", "value": "<p>Here is a function, printing rules of a scikit-learn decision tree under python 3 and with offsets for conditional blocks to make the structure more readable:</p>\n\n<pre><code>def print_decision_tree(tree, feature_names=None, offset_unit='    '):\n    '''Plots textual representation of rules of a decision tree\n    tree: scikit-learn representation of tree\n    feature_names: list of feature names. They are set to f1,f2,f3,... if not specified\n    offset_unit: a string of offset of the conditional block'''\n\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    value = tree.tree_.value\n    if feature_names is None:\n        features  = ['f%d'%i for i in tree.tree_.feature]\n    else:\n        features  = [feature_names[i] for i in tree.tree_.feature]        \n\n    def recurse(left, right, threshold, features, node, depth=0):\n            offset = offset_unit*depth\n            if (threshold[node] != -2):\n                    print(offset+\"if ( \" + features[node] + \" &lt;= \" + str(threshold[node]) + \" ) {\")\n                    if left[node] != -1:\n                            recurse (left, right, threshold, features,left[node],depth+1)\n                    print(offset+\"} else {\")\n                    if right[node] != -1:\n                            recurse (left, right, threshold, features,right[node],depth+1)\n                    print(offset+\"}\")\n            else:\n                    print(offset+\"return \" + str(value[node]))\n\n    recurse(left, right, threshold, features, 0,0)\n</code></pre>\n"}, "answer_14_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list?</p>\n\n<p>Something like: </p>\n\n<p>  </p>\n\n<p>Thanks for your help.</p>\n", "answer_wo_code": "<p>Here is a way to translate the whole tree into a single (not necessarily too human-readable) python expression using the <a href=\"https://github.com/konstantint/SKompiler\" rel=\"nofollow noreferrer\">SKompiler</a> library:</p>\n\n<pre> </pre>\n\n\n<p>Codes below is my approach under anaconda python 2.7 plus a package name \"pydot-ng\" to making a PDF file with decision rules. I hope it is helpful.</p>\n\n<pre> </pre>\n\n<p><a href=\"http://i.stack.imgur.com/OqHfU.png\" rel=\"nofollow\">a tree graphy show here</a></p>\n\n\n<p>This builds on @paulkernfeld 's answer. If you have a dataframe X with your features and a target dataframe y with your resonses and you you want to get an idea which y value ended in which node (and also ant to plot it accordingly) you can do the following:</p>\n\n<pre> </pre>\n\n<p>not the most elegant version but it does the job...  </p>\n\n\n<p>I believe that this answer is more correct than the other answers here:</p>\n\n<pre> </pre>\n\n<p>This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10.</p>\n\n<pre> </pre>\n\n<p>Here are some stumbling blocks that I see in other answers:</p>\n\n<ol>\n<li>Using   to decide whether a node is a leaf isn't a good idea. What if it's a real decision node with a threshold of -2? Instead, you should look at   or  .</li>\n<li>The line   crashes with my version of sklearn, because some values of   are -2 (specifically for leaf nodes).</li>\n<li>There is no need to have multiple if statements in the recursive function, just one is fine.</li>\n</ol>\n\n\n<p>I've been going through this, but i needed the rules to be written in this format </p>\n\n<pre> </pre>\n\n<p>So I adapted the answer of @paulkernfeld (thanks) that you can customize to your need</p>\n\n<pre> </pre>\n\n\n<p>I modified the code submitted by <a href=\"https://stackoverflow.com/users/919872/zelazny7\">Zelazny7</a> to print some pseudocode:</p>\n\n<pre> </pre>\n\n<p>if you call   on the same example you will obtain:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>You can see a digraph Tree. Then,   and   are array of nodes splitting feature and array of nodes values respectively. You can refer to more details from this <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/export.py\">github source</a>. </p>\n\n\n<p>There is a new <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\" rel=\"nofollow noreferrer\"> </a> method,  , in the <a href=\"http://scikit-learn.org/stable/whats_new.html#version-0-18\" rel=\"nofollow noreferrer\">0.18.0</a> release.  The developers provide an extensive (well-documented) <a href=\"http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\" rel=\"nofollow noreferrer\">walkthrough</a>.</p>\n\n<p>The first section of code in the walkthrough that prints the tree structure seems to be OK.  However, I modified the code in the second section to interrogate one sample.  My changes denoted with  </p>\n\n<p><strong>Edit</strong> The changes marked by   in the code below have since been updated in walkthrough link after the errors were pointed out in pull requests <a href=\"https://github.com/scikit-learn/scikit-learn/pull/8653\" rel=\"nofollow noreferrer\">#8653</a> and <a href=\"https://github.com/scikit-learn/scikit-learn/pull/10951\" rel=\"nofollow noreferrer\">#10951</a>. It's much easier to follow along now. </p>\n\n<pre> </pre>\n\n<p>Change the   to see the decision paths for other samples.  I haven't asked the developers about these changes, just seemed more intuitive when working through the example.</p>\n\n\n<p>Modified Zelazny7's code to fetch SQL from the decision tree.</p>\n\n<pre> </pre>\n\n\n<p>Just use the function from <strong>sklearn.tree</strong> like this</p>\n\n<pre> </pre>\n\n<p>And then look in your project folder for the file <strong>tree.dot</strong>, copy the ALL the content and paste it here <a href=\"http://www.webgraphviz.com/\" rel=\"nofollow noreferrer\">http://www.webgraphviz.com/</a> and generate your graph :)</p>\n\n\n<p>Apparently a long time ago somebody already decided to try to add the following function to the official scikit's tree export functions (which basically only supports export_graphviz)</p>\n\n<pre> </pre>\n\n<p>Here is his full commit:</p>\n\n<p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py\" rel=\"nofollow noreferrer\">https://github.com/scikit-learn/scikit-learn/blob/79bdc8f711d0af225ed6be9fdb708cea9f98a910/sklearn/tree/export.py</a></p>\n\n<p>Not exactly sure what happened to this comment. But you could also try to use that function.</p>\n\n<p>I think this warrants a serious documentation request to the good people of scikit-learn to properly document the   API which is the underlying tree structure that   exposes as its attribute  .</p>\n\n\n<p>Just because everyone was so helpful I'll just add a modification to Zelazny7 and Daniele's beautiful solutions. This one is for python 2.7, with tabs to make it more readable:</p>\n\n<pre> </pre>\n\n\n<p>I created my own function to extract the rules from the decision trees created by sklearn:</p>\n\n<pre> </pre>\n\n<p>This function first starts with the nodes (identified by -1 in the child arrays) and then recursively finds the parents. I call this a node's 'lineage'.  Along the way, I grab the values I need to create if/then/else SAS logic:</p>\n\n<pre> </pre>\n\n<p>The sets of tuples below contain everything I need to create SAS if/then/else statements. I do not like using   blocks in SAS which is why I create logic describing a node's entire path. The single integer after the tuples is the ID of the terminal node in a path. All of the preceding tuples combine to create that node.</p>\n\n<pre> </pre>\n\n<p><img src=\"https://i.stack.imgur.com/SWwtO.png\" alt=\"GraphViz output of example tree\"></p>\n\n\n<p>Here is a function, printing rules of a scikit-learn decision tree under python 3 and with offsets for conditional blocks to make the structure more readable:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_excel"}, "class_func_label": {"type": "literal", "value": "pandas.read_excel"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead an Excel file into a pandas DataFrame.\n\nSupport both `xls` and `xlsx` file extensions from a local filesystem or URL.\nSupport an option to read a single sheet or a list of sheets.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54007632"}, "title": {"type": "literal", "value": "Transfer data from excel worksheet (openpyxl) to database table (dbf)"}, "content": {"type": "literal", "value": "<p>I have a simple problem of reading an excel worksheet, treat every row containing about 83 columns as unique database record, add it to local datum record and ultimately append and write onto DBF file.</p>\n\n<p>I can extract all the values from excel and add them to the list. But the list is not correct syntax and I don't know how to prepare/convert the list to database record. I am using Openpyxl, dbf and python 3.7.</p>\n\n<p>At the moment I am only testing and trying to prepare the data for Row 3 (hence min_max rows = 3)</p>\n\n<p>I understand that the data should be in the format \n    (('','','', ... 83 entries), \\\n     ('','','', ... 83 entries) \\\n    )</p>\n\n<p>But I do not know how to convert the list data into record\nor, alternatively, how to read in excel data directly into a DF appendable format</p>\n\n<pre><code>tbl_tst.open(mode=dbf.READ_WRITE) # all fields character string\n\nfor everyrow in ws_IntMstDBF.iter_rows(min_row = 3, max_row = 3, max_col = ws_IntMstDBF.max_column-1):\n    datum = [] #set([83]), will defining datum as () help solve the problem?\n    for idx, cells in enumerate(everyrow):\n        if cells.value is None: # for None entries, enter empty string\n            datum.append(\"\")\n            continue\n        datum.append(cells.value) # else enter cell values \n\n     tbl_tst.append(datum) # append that record to table !!! list is not record error here\n\ntbl_tst.close()\n</code></pre>\n\n<p>The error is complaining about using list to append to table, and this should be a record etc. Please guide how I can convert excel rows into appendable DBF table data.  </p>\n\n<pre><code>raise TypeError(\"data to append must be a tuple, dict, record, or template; not a %r\" % type(data))\nTypeError: data to append must be a tuple, dict, record, or template; not a &lt;class 'list'&gt;\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Thank you for the responses, I went on a bit of tangent since last night while trying different solutions. </p>\n\n<p>One solution that worked for me is as follows: \nI made sure that the worksheet data I am using is all strings/Text and converted any null entries to String type and entered empty string. So the following code does this task: </p>\n\n<pre><code>#house keeping\nfor eachrow in ws_IntMstDBF.iter_rows(min_row=2, max_row=ws_IntMstDBF.max_row, max_col=ws_IntMstDBF.max_column):\n    for idx, cells in enumerate(eachrow):\n        if cells.value is None: # change every Null cell type to String and put 0x20 (space)\n            cells.data_type = 's'\n            cells.value = \" \"\n</code></pre>\n\n<p>After writing the worksheet, I reopened it using panda dataframe and verified if the contents were all string type and there were no \"nan\" values remaining in the dataframe. \nThen I used df2dbf function from \"Dani Arribas-Bel\", modified it to suit the data I am working with and converted to dbf.</p>\n\n<p>The code which imports dataframe and converts to dbf format is as follows:</p>\n\n<pre><code>abspath = Path(__file__).resolve() # resolve to relative path to absolute\nrootpath = abspath.parents[3] # root (my source file is3 sub directories deep\nxlspath = rootpath / 'sub-dir1' / 'sub-dir2' / 'sub-dir3' / 'test.xlsx'\n# above code is only resolving file location, ignore \npd_Mst_df = pd.read_excel(xlspath)\n#print(pd_Mst_df) # for debug \nprint(\"... Writing Master DBF file \")\ndf2dbf(pd_Mst_df, dbfpath) # dbf path is defined similar to pd_Mst path\n</code></pre>\n\n<p>The function df2dbg uses pysal to write dataframe in dbf format: \nI made some modifications to the code to detect the length row length and character types as follows:</p>\n\n<pre><code>import pandas as pd\nimport pysal as ps\nimport numpy as np\n\n# code from function df2dbf\nelse:\n    type2spec = {int: ('N', 20, 0),\n                 np.int64: ('N', 20, 0),\n                 float: ('N', 36, 15),\n                 np.float64: ('N', 36, 15),\n                 str: ('C', 200, 0)\n                 }\n    #types = [type(df[i].iloc[0]) for i in df.columns]\n    types = [type('C') for i in range(0, len(df.columns))] #84)] #df.columns)] #range(0,84)] # i not required, to be removed\n    specs = [type2spec[t] for t in types]\ndb = ps.open(dbf_path, 'w')\n# code continues from function df2dbf\n</code></pre>\n\n<p>Pandas dataframe didn't require further modifications as all source data was formatted correctly before being committed to excel file. </p>\n\n<p>I will provide the link to pysal and df2dbf as soon as I find it on stackoverflow. </p>\n\n\n<p>Change</p>\n\n<pre><code>tbl_tst.append(datum)\n</code></pre>\n\n<p>to</p>\n\n<pre><code>tbl_tst.append(tuple(datum))\n</code></pre>\n\n<p>and that will get rid of that error.  As long as all your cell data has the appropriate type then the append should work.</p>\n\n\n<p>Check out the Python Pandas library...</p>\n\n<p>To read the data from excel inta a Pandas dataframe, you could use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\" rel=\"nofollow noreferrer\">pandas.read_excel</a></p>\n\n<p>Once the date is read into a Pandas dataframe, you can manipulate it and afterwards write it to a database using <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.DataFrame.to_sql</a></p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sql\" rel=\"nofollow noreferrer\">See also this explanation for dealing with database io</a></p>\n"}, "answer_1": {"type": "literal", "value": "<p>Thank you for the responses, I went on a bit of tangent since last night while trying different solutions. </p>\n\n<p>One solution that worked for me is as follows: \nI made sure that the worksheet data I am using is all strings/Text and converted any null entries to String type and entered empty string. So the following code does this task: </p>\n\n<pre><code>#house keeping\nfor eachrow in ws_IntMstDBF.iter_rows(min_row=2, max_row=ws_IntMstDBF.max_row, max_col=ws_IntMstDBF.max_column):\n    for idx, cells in enumerate(eachrow):\n        if cells.value is None: # change every Null cell type to String and put 0x20 (space)\n            cells.data_type = 's'\n            cells.value = \" \"\n</code></pre>\n\n<p>After writing the worksheet, I reopened it using panda dataframe and verified if the contents were all string type and there were no \"nan\" values remaining in the dataframe. \nThen I used df2dbf function from \"Dani Arribas-Bel\", modified it to suit the data I am working with and converted to dbf.</p>\n\n<p>The code which imports dataframe and converts to dbf format is as follows:</p>\n\n<pre><code>abspath = Path(__file__).resolve() # resolve to relative path to absolute\nrootpath = abspath.parents[3] # root (my source file is3 sub directories deep\nxlspath = rootpath / 'sub-dir1' / 'sub-dir2' / 'sub-dir3' / 'test.xlsx'\n# above code is only resolving file location, ignore \npd_Mst_df = pd.read_excel(xlspath)\n#print(pd_Mst_df) # for debug \nprint(\"... Writing Master DBF file \")\ndf2dbf(pd_Mst_df, dbfpath) # dbf path is defined similar to pd_Mst path\n</code></pre>\n\n<p>The function df2dbg uses pysal to write dataframe in dbf format: \nI made some modifications to the code to detect the length row length and character types as follows:</p>\n\n<pre><code>import pandas as pd\nimport pysal as ps\nimport numpy as np\n\n# code from function df2dbf\nelse:\n    type2spec = {int: ('N', 20, 0),\n                 np.int64: ('N', 20, 0),\n                 float: ('N', 36, 15),\n                 np.float64: ('N', 36, 15),\n                 str: ('C', 200, 0)\n                 }\n    #types = [type(df[i].iloc[0]) for i in df.columns]\n    types = [type('C') for i in range(0, len(df.columns))] #84)] #df.columns)] #range(0,84)] # i not required, to be removed\n    specs = [type2spec[t] for t in types]\ndb = ps.open(dbf_path, 'w')\n# code continues from function df2dbf\n</code></pre>\n\n<p>Pandas dataframe didn't require further modifications as all source data was formatted correctly before being committed to excel file. </p>\n\n<p>I will provide the link to pysal and df2dbf as soon as I find it on stackoverflow. </p>\n"}, "answer_1_votes": {"type": "literal", "value": ""}, "answer_2": {"type": "literal", "value": "<p>Change</p>\n\n<pre><code>tbl_tst.append(datum)\n</code></pre>\n\n<p>to</p>\n\n<pre><code>tbl_tst.append(tuple(datum))\n</code></pre>\n\n<p>and that will get rid of that error.  As long as all your cell data has the appropriate type then the append should work.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "2"}, "answer_3": {"type": "literal", "value": "<p>Check out the Python Pandas library...</p>\n\n<p>To read the data from excel inta a Pandas dataframe, you could use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\" rel=\"nofollow noreferrer\">pandas.read_excel</a></p>\n\n<p>Once the date is read into a Pandas dataframe, you can manipulate it and afterwards write it to a database using <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.DataFrame.to_sql</a></p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sql\" rel=\"nofollow noreferrer\">See also this explanation for dealing with database io</a></p>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>I have a simple problem of reading an excel worksheet, treat every row containing about 83 columns as unique database record, add it to local datum record and ultimately append and write onto DBF file.</p>\n\n<p>I can extract all the values from excel and add them to the list. But the list is not correct syntax and I don't know how to prepare/convert the list to database record. I am using Openpyxl, dbf and python 3.7.</p>\n\n<p>At the moment I am only testing and trying to prepare the data for Row 3 (hence min_max rows = 3)</p>\n\n<p>I understand that the data should be in the format \n    (('','','', ... 83 entries), \\\n     ('','','', ... 83 entries) \\\n    )</p>\n\n<p>But I do not know how to convert the list data into record\nor, alternatively, how to read in excel data directly into a DF appendable format</p>\n\n<pre> </pre>\n\n<p>The error is complaining about using list to append to table, and this should be a record etc. Please guide how I can convert excel rows into appendable DBF table data.  </p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Thank you for the responses, I went on a bit of tangent since last night while trying different solutions. </p>\n\n<p>One solution that worked for me is as follows: \nI made sure that the worksheet data I am using is all strings/Text and converted any null entries to String type and entered empty string. So the following code does this task: </p>\n\n<pre> </pre>\n\n<p>After writing the worksheet, I reopened it using panda dataframe and verified if the contents were all string type and there were no \"nan\" values remaining in the dataframe. \nThen I used df2dbf function from \"Dani Arribas-Bel\", modified it to suit the data I am working with and converted to dbf.</p>\n\n<p>The code which imports dataframe and converts to dbf format is as follows:</p>\n\n<pre> </pre>\n\n<p>The function df2dbg uses pysal to write dataframe in dbf format: \nI made some modifications to the code to detect the length row length and character types as follows:</p>\n\n<pre> </pre>\n\n<p>Pandas dataframe didn't require further modifications as all source data was formatted correctly before being committed to excel file. </p>\n\n<p>I will provide the link to pysal and df2dbf as soon as I find it on stackoverflow. </p>\n\n\n<p>Change</p>\n\n<pre> </pre>\n\n<p>to</p>\n\n<pre> </pre>\n\n<p>and that will get rid of that error.  As long as all your cell data has the appropriate type then the append should work.</p>\n\n\n<p>Check out the Python Pandas library...</p>\n\n<p>To read the data from excel inta a Pandas dataframe, you could use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\" rel=\"nofollow noreferrer\">pandas.read_excel</a></p>\n\n<p>Once the date is read into a Pandas dataframe, you can manipulate it and afterwards write it to a database using <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\" rel=\"nofollow noreferrer\">pandas.DataFrame.to_sql</a></p>\n\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sql\" rel=\"nofollow noreferrer\">See also this explanation for dealing with database io</a></p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.DataFrame"}, "class_func_label": {"type": "literal", "value": "pandas.DataFrame"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/17156084"}, "title": {"type": "literal", "value": "unpacking a sql select into a pandas dataframe"}, "content": {"type": "literal", "value": "<p>Suppose I have a select roughly like this:</p>\n\n<pre><code>select instrument, price, date from my_prices;\n</code></pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n</code></pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n</code></pre>\n\n<p>...which is easy ;-)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "30"}, "answer_2": {"type": "literal", "value": "<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n"}, "answer_2_votes": {"type": "literal", "value": "30"}, "answer_3": {"type": "literal", "value": "<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p>Suppose I have a select roughly like this:</p>\n\n<pre> </pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre> </pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre> </pre>\n\n<p>...which is easy ;-)</p>\n", "answer_wo_code": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre> </pre>\n\n<p>then set index like</p>\n\n<pre> </pre>\n\n<p>or directly:</p>\n\n<pre> </pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions:   and  .</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre> </pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre> </pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre> </pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"> </a> the result:</p>\n\n<pre> </pre>\n\n<p><em>Note: You could miss out the   you don't specify an   in the  .</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/17156084"}, "title": {"type": "literal", "value": "unpacking a sql select into a pandas dataframe"}, "content": {"type": "literal", "value": "<p>Suppose I have a select roughly like this:</p>\n\n<pre><code>select instrument, price, date from my_prices;\n</code></pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n</code></pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n</code></pre>\n\n<p>...which is easy ;-)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "30"}, "answer_2": {"type": "literal", "value": "<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n"}, "answer_2_votes": {"type": "literal", "value": "30"}, "answer_3": {"type": "literal", "value": "<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p>Suppose I have a select roughly like this:</p>\n\n<pre> </pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre> </pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre> </pre>\n\n<p>...which is easy ;-)</p>\n", "answer_wo_code": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre> </pre>\n\n<p>then set index like</p>\n\n<pre> </pre>\n\n<p>or directly:</p>\n\n<pre> </pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions:   and  .</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre> </pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre> </pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre> </pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"> </a> the result:</p>\n\n<pre> </pre>\n\n<p><em>Note: You could miss out the   you don't specify an   in the  .</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql_query"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql_query"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query into a DataFrame.\n\nReturns a DataFrame corresponding to the result set of the query\nstring. Optionally provide an `index_col` parameter to use one of the\ncolumns as the index, otherwise default integer index will be used.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/17156084"}, "title": {"type": "literal", "value": "unpacking a sql select into a pandas dataframe"}, "content": {"type": "literal", "value": "<p>Suppose I have a select roughly like this:</p>\n\n<pre><code>select instrument, price, date from my_prices;\n</code></pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n</code></pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n</code></pre>\n\n<p>...which is easy ;-)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "30"}, "answer_2": {"type": "literal", "value": "<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n"}, "answer_2_votes": {"type": "literal", "value": "30"}, "answer_3": {"type": "literal", "value": "<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p>Suppose I have a select roughly like this:</p>\n\n<pre> </pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre> </pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre> </pre>\n\n<p>...which is easy ;-)</p>\n", "answer_wo_code": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre> </pre>\n\n<p>then set index like</p>\n\n<pre> </pre>\n\n<p>or directly:</p>\n\n<pre> </pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions:   and  .</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre> </pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre> </pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre> </pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"> </a> the result:</p>\n\n<pre> </pre>\n\n<p><em>Note: You could miss out the   you don't specify an   in the  .</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql_table"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql_table"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL database table into a DataFrame.\n\nGiven a table name and a SQLAlchemy connectable, returns a DataFrame.\nThis function does not support DBAPI connections.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/17156084"}, "title": {"type": "literal", "value": "unpacking a sql select into a pandas dataframe"}, "content": {"type": "literal", "value": "<p>Suppose I have a select roughly like this:</p>\n\n<pre><code>select instrument, price, date from my_prices;\n</code></pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n</code></pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n</code></pre>\n\n<p>...which is easy ;-)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre><code>import psycopg2\nconn = psycopg2.connect(\"dbname='db' user='user' host='host' password='pass'\")\ncur = conn.cursor()\ncur.execute(\"select instrument, price, date from my_prices\")\ndf = DataFrame(cur.fetchall(), columns=['instrument', 'price', 'date'])\n</code></pre>\n\n<p>then set index like</p>\n\n<pre><code>df.set_index('date', drop=False)\n</code></pre>\n\n<p>or directly:</p>\n\n<pre><code>df.index =  df['date']\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "30"}, "answer_2": {"type": "literal", "value": "<p><strong>Update</strong>: recent pandas have the following functions: <code>read_sql_table</code> and <code>read_sql_query</code>.</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre><code>from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine('mysql://scott:tiger@localhost/foo')\n</code></pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre><code>table_name = 'my_prices'\ndf = pd.read_sql_table(table_name, engine)\n</code></pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre><code>df = pd.read_sql_query(\"SELECT instrument, price, date FROM my_prices;\", engine)\n</code></pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"><code>pivot</code></a> the result:</p>\n\n<pre><code>df.reset_index().pivot('date', 'instrument', 'price')\n</code></pre>\n\n<p><em>Note: You could miss out the <code>reset_index</code> you don't specify an <code>index_col</code> in the <code>read_frame</code>.</em></p>\n"}, "answer_2_votes": {"type": "literal", "value": "30"}, "answer_3": {"type": "literal", "value": "<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre><code># CONNECT TO POSTGRES USING PANDAS\nimport psycopg2 as pg\nimport pandas.io.sql as psql\n</code></pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre><code>connection = pg.connect(\"host=192.168.0.1 dbname=db user=postgres\")\n</code></pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre><code>dataframe = psql.read_sql(\"SELECT * FROM DB.Table\", connection)\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p>Suppose I have a select roughly like this:</p>\n\n<pre> </pre>\n\n<p>How can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?</p>\n\n<p>To be clear: I'm looking for:</p>\n\n<pre> </pre>\n\n<p>I'm NOT looking for:</p>\n\n<pre> </pre>\n\n<p>...which is easy ;-)</p>\n", "answer_wo_code": "<p>You can pass a cursor object to the DataFrame constructor. For postgres:</p>\n\n<pre> </pre>\n\n<p>then set index like</p>\n\n<pre> </pre>\n\n<p>or directly:</p>\n\n<pre> </pre>\n\n\n<p><strong>Update</strong>: recent pandas have the following functions:   and  .</p>\n\n<p>First create a db engine (a connection can also work here):</p>\n\n<pre> </pre>\n\n<p><em>See <a href=\"http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\" rel=\"noreferrer\">sqlalchemy database urls</a>.</em></p>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html#pandas.read_sql_table\" rel=\"noreferrer\">pandas_read_sql_table</a></h3>\n\n<pre> </pre>\n\n<h3><a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html#pandas.read_sql_query\" rel=\"noreferrer\">pandas_read_sql_query</a></h3>\n\n<pre> </pre>\n\n<hr>\n\n<p>The old answer had referenced read_frame which is has been deprecated  (see the <a href=\"https://stackoverflow.com/posts/17156233/revisions\">version history</a> of this question for that answer).</p>\n\n<hr>\n\n<p>It's often makes sense to read first, and <em>then</em> perform transformations to your requirements (as these are usually efficient and readable in pandas). In your example, you can <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\" rel=\"noreferrer\"> </a> the result:</p>\n\n<pre> </pre>\n\n<p><em>Note: You could miss out the   you don't specify an   in the  .</em></p>\n\n\n<blockquote>\n  <p>This connect with postgres and pandas with remote postgresql</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to establish the connection with postgres db</p>\n</blockquote>\n\n<pre> </pre>\n\n<blockquote>\n  <p>this is used to read the table from postgres db</p>\n</blockquote>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/16249736"}, "title": {"type": "literal", "value": "How to import data from mongodb to pandas?"}, "content": {"type": "literal", "value": "<p>I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?</p>\n\n<p>I am new to pandas and numpy.</p>\n\n<p>EDIT:\nThe mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. </p>\n\n<p>Sample Data:</p>\n\n<pre><code>{\n\"_cls\" : \"SensorReport\",\n\"_id\" : ObjectId(\"515a963b78f6a035d9fa531b\"),\n\"_types\" : [\n    \"SensorReport\"\n],\n\"Readings\" : [\n    {\n        \"a\" : 0.958069536790466,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:26:35.297Z\"),\n        \"b\" : 6.296118156595,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 0.95574014778624,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:27:09.963Z\"),\n        \"b\" : 6.29651468650064,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 0.953648289182713,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:27:37.545Z\"),\n        \"b\" : 7.29679823731148,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 0.955931884300997,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:28:21.369Z\"),\n        \"b\" : 6.29642922525632,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 0.95821381,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:41:20.801Z\"),\n        \"b\" : 7.28956613,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 4.95821335,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:41:36.931Z\"),\n        \"b\" : 6.28956574,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 9.95821341,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:42:09.971Z\"),\n        \"b\" : 0.28956488,\n        \"_cls\" : \"Reading\"\n    },\n    {\n        \"a\" : 1.95667927,\n        \"_types\" : [\n            \"Reading\"\n        ],\n        \"ReadingUpdatedDate\" : ISODate(\"2013-04-02T08:43:55.463Z\"),\n        \"b\" : 0.29115237,\n        \"_cls\" : \"Reading\"\n    }\n],\n\"latestReportTime\" : ISODate(\"2013-04-02T08:43:55.463Z\"),\n\"sensorName\" : \"56847890-0\",\n\"reportCount\" : 8\n}\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p><a href=\"http://docs.mongodb.org/manual/reference/mongoexport\" rel=\"nofollow\">http://docs.mongodb.org/manual/reference/mongoexport</a></p>\n\n<p>export to csv and use <code>read_csv</code>\nor JSON and use <code>DataFrame.from_records</code></p>\n\n\n<p>A similar approach like Rafael Valero, waitingkuo and Deu Leung using <strong>pagination</strong>:</p>\n\n<pre><code>def read_mongo(\n       # db, \n       collection, query=None, \n       # host='localhost', port=27017, username=None, password=None,\n       chunksize = 100, page_num=1, no_id=True):\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Calculate number of documents to skip\n    skips = chunksize * (page_num - 1)\n\n    # Sorry, this is in spanish\n    # https://www.toptal.com/python/c%C3%B3digo-buggy-python-los-10-errores-m%C3%A1s-comunes-que-cometen-los-desarrolladores-python/es\n    if not query:\n        query = {}\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query).skip(skips).limit(chunksize)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df['_id']\n\n    return df\n</code></pre>\n\n\n<p>For dealing with out-of-core (not fitting into RAM) data efficiently (i.e. with parallel execution), you can try <a href=\"http://blaze.readthedocs.io/en/latest/index.html\" rel=\"noreferrer\">Python Blaze ecosystem</a>: Blaze / Dask / Odo.</p>\n\n<p>Blaze (and <a href=\"http://odo.pydata.org/en/latest/\" rel=\"noreferrer\">Odo</a>) has out-of-the-box functions to deal with MongoDB.</p>\n\n<p>A few useful articles to start off:</p>\n\n<ul>\n<li><a href=\"https://www.continuum.io/blog/developer/introducing-blaze-expressions\" rel=\"noreferrer\">Introducing Blaze Expessions</a> (with MongoDB query example)</li>\n<li><a href=\"http://danielfrg.com/blog/2015/07/21/reproduceit-reddit-word-count-dask/\" rel=\"noreferrer\">ReproduceIt: Reddit word count</a></li>\n<li><a href=\"http://dask.pydata.org/en/doc-test-build/array-blaze.html\" rel=\"noreferrer\">Difference between Dask Arrays and Blaze</a></li>\n</ul>\n\n<p>And an article which shows what amazing things are possible with Blaze stack: <a href=\"http://blaze.pydata.org/blog/2015/09/16/reddit-impala/\" rel=\"noreferrer\">Analyzing 1.7 Billion Reddit Comments with Blaze and Impala</a> (essentially, querying 975 Gb of Reddit comments in seconds).</p>\n\n<p>P.S. I'm not affiliated with any of these technologies.</p>\n\n\n<p>Following this great answer by <a href=\"https://stackoverflow.com/a/16255680/7127519\">waitingkuo</a> I would like to add the possibility of doing that using chunksize in line with <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\">.read_sql()</a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"nofollow noreferrer\">.read_csv()</a>. I enlarge the answer from <a href=\"https://stackoverflow.com/a/39446008/7127519\">Deu Leung</a> by avoiding go one by one each 'record' of the 'iterator' / 'cursor'.\nI will borrow previous <em>read_mongo</em> function.</p>\n\n<pre><code>def read_mongo(db, \n           collection, query={}, \n           host='localhost', port=27017, \n           username=None, password=None,\n           chunksize = 100, no_id=True):\n\"\"\" Read from Mongo and Store into DataFrame \"\"\"\n\n\n# Connect to MongoDB\n#db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\nclient = MongoClient(host=host, port=port)\n# Make a query to the specific DB and Collection\ndb_aux = client[db]\n\n\n# Some variables to create the chunks\nskips_variable = range(0, db_aux[collection].find(query).count(), int(chunksize))\nif len(skips_variable)&lt;=1:\n    skips_variable = [0,len(skips_variable)]\n\n# Iteration to create the dataframe in chunks.\nfor i in range(1,len(skips_variable)):\n\n    # Expand the cursor and construct the DataFrame\n    #df_aux =pd.DataFrame(list(cursor_aux[skips_variable[i-1]:skips_variable[i]]))\n    df_aux =pd.DataFrame(list(db_aux[collection].find(query)[skips_variable[i-1]:skips_variable[i]]))\n\n    if no_id:\n        del df_aux['_id']\n\n    # Concatenate the chunks into a unique df\n    if 'df' not in locals():\n        df =  df_aux\n    else:\n        df = pd.concat([df, df_aux], ignore_index=True)\n\nreturn df\n</code></pre>\n\n\n<p>Using </p>\n\n<pre><code>pandas.DataFrame(list(...))\n</code></pre>\n\n<p>will consume a lot of memory if the iterator/generator result is large</p>\n\n<p>better to generate small chunks and concat at the end</p>\n\n<pre><code>def iterator2dataframes(iterator, chunk_size: int):\n  \"\"\"Turn an iterator into multiple small pandas.DataFrame\n\n  This is a balance between memory and efficiency\n  \"\"\"\n  records = []\n  frames = []\n  for i, record in enumerate(iterator):\n    records.append(record)\n    if i % chunk_size == chunk_size - 1:\n      frames.append(pd.DataFrame(records))\n      records = []\n  if records:\n    frames.append(pd.DataFrame(records))\n  return pd.concat(frames)\n</code></pre>\n\n\n<p><a href=\"https://bitbucket.org/djcbeach/monary/wiki/Home\"><code>Monary</code></a> does exactly that, and it's <em>super fast</em>. (<a href=\"http://djcinnovations.com/index.php/archives/164\">another link</a>)</p>\n\n<p>See <a href=\"http://alexgaudio.com/2012/07/07/monarymongopandas.html\">this cool post</a> which includes a quick tutorial and some timings.</p>\n\n\n<p><code>pymongo</code> might give you a hand, followings are some codes I'm using:</p>\n\n<pre><code>import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    \"\"\" A util for making a connection to mongo \"\"\"\n\n    if username and password:\n        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df['_id']\n\n    return df\n</code></pre>\n\n\n<p>You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.</p>\n\n<pre><code>import pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\nclient = MongoClient()\ndb = client.database_name\ncollection = db.collection_name\ndata = pd.DataFrame(list(collection.find()))\n</code></pre>\n\n\n<p>As per PEP, simple is better than complicated:  </p>\n\n<pre><code>import pandas as pd\ndf = pd.DataFrame.from_records(db.&lt;database_name&gt;.&lt;collection_name&gt;.find())\n</code></pre>\n\n<p>You can include conditions as you would working with regular mongoDB database or even use find_one() to get only one element from the database, etc. </p>\n\n<p>and voila!</p>\n\n\n<p>Another option I found very useful is:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pandas.io.json import json_normalize\n\ncursor = my_collection.find()\ndf = json_normalize(cursor)\n</code></pre>\n\n<p>this way you get the unfolding of nested mongodb documents for free.</p>\n\n\n<pre><code>import pandas as pd\nfrom odo import odo\n\ndata = odo('mongodb://localhost/db::collection', pd.DataFrame)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p><a href=\"http://docs.mongodb.org/manual/reference/mongoexport\" rel=\"nofollow\">http://docs.mongodb.org/manual/reference/mongoexport</a></p>\n\n<p>export to csv and use <code>read_csv</code>\nor JSON and use <code>DataFrame.from_records</code></p>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p>A similar approach like Rafael Valero, waitingkuo and Deu Leung using <strong>pagination</strong>:</p>\n\n<pre><code>def read_mongo(\n       # db, \n       collection, query=None, \n       # host='localhost', port=27017, username=None, password=None,\n       chunksize = 100, page_num=1, no_id=True):\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Calculate number of documents to skip\n    skips = chunksize * (page_num - 1)\n\n    # Sorry, this is in spanish\n    # https://www.toptal.com/python/c%C3%B3digo-buggy-python-los-10-errores-m%C3%A1s-comunes-que-cometen-los-desarrolladores-python/es\n    if not query:\n        query = {}\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query).skip(skips).limit(chunksize)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df['_id']\n\n    return df\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>For dealing with out-of-core (not fitting into RAM) data efficiently (i.e. with parallel execution), you can try <a href=\"http://blaze.readthedocs.io/en/latest/index.html\" rel=\"noreferrer\">Python Blaze ecosystem</a>: Blaze / Dask / Odo.</p>\n\n<p>Blaze (and <a href=\"http://odo.pydata.org/en/latest/\" rel=\"noreferrer\">Odo</a>) has out-of-the-box functions to deal with MongoDB.</p>\n\n<p>A few useful articles to start off:</p>\n\n<ul>\n<li><a href=\"https://www.continuum.io/blog/developer/introducing-blaze-expressions\" rel=\"noreferrer\">Introducing Blaze Expessions</a> (with MongoDB query example)</li>\n<li><a href=\"http://danielfrg.com/blog/2015/07/21/reproduceit-reddit-word-count-dask/\" rel=\"noreferrer\">ReproduceIt: Reddit word count</a></li>\n<li><a href=\"http://dask.pydata.org/en/doc-test-build/array-blaze.html\" rel=\"noreferrer\">Difference between Dask Arrays and Blaze</a></li>\n</ul>\n\n<p>And an article which shows what amazing things are possible with Blaze stack: <a href=\"http://blaze.pydata.org/blog/2015/09/16/reddit-impala/\" rel=\"noreferrer\">Analyzing 1.7 Billion Reddit Comments with Blaze and Impala</a> (essentially, querying 975 Gb of Reddit comments in seconds).</p>\n\n<p>P.S. I'm not affiliated with any of these technologies.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "6"}, "answer_4": {"type": "literal", "value": "<p>Following this great answer by <a href=\"https://stackoverflow.com/a/16255680/7127519\">waitingkuo</a> I would like to add the possibility of doing that using chunksize in line with <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\">.read_sql()</a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"nofollow noreferrer\">.read_csv()</a>. I enlarge the answer from <a href=\"https://stackoverflow.com/a/39446008/7127519\">Deu Leung</a> by avoiding go one by one each 'record' of the 'iterator' / 'cursor'.\nI will borrow previous <em>read_mongo</em> function.</p>\n\n<pre><code>def read_mongo(db, \n           collection, query={}, \n           host='localhost', port=27017, \n           username=None, password=None,\n           chunksize = 100, no_id=True):\n\"\"\" Read from Mongo and Store into DataFrame \"\"\"\n\n\n# Connect to MongoDB\n#db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\nclient = MongoClient(host=host, port=port)\n# Make a query to the specific DB and Collection\ndb_aux = client[db]\n\n\n# Some variables to create the chunks\nskips_variable = range(0, db_aux[collection].find(query).count(), int(chunksize))\nif len(skips_variable)&lt;=1:\n    skips_variable = [0,len(skips_variable)]\n\n# Iteration to create the dataframe in chunks.\nfor i in range(1,len(skips_variable)):\n\n    # Expand the cursor and construct the DataFrame\n    #df_aux =pd.DataFrame(list(cursor_aux[skips_variable[i-1]:skips_variable[i]]))\n    df_aux =pd.DataFrame(list(db_aux[collection].find(query)[skips_variable[i-1]:skips_variable[i]]))\n\n    if no_id:\n        del df_aux['_id']\n\n    # Concatenate the chunks into a unique df\n    if 'df' not in locals():\n        df =  df_aux\n    else:\n        df = pd.concat([df, df_aux], ignore_index=True)\n\nreturn df\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "1"}, "answer_5": {"type": "literal", "value": "<p>Using </p>\n\n<pre><code>pandas.DataFrame(list(...))\n</code></pre>\n\n<p>will consume a lot of memory if the iterator/generator result is large</p>\n\n<p>better to generate small chunks and concat at the end</p>\n\n<pre><code>def iterator2dataframes(iterator, chunk_size: int):\n  \"\"\"Turn an iterator into multiple small pandas.DataFrame\n\n  This is a balance between memory and efficiency\n  \"\"\"\n  records = []\n  frames = []\n  for i, record in enumerate(iterator):\n    records.append(record)\n    if i % chunk_size == chunk_size - 1:\n      frames.append(pd.DataFrame(records))\n      records = []\n  if records:\n    frames.append(pd.DataFrame(records))\n  return pd.concat(frames)\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "3"}, "answer_6": {"type": "literal", "value": "<p><a href=\"https://bitbucket.org/djcbeach/monary/wiki/Home\"><code>Monary</code></a> does exactly that, and it's <em>super fast</em>. (<a href=\"http://djcinnovations.com/index.php/archives/164\">another link</a>)</p>\n\n<p>See <a href=\"http://alexgaudio.com/2012/07/07/monarymongopandas.html\">this cool post</a> which includes a quick tutorial and some timings.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "22"}, "answer_7": {"type": "literal", "value": "<p><code>pymongo</code> might give you a hand, followings are some codes I'm using:</p>\n\n<pre><code>import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    \"\"\" A util for making a connection to mongo \"\"\"\n\n    if username and password:\n        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df['_id']\n\n    return df\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "105"}, "answer_8": {"type": "literal", "value": "<p>You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.</p>\n\n<pre><code>import pymongo\nimport pandas as pd\nfrom pymongo import MongoClient\nclient = MongoClient()\ndb = client.database_name\ncollection = db.collection_name\ndata = pd.DataFrame(list(collection.find()))\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "30"}, "answer_9": {"type": "literal", "value": "<p>As per PEP, simple is better than complicated:  </p>\n\n<pre><code>import pandas as pd\ndf = pd.DataFrame.from_records(db.&lt;database_name&gt;.&lt;collection_name&gt;.find())\n</code></pre>\n\n<p>You can include conditions as you would working with regular mongoDB database or even use find_one() to get only one element from the database, etc. </p>\n\n<p>and voila!</p>\n"}, "answer_9_votes": {"type": "literal", "value": "11"}, "answer_10": {"type": "literal", "value": "<p>Another option I found very useful is:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pandas.io.json import json_normalize\n\ncursor = my_collection.find()\ndf = json_normalize(cursor)\n</code></pre>\n\n<p>this way you get the unfolding of nested mongodb documents for free.</p>\n"}, "answer_10_votes": {"type": "literal", "value": "4"}, "answer_11": {"type": "literal", "value": "<pre><code>import pandas as pd\nfrom odo import odo\n\ndata = odo('mongodb://localhost/db::collection', pd.DataFrame)\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "10"}, "content_wo_code": "<p>I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?</p>\n\n<p>I am new to pandas and numpy.</p>\n\n<p>EDIT:\nThe mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. </p>\n\n<p>Sample Data:</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p><a href=\"http://docs.mongodb.org/manual/reference/mongoexport\" rel=\"nofollow\">http://docs.mongodb.org/manual/reference/mongoexport</a></p>\n\n<p>export to csv and use  \nor JSON and use  </p>\n\n\n<p>A similar approach like Rafael Valero, waitingkuo and Deu Leung using <strong>pagination</strong>:</p>\n\n<pre> </pre>\n\n\n<p>For dealing with out-of-core (not fitting into RAM) data efficiently (i.e. with parallel execution), you can try <a href=\"http://blaze.readthedocs.io/en/latest/index.html\" rel=\"noreferrer\">Python Blaze ecosystem</a>: Blaze / Dask / Odo.</p>\n\n<p>Blaze (and <a href=\"http://odo.pydata.org/en/latest/\" rel=\"noreferrer\">Odo</a>) has out-of-the-box functions to deal with MongoDB.</p>\n\n<p>A few useful articles to start off:</p>\n\n<ul>\n<li><a href=\"https://www.continuum.io/blog/developer/introducing-blaze-expressions\" rel=\"noreferrer\">Introducing Blaze Expessions</a> (with MongoDB query example)</li>\n<li><a href=\"http://danielfrg.com/blog/2015/07/21/reproduceit-reddit-word-count-dask/\" rel=\"noreferrer\">ReproduceIt: Reddit word count</a></li>\n<li><a href=\"http://dask.pydata.org/en/doc-test-build/array-blaze.html\" rel=\"noreferrer\">Difference between Dask Arrays and Blaze</a></li>\n</ul>\n\n<p>And an article which shows what amazing things are possible with Blaze stack: <a href=\"http://blaze.pydata.org/blog/2015/09/16/reddit-impala/\" rel=\"noreferrer\">Analyzing 1.7 Billion Reddit Comments with Blaze and Impala</a> (essentially, querying 975 Gb of Reddit comments in seconds).</p>\n\n<p>P.S. I'm not affiliated with any of these technologies.</p>\n\n\n<p>Following this great answer by <a href=\"https://stackoverflow.com/a/16255680/7127519\">waitingkuo</a> I would like to add the possibility of doing that using chunksize in line with <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\">.read_sql()</a> and <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"nofollow noreferrer\">.read_csv()</a>. I enlarge the answer from <a href=\"https://stackoverflow.com/a/39446008/7127519\">Deu Leung</a> by avoiding go one by one each 'record' of the 'iterator' / 'cursor'.\nI will borrow previous <em>read_mongo</em> function.</p>\n\n<pre> </pre>\n\n\n<p>Using </p>\n\n<pre> </pre>\n\n<p>will consume a lot of memory if the iterator/generator result is large</p>\n\n<p>better to generate small chunks and concat at the end</p>\n\n<pre> </pre>\n\n\n<p><a href=\"https://bitbucket.org/djcbeach/monary/wiki/Home\"> </a> does exactly that, and it's <em>super fast</em>. (<a href=\"http://djcinnovations.com/index.php/archives/164\">another link</a>)</p>\n\n<p>See <a href=\"http://alexgaudio.com/2012/07/07/monarymongopandas.html\">this cool post</a> which includes a quick tutorial and some timings.</p>\n\n\n<p>  might give you a hand, followings are some codes I'm using:</p>\n\n<pre> </pre>\n\n\n<p>You can load your mongodb data to pandas DataFrame using this code. It works for me. Hopefully for you too.</p>\n\n<pre> </pre>\n\n\n<p>As per PEP, simple is better than complicated:  </p>\n\n<pre> </pre>\n\n<p>You can include conditions as you would working with regular mongoDB database or even use find_one() to get only one element from the database, etc. </p>\n\n<p>and voila!</p>\n\n\n<p>Another option I found very useful is:</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>this way you get the unfolding of nested mongodb documents for free.</p>\n\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/42468475"}, "title": {"type": "literal", "value": "Pandas.read_excel KeyError on reading set of xlsx files"}, "content": {"type": "literal", "value": "<p>I'm using Anaconda shell for data analitycs\nUploding to pandas bunch of excel files (25 files)\nOn This files <a href=\"https://www.dropbox.com/s/16ea1cw6k63i16p/Newdata.zip?dl=0\" rel=\"nofollow noreferrer\">https://www.dropbox.com/s/16ea1cw6k63i16p/Newdata.zip?dl=0</a>\nI get error. Cant find the reason why and how to fix it.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport os\n\nos.chdir(r\"C:\\Users\\Twentyouts\\Desktop\\Newdata\" )\npath = os.getcwd()\n\nfiles = os.listdir(path)\nfiles_xlsx = [f for f in files if f[-4:] == 'xlsx']\n\nfor f in files_xlsx:\n    print(f)\n    loading = pd.read_excel(f, heading = 0)\n    df = df.append(loading)\n2016-06-20\u20132016-06-26.xlsx\n2016-06-27\u20132016-07-03.xlsx\n2016-07-04\u20132016-07-10.xlsx\n2016-07-11\u20132016-07-17.xlsx\n2016-08-01\u20132016-08-07.xlsx\n2016-08-15\u20132016-08-21.xlsx\n</code></pre>\n\n<hr>\n\n<pre><code>KeyError                                  Traceback (most recent call last)\n&lt;ipython-input-23-5737d4d13b9f&gt; in &lt;module&gt;()\n      1 df = pd.DataFrame()\n----&gt; 2 pd.read_excel('2016-08-15\u20132016-08-21.xlsx')\n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py in read_excel(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, true_values, false_values, engine, squeeze, **kwds)\n    189 \n    190     if not isinstance(io, ExcelFile):\n--&gt; 191         io = ExcelFile(io, engine=engine)\n    192 \n    193     return io._parse_excel(\n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py in __init__(self, io, **kwds)\n    247             self.book = xlrd.open_workbook(file_contents=data)\n    248         elif isinstance(io, compat.string_types):\n--&gt; 249             self.book = xlrd.open_workbook(io)\n    250         else:\n    251             raise ValueError('Must explicitly set engine if not passing in'\n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py in open_workbook(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\n    420                 formatting_info=formatting_info,\n    421                 on_demand=on_demand,\n--&gt; 422                 ragged_rows=ragged_rows,\n    423                 )\n    424             return bk\n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\xlrd\\xlsx.py in open_workbook_2007_xml(zf, component_names, logfile, verbosity, use_mmap, formatting_info, on_demand, ragged_rows)\n    831         x12sheet = X12Sheet(sheet, logfile, verbosity)\n    832         heading = \"Sheet %r (sheetx=%d) from %r\" % (sheet.name, sheetx, fname)\n--&gt; 833         x12sheet.process_stream(zflo, heading)\n    834         del zflo\n    835 \n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\xlrd\\xlsx.py in own_process_stream(self, stream, heading)\n    546         for event, elem in ET.iterparse(stream):\n    547             if elem.tag == row_tag:\n--&gt; 548                 self_do_row(elem)\n    549                 elem.clear() # destroy all child elements (cells)\n    550             elif elem.tag == U_SSML12 + \"dimension\":\n\nC:\\Users\\Twentyouts\\Anaconda3\\lib\\site-packages\\xlrd\\xlsx.py in do_row(self, row_elem)\n    743                     else:\n    744                         bad_child_tag(child_tag)\n--&gt; 745                 value = error_code_from_text[tvalue]\n    746                 self.sheet.put_cell(rowx, colx, XL_CELL_ERROR, value, xf_index)\n    747             elif cell_type == \"inlineStr\":\n\nKeyError: None\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>It looks like some of your Excel files are malformed:</p>\n\n<pre><code>import os\nimport glob\nimport pandas as pd\n\nexcel_files_mask = r'D:\\temp\\.data\\42468475\\*.xlsx'\n\nfiles = glob.glob(excel_files_mask)\n\ndef merge_excel_files(files, **kwargs):\n    #return pd.concat([pd.read_excel(f, **kwargs) for f in files],\n    #                 ignore_index=True)\n    dfs = []\n    for f in files:\n        #print('processing: [{}]'.format(f))\n        try:\n            df = pd.read_excel(f, **kwargs)\n            dfs.append(df)\n            print('parsed: [{}], shape: {}'.format(f, df.shape))\n        except KeyError:\n            print(\"ERROR: file [{}] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\".format(f))\n    return pd.concat(dfs, ignore_index=True)\n\ndf = merge_excel_files(files, header=None, skiprows=1)\nprint(df.shape)\n</code></pre>\n\n<p>Yields:</p>\n\n<pre><code>parsed: [D:\\temp\\.data\\42468475\\2016-06-20\u20132016-06-26.xlsx], shape: (5912, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-06-27\u20132016-07-03.xlsx], shape: (5362, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-07-04\u20132016-07-10.xlsx], shape: (5387, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-07-11\u20132016-07-17.xlsx], shape: (5331, 28)\nparsed: [D:\\temp\\.data\\42468475\\2016-08-01\u20132016-08-07.xlsx], shape: (4965, 28)\nERROR: file [D:\\temp\\.data\\42468475\\2016-08-15\u20132016-08-21.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\2016-08-22\u20132016-08-28.xlsx], shape: (5179, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-08-29\u20132016-09-04.xlsx], shape: (5855, 27)\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-05\u20132016-09-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-12\u20132016-09-18.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-19\u20132016-09-25.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\2016-09-26\u20132016-10-02.xlsx], shape: (7018, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-09.xlsx], shape: (23874, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-10-03\u20132016-10-09.xlsx], shape: (6587, 27)\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-10\u20132016-10-12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-10\u20132016-10-13.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-17\u20132016-10-20.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-17\u20132016-10-23.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-24\u20132016-10-27.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-24\u20132016-10-30.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-31\u20132016-11-06.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-07\u20132016-11-13.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-14\u20132016-11-20.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-21.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-21\u20132016-11-23.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-28\u20132016-12-04.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-05\u20132016-12-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-12\u20132016-12-18.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-16\u20132016-12-22.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-26\u20132016-12-31.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-01\u20132017-01-08.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-09\u20132017-01-15.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-23\u20132017-01-29.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-30\u20132017-02-05.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-02-13\u20132017-02-12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-02-13\u20132017-02-19.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\test.xlsx], shape: (5315, 27)\nERROR: file [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 26-29.12.xlsx], shape: (4539, 27)\n(85324, 28)\n</code></pre>\n\n\n<p>Indeed as @MaxU points out, Excel files are malformed but interestingly does resolve when properly saved as an .xlsx file. Possibly, the invalid files were attempted to be upgraded from previous .xls version by simply changing the extension to .xlsx. These two file formats are not simple text files that can change extension without hazard but very different binary formats.</p>\n\n<p>Consider running a COM interface using <code>wn32com</code> module to properly save the malformed files to actual OpenXML workbooks using Excel's <a href=\"https://msdn.microsoft.com/en-us/library/office/ff841185.aspx\" rel=\"nofollow noreferrer\">Workbook.SaveAs</a> method. Note: this solution is only compliant in Python for Windows with installed MS Excel.</p>\n\n<pre><code>import pandas as pd\nimport glob\nimport win32com.client as win32\n\nxlsxfiles = glob.glob(\"C:\\\\Path\\\\To\\\\Workbooks\\\\*.xlsx\")\n\ndef save_xlsx(srcfile):\n    try:\n        newfile = srcfile.replace('.xlsx', '_new.xlsx')\n        print('Malformed file saved as {}'.format(newfile))\n        xlApp = win32.gencache.EnsureDispatch('Excel.Application')\n        wb = xlApp.Workbooks.Open(srcfile)\n        wb.SaveAs(newfile, 51)                 \n\n    except Exception as e:\n        print(e)            \n    finally:\n        wb.Close(True); wb = None\n        xlApp.Quit; xlApp = None    \n    return newfile\n\ndef xl_read():    \n    dfs = []\n    for f in xlsxfiles:        \n        try:\n            df = pd.read_excel(f)\n        except Exception as e:            \n            df = pd.read_excel(save_xlsx(f))\n\n        print('File: {}, Shape: {}'.format(f, df.shape))\n        dfs.append(df)            \n    return pd.concat(dfs)\n\nprint('Final dataframe shape: {}'.format(xl_read().shape))  \n</code></pre>\n\n<p><strong>Output</strong> <em>(final dataframe of 330,257 rows and 30 columns)</em></p>\n\n<pre><code>File: C:\\Path\\To\\Workbooks\\2016-06-20\u20132016-06-26.xlsx, Shape: (5912, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-06-27\u20132016-07-03.xlsx, Shape: (5362, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-07-04\u20132016-07-10.xlsx, Shape: (5387, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-07-11\u20132016-07-17.xlsx, Shape: (5331, 28)\nFile: C:\\Path\\To\\Workbooks\\2016-08-01\u20132016-08-07.xlsx, Shape: (4965, 28)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-08-15\u20132016-08-21_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-08-15\u20132016-08-21.xlsx, Shape: (5315, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-08-22\u20132016-08-28.xlsx, Shape: (5179, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-08-29\u20132016-09-04.xlsx, Shape: (5855, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-05\u20132016-09-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-05\u20132016-09-11.xlsx, Shape: (5838, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-12\u20132016-09-18_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-12\u20132016-09-18.xlsx, Shape: (5729, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-19\u20132016-09-25_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-19\u20132016-09-25.xlsx, Shape: (6401, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-09-26\u20132016-10-02.xlsx, Shape: (7018, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-09.xlsx, Shape: (23874, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-10-03\u20132016-10-09.xlsx, Shape: (6587, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-12.xlsx, Shape: (2883, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-13_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-13.xlsx, Shape: (4174, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-20_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-20.xlsx, Shape: (4560, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-23_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-23.xlsx, Shape: (7111, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-27_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-27.xlsx, Shape: (4921, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-30_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-30.xlsx, Shape: (8005, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-31\u20132016-11-06_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-31\u20132016-11-06.xlsx, Shape: (7029, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10.xlsx, Shape: (28098, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-07\u20132016-11-13_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-07\u20132016-11-13.xlsx, Shape: (7076, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-14\u20132016-11-20_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-14\u20132016-11-20.xlsx, Shape: (7758, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-21_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-21.xlsx, Shape: (1689, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-21\u20132016-11-23_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-21\u20132016-11-23.xlsx, Shape: (4711, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-28\u20132016-12-04_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-28\u20132016-12-04.xlsx, Shape: (9286, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11.xlsx, Shape: (30505, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-05\u20132016-12-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-05\u20132016-12-11.xlsx, Shape: (8802, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-12\u20132016-12-18_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-12\u20132016-12-18.xlsx, Shape: (8333, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-16\u20132016-12-22_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-16\u20132016-12-22.xlsx, Shape: (8592, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-26\u20132016-12-31_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-26\u20132016-12-31.xlsx, Shape: (5362, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-01\u20132017-01-08_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-01\u20132017-01-08.xlsx, Shape: (4322, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-09\u20132017-01-15_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-09\u20132017-01-15.xlsx, Shape: (7608, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-23\u20132017-01-29_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-23\u20132017-01-29.xlsx, Shape: (8903, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-30\u20132017-02-05_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-30\u20132017-02-05.xlsx, Shape: (9173, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-12.xlsx, Shape: (9144, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-19_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-19.xlsx, Shape: (9911, 27)\nFile: C:\\Path\\To\\Workbooks\\test.xlsx, Shape: (5315, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12.xlsx, Shape: (4818, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27.xlsx, Shape: (8876, 27)\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 26-29.12.xlsx, Shape: (4539, 27)\nFinal dataframe shape: (330257, 30)\n</code></pre>\n\n<hr>\n\n<p>Consider even a database engine approach using Windows' ACE Engine via <code>pyodbc</code> to query corresponding workbooks with pandas <a href=\"http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\"><code>read_sql</code></a> since each share same sheet name, <em>TDSheet</em>.</p>\n\n<pre><code>#...same as above\nimport pyodbc\n\ndef sql_read():    \n    dfs = [] \n    for f in xlsxfiles:                \n        try:\n            conn = pyodbc.connect('Driver={Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)};'+\\\n                      'DBQ=C:\\\\Path\\\\To\\\\Workbooks\\\\{};'.format(f), autocommit=True)\n            df =  pd.read_sql('SELECT * FROM [TDSheet$];', conn)\n\n        except Exception as e:\n            conn.close()\n            conn = pyodbc.connect('Driver={Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)};'+\\\n                      'DBQ=C:\\\\Path\\\\To\\\\Workbooks\\\\{};'.format(save_xlsx(f)), autocommit=True)\n            df = pd.read_excel('SELECT * FROM [TDSheet$];', conn)\n            conn.close()\n\n        print('File: {}, Shape: {}'.format(f, df.shape))\n        dfs.append(df)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>It looks like some of your Excel files are malformed:</p>\n\n<pre><code>import os\nimport glob\nimport pandas as pd\n\nexcel_files_mask = r'D:\\temp\\.data\\42468475\\*.xlsx'\n\nfiles = glob.glob(excel_files_mask)\n\ndef merge_excel_files(files, **kwargs):\n    #return pd.concat([pd.read_excel(f, **kwargs) for f in files],\n    #                 ignore_index=True)\n    dfs = []\n    for f in files:\n        #print('processing: [{}]'.format(f))\n        try:\n            df = pd.read_excel(f, **kwargs)\n            dfs.append(df)\n            print('parsed: [{}], shape: {}'.format(f, df.shape))\n        except KeyError:\n            print(\"ERROR: file [{}] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\".format(f))\n    return pd.concat(dfs, ignore_index=True)\n\ndf = merge_excel_files(files, header=None, skiprows=1)\nprint(df.shape)\n</code></pre>\n\n<p>Yields:</p>\n\n<pre><code>parsed: [D:\\temp\\.data\\42468475\\2016-06-20\u20132016-06-26.xlsx], shape: (5912, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-06-27\u20132016-07-03.xlsx], shape: (5362, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-07-04\u20132016-07-10.xlsx], shape: (5387, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-07-11\u20132016-07-17.xlsx], shape: (5331, 28)\nparsed: [D:\\temp\\.data\\42468475\\2016-08-01\u20132016-08-07.xlsx], shape: (4965, 28)\nERROR: file [D:\\temp\\.data\\42468475\\2016-08-15\u20132016-08-21.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\2016-08-22\u20132016-08-28.xlsx], shape: (5179, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-08-29\u20132016-09-04.xlsx], shape: (5855, 27)\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-05\u20132016-09-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-12\u20132016-09-18.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-09-19\u20132016-09-25.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\2016-09-26\u20132016-10-02.xlsx], shape: (7018, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-09.xlsx], shape: (23874, 27)\nparsed: [D:\\temp\\.data\\42468475\\2016-10-03\u20132016-10-09.xlsx], shape: (6587, 27)\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-10\u20132016-10-12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-10\u20132016-10-13.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-17\u20132016-10-20.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-17\u20132016-10-23.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-24\u20132016-10-27.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-24\u20132016-10-30.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10-31\u20132016-11-06.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-10.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-07\u20132016-11-13.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-14\u20132016-11-20.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-21.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-21\u20132016-11-23.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11-28\u20132016-12-04.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-05\u20132016-12-11.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-12\u20132016-12-18.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-16\u20132016-12-22.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2016-12-26\u20132016-12-31.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-01\u20132017-01-08.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-09\u20132017-01-15.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-23\u20132017-01-29.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-01-30\u20132017-02-05.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-02-13\u20132017-02-12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\2017-02-13\u20132017-02-19.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\test.xlsx], shape: (5315, 27)\nERROR: file [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nERROR: file [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27.xlsx] couldn't be parsed! Open it in Excel and save it as (.xlsx) file ...\nparsed: [D:\\temp\\.data\\42468475\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 26-29.12.xlsx], shape: (4539, 27)\n(85324, 28)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "answer_2": {"type": "literal", "value": "<p>Indeed as @MaxU points out, Excel files are malformed but interestingly does resolve when properly saved as an .xlsx file. Possibly, the invalid files were attempted to be upgraded from previous .xls version by simply changing the extension to .xlsx. These two file formats are not simple text files that can change extension without hazard but very different binary formats.</p>\n\n<p>Consider running a COM interface using <code>wn32com</code> module to properly save the malformed files to actual OpenXML workbooks using Excel's <a href=\"https://msdn.microsoft.com/en-us/library/office/ff841185.aspx\" rel=\"nofollow noreferrer\">Workbook.SaveAs</a> method. Note: this solution is only compliant in Python for Windows with installed MS Excel.</p>\n\n<pre><code>import pandas as pd\nimport glob\nimport win32com.client as win32\n\nxlsxfiles = glob.glob(\"C:\\\\Path\\\\To\\\\Workbooks\\\\*.xlsx\")\n\ndef save_xlsx(srcfile):\n    try:\n        newfile = srcfile.replace('.xlsx', '_new.xlsx')\n        print('Malformed file saved as {}'.format(newfile))\n        xlApp = win32.gencache.EnsureDispatch('Excel.Application')\n        wb = xlApp.Workbooks.Open(srcfile)\n        wb.SaveAs(newfile, 51)                 \n\n    except Exception as e:\n        print(e)            \n    finally:\n        wb.Close(True); wb = None\n        xlApp.Quit; xlApp = None    \n    return newfile\n\ndef xl_read():    \n    dfs = []\n    for f in xlsxfiles:        \n        try:\n            df = pd.read_excel(f)\n        except Exception as e:            \n            df = pd.read_excel(save_xlsx(f))\n\n        print('File: {}, Shape: {}'.format(f, df.shape))\n        dfs.append(df)            \n    return pd.concat(dfs)\n\nprint('Final dataframe shape: {}'.format(xl_read().shape))  \n</code></pre>\n\n<p><strong>Output</strong> <em>(final dataframe of 330,257 rows and 30 columns)</em></p>\n\n<pre><code>File: C:\\Path\\To\\Workbooks\\2016-06-20\u20132016-06-26.xlsx, Shape: (5912, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-06-27\u20132016-07-03.xlsx, Shape: (5362, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-07-04\u20132016-07-10.xlsx, Shape: (5387, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-07-11\u20132016-07-17.xlsx, Shape: (5331, 28)\nFile: C:\\Path\\To\\Workbooks\\2016-08-01\u20132016-08-07.xlsx, Shape: (4965, 28)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-08-15\u20132016-08-21_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-08-15\u20132016-08-21.xlsx, Shape: (5315, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-08-22\u20132016-08-28.xlsx, Shape: (5179, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-08-29\u20132016-09-04.xlsx, Shape: (5855, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-05\u20132016-09-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-05\u20132016-09-11.xlsx, Shape: (5838, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-12\u20132016-09-18_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-12\u20132016-09-18.xlsx, Shape: (5729, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-09-19\u20132016-09-25_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-09-19\u20132016-09-25.xlsx, Shape: (6401, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-09-26\u20132016-10-02.xlsx, Shape: (7018, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-09.xlsx, Shape: (23874, 27)\nFile: C:\\Path\\To\\Workbooks\\2016-10-03\u20132016-10-09.xlsx, Shape: (6587, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-12.xlsx, Shape: (2883, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-13_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-10\u20132016-10-13.xlsx, Shape: (4174, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-20_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-20.xlsx, Shape: (4560, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-23_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-17\u20132016-10-23.xlsx, Shape: (7111, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-27_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-27.xlsx, Shape: (4921, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-30_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-24\u20132016-10-30.xlsx, Shape: (8005, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10-31\u20132016-11-06_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10-31\u20132016-11-06.xlsx, Shape: (7029, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-10_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-10.xlsx, Shape: (28098, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-07\u20132016-11-13_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-07\u20132016-11-13.xlsx, Shape: (7076, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-14\u20132016-11-20_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-14\u20132016-11-20.xlsx, Shape: (7758, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-21_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-21.xlsx, Shape: (1689, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-21\u20132016-11-23_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-21\u20132016-11-23.xlsx, Shape: (4711, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11-28\u20132016-12-04_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11-28\u20132016-12-04.xlsx, Shape: (9286, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-11.xlsx, Shape: (30505, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-05\u20132016-12-11_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-05\u20132016-12-11.xlsx, Shape: (8802, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-12\u20132016-12-18_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-12\u20132016-12-18.xlsx, Shape: (8333, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-16\u20132016-12-22_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-16\u20132016-12-22.xlsx, Shape: (8592, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2016-12-26\u20132016-12-31_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2016-12-26\u20132016-12-31.xlsx, Shape: (5362, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-01\u20132017-01-08_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-01\u20132017-01-08.xlsx, Shape: (4322, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-09\u20132017-01-15_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-09\u20132017-01-15.xlsx, Shape: (7608, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-23\u20132017-01-29_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-23\u20132017-01-29.xlsx, Shape: (8903, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-01-30\u20132017-02-05_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-01-30\u20132017-02-05.xlsx, Shape: (9173, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-12.xlsx, Shape: (9144, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-19_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\2017-02-13\u20132017-02-19.xlsx, Shape: (9911, 27)\nFile: C:\\Path\\To\\Workbooks\\test.xlsx, Shape: (5315, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 12-15.12.xlsx, Shape: (4818, 27)\nMalformed file saved as C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27_new.xlsx\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 21-27.xlsx, Shape: (8876, 27)\nFile: C:\\Path\\To\\Workbooks\\\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 26-29.12.xlsx, Shape: (4539, 27)\nFinal dataframe shape: (330257, 30)\n</code></pre>\n\n<hr>\n\n<p>Consider even a database engine approach using Windows' ACE Engine via <code>pyodbc</code> to query corresponding workbooks with pandas <a href=\"http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\"><code>read_sql</code></a> since each share same sheet name, <em>TDSheet</em>.</p>\n\n<pre><code>#...same as above\nimport pyodbc\n\ndef sql_read():    \n    dfs = [] \n    for f in xlsxfiles:                \n        try:\n            conn = pyodbc.connect('Driver={Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)};'+\\\n                      'DBQ=C:\\\\Path\\\\To\\\\Workbooks\\\\{};'.format(f), autocommit=True)\n            df =  pd.read_sql('SELECT * FROM [TDSheet$];', conn)\n\n        except Exception as e:\n            conn.close()\n            conn = pyodbc.connect('Driver={Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)};'+\\\n                      'DBQ=C:\\\\Path\\\\To\\\\Workbooks\\\\{};'.format(save_xlsx(f)), autocommit=True)\n            df = pd.read_excel('SELECT * FROM [TDSheet$];', conn)\n            conn.close()\n\n        print('File: {}, Shape: {}'.format(f, df.shape))\n        dfs.append(df)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I'm using Anaconda shell for data analitycs\nUploding to pandas bunch of excel files (25 files)\nOn This files <a href=\"https://www.dropbox.com/s/16ea1cw6k63i16p/Newdata.zip?dl=0\" rel=\"nofollow noreferrer\">https://www.dropbox.com/s/16ea1cw6k63i16p/Newdata.zip?dl=0</a>\nI get error. Cant find the reason why and how to fix it.</p>\n\n<pre> </pre>\n\n<hr>\n\n<pre> </pre>\n", "answer_wo_code": "<p>It looks like some of your Excel files are malformed:</p>\n\n<pre> </pre>\n\n<p>Yields:</p>\n\n<pre> </pre>\n\n\n<p>Indeed as @MaxU points out, Excel files are malformed but interestingly does resolve when properly saved as an .xlsx file. Possibly, the invalid files were attempted to be upgraded from previous .xls version by simply changing the extension to .xlsx. These two file formats are not simple text files that can change extension without hazard but very different binary formats.</p>\n\n<p>Consider running a COM interface using   module to properly save the malformed files to actual OpenXML workbooks using Excel's <a href=\"https://msdn.microsoft.com/en-us/library/office/ff841185.aspx\" rel=\"nofollow noreferrer\">Workbook.SaveAs</a> method. Note: this solution is only compliant in Python for Windows with installed MS Excel.</p>\n\n<pre> </pre>\n\n<p><strong>Output</strong> <em>(final dataframe of 330,257 rows and 30 columns)</em></p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Consider even a database engine approach using Windows' ACE Engine via   to query corresponding workbooks with pandas <a href=\"http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.read_sql.html\" rel=\"nofollow noreferrer\"> </a> since each share same sheet name, <em>TDSheet</em>.</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/frac"}, "class_func_label": {"type": "literal", "value": "frac"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "Represents the fractional part of x\n\n    For real numbers it is defined [1]_ as\n\n    .. math::\n        x - \\left\\lfloor{x}\\right\\rfloor\n\n    Examples\n    ========\n\n    >>> from sympy import Symbol, frac, Rational, floor, ceiling, I\n    >>> frac(Rational(4, 3))\n    1/3\n    >>> frac(-Rational(4, 3))\n    2/3\n\n    returns zero for integer arguments\n\n    >>> n = Symbol('n', integer=True)\n    >>> frac(n)\n    0\n\n    rewrite as floor\n\n    >>> x = Symbol('x')\n    >>> frac(x).rewrite(floor)\n    x - floor(x)\n\n    for complex arguments\n\n    >>> r = Symbol('r', real=True)\n    >>> t = Symbol('t', real=True)\n    >>> frac(t + I*r)\n    I*frac(r) + frac(t)\n\n    See Also\n    ========\n\n    sympy.functions.elementary.integers.floor\n    sympy.functions.elementary.integers.ceiling\n\n    References\n    ===========\n\n    .. [1] https://en.wikipedia.org/wiki/Fractional_part\n    .. [2] http://mathworld.wolfram.com/FractionalPart.html\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/34468947"}, "title": {"type": "literal", "value": "Using Excel like solver in Python or SQL"}, "content": {"type": "literal", "value": "<p>Here is a simple calculation that I do in Excel. I will like to know if it can be done python or any other language. </p>\n\n<pre><code>Loan amount 7692\nPeriod : 12 months\nRate of interest 18 Per Annum\nThe formula in the B2 cell is =A1*18/100/12\nThe formula in the A2 cells is =A1+B2-C2\n</code></pre>\n\n<p>The column C is tentative amount the borrower may need to repay each month. All other cells next to C2 simply points to the first installment of 200. After using the solver as shown in the following image, I get the correct installment of 705.20 in the C column. </p>\n\n<p><a href=\"https://i.stack.imgur.com/elmP8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/elmP8.png\" alt=\"excel goal seak\"></a></p>\n\n<p>I will like to know if this calculation can be done using any scripting language like python (or SQL)</p>\n\n<p>Here is how the final version looks like...</p>\n\n<p><a href=\"https://i.stack.imgur.com/M0t9I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/M0t9I.png\" alt=\"enter image description here\"></a></p>\n\n<p>I tried something like this, but it does not exit the loop and prints all combinations.</p>\n\n<pre><code>loan_amount= 7692\ninterest = 18\nmonths =12\n\nfor rg in range(700, 710):\n    for i in range(months):\n        x = loan_amount * interest / 100 / 12\n        y = loan_amount + x - rg\n        if x &lt; 0: \n            print rg, i\n            exit\n        else:\n            loan_amount = y\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>As the title/tag also mentioned SQL, I will post an SQL solution:</p>\n\n<pre><code>create table loan (\n  amount decimal(10,2),\n  repay_months int,\n  yearly_interest_rate decimal(4, 4)\n);\n\ninsert into loan values (7692, 12, 0.18);\n\nselect amount * yearly_interest_rate/12 /\n           (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n           as monthly_payment\nfrom   loan;\n</code></pre>\n\n<p>Result: </p>\n\n<pre><code>monthly_payment\n-----------------\n705.2025054347173\n</code></pre>\n\n<p><a href=\"http://sqlfiddle.com/#!9/56254f/5\" rel=\"nofollow\">SQL Fiddle</a>.    </p>\n\n<p>If you want to get the whole amortization schedule, then an idea would be to first create a table with sequential month numbers (1, 2, ...), enough to cover for the longest loan pay-off duration you would have data for:</p>\n\n<pre><code>create table months (month int);\n\ninsert into months -- one year of months\n  values (1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12);\n-- now some record multiplication to avoid long literal lists:    \ninsert into months -- multiply to cover 2 years\n  select month + 12 from months;\ninsert into months -- multiply to cover 4 years\n  select month + 24 from months;\ninsert into months -- multiply to cover 8 years\n  select month + 48 from months;\ninsert into months -- multiply to cover 16 years\n  select month + 96 from months;\ninsert into months -- multiply to cover 32 years\n  select month + 192 from months;\n-- OK, we have now months from 1 to 384 (= 32 years)\n</code></pre>\n\n<p>Then use the following query, which has the above mentioned query as sub-select:</p>\n\n<pre><code>select month,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-repay_months)) \n                       / monthly_interest_rate\n                       as loan_balance,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-1-repay_months)) \n                       as interest,\n       monthly_payment\nfrom   months,\n       (\n        select amount,\n               repay_months,\n               yearly_interest_rate,\n               yearly_interest_rate/12 as monthly_interest_rate, \n               amount * yearly_interest_rate/12 /\n                   (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n                   as monthly_payment\n        from   loan\n       ) as loanX\nwhere  month &lt;= repay_months\norder by 1;\n</code></pre>\n\n<p>This produces the following output:</p>\n\n<pre><code>+-------+--------------------+---------------------+-------------------+\n| month | loan_balance       | interest            | monthly_payment   |\n+-------+--------------------+---------------------+-------------------+\n| 1     | 7102.177494565289  | 115.38              | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 2     | 6503.507651549055  | 106.53266241847933  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 3     | 5895.8577608875785 |  97.55261477323582  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 4     | 5279.093121866177  |  88.43786641331367  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 5     | 4653.077013259458  |  79.18639682799265  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 6     | 4017.6706630236345 |  69.79615519889187  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 7     | 3372.7332175342744 |  60.265059945354515 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 8     | 2718.12171036258   |  50.590998263014114 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 9     | 2053.6910305833035 |  40.7718256554387   | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 10    | 1379.2938906073389 |  30.805365458749552 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 11    |  694.7807935317352 |  20.689408359110082 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 12    |    0               |  10.421711902976027 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n</code></pre>\n\n<p>Here is the <a href=\"http://sqlfiddle.com/#!9/ddaa67/7\" rel=\"nofollow\">SQL fiddle</a>.</p>\n\n<p>The formula used is provided and derived in <a href=\"https://en.wikipedia.org/wiki/Amortization_calculator#The_formula\" rel=\"nofollow\">this Wikipedia article</a>.</p>\n\n\n<p>As the title/tag also mentioned SQL, I will post an SQL solution:</p>\n\n<pre><code>create table loan (\n  amount decimal(10,2),\n  repay_months int,\n  yearly_interest_rate decimal(4, 4)\n);\n\ninsert into loan values (7692, 12, 0.18);\n\nselect amount * yearly_interest_rate/12 /\n           (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n           as monthly_payment\nfrom   loan;\n</code></pre>\n\n<p>Result: </p>\n\n<pre><code>monthly_payment\n-----------------\n705.2025054347173\n</code></pre>\n\n<p><a href=\"http://sqlfiddle.com/#!9/56254f/5\" rel=\"nofollow\">SQL Fiddle</a>.    </p>\n\n<p>If you want to get the whole amortization schedule, then an idea would be to first create a table with sequential month numbers (1, 2, ...), enough to cover for the longest loan pay-off duration you would have data for:</p>\n\n<pre><code>create table months (month int);\n\ninsert into months -- one year of months\n  values (1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12);\n-- now some record multiplication to avoid long literal lists:    \ninsert into months -- multiply to cover 2 years\n  select month + 12 from months;\ninsert into months -- multiply to cover 4 years\n  select month + 24 from months;\ninsert into months -- multiply to cover 8 years\n  select month + 48 from months;\ninsert into months -- multiply to cover 16 years\n  select month + 96 from months;\ninsert into months -- multiply to cover 32 years\n  select month + 192 from months;\n-- OK, we have now months from 1 to 384 (= 32 years)\n</code></pre>\n\n<p>Then use the following query, which has the above mentioned query as sub-select:</p>\n\n<pre><code>select month,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-repay_months)) \n                       / monthly_interest_rate\n                       as loan_balance,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-1-repay_months)) \n                       as interest,\n       monthly_payment\nfrom   months,\n       (\n        select amount,\n               repay_months,\n               yearly_interest_rate,\n               yearly_interest_rate/12 as monthly_interest_rate, \n               amount * yearly_interest_rate/12 /\n                   (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n                   as monthly_payment\n        from   loan\n       ) as loanX\nwhere  month &lt;= repay_months\norder by 1;\n</code></pre>\n\n<p>This produces the following output:</p>\n\n<pre><code>+-------+--------------------+---------------------+-------------------+\n| month | loan_balance       | interest            | monthly_payment   |\n+-------+--------------------+---------------------+-------------------+\n| 1     | 7102.177494565289  | 115.38              | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 2     | 6503.507651549055  | 106.53266241847933  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 3     | 5895.8577608875785 |  97.55261477323582  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 4     | 5279.093121866177  |  88.43786641331367  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 5     | 4653.077013259458  |  79.18639682799265  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 6     | 4017.6706630236345 |  69.79615519889187  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 7     | 3372.7332175342744 |  60.265059945354515 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 8     | 2718.12171036258   |  50.590998263014114 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 9     | 2053.6910305833035 |  40.7718256554387   | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 10    | 1379.2938906073389 |  30.805365458749552 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 11    |  694.7807935317352 |  20.689408359110082 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 12    |    0               |  10.421711902976027 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n</code></pre>\n\n<p>Here is the <a href=\"http://sqlfiddle.com/#!9/ddaa67/7\" rel=\"nofollow\">SQL fiddle</a>.</p>\n\n<p>The formula used is provided and derived in <a href=\"https://en.wikipedia.org/wiki/Amortization_calculator#The_formula\" rel=\"nofollow\">this Wikipedia article</a>.</p>\n\n\n<p>Well, you can solve it using numerical method (as Excel does), you can solve it with brute force by checking every amount with some step within some range, or you can solve it analytically on a piece of paper.</p>\n\n<p>Using the following notation</p>\n\n<pre><code>L - initial loan amount = 7692\nR - monthly interest rate = 1 + 0.18/12\nm - number of months to repay the loan = 12\nP - monthly payment to pay the loan in full after m months = unknown\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/hc5r1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/hc5r1.png\" alt=\"L_{n}\"></a> is loan amount after the <code>n</code>-th month. <a href=\"https://i.stack.imgur.com/Fjn3W.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Fjn3W.png\" alt=\"L_{0}\"></a> is the initial loan amount (7692). <a href=\"https://i.stack.imgur.com/UQGZ9.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UQGZ9.png\" alt=\"L_{m}\"></a> is the loan amount after <code>m</code> months (0).</p>\n\n<p>The main relation between <code>n</code>-th and <code>(n-1)</code>-th month is:</p>\n\n<p><a href=\"https://i.stack.imgur.com/Tx4kh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Tx4kh.png\" alt=\"L_{n} = L_{n-1} * R - P\"></a></p>\n\n<p>So, analytical formula turns out to be:</p>\n\n<p><a href=\"https://i.stack.imgur.com/abGdN.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/abGdN.png\" alt=\"P = L * \\frac{R^{m}}{\\sum_{k=0}^{m-1}R^{k}} = L * R^{m} * \\frac{R-1}{R^{m}-1}\"></a></p>\n\n<p>Now it should be fairly straight-forward to calculate it in any programming language.</p>\n\n<p>For the given initial parameters </p>\n\n<p><a href=\"https://i.stack.imgur.com/9BXBX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9BXBX.png\" alt=\"R = 1 + \\frac{0.18}{12} = 1.015\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/nATZE.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/nATZE.png\" alt=\"P = 7692 * 1.015^{12} * \\frac{1.015-1}{1.015^{12}-1}\\approx 705.2025054\"></a></p>\n\n<hr>\n\n<p>By the way, if you are modelling how the real bank works, it may be tricky to calculate it correctly to the last cent.</p>\n\n<p>The answer that you get from precise analytical formulas like the one above is only approximate. </p>\n\n<p>In practice all monthly amounts (both payment and interest) are usually rounded to the cent. With each month there will be some rounding error, which would accumulate and grow. </p>\n\n<p>Apart from these rounding errors different months have different number of days and even though payments are the same for each month, the interest is usually calculated for each day of the month, so it varies from month to month. Then there are leap years with extra day, which also affects the monthly interest.</p>\n\n\n<p>Well, you can solve it using numerical method (as Excel does), you can solve it with brute force by checking every amount with some step within some range, or you can solve it analytically on a piece of paper.</p>\n\n<p>Using the following notation</p>\n\n<pre><code>L - initial loan amount = 7692\nR - monthly interest rate = 1 + 0.18/12\nm - number of months to repay the loan = 12\nP - monthly payment to pay the loan in full after m months = unknown\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/hc5r1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/hc5r1.png\" alt=\"L_{n}\"></a> is loan amount after the <code>n</code>-th month. <a href=\"https://i.stack.imgur.com/Fjn3W.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Fjn3W.png\" alt=\"L_{0}\"></a> is the initial loan amount (7692). <a href=\"https://i.stack.imgur.com/UQGZ9.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UQGZ9.png\" alt=\"L_{m}\"></a> is the loan amount after <code>m</code> months (0).</p>\n\n<p>The main relation between <code>n</code>-th and <code>(n-1)</code>-th month is:</p>\n\n<p><a href=\"https://i.stack.imgur.com/Tx4kh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Tx4kh.png\" alt=\"L_{n} = L_{n-1} * R - P\"></a></p>\n\n<p>So, analytical formula turns out to be:</p>\n\n<p><a href=\"https://i.stack.imgur.com/abGdN.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/abGdN.png\" alt=\"P = L * \\frac{R^{m}}{\\sum_{k=0}^{m-1}R^{k}} = L * R^{m} * \\frac{R-1}{R^{m}-1}\"></a></p>\n\n<p>Now it should be fairly straight-forward to calculate it in any programming language.</p>\n\n<p>For the given initial parameters </p>\n\n<p><a href=\"https://i.stack.imgur.com/9BXBX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9BXBX.png\" alt=\"R = 1 + \\frac{0.18}{12} = 1.015\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/nATZE.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/nATZE.png\" alt=\"P = 7692 * 1.015^{12} * \\frac{1.015-1}{1.015^{12}-1}\\approx 705.2025054\"></a></p>\n\n<hr>\n\n<p>By the way, if you are modelling how the real bank works, it may be tricky to calculate it correctly to the last cent.</p>\n\n<p>The answer that you get from precise analytical formulas like the one above is only approximate. </p>\n\n<p>In practice all monthly amounts (both payment and interest) are usually rounded to the cent. With each month there will be some rounding error, which would accumulate and grow. </p>\n\n<p>Apart from these rounding errors different months have different number of days and even though payments are the same for each month, the interest is usually calculated for each day of the month, so it varies from month to month. Then there are leap years with extra day, which also affects the monthly interest.</p>\n\n\n<p>A simple brute force approach in Python with the option to determine the level of accuracy you want. </p>\n\n<pre><code>\"\"\"\n    Calculate required monthly repayment for a given:\n        - loan amount, and\n        - annual interest rate, and\n        - period of repayments in months\n\n    You can nominate the accuracy required by adjusting the value of\n        ACCURACY_AS_PARTS_OF_CENT. For example:\n        - .01 = accurate to a dollar\n        - .1  = accurate to 10 cents\n        - 1   = accurate to cent\n        - 100 = accurate to 100th of a cent\n\"\"\"\n\n# Set constants.\nLOAN_AMOUNT = 7692\nANNUAL_INTEREST_PERCENT = 18\nREPAY_MONTHS = 12\nACCURACY_AS_PARTS_OF_CENT = 1\n\nloan_amount = int(LOAN_AMOUNT * 100 * ACCURACY_AS_PARTS_OF_CENT)\nmonthly_interest = float(ANNUAL_INTEREST_PERCENT / 100 / 12)\nrepay_guess_min = int((LOAN_AMOUNT / REPAY_MONTHS) - 1)\nresult_found = False\nrepayment_required = 0\n\nfor repay_guess in range(repay_guess_min, loan_amount):\n    if result_found:\n        break\n    loan_balance = loan_amount\n    for _ in range(REPAY_MONTHS):\n        interest_to_add = loan_balance * monthly_interest\n        loan_balance = loan_balance + interest_to_add - repay_guess\n        if loan_balance &lt;= 0:\n            repayment_required = repay_guess / 100 / ACCURACY_AS_PARTS_OF_CENT\n            result_found = True\n            break\n\nprint('Required monthly repayment = $' + str(repayment_required))\n</code></pre>\n\n\n<p>A simple brute force approach in Python with the option to determine the level of accuracy you want. </p>\n\n<pre><code>\"\"\"\n    Calculate required monthly repayment for a given:\n        - loan amount, and\n        - annual interest rate, and\n        - period of repayments in months\n\n    You can nominate the accuracy required by adjusting the value of\n        ACCURACY_AS_PARTS_OF_CENT. For example:\n        - .01 = accurate to a dollar\n        - .1  = accurate to 10 cents\n        - 1   = accurate to cent\n        - 100 = accurate to 100th of a cent\n\"\"\"\n\n# Set constants.\nLOAN_AMOUNT = 7692\nANNUAL_INTEREST_PERCENT = 18\nREPAY_MONTHS = 12\nACCURACY_AS_PARTS_OF_CENT = 1\n\nloan_amount = int(LOAN_AMOUNT * 100 * ACCURACY_AS_PARTS_OF_CENT)\nmonthly_interest = float(ANNUAL_INTEREST_PERCENT / 100 / 12)\nrepay_guess_min = int((LOAN_AMOUNT / REPAY_MONTHS) - 1)\nresult_found = False\nrepayment_required = 0\n\nfor repay_guess in range(repay_guess_min, loan_amount):\n    if result_found:\n        break\n    loan_balance = loan_amount\n    for _ in range(REPAY_MONTHS):\n        interest_to_add = loan_balance * monthly_interest\n        loan_balance = loan_balance + interest_to_add - repay_guess\n        if loan_balance &lt;= 0:\n            repayment_required = repay_guess / 100 / ACCURACY_AS_PARTS_OF_CENT\n            result_found = True\n            break\n\nprint('Required monthly repayment = $' + str(repayment_required))\n</code></pre>\n\n\n<p>Code:</p>\n\n<pre><code>from __future__ import print_function\n\n\"\"\"\nFormulas: http://mathforum.org/dr.math/faq/faq.interest.html\n\"\"\"\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the Principal \n    n   - Number of years\n    q   - the number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.04 for 4% interest)\n    \"\"\"\n    if debug:\n        print('P = %s\\t(amount of the Principal)' %P)\n        print('n = %s\\t\\t(# of years)' %n)\n        print('q = %s\\t\\t(# of periods per year)' %q)\n        print('i = %s %%\\t(Annual interest)' %(i*100))\n    return P*i/( q*(1 - pow(1 + i/q, -n*q)) )\n\n\n### Given :\nP = 7692\nn = 1\nq = 12\ni = 18/100\n\nprint('M = %s' %annuity_monthly_payment(P=P, n=n, q=q, i=i, debug=True))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>P = 7692        (amount of the Principal)\nn = 1           (# of years)\nq = 12          (# of periods per year)\ni = 18.0 %      (Annual interest)\nM = 705.2025054347173\n</code></pre>\n\n\n<p>Code:</p>\n\n<pre><code>from __future__ import print_function\n\n\"\"\"\nFormulas: http://mathforum.org/dr.math/faq/faq.interest.html\n\"\"\"\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the Principal \n    n   - Number of years\n    q   - the number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.04 for 4% interest)\n    \"\"\"\n    if debug:\n        print('P = %s\\t(amount of the Principal)' %P)\n        print('n = %s\\t\\t(# of years)' %n)\n        print('q = %s\\t\\t(# of periods per year)' %q)\n        print('i = %s %%\\t(Annual interest)' %(i*100))\n    return P*i/( q*(1 - pow(1 + i/q, -n*q)) )\n\n\n### Given :\nP = 7692\nn = 1\nq = 12\ni = 18/100\n\nprint('M = %s' %annuity_monthly_payment(P=P, n=n, q=q, i=i, debug=True))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>P = 7692        (amount of the Principal)\nn = 1           (# of years)\nq = 12          (# of periods per year)\ni = 18.0 %      (Annual interest)\nM = 705.2025054347173\n</code></pre>\n\n\n<p>Your python code has some problems.  For one thing, the command to exit is <code>exit()</code>, not <code>exit</code>.  Here is a revised version:</p>\n\n<pre><code>loan_amount= 7692\ninterest = 18\nmonths = 12\n\nfor rg in range(700, 710):\n    y = loan_amount\n    for i in range(months):\n        x = y * interest / 100. / 12.\n        y = y + x - rg\n        if y &lt; 0: \n            print(rg)\n            exit()\n</code></pre>\n\n<p>This prints <code>706</code>, which is the nearest whole number approximation of 705.20.</p>\n\n<p>If you want python code that prints exactly 705.20, that is certainly possible.  However the code will be more complex and take quite a bit of effort to write.  Spreadsheets seem better suited for this job.</p>\n\n\n<p>Your python code has some problems.  For one thing, the command to exit is <code>exit()</code>, not <code>exit</code>.  Here is a revised version:</p>\n\n<pre><code>loan_amount= 7692\ninterest = 18\nmonths = 12\n\nfor rg in range(700, 710):\n    y = loan_amount\n    for i in range(months):\n        x = y * interest / 100. / 12.\n        y = y + x - rg\n        if y &lt; 0: \n            print(rg)\n            exit()\n</code></pre>\n\n<p>This prints <code>706</code>, which is the nearest whole number approximation of 705.20.</p>\n\n<p>If you want python code that prints exactly 705.20, that is certainly possible.  However the code will be more complex and take quite a bit of effort to write.  Spreadsheets seem better suited for this job.</p>\n\n\n<p>I think these tabular/vector/matrix type analyses are perfect for numpy and pandas. You often can write more compact code that is also easy to read. See if you agree.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndef mpmt(amt, i, nper):\n    \"\"\"\n    Calculate the monthly payments on a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return amt*i1**nper*i/(i1**nper-1)\n\ndef ipmt(amt, i, per, nper):\n    \"\"\"\n    Calculate interest paid in a specific period, per, of a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return (amt*i*(i1**(nper+1)-i1**per))/(i1*(i1**nper-1))\n\ndef amorttable(amt, i, nper):\n    \"\"\"\n    Create an amortization table for a loan/mortgage\n    \"\"\"\n    monthlypmt = mpmt(amt, i, nper)\n\n    # the following calculations are vectorized\n    df = pd.DataFrame({'month':np.arange(1, nper+1)})\n    df['intpaid'] = ipmt(amt, i, df['month'], nper)\n    df['prinpaid'] = monthlypmt - df['intpaid']\n    df['balance'] = amt\n    df['balance'] -= np.cumsum(df['prinpaid'])\n    return df\n\n\nprint(amorttable(7692, .18, 12).round(2))\n</code></pre>\n\n<p>Here's the result:</p>\n\n<pre><code>    month  intpaid  prinpaid  balance\n0       1   115.38    589.82  7102.18\n1       2   106.53    598.67  6503.51\n2       3    97.55    607.65  5895.86\n3       4    88.44    616.76  5279.09\n4       5    79.19    626.02  4653.08\n5       6    69.80    635.41  4017.67\n6       7    60.27    644.94  3372.73\n7       8    50.59    654.61  2718.12\n8       9    40.77    664.43  2053.69\n9      10    30.81    674.40  1379.29\n10     11    20.69    684.51   694.78\n11     12    10.42    694.78    -0.00\n</code></pre>\n\n\n<p>I think these tabular/vector/matrix type analyses are perfect for numpy and pandas. You often can write more compact code that is also easy to read. See if you agree.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndef mpmt(amt, i, nper):\n    \"\"\"\n    Calculate the monthly payments on a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return amt*i1**nper*i/(i1**nper-1)\n\ndef ipmt(amt, i, per, nper):\n    \"\"\"\n    Calculate interest paid in a specific period, per, of a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return (amt*i*(i1**(nper+1)-i1**per))/(i1*(i1**nper-1))\n\ndef amorttable(amt, i, nper):\n    \"\"\"\n    Create an amortization table for a loan/mortgage\n    \"\"\"\n    monthlypmt = mpmt(amt, i, nper)\n\n    # the following calculations are vectorized\n    df = pd.DataFrame({'month':np.arange(1, nper+1)})\n    df['intpaid'] = ipmt(amt, i, df['month'], nper)\n    df['prinpaid'] = monthlypmt - df['intpaid']\n    df['balance'] = amt\n    df['balance'] -= np.cumsum(df['prinpaid'])\n    return df\n\n\nprint(amorttable(7692, .18, 12).round(2))\n</code></pre>\n\n<p>Here's the result:</p>\n\n<pre><code>    month  intpaid  prinpaid  balance\n0       1   115.38    589.82  7102.18\n1       2   106.53    598.67  6503.51\n2       3    97.55    607.65  5895.86\n3       4    88.44    616.76  5279.09\n4       5    79.19    626.02  4653.08\n5       6    69.80    635.41  4017.67\n6       7    60.27    644.94  3372.73\n7       8    50.59    654.61  2718.12\n8       9    40.77    664.43  2053.69\n9      10    30.81    674.40  1379.29\n10     11    20.69    684.51   694.78\n11     12    10.42    694.78    -0.00\n</code></pre>\n\n\n<p>I decided to tune it up for myself. So he is the last version i came up with, which now includes a \"<strong>amortization_tab</strong>\" function for printing an amortization table (PrettyTable and CSV formats) and another useful functions: \"<strong>rest_amount</strong>\", \"<strong>amount_can_be_payed_in_n_years</strong>\", \"<strong>years_to_pay_off</strong>\".\nI have added CSV format, so one can generate an initial calculation, import it into Excel and continue there. \nSo now it's a more or less complete library for <strong>annuity</strong> loan/mortgage math.</p>\n\n<p>PS it's been tested with Python v3.5.1, but it should also <em>theoretically</em> work with another Python versions.</p>\n\n<pre><code>from __future__ import print_function\nimport sys\nimport math\nimport io\nimport csv\nimport prettytable\n\n# Formulas:   http://mathforum.org/dr.math/faq/faq.interest.html\n\ndescription = {\n    'P': 'amount of the principal',\n    'i': 'annual interest rate',\n    'n': 'number of years',\n    'q': 'number of times per year that the interest is compounded',\n    'M': 'fixed monthly payment',\n    'k': 'number of \"payed\" payments',\n}\n\n\ndef pr_debug(P=None, i=None, n=None, q=None, M=None, k=None, debug=False):\n    if not debug:\n        return\n\n    columns = ['var','value', 'description']\n    t = prettytable.PrettyTable(columns)\n    t.align['var'] = 'l'\n    t.align['value'] = 'r'\n    t.align['description'] = 'l'\n    t.padding_width = 1\n    t.float_format = '.2'\n\n    if P:\n        t.add_row(['P', P, description['P']])\n    if i:\n        t.add_row(['i', i, description['i']])\n    if n:\n        t.add_row(['n', n, description['n']])\n    if q:\n        t.add_row(['q', q, description['q']])\n    if M:\n        t.add_row(['M', M, description['M']])\n    if k:\n        t.add_row(['k', k, description['k']])\n\n    print(t.get_string() + '\\n')\n\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the principal \n    n   - number of years\n    q   - number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.045 for 4.5% interest)\n    \"\"\"\n    pr_debug(P=P, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(P*i/( q*(1 - pow(1 + i/q, -n*q)) ), 2)\n\n\ndef rest_amount(P, M, k, q, i, debug = False):\n    \"\"\"\n    Calculates rest amount after 'k' payed payments \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    k   - # of payments\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, k=k, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round((P - M*q/i) * pow(1 + i/q, k) + M*q/i, 2)\n\n\ndef amount_can_be_payed_in_n_years(M, n, q, i, debug = False):\n    \"\"\"\n    Returns the amount of principal that can be paid off in n years \n    M   - fixed amount that have been payed 'k' times\n    n   - Number of years\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(M=M, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round( M*(1 - pow(1 + i/q, -n*q) )*q/i, 2)\n\n\ndef years_to_pay_off(P, M, q, i, debug = False):\n    \"\"\"\n    Returns number of years needed to pay off the loan \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(-math.log(1 - (P*i/M/q)) / (q*math.log(1 + i/q)), 2)\n\n\ndef amortization_tab(P, n, q, i, M=None, fmt='txt', debug=False):\n    \"\"\"\n    Generates amortization table \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    assert any(fmt in x for x in ['txt', 'csv'])\n\n    # calculate monthly payment if it's not given\n    if not M:\n        M = annuity_monthly_payment(P=P, n=n, q=q, i=i)\n\n    pr_debug(P=P, M=M, n=n, q=q, i=i, debug=debug)\n\n    # column headers for the output table\n    columns=['pmt#','beg_bal','pmt','interest','applied', 'end_bal']\n\n    i /= 100\n\n    beg_bal = P\n    term = n*q\n\n    if fmt.lower() == 'txt':\n        t = prettytable.PrettyTable(columns)\n        t.align = 'r'\n        t.padding_width = 1\n        t.float_format = '.2'\n    elif fmt.lower() == 'csv':\n        if sys.version_info &gt;= (2,7,0):\n            out = io.StringIO()\n        else:\n            out = io.BytesIO()\n        t = csv.writer(out, quoting=csv.QUOTE_NONNUMERIC)\n        t.writerow(columns)\n\n    for num in range(1, term+1):\n        interest = round(beg_bal*i/q , 2)\n        applied = round(M - interest, 2)\n        end_bal = round(beg_bal - applied,2)\n        row = [num, beg_bal, M, interest, applied, end_bal]\n        if fmt.lower() == 'txt':\n            t.add_row(row)\n        elif fmt.lower() == 'csv':\n            t.writerow(row)\n        beg_bal = end_bal\n\n    if fmt.lower() == 'txt':\n        return t.get_string()\n    elif fmt.lower() == 'csv':\n        return out.getvalue()\n\n############################\nP = 7692.0\nn = 1\nq = 12\ni = 18\nprint(amortization_tab(P, n, q, i, debug=True))\n\nprint('#' * 80)\nprint('#' * 80)\n############################\n# another example\nP = 100000.0\nn = 5\nq = 12\ni = 3.5\nk = 36\nM = 1200\nprint(amortization_tab(P, n, q, i, M, fmt='csv', debug=True))\nprint('*' * 80)\nprint('Rest amount after %s payments:\\t%s' %(k, rest_amount(P=P, M=M, k=k, q=q, i=i)))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>+-----+---------+----------------------------------------------------------+\n| var |   value | description                                              |\n+-----+---------+----------------------------------------------------------+\n| P   | 7692.00 | amount of the principal                                  |\n| i   |      18 | annual interest rate                                     |\n| n   |       1 | number of years                                          |\n| q   |      12 | number of times per year that the interest is compounded |\n| M   |  705.20 | fixed monthly payment                                    |\n+-----+---------+----------------------------------------------------------+\n\n+------+---------+--------+----------+---------+---------+\n| pmt# | beg_bal |    pmt | interest | applied | end_bal |\n+------+---------+--------+----------+---------+---------+\n|    1 | 7692.00 | 705.20 |   115.38 |  589.82 | 7102.18 |\n|    2 | 7102.18 | 705.20 |   106.53 |  598.67 | 6503.51 |\n|    3 | 6503.51 | 705.20 |    97.55 |  607.65 | 5895.86 |\n|    4 | 5895.86 | 705.20 |    88.44 |  616.76 | 5279.10 |\n|    5 | 5279.10 | 705.20 |    79.19 |  626.01 | 4653.09 |\n|    6 | 4653.09 | 705.20 |    69.80 |  635.40 | 4017.69 |\n|    7 | 4017.69 | 705.20 |    60.27 |  644.93 | 3372.76 |\n|    8 | 3372.76 | 705.20 |    50.59 |  654.61 | 2718.15 |\n|    9 | 2718.15 | 705.20 |    40.77 |  664.43 | 2053.72 |\n|   10 | 2053.72 | 705.20 |    30.81 |  674.39 | 1379.33 |\n|   11 | 1379.33 | 705.20 |    20.69 |  684.51 |  694.82 |\n|   12 |  694.82 | 705.20 |    10.42 |  694.78 |    0.04 |\n+------+---------+--------+----------+---------+---------+\n################################################################################\n################################################################################\n+-----+-----------+----------------------------------------------------------+\n| var |     value | description                                              |\n+-----+-----------+----------------------------------------------------------+\n| P   | 100000.00 | amount of the principal                                  |\n| i   |      3.50 | annual interest rate                                     |\n| n   |         5 | number of years                                          |\n| q   |        12 | number of times per year that the interest is compounded |\n| M   |      1200 | fixed monthly payment                                    |\n+-----+-----------+----------------------------------------------------------+\n\n\"pmt#\",\"beg_bal\",\"pmt\",\"interest\",\"applied\",\"end_bal\"\n1,100000.0,1200,291.67,908.33,99091.67\n2,99091.67,1200,289.02,910.98,98180.69\n3,98180.69,1200,286.36,913.64,97267.05\n4,97267.05,1200,283.7,916.3,96350.75\n5,96350.75,1200,281.02,918.98,95431.77\n6,95431.77,1200,278.34,921.66,94510.11\n7,94510.11,1200,275.65,924.35,93585.76\n8,93585.76,1200,272.96,927.04,92658.72\n9,92658.72,1200,270.25,929.75,91728.97\n10,91728.97,1200,267.54,932.46,90796.51\n11,90796.51,1200,264.82,935.18,89861.33\n12,89861.33,1200,262.1,937.9,88923.43\n13,88923.43,1200,259.36,940.64,87982.79\n14,87982.79,1200,256.62,943.38,87039.41\n15,87039.41,1200,253.86,946.14,86093.27\n16,86093.27,1200,251.11,948.89,85144.38\n17,85144.38,1200,248.34,951.66,84192.72\n18,84192.72,1200,245.56,954.44,83238.28\n19,83238.28,1200,242.78,957.22,82281.06\n20,82281.06,1200,239.99,960.01,81321.05\n21,81321.05,1200,237.19,962.81,80358.24\n22,80358.24,1200,234.38,965.62,79392.62\n23,79392.62,1200,231.56,968.44,78424.18\n24,78424.18,1200,228.74,971.26,77452.92\n25,77452.92,1200,225.9,974.1,76478.82\n26,76478.82,1200,223.06,976.94,75501.88\n27,75501.88,1200,220.21,979.79,74522.09\n28,74522.09,1200,217.36,982.64,73539.45\n29,73539.45,1200,214.49,985.51,72553.94\n30,72553.94,1200,211.62,988.38,71565.56\n31,71565.56,1200,208.73,991.27,70574.29\n32,70574.29,1200,205.84,994.16,69580.13\n33,69580.13,1200,202.94,997.06,68583.07\n34,68583.07,1200,200.03,999.97,67583.1\n35,67583.1,1200,197.12,1002.88,66580.22\n36,66580.22,1200,194.19,1005.81,65574.41\n37,65574.41,1200,191.26,1008.74,64565.67\n38,64565.67,1200,188.32,1011.68,63553.99\n39,63553.99,1200,185.37,1014.63,62539.36\n40,62539.36,1200,182.41,1017.59,61521.77\n41,61521.77,1200,179.44,1020.56,60501.21\n42,60501.21,1200,176.46,1023.54,59477.67\n43,59477.67,1200,173.48,1026.52,58451.15\n44,58451.15,1200,170.48,1029.52,57421.63\n45,57421.63,1200,167.48,1032.52,56389.11\n46,56389.11,1200,164.47,1035.53,55353.58\n47,55353.58,1200,161.45,1038.55,54315.03\n48,54315.03,1200,158.42,1041.58,53273.45\n49,53273.45,1200,155.38,1044.62,52228.83\n50,52228.83,1200,152.33,1047.67,51181.16\n51,51181.16,1200,149.28,1050.72,50130.44\n52,50130.44,1200,146.21,1053.79,49076.65\n53,49076.65,1200,143.14,1056.86,48019.79\n54,48019.79,1200,140.06,1059.94,46959.85\n55,46959.85,1200,136.97,1063.03,45896.82\n56,45896.82,1200,133.87,1066.13,44830.69\n57,44830.69,1200,130.76,1069.24,43761.45\n58,43761.45,1200,127.64,1072.36,42689.09\n59,42689.09,1200,124.51,1075.49,41613.6\n60,41613.6,1200,121.37,1078.63,40534.97\n\n********************************************************************************\nRest amount after 36 payments:  65574.41\n</code></pre>\n\n\n<p>I decided to tune it up for myself. So he is the last version i came up with, which now includes a \"<strong>amortization_tab</strong>\" function for printing an amortization table (PrettyTable and CSV formats) and another useful functions: \"<strong>rest_amount</strong>\", \"<strong>amount_can_be_payed_in_n_years</strong>\", \"<strong>years_to_pay_off</strong>\".\nI have added CSV format, so one can generate an initial calculation, import it into Excel and continue there. \nSo now it's a more or less complete library for <strong>annuity</strong> loan/mortgage math.</p>\n\n<p>PS it's been tested with Python v3.5.1, but it should also <em>theoretically</em> work with another Python versions.</p>\n\n<pre><code>from __future__ import print_function\nimport sys\nimport math\nimport io\nimport csv\nimport prettytable\n\n# Formulas:   http://mathforum.org/dr.math/faq/faq.interest.html\n\ndescription = {\n    'P': 'amount of the principal',\n    'i': 'annual interest rate',\n    'n': 'number of years',\n    'q': 'number of times per year that the interest is compounded',\n    'M': 'fixed monthly payment',\n    'k': 'number of \"payed\" payments',\n}\n\n\ndef pr_debug(P=None, i=None, n=None, q=None, M=None, k=None, debug=False):\n    if not debug:\n        return\n\n    columns = ['var','value', 'description']\n    t = prettytable.PrettyTable(columns)\n    t.align['var'] = 'l'\n    t.align['value'] = 'r'\n    t.align['description'] = 'l'\n    t.padding_width = 1\n    t.float_format = '.2'\n\n    if P:\n        t.add_row(['P', P, description['P']])\n    if i:\n        t.add_row(['i', i, description['i']])\n    if n:\n        t.add_row(['n', n, description['n']])\n    if q:\n        t.add_row(['q', q, description['q']])\n    if M:\n        t.add_row(['M', M, description['M']])\n    if k:\n        t.add_row(['k', k, description['k']])\n\n    print(t.get_string() + '\\n')\n\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the principal \n    n   - number of years\n    q   - number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.045 for 4.5% interest)\n    \"\"\"\n    pr_debug(P=P, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(P*i/( q*(1 - pow(1 + i/q, -n*q)) ), 2)\n\n\ndef rest_amount(P, M, k, q, i, debug = False):\n    \"\"\"\n    Calculates rest amount after 'k' payed payments \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    k   - # of payments\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, k=k, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round((P - M*q/i) * pow(1 + i/q, k) + M*q/i, 2)\n\n\ndef amount_can_be_payed_in_n_years(M, n, q, i, debug = False):\n    \"\"\"\n    Returns the amount of principal that can be paid off in n years \n    M   - fixed amount that have been payed 'k' times\n    n   - Number of years\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(M=M, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round( M*(1 - pow(1 + i/q, -n*q) )*q/i, 2)\n\n\ndef years_to_pay_off(P, M, q, i, debug = False):\n    \"\"\"\n    Returns number of years needed to pay off the loan \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(-math.log(1 - (P*i/M/q)) / (q*math.log(1 + i/q)), 2)\n\n\ndef amortization_tab(P, n, q, i, M=None, fmt='txt', debug=False):\n    \"\"\"\n    Generates amortization table \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    assert any(fmt in x for x in ['txt', 'csv'])\n\n    # calculate monthly payment if it's not given\n    if not M:\n        M = annuity_monthly_payment(P=P, n=n, q=q, i=i)\n\n    pr_debug(P=P, M=M, n=n, q=q, i=i, debug=debug)\n\n    # column headers for the output table\n    columns=['pmt#','beg_bal','pmt','interest','applied', 'end_bal']\n\n    i /= 100\n\n    beg_bal = P\n    term = n*q\n\n    if fmt.lower() == 'txt':\n        t = prettytable.PrettyTable(columns)\n        t.align = 'r'\n        t.padding_width = 1\n        t.float_format = '.2'\n    elif fmt.lower() == 'csv':\n        if sys.version_info &gt;= (2,7,0):\n            out = io.StringIO()\n        else:\n            out = io.BytesIO()\n        t = csv.writer(out, quoting=csv.QUOTE_NONNUMERIC)\n        t.writerow(columns)\n\n    for num in range(1, term+1):\n        interest = round(beg_bal*i/q , 2)\n        applied = round(M - interest, 2)\n        end_bal = round(beg_bal - applied,2)\n        row = [num, beg_bal, M, interest, applied, end_bal]\n        if fmt.lower() == 'txt':\n            t.add_row(row)\n        elif fmt.lower() == 'csv':\n            t.writerow(row)\n        beg_bal = end_bal\n\n    if fmt.lower() == 'txt':\n        return t.get_string()\n    elif fmt.lower() == 'csv':\n        return out.getvalue()\n\n############################\nP = 7692.0\nn = 1\nq = 12\ni = 18\nprint(amortization_tab(P, n, q, i, debug=True))\n\nprint('#' * 80)\nprint('#' * 80)\n############################\n# another example\nP = 100000.0\nn = 5\nq = 12\ni = 3.5\nk = 36\nM = 1200\nprint(amortization_tab(P, n, q, i, M, fmt='csv', debug=True))\nprint('*' * 80)\nprint('Rest amount after %s payments:\\t%s' %(k, rest_amount(P=P, M=M, k=k, q=q, i=i)))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>+-----+---------+----------------------------------------------------------+\n| var |   value | description                                              |\n+-----+---------+----------------------------------------------------------+\n| P   | 7692.00 | amount of the principal                                  |\n| i   |      18 | annual interest rate                                     |\n| n   |       1 | number of years                                          |\n| q   |      12 | number of times per year that the interest is compounded |\n| M   |  705.20 | fixed monthly payment                                    |\n+-----+---------+----------------------------------------------------------+\n\n+------+---------+--------+----------+---------+---------+\n| pmt# | beg_bal |    pmt | interest | applied | end_bal |\n+------+---------+--------+----------+---------+---------+\n|    1 | 7692.00 | 705.20 |   115.38 |  589.82 | 7102.18 |\n|    2 | 7102.18 | 705.20 |   106.53 |  598.67 | 6503.51 |\n|    3 | 6503.51 | 705.20 |    97.55 |  607.65 | 5895.86 |\n|    4 | 5895.86 | 705.20 |    88.44 |  616.76 | 5279.10 |\n|    5 | 5279.10 | 705.20 |    79.19 |  626.01 | 4653.09 |\n|    6 | 4653.09 | 705.20 |    69.80 |  635.40 | 4017.69 |\n|    7 | 4017.69 | 705.20 |    60.27 |  644.93 | 3372.76 |\n|    8 | 3372.76 | 705.20 |    50.59 |  654.61 | 2718.15 |\n|    9 | 2718.15 | 705.20 |    40.77 |  664.43 | 2053.72 |\n|   10 | 2053.72 | 705.20 |    30.81 |  674.39 | 1379.33 |\n|   11 | 1379.33 | 705.20 |    20.69 |  684.51 |  694.82 |\n|   12 |  694.82 | 705.20 |    10.42 |  694.78 |    0.04 |\n+------+---------+--------+----------+---------+---------+\n################################################################################\n################################################################################\n+-----+-----------+----------------------------------------------------------+\n| var |     value | description                                              |\n+-----+-----------+----------------------------------------------------------+\n| P   | 100000.00 | amount of the principal                                  |\n| i   |      3.50 | annual interest rate                                     |\n| n   |         5 | number of years                                          |\n| q   |        12 | number of times per year that the interest is compounded |\n| M   |      1200 | fixed monthly payment                                    |\n+-----+-----------+----------------------------------------------------------+\n\n\"pmt#\",\"beg_bal\",\"pmt\",\"interest\",\"applied\",\"end_bal\"\n1,100000.0,1200,291.67,908.33,99091.67\n2,99091.67,1200,289.02,910.98,98180.69\n3,98180.69,1200,286.36,913.64,97267.05\n4,97267.05,1200,283.7,916.3,96350.75\n5,96350.75,1200,281.02,918.98,95431.77\n6,95431.77,1200,278.34,921.66,94510.11\n7,94510.11,1200,275.65,924.35,93585.76\n8,93585.76,1200,272.96,927.04,92658.72\n9,92658.72,1200,270.25,929.75,91728.97\n10,91728.97,1200,267.54,932.46,90796.51\n11,90796.51,1200,264.82,935.18,89861.33\n12,89861.33,1200,262.1,937.9,88923.43\n13,88923.43,1200,259.36,940.64,87982.79\n14,87982.79,1200,256.62,943.38,87039.41\n15,87039.41,1200,253.86,946.14,86093.27\n16,86093.27,1200,251.11,948.89,85144.38\n17,85144.38,1200,248.34,951.66,84192.72\n18,84192.72,1200,245.56,954.44,83238.28\n19,83238.28,1200,242.78,957.22,82281.06\n20,82281.06,1200,239.99,960.01,81321.05\n21,81321.05,1200,237.19,962.81,80358.24\n22,80358.24,1200,234.38,965.62,79392.62\n23,79392.62,1200,231.56,968.44,78424.18\n24,78424.18,1200,228.74,971.26,77452.92\n25,77452.92,1200,225.9,974.1,76478.82\n26,76478.82,1200,223.06,976.94,75501.88\n27,75501.88,1200,220.21,979.79,74522.09\n28,74522.09,1200,217.36,982.64,73539.45\n29,73539.45,1200,214.49,985.51,72553.94\n30,72553.94,1200,211.62,988.38,71565.56\n31,71565.56,1200,208.73,991.27,70574.29\n32,70574.29,1200,205.84,994.16,69580.13\n33,69580.13,1200,202.94,997.06,68583.07\n34,68583.07,1200,200.03,999.97,67583.1\n35,67583.1,1200,197.12,1002.88,66580.22\n36,66580.22,1200,194.19,1005.81,65574.41\n37,65574.41,1200,191.26,1008.74,64565.67\n38,64565.67,1200,188.32,1011.68,63553.99\n39,63553.99,1200,185.37,1014.63,62539.36\n40,62539.36,1200,182.41,1017.59,61521.77\n41,61521.77,1200,179.44,1020.56,60501.21\n42,60501.21,1200,176.46,1023.54,59477.67\n43,59477.67,1200,173.48,1026.52,58451.15\n44,58451.15,1200,170.48,1029.52,57421.63\n45,57421.63,1200,167.48,1032.52,56389.11\n46,56389.11,1200,164.47,1035.53,55353.58\n47,55353.58,1200,161.45,1038.55,54315.03\n48,54315.03,1200,158.42,1041.58,53273.45\n49,53273.45,1200,155.38,1044.62,52228.83\n50,52228.83,1200,152.33,1047.67,51181.16\n51,51181.16,1200,149.28,1050.72,50130.44\n52,50130.44,1200,146.21,1053.79,49076.65\n53,49076.65,1200,143.14,1056.86,48019.79\n54,48019.79,1200,140.06,1059.94,46959.85\n55,46959.85,1200,136.97,1063.03,45896.82\n56,45896.82,1200,133.87,1066.13,44830.69\n57,44830.69,1200,130.76,1069.24,43761.45\n58,43761.45,1200,127.64,1072.36,42689.09\n59,42689.09,1200,124.51,1075.49,41613.6\n60,41613.6,1200,121.37,1078.63,40534.97\n\n********************************************************************************\nRest amount after 36 payments:  65574.41\n</code></pre>\n\n\n<pre><code>import numpy as np\n\nfor pmt in np.linspace(200, 800, 20):\n    loan = 7692.00\n    for n in range(1, 13):\n        new_balance = loan + ((loan*(1+(0.18/12)))-loan) - pmt\n        loan = new_balance\n    print(round(pmt, 2), '-&gt;', round(loan,2))\n</code></pre>\n\n<p>The first column shows what the equal 12 month payment would be and the right column shows what the balance would be after 12 months.  See how the balance approaches zero around 705.26?  That indicates zero is somewhere around there.</p>\n\n<pre><code>200.0 -&gt; 6588.45\n231.58 -&gt; 6176.62\n263.16 -&gt; 5764.8\n294.74 -&gt; 5352.97\n326.32 -&gt; 4941.14\n357.89 -&gt; 4529.31\n389.47 -&gt; 4117.49\n421.05 -&gt; 3705.66\n452.63 -&gt; 3293.83\n484.21 -&gt; 2882.0\n515.79 -&gt; 2470.18\n547.37 -&gt; 2058.35\n578.95 -&gt; 1646.52\n610.53 -&gt; 1234.69\n642.11 -&gt; 822.86\n673.68 -&gt; 411.04\n705.26 -&gt; -0.79\n736.84 -&gt; -412.62\n768.42 -&gt; -824.45\n800.0 -&gt; -1236.27\n</code></pre>\n\n<p>I had a similar <a href=\"https://stackoverflow.com/questions/33160262/linear-programming-simplex-lp-pulp\">question using linear programming</a>.  Might be worth checking out.</p>\n\n\n<pre><code>import numpy as np\n\nfor pmt in np.linspace(200, 800, 20):\n    loan = 7692.00\n    for n in range(1, 13):\n        new_balance = loan + ((loan*(1+(0.18/12)))-loan) - pmt\n        loan = new_balance\n    print(round(pmt, 2), '-&gt;', round(loan,2))\n</code></pre>\n\n<p>The first column shows what the equal 12 month payment would be and the right column shows what the balance would be after 12 months.  See how the balance approaches zero around 705.26?  That indicates zero is somewhere around there.</p>\n\n<pre><code>200.0 -&gt; 6588.45\n231.58 -&gt; 6176.62\n263.16 -&gt; 5764.8\n294.74 -&gt; 5352.97\n326.32 -&gt; 4941.14\n357.89 -&gt; 4529.31\n389.47 -&gt; 4117.49\n421.05 -&gt; 3705.66\n452.63 -&gt; 3293.83\n484.21 -&gt; 2882.0\n515.79 -&gt; 2470.18\n547.37 -&gt; 2058.35\n578.95 -&gt; 1646.52\n610.53 -&gt; 1234.69\n642.11 -&gt; 822.86\n673.68 -&gt; 411.04\n705.26 -&gt; -0.79\n736.84 -&gt; -412.62\n768.42 -&gt; -824.45\n800.0 -&gt; -1236.27\n</code></pre>\n\n<p>I had a similar <a href=\"https://stackoverflow.com/questions/33160262/linear-programming-simplex-lp-pulp\">question using linear programming</a>.  Might be worth checking out.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>As the title/tag also mentioned SQL, I will post an SQL solution:</p>\n\n<pre><code>create table loan (\n  amount decimal(10,2),\n  repay_months int,\n  yearly_interest_rate decimal(4, 4)\n);\n\ninsert into loan values (7692, 12, 0.18);\n\nselect amount * yearly_interest_rate/12 /\n           (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n           as monthly_payment\nfrom   loan;\n</code></pre>\n\n<p>Result: </p>\n\n<pre><code>monthly_payment\n-----------------\n705.2025054347173\n</code></pre>\n\n<p><a href=\"http://sqlfiddle.com/#!9/56254f/5\" rel=\"nofollow\">SQL Fiddle</a>.    </p>\n\n<p>If you want to get the whole amortization schedule, then an idea would be to first create a table with sequential month numbers (1, 2, ...), enough to cover for the longest loan pay-off duration you would have data for:</p>\n\n<pre><code>create table months (month int);\n\ninsert into months -- one year of months\n  values (1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12);\n-- now some record multiplication to avoid long literal lists:    \ninsert into months -- multiply to cover 2 years\n  select month + 12 from months;\ninsert into months -- multiply to cover 4 years\n  select month + 24 from months;\ninsert into months -- multiply to cover 8 years\n  select month + 48 from months;\ninsert into months -- multiply to cover 16 years\n  select month + 96 from months;\ninsert into months -- multiply to cover 32 years\n  select month + 192 from months;\n-- OK, we have now months from 1 to 384 (= 32 years)\n</code></pre>\n\n<p>Then use the following query, which has the above mentioned query as sub-select:</p>\n\n<pre><code>select month,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-repay_months)) \n                       / monthly_interest_rate\n                       as loan_balance,\n       monthly_payment * (1 - pow(1 + monthly_interest_rate, month-1-repay_months)) \n                       as interest,\n       monthly_payment\nfrom   months,\n       (\n        select amount,\n               repay_months,\n               yearly_interest_rate,\n               yearly_interest_rate/12 as monthly_interest_rate, \n               amount * yearly_interest_rate/12 /\n                   (1 - pow(1 + yearly_interest_rate/12, -repay_months))\n                   as monthly_payment\n        from   loan\n       ) as loanX\nwhere  month &lt;= repay_months\norder by 1;\n</code></pre>\n\n<p>This produces the following output:</p>\n\n<pre><code>+-------+--------------------+---------------------+-------------------+\n| month | loan_balance       | interest            | monthly_payment   |\n+-------+--------------------+---------------------+-------------------+\n| 1     | 7102.177494565289  | 115.38              | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 2     | 6503.507651549055  | 106.53266241847933  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 3     | 5895.8577608875785 |  97.55261477323582  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 4     | 5279.093121866177  |  88.43786641331367  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 5     | 4653.077013259458  |  79.18639682799265  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 6     | 4017.6706630236345 |  69.79615519889187  | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 7     | 3372.7332175342744 |  60.265059945354515 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 8     | 2718.12171036258   |  50.590998263014114 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 9     | 2053.6910305833035 |  40.7718256554387   | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 10    | 1379.2938906073389 |  30.805365458749552 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 11    |  694.7807935317352 |  20.689408359110082 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n| 12    |    0               |  10.421711902976027 | 705.2025054347173 |\n+-------+--------------------+---------------------+-------------------+\n</code></pre>\n\n<p>Here is the <a href=\"http://sqlfiddle.com/#!9/ddaa67/7\" rel=\"nofollow\">SQL fiddle</a>.</p>\n\n<p>The formula used is provided and derived in <a href=\"https://en.wikipedia.org/wiki/Amortization_calculator#The_formula\" rel=\"nofollow\">this Wikipedia article</a>.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "answer_2": {"type": "literal", "value": "<p>Well, you can solve it using numerical method (as Excel does), you can solve it with brute force by checking every amount with some step within some range, or you can solve it analytically on a piece of paper.</p>\n\n<p>Using the following notation</p>\n\n<pre><code>L - initial loan amount = 7692\nR - monthly interest rate = 1 + 0.18/12\nm - number of months to repay the loan = 12\nP - monthly payment to pay the loan in full after m months = unknown\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/hc5r1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/hc5r1.png\" alt=\"L_{n}\"></a> is loan amount after the <code>n</code>-th month. <a href=\"https://i.stack.imgur.com/Fjn3W.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Fjn3W.png\" alt=\"L_{0}\"></a> is the initial loan amount (7692). <a href=\"https://i.stack.imgur.com/UQGZ9.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UQGZ9.png\" alt=\"L_{m}\"></a> is the loan amount after <code>m</code> months (0).</p>\n\n<p>The main relation between <code>n</code>-th and <code>(n-1)</code>-th month is:</p>\n\n<p><a href=\"https://i.stack.imgur.com/Tx4kh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Tx4kh.png\" alt=\"L_{n} = L_{n-1} * R - P\"></a></p>\n\n<p>So, analytical formula turns out to be:</p>\n\n<p><a href=\"https://i.stack.imgur.com/abGdN.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/abGdN.png\" alt=\"P = L * \\frac{R^{m}}{\\sum_{k=0}^{m-1}R^{k}} = L * R^{m} * \\frac{R-1}{R^{m}-1}\"></a></p>\n\n<p>Now it should be fairly straight-forward to calculate it in any programming language.</p>\n\n<p>For the given initial parameters </p>\n\n<p><a href=\"https://i.stack.imgur.com/9BXBX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9BXBX.png\" alt=\"R = 1 + \\frac{0.18}{12} = 1.015\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/nATZE.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/nATZE.png\" alt=\"P = 7692 * 1.015^{12} * \\frac{1.015-1}{1.015^{12}-1}\\approx 705.2025054\"></a></p>\n\n<hr>\n\n<p>By the way, if you are modelling how the real bank works, it may be tricky to calculate it correctly to the last cent.</p>\n\n<p>The answer that you get from precise analytical formulas like the one above is only approximate. </p>\n\n<p>In practice all monthly amounts (both payment and interest) are usually rounded to the cent. With each month there will be some rounding error, which would accumulate and grow. </p>\n\n<p>Apart from these rounding errors different months have different number of days and even though payments are the same for each month, the interest is usually calculated for each day of the month, so it varies from month to month. Then there are leap years with extra day, which also affects the monthly interest.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "7"}, "answer_3": {"type": "literal", "value": "<p>A simple brute force approach in Python with the option to determine the level of accuracy you want. </p>\n\n<pre><code>\"\"\"\n    Calculate required monthly repayment for a given:\n        - loan amount, and\n        - annual interest rate, and\n        - period of repayments in months\n\n    You can nominate the accuracy required by adjusting the value of\n        ACCURACY_AS_PARTS_OF_CENT. For example:\n        - .01 = accurate to a dollar\n        - .1  = accurate to 10 cents\n        - 1   = accurate to cent\n        - 100 = accurate to 100th of a cent\n\"\"\"\n\n# Set constants.\nLOAN_AMOUNT = 7692\nANNUAL_INTEREST_PERCENT = 18\nREPAY_MONTHS = 12\nACCURACY_AS_PARTS_OF_CENT = 1\n\nloan_amount = int(LOAN_AMOUNT * 100 * ACCURACY_AS_PARTS_OF_CENT)\nmonthly_interest = float(ANNUAL_INTEREST_PERCENT / 100 / 12)\nrepay_guess_min = int((LOAN_AMOUNT / REPAY_MONTHS) - 1)\nresult_found = False\nrepayment_required = 0\n\nfor repay_guess in range(repay_guess_min, loan_amount):\n    if result_found:\n        break\n    loan_balance = loan_amount\n    for _ in range(REPAY_MONTHS):\n        interest_to_add = loan_balance * monthly_interest\n        loan_balance = loan_balance + interest_to_add - repay_guess\n        if loan_balance &lt;= 0:\n            repayment_required = repay_guess / 100 / ACCURACY_AS_PARTS_OF_CENT\n            result_found = True\n            break\n\nprint('Required monthly repayment = $' + str(repayment_required))\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "2"}, "answer_4": {"type": "literal", "value": "<p>Code:</p>\n\n<pre><code>from __future__ import print_function\n\n\"\"\"\nFormulas: http://mathforum.org/dr.math/faq/faq.interest.html\n\"\"\"\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the Principal \n    n   - Number of years\n    q   - the number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.04 for 4% interest)\n    \"\"\"\n    if debug:\n        print('P = %s\\t(amount of the Principal)' %P)\n        print('n = %s\\t\\t(# of years)' %n)\n        print('q = %s\\t\\t(# of periods per year)' %q)\n        print('i = %s %%\\t(Annual interest)' %(i*100))\n    return P*i/( q*(1 - pow(1 + i/q, -n*q)) )\n\n\n### Given :\nP = 7692\nn = 1\nq = 12\ni = 18/100\n\nprint('M = %s' %annuity_monthly_payment(P=P, n=n, q=q, i=i, debug=True))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>P = 7692        (amount of the Principal)\nn = 1           (# of years)\nq = 12          (# of periods per year)\ni = 18.0 %      (Annual interest)\nM = 705.2025054347173\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "5"}, "answer_5": {"type": "literal", "value": "<p>Your python code has some problems.  For one thing, the command to exit is <code>exit()</code>, not <code>exit</code>.  Here is a revised version:</p>\n\n<pre><code>loan_amount= 7692\ninterest = 18\nmonths = 12\n\nfor rg in range(700, 710):\n    y = loan_amount\n    for i in range(months):\n        x = y * interest / 100. / 12.\n        y = y + x - rg\n        if y &lt; 0: \n            print(rg)\n            exit()\n</code></pre>\n\n<p>This prints <code>706</code>, which is the nearest whole number approximation of 705.20.</p>\n\n<p>If you want python code that prints exactly 705.20, that is certainly possible.  However the code will be more complex and take quite a bit of effort to write.  Spreadsheets seem better suited for this job.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "2"}, "answer_6": {"type": "literal", "value": "<p>I think these tabular/vector/matrix type analyses are perfect for numpy and pandas. You often can write more compact code that is also easy to read. See if you agree.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\n\ndef mpmt(amt, i, nper):\n    \"\"\"\n    Calculate the monthly payments on a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return amt*i1**nper*i/(i1**nper-1)\n\ndef ipmt(amt, i, per, nper):\n    \"\"\"\n    Calculate interest paid in a specific period, per, of a loan/mortgage\n    \"\"\"\n    i = i/12  # convert to monthly interest\n    i1 = i + 1  # used multiple times in formula below\n    return (amt*i*(i1**(nper+1)-i1**per))/(i1*(i1**nper-1))\n\ndef amorttable(amt, i, nper):\n    \"\"\"\n    Create an amortization table for a loan/mortgage\n    \"\"\"\n    monthlypmt = mpmt(amt, i, nper)\n\n    # the following calculations are vectorized\n    df = pd.DataFrame({'month':np.arange(1, nper+1)})\n    df['intpaid'] = ipmt(amt, i, df['month'], nper)\n    df['prinpaid'] = monthlypmt - df['intpaid']\n    df['balance'] = amt\n    df['balance'] -= np.cumsum(df['prinpaid'])\n    return df\n\n\nprint(amorttable(7692, .18, 12).round(2))\n</code></pre>\n\n<p>Here's the result:</p>\n\n<pre><code>    month  intpaid  prinpaid  balance\n0       1   115.38    589.82  7102.18\n1       2   106.53    598.67  6503.51\n2       3    97.55    607.65  5895.86\n3       4    88.44    616.76  5279.09\n4       5    79.19    626.02  4653.08\n5       6    69.80    635.41  4017.67\n6       7    60.27    644.94  3372.73\n7       8    50.59    654.61  2718.12\n8       9    40.77    664.43  2053.69\n9      10    30.81    674.40  1379.29\n10     11    20.69    684.51   694.78\n11     12    10.42    694.78    -0.00\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "6"}, "answer_7": {"type": "literal", "value": "<p>I decided to tune it up for myself. So he is the last version i came up with, which now includes a \"<strong>amortization_tab</strong>\" function for printing an amortization table (PrettyTable and CSV formats) and another useful functions: \"<strong>rest_amount</strong>\", \"<strong>amount_can_be_payed_in_n_years</strong>\", \"<strong>years_to_pay_off</strong>\".\nI have added CSV format, so one can generate an initial calculation, import it into Excel and continue there. \nSo now it's a more or less complete library for <strong>annuity</strong> loan/mortgage math.</p>\n\n<p>PS it's been tested with Python v3.5.1, but it should also <em>theoretically</em> work with another Python versions.</p>\n\n<pre><code>from __future__ import print_function\nimport sys\nimport math\nimport io\nimport csv\nimport prettytable\n\n# Formulas:   http://mathforum.org/dr.math/faq/faq.interest.html\n\ndescription = {\n    'P': 'amount of the principal',\n    'i': 'annual interest rate',\n    'n': 'number of years',\n    'q': 'number of times per year that the interest is compounded',\n    'M': 'fixed monthly payment',\n    'k': 'number of \"payed\" payments',\n}\n\n\ndef pr_debug(P=None, i=None, n=None, q=None, M=None, k=None, debug=False):\n    if not debug:\n        return\n\n    columns = ['var','value', 'description']\n    t = prettytable.PrettyTable(columns)\n    t.align['var'] = 'l'\n    t.align['value'] = 'r'\n    t.align['description'] = 'l'\n    t.padding_width = 1\n    t.float_format = '.2'\n\n    if P:\n        t.add_row(['P', P, description['P']])\n    if i:\n        t.add_row(['i', i, description['i']])\n    if n:\n        t.add_row(['n', n, description['n']])\n    if q:\n        t.add_row(['q', q, description['q']])\n    if M:\n        t.add_row(['M', M, description['M']])\n    if k:\n        t.add_row(['k', k, description['k']])\n\n    print(t.get_string() + '\\n')\n\n\ndef annuity_monthly_payment(P, n, q, i, debug = False):\n    \"\"\"\n    Calculates fixed monthly annuity payment\n    P   - amount of the principal \n    n   - number of years\n    q   - number of times per year that the interest is compounded\n    i   - yearly rate of interest (for example: 0.045 for 4.5% interest)\n    \"\"\"\n    pr_debug(P=P, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(P*i/( q*(1 - pow(1 + i/q, -n*q)) ), 2)\n\n\ndef rest_amount(P, M, k, q, i, debug = False):\n    \"\"\"\n    Calculates rest amount after 'k' payed payments \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    k   - # of payments\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, k=k, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round((P - M*q/i) * pow(1 + i/q, k) + M*q/i, 2)\n\n\ndef amount_can_be_payed_in_n_years(M, n, q, i, debug = False):\n    \"\"\"\n    Returns the amount of principal that can be paid off in n years \n    M   - fixed amount that have been payed 'k' times\n    n   - Number of years\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(M=M, n=n, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round( M*(1 - pow(1 + i/q, -n*q) )*q/i, 2)\n\n\ndef years_to_pay_off(P, M, q, i, debug = False):\n    \"\"\"\n    Returns number of years needed to pay off the loan \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    pr_debug(P=P, M=M, q=q, i=i, debug=debug)\n\n    i /= 100\n    return round(-math.log(1 - (P*i/M/q)) / (q*math.log(1 + i/q)), 2)\n\n\ndef amortization_tab(P, n, q, i, M=None, fmt='txt', debug=False):\n    \"\"\"\n    Generates amortization table \n    P   - Principal amount\n    M   - fixed amount that have been payed 'k' times\n    q   - # of periods (12 times per year)\n    i   - yearly interest rate (for example: 0.04 for 4%)\n    \"\"\"\n    assert any(fmt in x for x in ['txt', 'csv'])\n\n    # calculate monthly payment if it's not given\n    if not M:\n        M = annuity_monthly_payment(P=P, n=n, q=q, i=i)\n\n    pr_debug(P=P, M=M, n=n, q=q, i=i, debug=debug)\n\n    # column headers for the output table\n    columns=['pmt#','beg_bal','pmt','interest','applied', 'end_bal']\n\n    i /= 100\n\n    beg_bal = P\n    term = n*q\n\n    if fmt.lower() == 'txt':\n        t = prettytable.PrettyTable(columns)\n        t.align = 'r'\n        t.padding_width = 1\n        t.float_format = '.2'\n    elif fmt.lower() == 'csv':\n        if sys.version_info &gt;= (2,7,0):\n            out = io.StringIO()\n        else:\n            out = io.BytesIO()\n        t = csv.writer(out, quoting=csv.QUOTE_NONNUMERIC)\n        t.writerow(columns)\n\n    for num in range(1, term+1):\n        interest = round(beg_bal*i/q , 2)\n        applied = round(M - interest, 2)\n        end_bal = round(beg_bal - applied,2)\n        row = [num, beg_bal, M, interest, applied, end_bal]\n        if fmt.lower() == 'txt':\n            t.add_row(row)\n        elif fmt.lower() == 'csv':\n            t.writerow(row)\n        beg_bal = end_bal\n\n    if fmt.lower() == 'txt':\n        return t.get_string()\n    elif fmt.lower() == 'csv':\n        return out.getvalue()\n\n############################\nP = 7692.0\nn = 1\nq = 12\ni = 18\nprint(amortization_tab(P, n, q, i, debug=True))\n\nprint('#' * 80)\nprint('#' * 80)\n############################\n# another example\nP = 100000.0\nn = 5\nq = 12\ni = 3.5\nk = 36\nM = 1200\nprint(amortization_tab(P, n, q, i, M, fmt='csv', debug=True))\nprint('*' * 80)\nprint('Rest amount after %s payments:\\t%s' %(k, rest_amount(P=P, M=M, k=k, q=q, i=i)))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>+-----+---------+----------------------------------------------------------+\n| var |   value | description                                              |\n+-----+---------+----------------------------------------------------------+\n| P   | 7692.00 | amount of the principal                                  |\n| i   |      18 | annual interest rate                                     |\n| n   |       1 | number of years                                          |\n| q   |      12 | number of times per year that the interest is compounded |\n| M   |  705.20 | fixed monthly payment                                    |\n+-----+---------+----------------------------------------------------------+\n\n+------+---------+--------+----------+---------+---------+\n| pmt# | beg_bal |    pmt | interest | applied | end_bal |\n+------+---------+--------+----------+---------+---------+\n|    1 | 7692.00 | 705.20 |   115.38 |  589.82 | 7102.18 |\n|    2 | 7102.18 | 705.20 |   106.53 |  598.67 | 6503.51 |\n|    3 | 6503.51 | 705.20 |    97.55 |  607.65 | 5895.86 |\n|    4 | 5895.86 | 705.20 |    88.44 |  616.76 | 5279.10 |\n|    5 | 5279.10 | 705.20 |    79.19 |  626.01 | 4653.09 |\n|    6 | 4653.09 | 705.20 |    69.80 |  635.40 | 4017.69 |\n|    7 | 4017.69 | 705.20 |    60.27 |  644.93 | 3372.76 |\n|    8 | 3372.76 | 705.20 |    50.59 |  654.61 | 2718.15 |\n|    9 | 2718.15 | 705.20 |    40.77 |  664.43 | 2053.72 |\n|   10 | 2053.72 | 705.20 |    30.81 |  674.39 | 1379.33 |\n|   11 | 1379.33 | 705.20 |    20.69 |  684.51 |  694.82 |\n|   12 |  694.82 | 705.20 |    10.42 |  694.78 |    0.04 |\n+------+---------+--------+----------+---------+---------+\n################################################################################\n################################################################################\n+-----+-----------+----------------------------------------------------------+\n| var |     value | description                                              |\n+-----+-----------+----------------------------------------------------------+\n| P   | 100000.00 | amount of the principal                                  |\n| i   |      3.50 | annual interest rate                                     |\n| n   |         5 | number of years                                          |\n| q   |        12 | number of times per year that the interest is compounded |\n| M   |      1200 | fixed monthly payment                                    |\n+-----+-----------+----------------------------------------------------------+\n\n\"pmt#\",\"beg_bal\",\"pmt\",\"interest\",\"applied\",\"end_bal\"\n1,100000.0,1200,291.67,908.33,99091.67\n2,99091.67,1200,289.02,910.98,98180.69\n3,98180.69,1200,286.36,913.64,97267.05\n4,97267.05,1200,283.7,916.3,96350.75\n5,96350.75,1200,281.02,918.98,95431.77\n6,95431.77,1200,278.34,921.66,94510.11\n7,94510.11,1200,275.65,924.35,93585.76\n8,93585.76,1200,272.96,927.04,92658.72\n9,92658.72,1200,270.25,929.75,91728.97\n10,91728.97,1200,267.54,932.46,90796.51\n11,90796.51,1200,264.82,935.18,89861.33\n12,89861.33,1200,262.1,937.9,88923.43\n13,88923.43,1200,259.36,940.64,87982.79\n14,87982.79,1200,256.62,943.38,87039.41\n15,87039.41,1200,253.86,946.14,86093.27\n16,86093.27,1200,251.11,948.89,85144.38\n17,85144.38,1200,248.34,951.66,84192.72\n18,84192.72,1200,245.56,954.44,83238.28\n19,83238.28,1200,242.78,957.22,82281.06\n20,82281.06,1200,239.99,960.01,81321.05\n21,81321.05,1200,237.19,962.81,80358.24\n22,80358.24,1200,234.38,965.62,79392.62\n23,79392.62,1200,231.56,968.44,78424.18\n24,78424.18,1200,228.74,971.26,77452.92\n25,77452.92,1200,225.9,974.1,76478.82\n26,76478.82,1200,223.06,976.94,75501.88\n27,75501.88,1200,220.21,979.79,74522.09\n28,74522.09,1200,217.36,982.64,73539.45\n29,73539.45,1200,214.49,985.51,72553.94\n30,72553.94,1200,211.62,988.38,71565.56\n31,71565.56,1200,208.73,991.27,70574.29\n32,70574.29,1200,205.84,994.16,69580.13\n33,69580.13,1200,202.94,997.06,68583.07\n34,68583.07,1200,200.03,999.97,67583.1\n35,67583.1,1200,197.12,1002.88,66580.22\n36,66580.22,1200,194.19,1005.81,65574.41\n37,65574.41,1200,191.26,1008.74,64565.67\n38,64565.67,1200,188.32,1011.68,63553.99\n39,63553.99,1200,185.37,1014.63,62539.36\n40,62539.36,1200,182.41,1017.59,61521.77\n41,61521.77,1200,179.44,1020.56,60501.21\n42,60501.21,1200,176.46,1023.54,59477.67\n43,59477.67,1200,173.48,1026.52,58451.15\n44,58451.15,1200,170.48,1029.52,57421.63\n45,57421.63,1200,167.48,1032.52,56389.11\n46,56389.11,1200,164.47,1035.53,55353.58\n47,55353.58,1200,161.45,1038.55,54315.03\n48,54315.03,1200,158.42,1041.58,53273.45\n49,53273.45,1200,155.38,1044.62,52228.83\n50,52228.83,1200,152.33,1047.67,51181.16\n51,51181.16,1200,149.28,1050.72,50130.44\n52,50130.44,1200,146.21,1053.79,49076.65\n53,49076.65,1200,143.14,1056.86,48019.79\n54,48019.79,1200,140.06,1059.94,46959.85\n55,46959.85,1200,136.97,1063.03,45896.82\n56,45896.82,1200,133.87,1066.13,44830.69\n57,44830.69,1200,130.76,1069.24,43761.45\n58,43761.45,1200,127.64,1072.36,42689.09\n59,42689.09,1200,124.51,1075.49,41613.6\n60,41613.6,1200,121.37,1078.63,40534.97\n\n********************************************************************************\nRest amount after 36 payments:  65574.41\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "2"}, "answer_8": {"type": "literal", "value": "<pre><code>import numpy as np\n\nfor pmt in np.linspace(200, 800, 20):\n    loan = 7692.00\n    for n in range(1, 13):\n        new_balance = loan + ((loan*(1+(0.18/12)))-loan) - pmt\n        loan = new_balance\n    print(round(pmt, 2), '-&gt;', round(loan,2))\n</code></pre>\n\n<p>The first column shows what the equal 12 month payment would be and the right column shows what the balance would be after 12 months.  See how the balance approaches zero around 705.26?  That indicates zero is somewhere around there.</p>\n\n<pre><code>200.0 -&gt; 6588.45\n231.58 -&gt; 6176.62\n263.16 -&gt; 5764.8\n294.74 -&gt; 5352.97\n326.32 -&gt; 4941.14\n357.89 -&gt; 4529.31\n389.47 -&gt; 4117.49\n421.05 -&gt; 3705.66\n452.63 -&gt; 3293.83\n484.21 -&gt; 2882.0\n515.79 -&gt; 2470.18\n547.37 -&gt; 2058.35\n578.95 -&gt; 1646.52\n610.53 -&gt; 1234.69\n642.11 -&gt; 822.86\n673.68 -&gt; 411.04\n705.26 -&gt; -0.79\n736.84 -&gt; -412.62\n768.42 -&gt; -824.45\n800.0 -&gt; -1236.27\n</code></pre>\n\n<p>I had a similar <a href=\"https://stackoverflow.com/questions/33160262/linear-programming-simplex-lp-pulp\">question using linear programming</a>.  Might be worth checking out.</p>\n"}, "answer_8_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>Here is a simple calculation that I do in Excel. I will like to know if it can be done python or any other language. </p>\n\n<pre> </pre>\n\n<p>The column C is tentative amount the borrower may need to repay each month. All other cells next to C2 simply points to the first installment of 200. After using the solver as shown in the following image, I get the correct installment of 705.20 in the C column. </p>\n\n<p><a href=\"https://i.stack.imgur.com/elmP8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/elmP8.png\" alt=\"excel goal seak\"></a></p>\n\n<p>I will like to know if this calculation can be done using any scripting language like python (or SQL)</p>\n\n<p>Here is how the final version looks like...</p>\n\n<p><a href=\"https://i.stack.imgur.com/M0t9I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/M0t9I.png\" alt=\"enter image description here\"></a></p>\n\n<p>I tried something like this, but it does not exit the loop and prints all combinations.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>As the title/tag also mentioned SQL, I will post an SQL solution:</p>\n\n<pre> </pre>\n\n<p>Result: </p>\n\n<pre> </pre>\n\n<p><a href=\"http://sqlfiddle.com/#!9/56254f/5\" rel=\"nofollow\">SQL Fiddle</a>.    </p>\n\n<p>If you want to get the whole amortization schedule, then an idea would be to first create a table with sequential month numbers (1, 2, ...), enough to cover for the longest loan pay-off duration you would have data for:</p>\n\n<pre> </pre>\n\n<p>Then use the following query, which has the above mentioned query as sub-select:</p>\n\n<pre> </pre>\n\n<p>This produces the following output:</p>\n\n<pre> </pre>\n\n<p>Here is the <a href=\"http://sqlfiddle.com/#!9/ddaa67/7\" rel=\"nofollow\">SQL fiddle</a>.</p>\n\n<p>The formula used is provided and derived in <a href=\"https://en.wikipedia.org/wiki/Amortization_calculator#The_formula\" rel=\"nofollow\">this Wikipedia article</a>.</p>\n\n\n<p>As the title/tag also mentioned SQL, I will post an SQL solution:</p>\n\n<pre> </pre>\n\n<p>Result: </p>\n\n<pre> </pre>\n\n<p><a href=\"http://sqlfiddle.com/#!9/56254f/5\" rel=\"nofollow\">SQL Fiddle</a>.    </p>\n\n<p>If you want to get the whole amortization schedule, then an idea would be to first create a table with sequential month numbers (1, 2, ...), enough to cover for the longest loan pay-off duration you would have data for:</p>\n\n<pre> </pre>\n\n<p>Then use the following query, which has the above mentioned query as sub-select:</p>\n\n<pre> </pre>\n\n<p>This produces the following output:</p>\n\n<pre> </pre>\n\n<p>Here is the <a href=\"http://sqlfiddle.com/#!9/ddaa67/7\" rel=\"nofollow\">SQL fiddle</a>.</p>\n\n<p>The formula used is provided and derived in <a href=\"https://en.wikipedia.org/wiki/Amortization_calculator#The_formula\" rel=\"nofollow\">this Wikipedia article</a>.</p>\n\n\n<p>Well, you can solve it using numerical method (as Excel does), you can solve it with brute force by checking every amount with some step within some range, or you can solve it analytically on a piece of paper.</p>\n\n<p>Using the following notation</p>\n\n<pre> </pre>\n\n<p><a href=\"https://i.stack.imgur.com/hc5r1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/hc5r1.png\" alt=\"L_{n}\"></a> is loan amount after the  -th month. <a href=\"https://i.stack.imgur.com/Fjn3W.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Fjn3W.png\" alt=\"L_{0}\"></a> is the initial loan amount (7692). <a href=\"https://i.stack.imgur.com/UQGZ9.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UQGZ9.png\" alt=\"L_{m}\"></a> is the loan amount after   months (0).</p>\n\n<p>The main relation between  -th and  -th month is:</p>\n\n<p><a href=\"https://i.stack.imgur.com/Tx4kh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Tx4kh.png\" alt=\"L_{n} = L_{n-1} * R - P\"></a></p>\n\n<p>So, analytical formula turns out to be:</p>\n\n<p><a href=\"https://i.stack.imgur.com/abGdN.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/abGdN.png\" alt=\"P = L * \\frac{R^{m}}{\\sum_{k=0}^{m-1}R^{k}} = L * R^{m} * \\frac{R-1}{R^{m}-1}\"></a></p>\n\n<p>Now it should be fairly straight-forward to calculate it in any programming language.</p>\n\n<p>For the given initial parameters </p>\n\n<p><a href=\"https://i.stack.imgur.com/9BXBX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9BXBX.png\" alt=\"R = 1 + \\frac{0.18}{12} = 1.015\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/nATZE.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/nATZE.png\" alt=\"P = 7692 * 1.015^{12} * \\frac{1.015-1}{1.015^{12}-1}\\approx 705.2025054\"></a></p>\n\n<hr>\n\n<p>By the way, if you are modelling how the real bank works, it may be tricky to calculate it correctly to the last cent.</p>\n\n<p>The answer that you get from precise analytical formulas like the one above is only approximate. </p>\n\n<p>In practice all monthly amounts (both payment and interest) are usually rounded to the cent. With each month there will be some rounding error, which would accumulate and grow. </p>\n\n<p>Apart from these rounding errors different months have different number of days and even though payments are the same for each month, the interest is usually calculated for each day of the month, so it varies from month to month. Then there are leap years with extra day, which also affects the monthly interest.</p>\n\n\n<p>Well, you can solve it using numerical method (as Excel does), you can solve it with brute force by checking every amount with some step within some range, or you can solve it analytically on a piece of paper.</p>\n\n<p>Using the following notation</p>\n\n<pre> </pre>\n\n<p><a href=\"https://i.stack.imgur.com/hc5r1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/hc5r1.png\" alt=\"L_{n}\"></a> is loan amount after the  -th month. <a href=\"https://i.stack.imgur.com/Fjn3W.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Fjn3W.png\" alt=\"L_{0}\"></a> is the initial loan amount (7692). <a href=\"https://i.stack.imgur.com/UQGZ9.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UQGZ9.png\" alt=\"L_{m}\"></a> is the loan amount after   months (0).</p>\n\n<p>The main relation between  -th and  -th month is:</p>\n\n<p><a href=\"https://i.stack.imgur.com/Tx4kh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Tx4kh.png\" alt=\"L_{n} = L_{n-1} * R - P\"></a></p>\n\n<p>So, analytical formula turns out to be:</p>\n\n<p><a href=\"https://i.stack.imgur.com/abGdN.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/abGdN.png\" alt=\"P = L * \\frac{R^{m}}{\\sum_{k=0}^{m-1}R^{k}} = L * R^{m} * \\frac{R-1}{R^{m}-1}\"></a></p>\n\n<p>Now it should be fairly straight-forward to calculate it in any programming language.</p>\n\n<p>For the given initial parameters </p>\n\n<p><a href=\"https://i.stack.imgur.com/9BXBX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9BXBX.png\" alt=\"R = 1 + \\frac{0.18}{12} = 1.015\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/nATZE.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/nATZE.png\" alt=\"P = 7692 * 1.015^{12} * \\frac{1.015-1}{1.015^{12}-1}\\approx 705.2025054\"></a></p>\n\n<hr>\n\n<p>By the way, if you are modelling how the real bank works, it may be tricky to calculate it correctly to the last cent.</p>\n\n<p>The answer that you get from precise analytical formulas like the one above is only approximate. </p>\n\n<p>In practice all monthly amounts (both payment and interest) are usually rounded to the cent. With each month there will be some rounding error, which would accumulate and grow. </p>\n\n<p>Apart from these rounding errors different months have different number of days and even though payments are the same for each month, the interest is usually calculated for each day of the month, so it varies from month to month. Then there are leap years with extra day, which also affects the monthly interest.</p>\n\n\n<p>A simple brute force approach in Python with the option to determine the level of accuracy you want. </p>\n\n<pre> </pre>\n\n\n<p>A simple brute force approach in Python with the option to determine the level of accuracy you want. </p>\n\n<pre> </pre>\n\n\n<p>Code:</p>\n\n<pre> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<p>Code:</p>\n\n<pre> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<p>Your python code has some problems.  For one thing, the command to exit is  , not  .  Here is a revised version:</p>\n\n<pre> </pre>\n\n<p>This prints  , which is the nearest whole number approximation of 705.20.</p>\n\n<p>If you want python code that prints exactly 705.20, that is certainly possible.  However the code will be more complex and take quite a bit of effort to write.  Spreadsheets seem better suited for this job.</p>\n\n\n<p>Your python code has some problems.  For one thing, the command to exit is  , not  .  Here is a revised version:</p>\n\n<pre> </pre>\n\n<p>This prints  , which is the nearest whole number approximation of 705.20.</p>\n\n<p>If you want python code that prints exactly 705.20, that is certainly possible.  However the code will be more complex and take quite a bit of effort to write.  Spreadsheets seem better suited for this job.</p>\n\n\n<p>I think these tabular/vector/matrix type analyses are perfect for numpy and pandas. You often can write more compact code that is also easy to read. See if you agree.</p>\n\n<pre> </pre>\n\n<p>Here's the result:</p>\n\n<pre> </pre>\n\n\n<p>I think these tabular/vector/matrix type analyses are perfect for numpy and pandas. You often can write more compact code that is also easy to read. See if you agree.</p>\n\n<pre> </pre>\n\n<p>Here's the result:</p>\n\n<pre> </pre>\n\n\n<p>I decided to tune it up for myself. So he is the last version i came up with, which now includes a \"<strong>amortization_tab</strong>\" function for printing an amortization table (PrettyTable and CSV formats) and another useful functions: \"<strong>rest_amount</strong>\", \"<strong>amount_can_be_payed_in_n_years</strong>\", \"<strong>years_to_pay_off</strong>\".\nI have added CSV format, so one can generate an initial calculation, import it into Excel and continue there. \nSo now it's a more or less complete library for <strong>annuity</strong> loan/mortgage math.</p>\n\n<p>PS it's been tested with Python v3.5.1, but it should also <em>theoretically</em> work with another Python versions.</p>\n\n<pre> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<p>I decided to tune it up for myself. So he is the last version i came up with, which now includes a \"<strong>amortization_tab</strong>\" function for printing an amortization table (PrettyTable and CSV formats) and another useful functions: \"<strong>rest_amount</strong>\", \"<strong>amount_can_be_payed_in_n_years</strong>\", \"<strong>years_to_pay_off</strong>\".\nI have added CSV format, so one can generate an initial calculation, import it into Excel and continue there. \nSo now it's a more or less complete library for <strong>annuity</strong> loan/mortgage math.</p>\n\n<p>PS it's been tested with Python v3.5.1, but it should also <em>theoretically</em> work with another Python versions.</p>\n\n<pre> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>The first column shows what the equal 12 month payment would be and the right column shows what the balance would be after 12 months.  See how the balance approaches zero around 705.26?  That indicates zero is somewhere around there.</p>\n\n<pre> </pre>\n\n<p>I had a similar <a href=\"https://stackoverflow.com/questions/33160262/linear-programming-simplex-lp-pulp\">question using linear programming</a>.  Might be worth checking out.</p>\n\n\n<pre> </pre>\n\n<p>The first column shows what the equal 12 month payment would be and the right column shows what the balance would be after 12 months.  See how the balance approaches zero around 705.26?  That indicates zero is somewhere around there.</p>\n\n<pre> </pre>\n\n<p>I had a similar <a href=\"https://stackoverflow.com/questions/33160262/linear-programming-simplex-lp-pulp\">question using linear programming</a>.  Might be worth checking out.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/datetime.date"}, "class_func_label": {"type": "literal", "value": "datetime.date"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "date(year, month, day) --> date object"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/466345"}, "title": {"type": "literal", "value": "Converting string into datetime"}, "content": {"type": "literal", "value": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre><code>Jun 1 2005  1:33PM\nAug 28 1999 12:00AM\n</code></pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n\n<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n\n\n<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n\n\n<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "25"}, "answer_2": {"type": "literal", "value": "<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "5"}, "answer_3": {"type": "literal", "value": "<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n"}, "answer_3_votes": {"type": "literal", "value": "3119"}, "answer_4": {"type": "literal", "value": "<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "749"}, "answer_5": {"type": "literal", "value": "<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "4"}, "answer_6": {"type": "literal", "value": "<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "15"}, "answer_7": {"type": "literal", "value": "<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "33"}, "answer_8": {"type": "literal", "value": "<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "13"}, "answer_9": {"type": "literal", "value": "<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "106"}, "answer_10": {"type": "literal", "value": "<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "6"}, "answer_11": {"type": "literal", "value": "<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "2"}, "answer_12": {"type": "literal", "value": "<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "21"}, "answer_13": {"type": "literal", "value": "<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": "44"}, "answer_14": {"type": "literal", "value": "<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n"}, "answer_14_votes": {"type": "literal", "value": "21"}, "answer_15": {"type": "literal", "value": "<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n"}, "answer_15_votes": {"type": "literal", "value": "7"}, "answer_16": {"type": "literal", "value": "<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n"}, "answer_16_votes": {"type": "literal", "value": "498"}, "answer_17": {"type": "literal", "value": "<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n"}, "answer_17_votes": {"type": "literal", "value": "1"}, "answer_18": {"type": "literal", "value": "<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n"}, "answer_18_votes": {"type": "literal", "value": "25"}, "answer_19": {"type": "literal", "value": "<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_19_votes": {"type": "literal", "value": "10"}, "content_wo_code": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre> </pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n", "answer_wo_code": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre> </pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre> </pre>\n\n\n<p>  is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre> </pre>\n\n<p>The resulting   object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for  : <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for  /  format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>  = \"string parse time\"</li>\n<li>  = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre> </pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre> </pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre> </pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre> </pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre> </pre>\n\n<p>This conversion is very important for Django and Python when you have  :</p>\n\n<pre> </pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n \n\n<pre> </pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre> </pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from  ) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre> </pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>,   could be used.</p>\n\n<pre> </pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object =  </p>\n\n<p>datetime object to other formats =  </p>\n\n<p> </p>\n\n<p>is equals to</p>\n\n<p> </p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting   to </p>\n\n<pre> </pre>\n\n<p>Output</p>\n\n<pre> </pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre> </pre>\n\n<p>OutPut</p>\n\n<pre> </pre>\n\n\n<p>I personally like the solution using the   module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with  .</p>\n\n<pre> </pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the   method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre> </pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n<p>By using   option in   mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre> </pre>\n\n<p><strong>Timings</strong></p>\n\n<pre> </pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre> </pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using  , so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to  </p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre> </pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/datetime.datetime"}, "class_func_label": {"type": "literal", "value": "datetime.datetime"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/466345"}, "title": {"type": "literal", "value": "Converting string into datetime"}, "content": {"type": "literal", "value": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre><code>Jun 1 2005  1:33PM\nAug 28 1999 12:00AM\n</code></pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n\n<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n\n\n<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n\n\n<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "25"}, "answer_2": {"type": "literal", "value": "<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "5"}, "answer_3": {"type": "literal", "value": "<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n"}, "answer_3_votes": {"type": "literal", "value": "3119"}, "answer_4": {"type": "literal", "value": "<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "749"}, "answer_5": {"type": "literal", "value": "<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "4"}, "answer_6": {"type": "literal", "value": "<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "15"}, "answer_7": {"type": "literal", "value": "<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "33"}, "answer_8": {"type": "literal", "value": "<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "13"}, "answer_9": {"type": "literal", "value": "<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "106"}, "answer_10": {"type": "literal", "value": "<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "6"}, "answer_11": {"type": "literal", "value": "<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "2"}, "answer_12": {"type": "literal", "value": "<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "21"}, "answer_13": {"type": "literal", "value": "<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": "44"}, "answer_14": {"type": "literal", "value": "<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n"}, "answer_14_votes": {"type": "literal", "value": "21"}, "answer_15": {"type": "literal", "value": "<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n"}, "answer_15_votes": {"type": "literal", "value": "7"}, "answer_16": {"type": "literal", "value": "<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n"}, "answer_16_votes": {"type": "literal", "value": "498"}, "answer_17": {"type": "literal", "value": "<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n"}, "answer_17_votes": {"type": "literal", "value": "1"}, "answer_18": {"type": "literal", "value": "<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n"}, "answer_18_votes": {"type": "literal", "value": "25"}, "answer_19": {"type": "literal", "value": "<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_19_votes": {"type": "literal", "value": "10"}, "content_wo_code": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre> </pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n", "answer_wo_code": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre> </pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre> </pre>\n\n\n<p>  is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre> </pre>\n\n<p>The resulting   object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for  : <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for  /  format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>  = \"string parse time\"</li>\n<li>  = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre> </pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre> </pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre> </pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre> </pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre> </pre>\n\n<p>This conversion is very important for Django and Python when you have  :</p>\n\n<pre> </pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n \n\n<pre> </pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre> </pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from  ) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre> </pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>,   could be used.</p>\n\n<pre> </pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object =  </p>\n\n<p>datetime object to other formats =  </p>\n\n<p> </p>\n\n<p>is equals to</p>\n\n<p> </p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting   to </p>\n\n<pre> </pre>\n\n<p>Output</p>\n\n<pre> </pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre> </pre>\n\n<p>OutPut</p>\n\n<pre> </pre>\n\n\n<p>I personally like the solution using the   module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with  .</p>\n\n<pre> </pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the   method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre> </pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n<p>By using   option in   mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre> </pre>\n\n<p><strong>Timings</strong></p>\n\n<pre> </pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre> </pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using  , so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to  </p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre> </pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.to_datetime"}, "class_func_label": {"type": "literal", "value": "pandas.to_datetime"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nConvert argument to datetime.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/466345"}, "title": {"type": "literal", "value": "Converting string into datetime"}, "content": {"type": "literal", "value": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre><code>Jun 1 2005  1:33PM\nAug 28 1999 12:00AM\n</code></pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n\n<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n\n\n<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n\n\n<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "25"}, "answer_2": {"type": "literal", "value": "<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "5"}, "answer_3": {"type": "literal", "value": "<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n"}, "answer_3_votes": {"type": "literal", "value": "3119"}, "answer_4": {"type": "literal", "value": "<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "749"}, "answer_5": {"type": "literal", "value": "<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "4"}, "answer_6": {"type": "literal", "value": "<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "15"}, "answer_7": {"type": "literal", "value": "<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "33"}, "answer_8": {"type": "literal", "value": "<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "13"}, "answer_9": {"type": "literal", "value": "<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "106"}, "answer_10": {"type": "literal", "value": "<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "6"}, "answer_11": {"type": "literal", "value": "<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "2"}, "answer_12": {"type": "literal", "value": "<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "21"}, "answer_13": {"type": "literal", "value": "<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": "44"}, "answer_14": {"type": "literal", "value": "<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n"}, "answer_14_votes": {"type": "literal", "value": "21"}, "answer_15": {"type": "literal", "value": "<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n"}, "answer_15_votes": {"type": "literal", "value": "7"}, "answer_16": {"type": "literal", "value": "<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n"}, "answer_16_votes": {"type": "literal", "value": "498"}, "answer_17": {"type": "literal", "value": "<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n"}, "answer_17_votes": {"type": "literal", "value": "1"}, "answer_18": {"type": "literal", "value": "<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n"}, "answer_18_votes": {"type": "literal", "value": "25"}, "answer_19": {"type": "literal", "value": "<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_19_votes": {"type": "literal", "value": "10"}, "content_wo_code": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre> </pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n", "answer_wo_code": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre> </pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre> </pre>\n\n\n<p>  is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre> </pre>\n\n<p>The resulting   object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for  : <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for  /  format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>  = \"string parse time\"</li>\n<li>  = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre> </pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre> </pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre> </pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre> </pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre> </pre>\n\n<p>This conversion is very important for Django and Python when you have  :</p>\n\n<pre> </pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n \n\n<pre> </pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre> </pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from  ) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre> </pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>,   could be used.</p>\n\n<pre> </pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object =  </p>\n\n<p>datetime object to other formats =  </p>\n\n<p> </p>\n\n<p>is equals to</p>\n\n<p> </p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting   to </p>\n\n<pre> </pre>\n\n<p>Output</p>\n\n<pre> </pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre> </pre>\n\n<p>OutPut</p>\n\n<pre> </pre>\n\n\n<p>I personally like the solution using the   module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with  .</p>\n\n<pre> </pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the   method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre> </pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n<p>By using   option in   mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre> </pre>\n\n<p><strong>Timings</strong></p>\n\n<pre> </pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre> </pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using  , so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to  </p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre> </pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/time.strptime"}, "class_func_label": {"type": "literal", "value": "time.strptime"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "strptime(string, format) -> struct_time\n\nParse a string to a time tuple according to a format specification.\nSee the library reference manual for formatting codes (same as\nstrftime()).\n\nCommonly used format codes:\n\n%Y  Year with century as a decimal number.\n%m  Month as a decimal number [01,12].\n%d  Day of the month as a decimal number [01,31].\n%H  Hour (24-hour clock) as a decimal number [00,23].\n%M  Minute as a decimal number [00,59].\n%S  Second as a decimal number [00,61].\n%z  Time zone offset from UTC.\n%a  Locale's abbreviated weekday name.\n%A  Locale's full weekday name.\n%b  Locale's abbreviated month name.\n%B  Locale's full month name.\n%c  Locale's appropriate date and time representation.\n%I  Hour (12-hour clock) as a decimal number [01,12].\n%p  Locale's equivalent of either AM or PM.\n\nOther codes may be available on your platform.  See documentation for\nthe C library strftime function."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/466345"}, "title": {"type": "literal", "value": "Converting string into datetime"}, "content": {"type": "literal", "value": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre><code>Jun 1 2005  1:33PM\nAug 28 1999 12:00AM\n</code></pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n\n<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n\n\n<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n\n\n<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre><code>import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens &gt; 10 and tens &lt; 20:\n        return suf[0] # teens with 'th'\n    elif units &lt;= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "25"}, "answer_2": {"type": "literal", "value": "<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre><code>import date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "5"}, "answer_3": {"type": "literal", "value": "<p><code>datetime.strptime</code> is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre><code>from datetime import datetime\n\ndatetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n</code></pre>\n\n<p>The resulting <code>datetime</code> object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for <code>strptime</code>: <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for <code>strptime</code>/<code>strftime</code> format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li><code>strptime</code> = \"string parse time\"</li>\n<li><code>strftime</code> = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n"}, "answer_3_votes": {"type": "literal", "value": "3119"}, "answer_4": {"type": "literal", "value": "<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre><code>from dateutil import parser\ndt = parser.parse(\"Aug 28 1999 12:00AM\")\n</code></pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre><code>pip install python-dateutil\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "749"}, "answer_5": {"type": "literal", "value": "<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; date = datetime.date(int('2017'),int('12'),int('21'))\n&gt;&gt;&gt; date\ndatetime.date(2017, 12, 21)\n&gt;&gt;&gt; type(date)\n&lt;type 'datetime.date'&gt;\n</code></pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre><code>selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\n</code></pre>\n\n<p>You will get the resulting value in date format.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "4"}, "answer_6": {"type": "literal", "value": "<p>Django Timezone aware datetime object example.</p>\n\n<pre><code>import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\n</code></pre>\n\n<p>This conversion is very important for Django and Python when you have <code>USE_TZ = True</code>:</p>\n\n<pre><code>RuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "15"}, "answer_7": {"type": "literal", "value": "<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre><code>&gt;&gt;&gt; datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "33"}, "answer_8": {"type": "literal", "value": "<pre><code>In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "13"}, "answer_9": {"type": "literal", "value": "<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n<code>pip install timestring</code>\n\n<pre><code>&gt;&gt;&gt; import timestring\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n&lt;timestring.Date 2015-08-15 20:40:00 4491909392&gt;\n&gt;&gt;&gt; timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n&gt;&gt;&gt; timestring.Range('next week')\n&lt;timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880&gt;\n&gt;&gt;&gt; (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "106"}, "answer_10": {"type": "literal", "value": "<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre><code>def convert_string_to_time(date_string, timezone):\n    from datetime import datetime\n    import pytz\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "6"}, "answer_11": {"type": "literal", "value": "<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from <code>strptime()</code>) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre><code>def try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "2"}, "answer_12": {"type": "literal", "value": "<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>, <code>datetime.fromisoformat</code> could be used.</p>\n\n<pre><code>&gt;&gt;&gt; from datetime import datetime\n\n&gt;&gt;&gt; date_string = \"2012-12-12 10:10:10\"\n&gt;&gt;&gt; print (datetime.fromisoformat(date_string))\n&gt;&gt;&gt; 2012-12-12 10:10:10\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "21"}, "answer_13": {"type": "literal", "value": "<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object = <code>strptime</code></p>\n\n<p>datetime object to other formats = <code>strftime</code></p>\n\n<p><code>Jun 1 2005  1:33PM</code></p>\n\n<p>is equals to</p>\n\n<p><code>%b %d %Y %I:%M%p</code></p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting <code>string</code> to </p>\n\n<pre><code>&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('Jun 1 2005  1:33PM')\n&gt;&gt;&gt; dates.append('Aug 28 1999 12:00AM')\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n</code></pre>\n\n<p>Output</p>\n\n<pre><code>&lt;type 'datetime.datetime'&gt;\n2005-06-01 13:33:00\n&lt;type 'datetime.datetime'&gt;\n1999-08-28 00:00:00\n</code></pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre><code>&gt;&gt;&gt; import dateutil\n&gt;&gt;&gt; dates = []\n&gt;&gt;&gt; dates.append('12 1 2017')\n&gt;&gt;&gt; dates.append('1 1 2017')\n&gt;&gt;&gt; dates.append('1 12 2017')\n&gt;&gt;&gt; dates.append('June 1 2017 1:30:00AM')\n&gt;&gt;&gt; [parser.parse(x) for x in dates]\n</code></pre>\n\n<p>OutPut</p>\n\n<pre><code>[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": "44"}, "answer_14": {"type": "literal", "value": "<p>I personally like the solution using the <code>parser</code> module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with <code>strptime</code>.</p>\n\n<pre><code>from dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\n&gt;10.70296801342902\n&gt;1.3627995655316933\n</code></pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the <code>parser</code> method is more convenient and will handle most of the time formats automatically.</p>\n"}, "answer_14_votes": {"type": "literal", "value": "21"}, "answer_15": {"type": "literal", "value": "<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre><code>&gt;&gt;&gt; import arrow\n&gt;&gt;&gt; dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n&gt;&gt;&gt; for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n</code></pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n"}, "answer_15_votes": {"type": "literal", "value": "7"}, "answer_16": {"type": "literal", "value": "<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre><code>$ python\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n</code></pre>\n"}, "answer_16_votes": {"type": "literal", "value": "498"}, "answer_17": {"type": "literal", "value": "<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\n</code></pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null object\n\nLast Login Time      1000 non-null object\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n</code></pre>\n\n<p>By using <code>parse_dates</code> option in <code>read_csv</code> mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre><code>emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n</code></pre>\n\n<blockquote>\n<pre><code>Start Date           1000 non-null datetime64[ns]\nLast Login Time      1000 non-null datetime64[ns]\n</code></pre>\n</blockquote>\n\n<pre><code>Salary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n</code></pre>\n"}, "answer_17_votes": {"type": "literal", "value": "1"}, "answer_18": {"type": "literal", "value": "<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre><code>import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n&gt;&gt;&gt; [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n&gt;&gt;&gt; pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n</code></pre>\n\n<p><strong>Timings</strong></p>\n\n<pre><code>dates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n&gt;&gt;&gt; %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n&gt;&gt;&gt; %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\n</code></pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre><code>datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n&gt;&gt;&gt; pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\n</code></pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using <code>to_datetime</code>, so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to <code>.date</code></p>\n"}, "answer_18_votes": {"type": "literal", "value": "25"}, "answer_19": {"type": "literal", "value": "<p>Create a small utility function like:</p>\n\n<pre><code>def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\n</code></pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, "answer_19_votes": {"type": "literal", "value": "10"}, "content_wo_code": "<p>Short and simple. I've got a huge list of date-times like this as strings:</p>\n\n<pre> </pre>\n\n<p>I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. </p>\n\n<p>Any help (even if it's just a kick in the right direction) would be appreciated.</p>\n\n<p>Edit: This is going through Django's ORM so I can't use SQL to do the conversion on insert.</p>\n", "answer_wo_code": "<p>Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.</p>\n\n<pre> </pre>\n\n\n<p>You can use <a href=\"https://github.com/ralphavalon/easy_date\" rel=\"noreferrer\">easy_date</a> to make it easy:</p>\n\n<pre> </pre>\n\n\n<p>  is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:</p>\n\n<pre> </pre>\n\n<p>The resulting   object is timezone-naive.</p>\n\n<p>Links:</p>\n\n<ul>\n<li><p>Python documentation for  : <a href=\"https://docs.python.org/2/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\" title=\"datetime.datetime.strptime\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p>Python documentation for  /  format strings: <a href=\"https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\" title=\"strftime-and-strptime-behavior\">Python 2</a>, <a href=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\" rel=\"noreferrer\">Python 3</a></p></li>\n<li><p><a href=\"http://strftime.org/\" rel=\"noreferrer\">strftime.org</a> is also a really nice reference for strftime</p></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>  = \"string parse time\"</li>\n<li>  = \"string format time\"</li>\n<li>Pronounce it out loud today &amp; you won't have to search for it again in 6 months.</li>\n</ul>\n\n\n<p>Use the third party <a href=\"http://labix.org/python-dateutil\" rel=\"noreferrer\">dateutil</a> library:</p>\n\n<pre> </pre>\n\n<p>It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.</p>\n\n<p>It very useful for writing tests, where readability is more important than performance.</p>\n\n<p>You can install it with:</p>\n\n<pre> </pre>\n\n\n<p>If you want only date format then you can manually convert it by passing your individual fields like:</p>\n\n<pre> </pre>\n\n<p>You can pass your split string values to convert it into date type like:</p>\n\n<pre> </pre>\n\n<p>You will get the resulting value in date format.</p>\n\n\n<p>Django Timezone aware datetime object example.</p>\n\n<pre> </pre>\n\n<p>This conversion is very important for Django and Python when you have  :</p>\n\n<pre> </pre>\n\n\n<p>Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.</p>\n\n<p>Python 3.2+:</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n\n<p>I have put together a project that can convert some really neat expressions. Check out <strong><a href=\"http://github.com/stevepeak/timestring\" rel=\"noreferrer\">timestring</a></strong>. </p>\n\n<h2>Here are some examples below:</h2>\n\n \n\n<pre> </pre>\n\n\n<p>It would do the helpful for converting string to datetime and also with time zone</p>\n\n<pre> </pre>\n\n\n<p>See <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my answer</a>.</p>\n\n<p>In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.</p>\n\n<p>We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from  ) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From <a href=\"https://stackoverflow.com/questions/6615533/is-there-a-python-equivalent-to-cs-datetime-tryparse/47876446#47876446\">my solution</a></p>\n\n<pre> </pre>\n\n\n<p>In Python >= 3.7.0,</p>\n\n<p>to convert <strong>YYYY-MM-DD string to datetime object</strong>,   could be used.</p>\n\n<pre> </pre>\n\n\n<p>Remember this and you didn't need to get confused in datetime conversion again.</p>\n\n<p>String to datetime object =  </p>\n\n<p>datetime object to other formats =  </p>\n\n<p> </p>\n\n<p>is equals to</p>\n\n<p> </p>\n\n<blockquote>\n  <p>%b    Month as locale\u2019s abbreviated name(Jun)</p>\n  \n  <p>%d    Day of the month as a zero-padded decimal number(1)</p>\n  \n  <p>%Y    Year with century as a decimal number(2015)</p>\n  \n  <p>%I    Hour (12-hour clock) as a zero-padded decimal number(01)</p>\n  \n  <p>%M    Minute as a zero-padded decimal number(33)</p>\n  \n  <p>%p    Locale\u2019s equivalent of either AM or PM(PM)</p>\n</blockquote>\n\n<p>so you need strptime i-e converting   to </p>\n\n<pre> </pre>\n\n<p>Output</p>\n\n<pre> </pre>\n\n<p>What if you have different format of dates you can use panda or dateutil.parse</p>\n\n<pre> </pre>\n\n<p>OutPut</p>\n\n<pre> </pre>\n\n\n<p>I personally like the solution using the   module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. <strong>BUT</strong>, one downside is that it is <strong>90% slower</strong> than the accepted answer with  .</p>\n\n<pre> </pre>\n\n<p>As long as you are not doing this <em>a million</em> times over and over again, I still  think the   method is more convenient and will handle most of the time formats automatically.</p>\n\n\n<p><strong>arrow</strong> offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.</p>\n\n<pre> </pre>\n\n<p>See <a href=\"http://arrow.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">http://arrow.readthedocs.io/en/latest/</a> for more.</p>\n\n\n<p>Check out <a href=\"http://docs.python.org/3/library/time.html#time.strptime\" rel=\"noreferrer\">strptime</a> in the <a href=\"http://docs.python.org/3/library/time.html\" rel=\"noreferrer\">time</a> module.  It is the inverse of <a href=\"http://docs.python.org/3/library/time.html#time.strftime\" rel=\"noreferrer\">strftime</a>.</p>\n\n<pre> </pre>\n\n\n<pre> </pre>\n\n<p>it shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n<p>By using   option in   mention you can convert your string datetime into pandas datetime format.</p>\n\n<pre> </pre>\n\n<blockquote>\n<pre> </pre>\n</blockquote>\n\n<pre> </pre>\n\n\n<p>Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.</p>\n\n<pre> </pre>\n\n<p><strong>Timings</strong></p>\n\n<pre> </pre>\n\n<p>And here is how to convert the OP's original date-time examples:</p>\n\n<pre> </pre>\n\n<p>There are many options for converting from the strings to Pandas Timestamps using  , so check the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\" rel=\"noreferrer\">docs</a> if you need anything special.</p>\n\n<p>Likewise, Timestamps have many <a href=\"http://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties\" rel=\"noreferrer\">properties and methods</a> that can be accessed in addition to  </p>\n\n\n<p>Create a small utility function like:</p>\n\n<pre> </pre>\n\n<p>This is versatile enough:</p>\n\n<ul>\n<li>If you don't pass any arguments it will return today's date.</li>\n<li>There's a date format as default that you can override.</li>\n<li>You can easily modify it to return a datetime.</li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/django.db.models.query.QuerySet"}, "class_func_label": {"type": "literal", "value": "django.db.models.query.QuerySet"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Represents a lazy database lookup for a set of objects.\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/4294088"}, "title": {"type": "literal", "value": "Accelerate bulk insert using Django's ORM?"}, "content": {"type": "literal", "value": "<p>I'm planning to upload a billion records taken from ~750 files (each ~250MB) to a db using django's ORM.\nCurrently each file takes ~20min to process, and I was wondering if there's any way to accelerate this process.</p>\n\n<p>I've taken the following measures:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/1136106/efficent-way-to-insert-thousands-of-records-into-a-table-sqlite-python-django\">Use @transaction.commit_manually</a> and commit once every 5000 records</li>\n<li>Set DEBUG=False so that django <a href=\"https://stackoverflow.com/questions/4292963/python-process-keeps-growing-in-django-db-upload-script\">won't accumulate all the sql commands in memory</a></li>\n<li>The loop that runs over records in a single file is completely contained in a single function (minimize stack changes)</li>\n<li>Refrained from hitting the db for queries (used a local hash of objects already in the db <a href=\"https://stackoverflow.com/questions/2252530/efficent-way-to-bulk-insert-with-get-or-create-in-django-sql-python-django\">instead of using get_or_create</a>)</li>\n<li><a href=\"http://docs.djangoproject.com/en/dev/ref/models/instances/#forcing-an-insert-or-update\" rel=\"noreferrer\">Set force_insert=True in the save()</a> in hopes it will save django some logic</li>\n<li><a href=\"http://docs.djangoproject.com/en/dev/ref/models/instances/#explicitly-specifying-auto-primary-key-values\" rel=\"noreferrer\">Explicitly set the id</a> in hopes it will save django some logic</li>\n<li>General code minimization and optimization</li>\n</ul>\n\n<p>What else can I do to speed things up? Here are some of my thoughts:</p>\n\n<ul>\n<li>Use some kind of Python compiler or version which is quicker (Psyco?)</li>\n<li><a href=\"https://stackoverflow.com/questions/2252530/efficent-way-to-bulk-insert-with-get-or-create-in-django-sql-python-django\">Override the ORM and use SQL directly</a></li>\n<li>Use some 3rd party code that might be better (<a href=\"http://code.google.com/p/django-batchimport/\" rel=\"noreferrer\">1</a>, <a href=\"http://djangosnippets.org/snippets/446/\" rel=\"noreferrer\">2</a>)</li>\n<li><a href=\"http://code.djangoproject.com/ticket/661\" rel=\"noreferrer\">Beg the django community</a> to create a bulk_insert function</li>\n</ul>\n\n<p>Any pointers regarding these items or any other idea would be welcome :)</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Also, if you want something quick and simple, you could try this: <a href=\"http://djangosnippets.org/snippets/2362/\" rel=\"nofollow\">http://djangosnippets.org/snippets/2362/</a>. It's a simple manager I used on a project.</p>\n\n<p>The other snippet wasn't as simple and was really focused on bulk inserts for relationships. This is just a plain bulk insert and just uses the same INSERT query.</p>\n\n\n<p>Django 1.4 provides a <code>bulk_create()</code> method on the QuerySet object, see:</p>\n\n<ul>\n<li><a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></li>\n<li><a href=\"https://docs.djangoproject.com/en/dev/releases/1.4/\">https://docs.djangoproject.com/en/dev/releases/1.4/</a></li>\n<li><a href=\"https://code.djangoproject.com/ticket/7596\">https://code.djangoproject.com/ticket/7596</a></li>\n</ul>\n\n\n<p>There is also a bulk insert snippet at <a href=\"http://djangosnippets.org/snippets/446/\" rel=\"noreferrer\">http://djangosnippets.org/snippets/446/</a>.</p>\n\n<p>This gives one insert command multiple value pairs (INSERT INTO x (val1, val2) VALUES (1,2), (3,4) --etc etc). This should greatly improve performance.</p>\n\n<p>It also appears to be heavily documented, which is always a plus.</p>\n\n\n<p>This is not specific to Django ORM, but recently I had to bulk insert >60 Million rows of 8 columns of data from over 2000 files into a sqlite3 database. And I learned that the following three things reduced the insert time from over 48 hours to ~1 hour:</p>\n\n<ol>\n<li><p>increase the cache size setting of your DB to use more RAM (default ones always very\nsmall, I used 3GB); in sqlite, this is done by PRAGMA cache_size = n_of_pages; </p></li>\n<li><p>do journalling in RAM instead of disk (this does cause slight\nproblem if system fails, but something I consider to be negligible\ngiven that you have the source data on disk already); in sqlite this is done by PRAGMA journal_mode = MEMORY </p></li>\n<li><p>last and perhaps most important one: do not build index while\ninserting. This also means to not declare UNIQUE or other constraint that might cause DB to build index. Build index only after you are done inserting. </p></li>\n</ol>\n\n<p>As someone mentioned previously, you should also use cursor.executemany() (or just the shortcut conn.executemany()). To use it, do:</p>\n\n<pre><code>cursor.executemany('INSERT INTO mytable (field1, field2, field3) VALUES (?, ?, ?)', iterable_data)\n</code></pre>\n\n<p>The iterable_data could be a list or something alike, or even an open file reader. </p>\n\n\n<p>Development django got bulk_create: <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\" rel=\"nofollow\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></p>\n\n\n<p><a href=\"http://docs.djangoproject.com/en/dev/topics/db/sql/#executing-custom-sql-directly\" rel=\"noreferrer\">Drop to DB-API</a> and use <code>cursor.executemany()</code>. See <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP 249</a> for details.</p>\n\n\n<p>I ran some tests on Django 1.10 / Postgresql 9.4 / Pandas 0.19.0 and got the following timings:</p>\n\n<ul>\n<li>Insert 3000 rows individually and get ids from populated objects using Django ORM: <strong>3200ms</strong></li>\n<li>Insert 3000 rows with Pandas <code>DataFrame.to_sql()</code> and don't get IDs: <strong>774ms</strong> \n\n<ul>\n<li>Update 2019: Pandas 0.24.0 <code>df.to_sql()</code> is quicker by optionally <a href=\"https://github.com/pandas-dev/pandas/issues/8953\" rel=\"nofollow noreferrer\">combining multiple inserts into a single statement</a> - I haven't tested it</li>\n</ul></li>\n<li>Insert 3000 rows with Django manager <code>.bulk_create(Model(**df.to_records()))</code> and don't get IDs: <strong>574ms</strong></li>\n<li>Insert 3000 rows with <code>to_csv</code> to <code>StringIO</code> buffer and <code>COPY</code> (<code>cur.copy_from()</code>) and don't get IDs: <strong>118ms</strong></li>\n<li>Insert 3000 rows with <code>to_csv</code> and <code>COPY</code> and get IDs via simple <code>SELECT WHERE ID &gt; [max ID before insert]</code> (probably not threadsafe unless <code>COPY</code> holds a lock on the table preventing simultaneous inserts?): <strong>201ms</strong></li>\n</ul>\n\n\n\n<pre><code>def bulk_to_sql(df, columns, model_cls):\n    \"\"\" Inserting 3000 takes 774ms avg \"\"\"\n    engine = ExcelImportProcessor._get_sqlalchemy_engine()\n    df[columns].to_sql(model_cls._meta.db_table, con=engine, if_exists='append', index=False)\n\n\ndef bulk_via_csv(df, columns, model_cls):\n    \"\"\" Inserting 3000 takes 118ms avg \"\"\"\n    engine = ExcelImportProcessor._get_sqlalchemy_engine()\n    connection = engine.raw_connection()\n    cursor = connection.cursor()\n    output = StringIO()\n    df[columns].to_csv(output, sep='\\t', header=False, index=False)\n    output.seek(0)\n    contents = output.getvalue()\n    cur = connection.cursor()\n    cur.copy_from(output, model_cls._meta.db_table, null=\"\", columns=columns)\n    connection.commit()\n    cur.close()\n</code></pre>\n\n<p>The performance stats were all obtained on a table already containing 3,000 rows running on OS X (i7 SSD 16GB), average of ten runs using <code>timeit</code>.</p>\n\n<p>I get my inserted primary keys back by assigning an import batch id and sorting by primary key, although I'm not 100% certain primary keys will always be assigned in the order the rows are serialized for the <code>COPY</code> command - would appreciate opinions either way.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Also, if you want something quick and simple, you could try this: <a href=\"http://djangosnippets.org/snippets/2362/\" rel=\"nofollow\">http://djangosnippets.org/snippets/2362/</a>. It's a simple manager I used on a project.</p>\n\n<p>The other snippet wasn't as simple and was really focused on bulk inserts for relationships. This is just a plain bulk insert and just uses the same INSERT query.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "3"}, "answer_2": {"type": "literal", "value": "<p>Django 1.4 provides a <code>bulk_create()</code> method on the QuerySet object, see:</p>\n\n<ul>\n<li><a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></li>\n<li><a href=\"https://docs.djangoproject.com/en/dev/releases/1.4/\">https://docs.djangoproject.com/en/dev/releases/1.4/</a></li>\n<li><a href=\"https://code.djangoproject.com/ticket/7596\">https://code.djangoproject.com/ticket/7596</a></li>\n</ul>\n"}, "answer_2_votes": {"type": "literal", "value": "34"}, "answer_3": {"type": "literal", "value": "<p>There is also a bulk insert snippet at <a href=\"http://djangosnippets.org/snippets/446/\" rel=\"noreferrer\">http://djangosnippets.org/snippets/446/</a>.</p>\n\n<p>This gives one insert command multiple value pairs (INSERT INTO x (val1, val2) VALUES (1,2), (3,4) --etc etc). This should greatly improve performance.</p>\n\n<p>It also appears to be heavily documented, which is always a plus.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "5"}, "answer_4": {"type": "literal", "value": "<p>This is not specific to Django ORM, but recently I had to bulk insert >60 Million rows of 8 columns of data from over 2000 files into a sqlite3 database. And I learned that the following three things reduced the insert time from over 48 hours to ~1 hour:</p>\n\n<ol>\n<li><p>increase the cache size setting of your DB to use more RAM (default ones always very\nsmall, I used 3GB); in sqlite, this is done by PRAGMA cache_size = n_of_pages; </p></li>\n<li><p>do journalling in RAM instead of disk (this does cause slight\nproblem if system fails, but something I consider to be negligible\ngiven that you have the source data on disk already); in sqlite this is done by PRAGMA journal_mode = MEMORY </p></li>\n<li><p>last and perhaps most important one: do not build index while\ninserting. This also means to not declare UNIQUE or other constraint that might cause DB to build index. Build index only after you are done inserting. </p></li>\n</ol>\n\n<p>As someone mentioned previously, you should also use cursor.executemany() (or just the shortcut conn.executemany()). To use it, do:</p>\n\n<pre><code>cursor.executemany('INSERT INTO mytable (field1, field2, field3) VALUES (?, ?, ?)', iterable_data)\n</code></pre>\n\n<p>The iterable_data could be a list or something alike, or even an open file reader. </p>\n"}, "answer_4_votes": {"type": "literal", "value": "16"}, "answer_5": {"type": "literal", "value": "<p>Development django got bulk_create: <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\" rel=\"nofollow\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></p>\n"}, "answer_5_votes": {"type": "literal", "value": "3"}, "answer_6": {"type": "literal", "value": "<p><a href=\"http://docs.djangoproject.com/en/dev/topics/db/sql/#executing-custom-sql-directly\" rel=\"noreferrer\">Drop to DB-API</a> and use <code>cursor.executemany()</code>. See <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP 249</a> for details.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "13"}, "answer_7": {"type": "literal", "value": "<p>I ran some tests on Django 1.10 / Postgresql 9.4 / Pandas 0.19.0 and got the following timings:</p>\n\n<ul>\n<li>Insert 3000 rows individually and get ids from populated objects using Django ORM: <strong>3200ms</strong></li>\n<li>Insert 3000 rows with Pandas <code>DataFrame.to_sql()</code> and don't get IDs: <strong>774ms</strong> \n\n<ul>\n<li>Update 2019: Pandas 0.24.0 <code>df.to_sql()</code> is quicker by optionally <a href=\"https://github.com/pandas-dev/pandas/issues/8953\" rel=\"nofollow noreferrer\">combining multiple inserts into a single statement</a> - I haven't tested it</li>\n</ul></li>\n<li>Insert 3000 rows with Django manager <code>.bulk_create(Model(**df.to_records()))</code> and don't get IDs: <strong>574ms</strong></li>\n<li>Insert 3000 rows with <code>to_csv</code> to <code>StringIO</code> buffer and <code>COPY</code> (<code>cur.copy_from()</code>) and don't get IDs: <strong>118ms</strong></li>\n<li>Insert 3000 rows with <code>to_csv</code> and <code>COPY</code> and get IDs via simple <code>SELECT WHERE ID &gt; [max ID before insert]</code> (probably not threadsafe unless <code>COPY</code> holds a lock on the table preventing simultaneous inserts?): <strong>201ms</strong></li>\n</ul>\n\n\n\n<pre><code>def bulk_to_sql(df, columns, model_cls):\n    \"\"\" Inserting 3000 takes 774ms avg \"\"\"\n    engine = ExcelImportProcessor._get_sqlalchemy_engine()\n    df[columns].to_sql(model_cls._meta.db_table, con=engine, if_exists='append', index=False)\n\n\ndef bulk_via_csv(df, columns, model_cls):\n    \"\"\" Inserting 3000 takes 118ms avg \"\"\"\n    engine = ExcelImportProcessor._get_sqlalchemy_engine()\n    connection = engine.raw_connection()\n    cursor = connection.cursor()\n    output = StringIO()\n    df[columns].to_csv(output, sep='\\t', header=False, index=False)\n    output.seek(0)\n    contents = output.getvalue()\n    cur = connection.cursor()\n    cur.copy_from(output, model_cls._meta.db_table, null=\"\", columns=columns)\n    connection.commit()\n    cur.close()\n</code></pre>\n\n<p>The performance stats were all obtained on a table already containing 3,000 rows running on OS X (i7 SSD 16GB), average of ten runs using <code>timeit</code>.</p>\n\n<p>I get my inserted primary keys back by assigning an import batch id and sorting by primary key, although I'm not 100% certain primary keys will always be assigned in the order the rows are serialized for the <code>COPY</code> command - would appreciate opinions either way.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "7"}, "content_wo_code": "<p>I'm planning to upload a billion records taken from ~750 files (each ~250MB) to a db using django's ORM.\nCurrently each file takes ~20min to process, and I was wondering if there's any way to accelerate this process.</p>\n\n<p>I've taken the following measures:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/1136106/efficent-way-to-insert-thousands-of-records-into-a-table-sqlite-python-django\">Use @transaction.commit_manually</a> and commit once every 5000 records</li>\n<li>Set DEBUG=False so that django <a href=\"https://stackoverflow.com/questions/4292963/python-process-keeps-growing-in-django-db-upload-script\">won't accumulate all the sql commands in memory</a></li>\n<li>The loop that runs over records in a single file is completely contained in a single function (minimize stack changes)</li>\n<li>Refrained from hitting the db for queries (used a local hash of objects already in the db <a href=\"https://stackoverflow.com/questions/2252530/efficent-way-to-bulk-insert-with-get-or-create-in-django-sql-python-django\">instead of using get_or_create</a>)</li>\n<li><a href=\"http://docs.djangoproject.com/en/dev/ref/models/instances/#forcing-an-insert-or-update\" rel=\"noreferrer\">Set force_insert=True in the save()</a> in hopes it will save django some logic</li>\n<li><a href=\"http://docs.djangoproject.com/en/dev/ref/models/instances/#explicitly-specifying-auto-primary-key-values\" rel=\"noreferrer\">Explicitly set the id</a> in hopes it will save django some logic</li>\n<li>General code minimization and optimization</li>\n</ul>\n\n<p>What else can I do to speed things up? Here are some of my thoughts:</p>\n\n<ul>\n<li>Use some kind of Python compiler or version which is quicker (Psyco?)</li>\n<li><a href=\"https://stackoverflow.com/questions/2252530/efficent-way-to-bulk-insert-with-get-or-create-in-django-sql-python-django\">Override the ORM and use SQL directly</a></li>\n<li>Use some 3rd party code that might be better (<a href=\"http://code.google.com/p/django-batchimport/\" rel=\"noreferrer\">1</a>, <a href=\"http://djangosnippets.org/snippets/446/\" rel=\"noreferrer\">2</a>)</li>\n<li><a href=\"http://code.djangoproject.com/ticket/661\" rel=\"noreferrer\">Beg the django community</a> to create a bulk_insert function</li>\n</ul>\n\n<p>Any pointers regarding these items or any other idea would be welcome :)</p>\n", "answer_wo_code": "<p>Also, if you want something quick and simple, you could try this: <a href=\"http://djangosnippets.org/snippets/2362/\" rel=\"nofollow\">http://djangosnippets.org/snippets/2362/</a>. It's a simple manager I used on a project.</p>\n\n<p>The other snippet wasn't as simple and was really focused on bulk inserts for relationships. This is just a plain bulk insert and just uses the same INSERT query.</p>\n\n\n<p>Django 1.4 provides a   method on the QuerySet object, see:</p>\n\n<ul>\n<li><a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></li>\n<li><a href=\"https://docs.djangoproject.com/en/dev/releases/1.4/\">https://docs.djangoproject.com/en/dev/releases/1.4/</a></li>\n<li><a href=\"https://code.djangoproject.com/ticket/7596\">https://code.djangoproject.com/ticket/7596</a></li>\n</ul>\n\n\n<p>There is also a bulk insert snippet at <a href=\"http://djangosnippets.org/snippets/446/\" rel=\"noreferrer\">http://djangosnippets.org/snippets/446/</a>.</p>\n\n<p>This gives one insert command multiple value pairs (INSERT INTO x (val1, val2) VALUES (1,2), (3,4) --etc etc). This should greatly improve performance.</p>\n\n<p>It also appears to be heavily documented, which is always a plus.</p>\n\n\n<p>This is not specific to Django ORM, but recently I had to bulk insert >60 Million rows of 8 columns of data from over 2000 files into a sqlite3 database. And I learned that the following three things reduced the insert time from over 48 hours to ~1 hour:</p>\n\n<ol>\n<li><p>increase the cache size setting of your DB to use more RAM (default ones always very\nsmall, I used 3GB); in sqlite, this is done by PRAGMA cache_size = n_of_pages; </p></li>\n<li><p>do journalling in RAM instead of disk (this does cause slight\nproblem if system fails, but something I consider to be negligible\ngiven that you have the source data on disk already); in sqlite this is done by PRAGMA journal_mode = MEMORY </p></li>\n<li><p>last and perhaps most important one: do not build index while\ninserting. This also means to not declare UNIQUE or other constraint that might cause DB to build index. Build index only after you are done inserting. </p></li>\n</ol>\n\n<p>As someone mentioned previously, you should also use cursor.executemany() (or just the shortcut conn.executemany()). To use it, do:</p>\n\n<pre> </pre>\n\n<p>The iterable_data could be a list or something alike, or even an open file reader. </p>\n\n\n<p>Development django got bulk_create: <a href=\"https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create\" rel=\"nofollow\">https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.bulk_create</a></p>\n\n\n<p><a href=\"http://docs.djangoproject.com/en/dev/topics/db/sql/#executing-custom-sql-directly\" rel=\"noreferrer\">Drop to DB-API</a> and use  . See <a href=\"http://www.python.org/dev/peps/pep-0249/\" rel=\"noreferrer\">PEP 249</a> for details.</p>\n\n\n<p>I ran some tests on Django 1.10 / Postgresql 9.4 / Pandas 0.19.0 and got the following timings:</p>\n\n<ul>\n<li>Insert 3000 rows individually and get ids from populated objects using Django ORM: <strong>3200ms</strong></li>\n<li>Insert 3000 rows with Pandas   and don't get IDs: <strong>774ms</strong> \n\n<ul>\n<li>Update 2019: Pandas 0.24.0   is quicker by optionally <a href=\"https://github.com/pandas-dev/pandas/issues/8953\" rel=\"nofollow noreferrer\">combining multiple inserts into a single statement</a> - I haven't tested it</li>\n</ul></li>\n<li>Insert 3000 rows with Django manager   and don't get IDs: <strong>574ms</strong></li>\n<li>Insert 3000 rows with   to   buffer and   ( ) and don't get IDs: <strong>118ms</strong></li>\n<li>Insert 3000 rows with   and   and get IDs via simple   (probably not threadsafe unless   holds a lock on the table preventing simultaneous inserts?): <strong>201ms</strong></li>\n</ul>\n\n\n\n<pre> </pre>\n\n<p>The performance stats were all obtained on a table already containing 3,000 rows running on OS X (i7 SSD 16GB), average of ten runs using  .</p>\n\n<p>I get my inserted primary keys back by assigning an import batch id and sorting by primary key, although I'm not 100% certain primary keys will always be assigned in the order the rows are serialized for the   command - would appreciate opinions either way.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.DataFrame"}, "class_func_label": {"type": "literal", "value": "pandas.DataFrame"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/14431646"}, "title": {"type": "literal", "value": "How to write Pandas dataframe to sqlite with Index"}, "content": {"type": "literal", "value": "<p>I have a list of stockmarket data pulled from Yahoo in a pandas DataFrame (see format below). The date is serving as the index in the DataFrame. I want to write the data (including the index) out to a SQLite database. </p>\n\n<pre><code>             AAPL     GE\nDate\n2009-01-02  89.95  14.76\n2009-01-05  93.75  14.38\n2009-01-06  92.20  14.58\n2009-01-07  90.21  13.93\n2009-01-08  91.88  13.95\n</code></pre>\n\n<p>Based on my reading of the write_frame code for Pandas, it <a href=\"https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L163\">does not currently support writing the index</a>. I've attempted to use to_records instead, but ran into the <a href=\"https://github.com/pydata/pandas/issues/1908\">issue with Numpy 1.6.2 and datetimes</a>. Now I'm trying to write tuples using .itertuples, but SQLite throws an error that the data type isn't supported (see code and result below). I'm relatively new to Python, Pandas and Numpy, so it is entirely possible I'm missing something obvious. I think I'm running into a problem trying to write a datetime to SQLite, but I think I might be overcomplicating this. </p>\n\n<p>I think I <em>may</em> be able to fix the issue by upgrading to Numpy 1.7 or the development version of Pandas, which has a fix posted on GitHub. I'd prefer to develop using release versions of software - I'm new to this and I don't want stability issues confusing matters further. </p>\n\n<p>Is there a way to accomplish this using Python 2.7.2, Pandas 0.10.0, and Numpy 1.6.2? Perhaps cleaning the datetimes somehow? I'm in a bit over my head, any help would be appreciated. </p>\n\n<p><strong>Code:</strong></p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\nfor ticker in ['AAPL', 'GE']:\n    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, '1/1/2009','12/31/2012')\n\n# create a data frame\nprice = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled \"Demo\"\ncon = db.connect('c:/Python27/test.db')\nwildcards = ','.join(['?'] * 3)\ninsert_sql = 'INSERT INTO Demo VALUES (%s)' % wildcards\ncon.executemany(insert_sql, data)\n</code></pre>\n\n<p><strong>Result:</strong></p>\n\n<pre><code>---------------------------------------------------------------------------\nInterfaceError                            Traceback (most recent call last)\n&lt;ipython-input-15-680cc9889c56&gt; in &lt;module&gt;()\n----&gt; 1 con.executemany(insert_sql, data)\n\nInterfaceError: Error binding parameter 0 - probably unsupported type.\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Unfortunately, <code>pandas.io.write_frame</code> no longer exists in more recent versions of Pandas in regards to the current accepted answer. For example I'm using pandas 0.19.2. You can do something like </p>\n\n<pre><code>from sqlalchemy import create_engine\n\ndisk_engine = create_engine('sqlite:///my_lite_store.db')\nprice.to_sql('stock_price', disk_engine, if_exists='append')\n</code></pre>\n\n<p>And then in turn preview your table with the following:</p>\n\n<pre><code>df = pd.read_sql_query('SELECT * FROM stock_price LIMIT 3',disk_engine)\ndf.head()\n</code></pre>\n\n\n<p>Below is the code which worked for me. I was able to write it to SQLite DB.</p>\n\n<pre><code>import pandas as pd\nimport sqlite3 as sq\ndata = &lt;This is going to be your pandas dataframe&gt;\nsql_data = 'D:\\\\SA.sqlite' #- Creates DB names SQLite\nconn = sq.connect(sql_data)\ncur = conn.cursor()\ncur.execute('''DROP TABLE IF EXISTS SA''')\ndata.to_sql('SA', conn, if_exists='replace', index=False) # - writes the pd.df to SQLIte DB\npd.read_sql('select * from SentimentAnalysis', conn)\nconn.commit()\nconn.close()\n</code></pre>\n\n\n<p>In recent pandas the index will be saved in the database (you used to have to <a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a> first).</p>\n\n<p>Following the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql\" rel=\"nofollow noreferrer\">docs</a> (setting a SQLite connection in memory):</p>\n\n<pre><code>import sqlite3\n# Create your connection.\ncnx = sqlite3.connect(':memory:')\n</code></pre>\n\n<p><em>Note: You can also pass a SQLAlchemy engine here (see end of answer).</em></p>\n\n<p>We can save <code>price2</code> to <code>cnx</code>:</p>\n\n<pre><code>price2.to_sql(name='price2', con=cnx)\n</code></pre>\n\n<p>We can retrieve via <code>read_sql</code>:</p>\n\n<pre><code>p2 = pd.read_sql('select * from price2', cnx)\n</code></pre>\n\n<p>However, when stored (and retrieved) <strong>dates are <code>unicode</code></strong> rather than <code>Timestamp</code>. To convert back to what we started with we can use <code>pd.to_datetime</code>:</p>\n\n<pre><code>p2.Date = pd.to_datetime(p2.Date)\np = p2.set_index('Date')\n</code></pre>\n\n<p>We get back the same DataFrame as <code>prices</code>:</p>\n\n<pre><code>In [11]: p2\nOut[11]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1006 entries, 2009-01-02 00:00:00 to 2012-12-31 00:00:00\nData columns:\nAAPL    1006  non-null values\nGE      1006  non-null values\ndtypes: float64(2)\n</code></pre>\n\n<hr>\n\n<p>You can also use a <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\" rel=\"nofollow noreferrer\">SQLAlchemy engine</a>:</p>\n\n<pre><code>from sqlalchemy import create_engine\ne = create_engine('sqlite://')  # pass your db url\n\nprice2.to_sql(name='price2', con=cnx)\n</code></pre>\n\n<p>This allows you to use <code>read_sql_table</code> (which can only be used with SQLAlchemy):</p>\n\n<pre><code>pd.read_sql_table(table_name='price2', con=e)\n#         Date   AAPL     GE\n# 0 2009-01-02  89.95  14.76\n# 1 2009-01-05  93.75  14.38\n# 2 2009-01-06  92.20  14.58\n# 3 2009-01-07  90.21  13.93\n# 4 2009-01-08  91.88  13.95\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Unfortunately, <code>pandas.io.write_frame</code> no longer exists in more recent versions of Pandas in regards to the current accepted answer. For example I'm using pandas 0.19.2. You can do something like </p>\n\n<pre><code>from sqlalchemy import create_engine\n\ndisk_engine = create_engine('sqlite:///my_lite_store.db')\nprice.to_sql('stock_price', disk_engine, if_exists='append')\n</code></pre>\n\n<p>And then in turn preview your table with the following:</p>\n\n<pre><code>df = pd.read_sql_query('SELECT * FROM stock_price LIMIT 3',disk_engine)\ndf.head()\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "11"}, "answer_2": {"type": "literal", "value": "<p>Below is the code which worked for me. I was able to write it to SQLite DB.</p>\n\n<pre><code>import pandas as pd\nimport sqlite3 as sq\ndata = &lt;This is going to be your pandas dataframe&gt;\nsql_data = 'D:\\\\SA.sqlite' #- Creates DB names SQLite\nconn = sq.connect(sql_data)\ncur = conn.cursor()\ncur.execute('''DROP TABLE IF EXISTS SA''')\ndata.to_sql('SA', conn, if_exists='replace', index=False) # - writes the pd.df to SQLIte DB\npd.read_sql('select * from SentimentAnalysis', conn)\nconn.commit()\nconn.close()\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>In recent pandas the index will be saved in the database (you used to have to <a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a> first).</p>\n\n<p>Following the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql\" rel=\"nofollow noreferrer\">docs</a> (setting a SQLite connection in memory):</p>\n\n<pre><code>import sqlite3\n# Create your connection.\ncnx = sqlite3.connect(':memory:')\n</code></pre>\n\n<p><em>Note: You can also pass a SQLAlchemy engine here (see end of answer).</em></p>\n\n<p>We can save <code>price2</code> to <code>cnx</code>:</p>\n\n<pre><code>price2.to_sql(name='price2', con=cnx)\n</code></pre>\n\n<p>We can retrieve via <code>read_sql</code>:</p>\n\n<pre><code>p2 = pd.read_sql('select * from price2', cnx)\n</code></pre>\n\n<p>However, when stored (and retrieved) <strong>dates are <code>unicode</code></strong> rather than <code>Timestamp</code>. To convert back to what we started with we can use <code>pd.to_datetime</code>:</p>\n\n<pre><code>p2.Date = pd.to_datetime(p2.Date)\np = p2.set_index('Date')\n</code></pre>\n\n<p>We get back the same DataFrame as <code>prices</code>:</p>\n\n<pre><code>In [11]: p2\nOut[11]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1006 entries, 2009-01-02 00:00:00 to 2012-12-31 00:00:00\nData columns:\nAAPL    1006  non-null values\nGE      1006  non-null values\ndtypes: float64(2)\n</code></pre>\n\n<hr>\n\n<p>You can also use a <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\" rel=\"nofollow noreferrer\">SQLAlchemy engine</a>:</p>\n\n<pre><code>from sqlalchemy import create_engine\ne = create_engine('sqlite://')  # pass your db url\n\nprice2.to_sql(name='price2', con=cnx)\n</code></pre>\n\n<p>This allows you to use <code>read_sql_table</code> (which can only be used with SQLAlchemy):</p>\n\n<pre><code>pd.read_sql_table(table_name='price2', con=e)\n#         Date   AAPL     GE\n# 0 2009-01-02  89.95  14.76\n# 1 2009-01-05  93.75  14.38\n# 2 2009-01-06  92.20  14.58\n# 3 2009-01-07  90.21  13.93\n# 4 2009-01-08  91.88  13.95\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": "30"}, "content_wo_code": "<p>I have a list of stockmarket data pulled from Yahoo in a pandas DataFrame (see format below). The date is serving as the index in the DataFrame. I want to write the data (including the index) out to a SQLite database. </p>\n\n<pre> </pre>\n\n<p>Based on my reading of the write_frame code for Pandas, it <a href=\"https://github.com/pydata/pandas/blob/master/pandas/io/sql.py#L163\">does not currently support writing the index</a>. I've attempted to use to_records instead, but ran into the <a href=\"https://github.com/pydata/pandas/issues/1908\">issue with Numpy 1.6.2 and datetimes</a>. Now I'm trying to write tuples using .itertuples, but SQLite throws an error that the data type isn't supported (see code and result below). I'm relatively new to Python, Pandas and Numpy, so it is entirely possible I'm missing something obvious. I think I'm running into a problem trying to write a datetime to SQLite, but I think I might be overcomplicating this. </p>\n\n<p>I think I <em>may</em> be able to fix the issue by upgrading to Numpy 1.7 or the development version of Pandas, which has a fix posted on GitHub. I'd prefer to develop using release versions of software - I'm new to this and I don't want stability issues confusing matters further. </p>\n\n<p>Is there a way to accomplish this using Python 2.7.2, Pandas 0.10.0, and Numpy 1.6.2? Perhaps cleaning the datetimes somehow? I'm in a bit over my head, any help would be appreciated. </p>\n\n<p><strong>Code:</strong></p>\n\n<pre> </pre>\n\n<p><strong>Result:</strong></p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Unfortunately,   no longer exists in more recent versions of Pandas in regards to the current accepted answer. For example I'm using pandas 0.19.2. You can do something like </p>\n\n<pre> </pre>\n\n<p>And then in turn preview your table with the following:</p>\n\n<pre> </pre>\n\n\n<p>Below is the code which worked for me. I was able to write it to SQLite DB.</p>\n\n<pre> </pre>\n\n\n<p>In recent pandas the index will be saved in the database (you used to have to <a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"> </a> first).</p>\n\n<p>Following the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql\" rel=\"nofollow noreferrer\">docs</a> (setting a SQLite connection in memory):</p>\n\n<pre> </pre>\n\n<p><em>Note: You can also pass a SQLAlchemy engine here (see end of answer).</em></p>\n\n<p>We can save   to  :</p>\n\n<pre> </pre>\n\n<p>We can retrieve via  :</p>\n\n<pre> </pre>\n\n<p>However, when stored (and retrieved) <strong>dates are  </strong> rather than  . To convert back to what we started with we can use  :</p>\n\n<pre> </pre>\n\n<p>We get back the same DataFrame as  :</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>You can also use a <a href=\"https://docs.sqlalchemy.org/en/latest/core/engines.html\" rel=\"nofollow noreferrer\">SQLAlchemy engine</a>:</p>\n\n<pre> </pre>\n\n<p>This allows you to use   (which can only be used with SQLAlchemy):</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/UnicodeEncodeError"}, "class_func_label": {"type": "literal", "value": "UnicodeEncodeError"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "Unicode encoding error."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/33337798"}, "title": {"type": "literal", "value": "UnicodeEncodeError when using pandas method to_sql on a dataframe with unicode column names"}, "content": {"type": "literal", "value": "<p>This is my first time posting on stack overflow, so bear with me. I have been scouring the internet for an entire day and I have not been able to fix this problem.</p>\n\n<p>Basically, I have a Pandas DataFrame with unicode characters in the column names, and I am getting a UnicodeEncodeError when I try to use to_sql to store the data in a database. I can reproduce the error with the following code:</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\ndf = pd.DataFrame([[1,2],[3,4]], columns = [u'\\xe9',u'b'])\nengine = create_engine('mysql://root:pass@localhost/testdb')\ndf.to_sql('data', engine, if_exists = 'replace', index = False)\n</code></pre>\n\n<p>The last line of the traceback looks like this:</p>\n\n<pre><code>C:\\Users\\isaac_000\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.pyc in _get_column_names_and_types(self, dtype_mapper)\n857              dtype_mapper(self.frame.iloc[:, i]),\n858              False)\n--&gt; 859             for i in range(len(self.frame.columns))\n860             ]\n861 \n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xe9' in position 0: ordinal not in range(128)\n</code></pre>\n\n<p>I would appreciate any help you guys can provide. Let me know if you need any more information. Thanks!</p>\n"}, "answerContent": {"type": "literal", "value": "<p>This is a bug in the current <code>to_sql</code> method, and I filed it here: <a href=\"https://github.com/pydata/pandas/issues/11431\" rel=\"nofollow\">https://github.com/pydata/pandas/issues/11431</a> (and will probably be fixed in version 0.17.1)</p>\n\n<p>As a workaround, I would suggest to </p>\n\n<ul>\n<li>remove the special unicode character for now in the column names</li>\n<li>or patch your pandas installation (the fix is very simple and small, see here what you should have to change: <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow\">https://github.com/pydata/pandas/pull/11432</a>)</li>\n</ul>\n\n\n<p>This is a bug in the current <code>to_sql</code> method, and I filed it here: <a href=\"https://github.com/pydata/pandas/issues/11431\" rel=\"nofollow\">https://github.com/pydata/pandas/issues/11431</a> (and will probably be fixed in version 0.17.1)</p>\n\n<p>As a workaround, I would suggest to </p>\n\n<ul>\n<li>remove the special unicode character for now in the column names</li>\n<li>or patch your pandas installation (the fix is very simple and small, see here what you should have to change: <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow\">https://github.com/pydata/pandas/pull/11432</a>)</li>\n</ul>\n"}, "answer_1": {"type": "literal", "value": "<p>This is a bug in the current <code>to_sql</code> method, and I filed it here: <a href=\"https://github.com/pydata/pandas/issues/11431\" rel=\"nofollow\">https://github.com/pydata/pandas/issues/11431</a> (and will probably be fixed in version 0.17.1)</p>\n\n<p>As a workaround, I would suggest to </p>\n\n<ul>\n<li>remove the special unicode character for now in the column names</li>\n<li>or patch your pandas installation (the fix is very simple and small, see here what you should have to change: <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow\">https://github.com/pydata/pandas/pull/11432</a>)</li>\n</ul>\n"}, "answer_1_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>This is my first time posting on stack overflow, so bear with me. I have been scouring the internet for an entire day and I have not been able to fix this problem.</p>\n\n<p>Basically, I have a Pandas DataFrame with unicode characters in the column names, and I am getting a UnicodeEncodeError when I try to use to_sql to store the data in a database. I can reproduce the error with the following code:</p>\n\n<pre> </pre>\n\n<p>The last line of the traceback looks like this:</p>\n\n<pre> </pre>\n\n<p>I would appreciate any help you guys can provide. Let me know if you need any more information. Thanks!</p>\n", "answer_wo_code": "<p>This is a bug in the current   method, and I filed it here: <a href=\"https://github.com/pydata/pandas/issues/11431\" rel=\"nofollow\">https://github.com/pydata/pandas/issues/11431</a> (and will probably be fixed in version 0.17.1)</p>\n\n<p>As a workaround, I would suggest to </p>\n\n<ul>\n<li>remove the special unicode character for now in the column names</li>\n<li>or patch your pandas installation (the fix is very simple and small, see here what you should have to change: <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow\">https://github.com/pydata/pandas/pull/11432</a>)</li>\n</ul>\n\n\n<p>This is a bug in the current   method, and I filed it here: <a href=\"https://github.com/pydata/pandas/issues/11431\" rel=\"nofollow\">https://github.com/pydata/pandas/issues/11431</a> (and will probably be fixed in version 0.17.1)</p>\n\n<p>As a workaround, I would suggest to </p>\n\n<ul>\n<li>remove the special unicode character for now in the column names</li>\n<li>or patch your pandas installation (the fix is very simple and small, see here what you should have to change: <a href=\"https://github.com/pydata/pandas/pull/11432\" rel=\"nofollow\">https://github.com/pydata/pandas/pull/11432</a>)</li>\n</ul>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/lineno"}, "class_func_label": {"type": "literal", "value": "lineno"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nReturn the cumulative line number of the line that has just been read.\nBefore the first line has been read, returns 0. After the last line\nof the last file has been read, returns the line number of that line."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/14355151"}, "title": {"type": "literal", "value": "how to make pandas HDFStore 'put' operation faster"}, "content": {"type": "literal", "value": "<p>I'm trying to build a ETL toolkit with pandas, hdf5.</p>\n\n<p>My plan was  </p>\n\n<ol>\n<li>extracting a table from mysql to a DataFrame;  </li>\n<li>put this DataFrame into a HDFStore;</li>\n</ol>\n\n<p>But when i was doing the step 2, i found putting a dataframe into a *.h5 file costs too much time.</p>\n\n<ul>\n<li>the size of table in source mysql server: 498MB\n<ul>\n<li>52 columns</li>\n<li>924,624 records</li>\n</ul></li>\n<li>the size of *.h5 file after putting the dataframe inside : 513MB\n<ul>\n<li>the 'put' operation costs 849.345677137 seconds</li>\n</ul></li>\n</ul>\n\n<p>My questions are:<br>\nIs this time costs normal?<br>\nIs there any way to make it faster?</p>\n\n<hr>\n\n<h2>Update 1</h2>\n\n<p>thanks Jeff</p>\n\n<ul>\n<li><p>my codes are pretty simple:</p>\n\n<p>extract_store = HDFStore('extract_store.h5')<br>\nextract_store['df_staff'] = df_staff</p></li>\n<li>and when i trying 'ptdump -av file.h5', i got an error, but i still could load the dataframe object from this h5 file:</li>\n</ul>\n\n<blockquote>\n  <p>tables.exceptions.HDF5ExtError: HDF5 error back trace</p>\n  \n  <p>File \"../../../src/H5F.c\", line 1512, in H5Fopen<br>\n      unable to open file   File \"../../../src/H5F.c\", line 1307, in H5F_open<br>\n      unable to read superblock   File \"../../../src/H5Fsuper.c\", line 305, in H5F_super_read<br>\n      unable to find file signature   File \"../../../src/H5Fsuper.c\", line 153, in H5F_locate_signature<br>\n      unable to find a valid file signature  </p>\n  \n  <p>End of HDF5 error back trace  </p>\n  \n  <p>Unable to open/create file 'extract_store.h5'  </p>\n</blockquote>\n\n<ul>\n<li>some other infos:<br>\n<ul>\n<li>pandas version: '0.10.0'</li>\n<li>os: ubuntu server 10.04 x86_64</li>\n<li>cpu: 8 * Intel(R) Xeon(R) CPU X5670  @ 2.93GHz</li>\n<li>MemTotal: 51634016 kB</li>\n</ul></li>\n</ul>\n\n<p>I will update the pandas to 0.10.1-dev and try again.</p>\n\n<hr>\n\n<h2>Update 2</h2>\n\n<ul>\n<li>I had updated pandas to '0.10.1.dev-6e2b6ea'</li>\n<li>but the time costs wasn't decreased, it costs 884.15 s seconds this time</li>\n<li>the output of 'ptdump -av file.h5 ' is :</li>\n</ul>\n\n<pre>\n    / (RootGroup) ''  \n      /._v_attrs (AttributeSet), 4 attributes:  \n       [CLASS := 'GROUP',  \n        PYTABLES_FORMAT_VERSION := '2.0',  \n        TITLE := '',  \n        VERSION := '1.0']  \n    /df_bugs (Group) ''  \n      /df_bugs._v_attrs (AttributeSet), 12 attributes:  \n       [CLASS := 'GROUP',  \n        TITLE := '',  \n        VERSION := '1.0',  \n        axis0_variety := 'regular',  \n        axis1_variety := 'regular',  \n        block0_items_variety := 'regular',  \n        block1_items_variety := 'regular',  \n        block2_items_variety := 'regular',  \n        nblocks := 3,  \n        ndim := 2,  \n        pandas_type := 'frame',  \n        pandas_version := '0.10.1']  \n    /df_bugs/axis0 (Array(52,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/axis0._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/axis1 (Array(924624,)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/axis1._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'integer',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_items (Array(5,)) ''  \n      atom := StringAtom(itemsize=12, shape=(), dflt='')  \n      maindim := 0   \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block0_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_values (Array(924624, 5)) ''  \n      atom := Float64Atom(shape=(), dflt=0.0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block0_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block1_items (Array(19,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block1_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block1_values (Array(924624, 19)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block1_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',   \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block2_items (Array(28,)) ''  \n      atom := StringAtom(itemsize=18, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block2_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',\n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block2_values (VLArray(1,)) ''  \n      atom = ObjectAtom()  \n      byteorder = 'irrelevant'  \n      nrows = 1  \n      flavor = 'numpy'  \n      /df_bugs/block2_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'VLARRAY',  \n        PSEUDOATOM := 'object',  \n        TITLE := '',   \n        VERSION := '1.3',  \n        transposed := True]  \n</pre>\n\n<ul>\n<li>and I had tried your code below (putting the dataframe into hdfstore with the param 'table' is True) , but got an error instead, it seemed like python's datatime type was not supported :</li>\n</ul>\n\n<blockquote>\n  <p>Exception: cannot find the correct atom type -> [dtype->object] object\n  of type 'datetime.datetime' has no len()</p>\n</blockquote>\n\n<hr>\n\n<h2>Update 3</h2>\n\n<p>thanks jeff. \nSorry for the delay.</p>\n\n<ul>\n<li>tables.<strong>version</strong> : '2.4.0'.</li>\n<li>yes, the 884 seconds is only the put operation costs without the pull operation from mysql</li>\n<li>a row of dataframe (df.ix[0]):</li>\n</ul>\n\n<pre>\nbug_id                                   1\nassigned_to                            185\nbug_file_loc                          None\nbug_severity                      critical\nbug_status                          closed\ncreation_ts            1998-05-06 21:27:00\ndelta_ts               2012-05-09 14:41:41\nshort_desc                    Two cursors.\nhost_op_sys                        Unknown\nguest_op_sys                       Unknown\npriority                                P3\nrep_platform                          IA32\nreporter                                56\nproduct_id                               7\ncategory_id                            983\ncomponent_id                         12925\nresolution                           fixed\ntarget_milestone                       ws1\nqa_contact                             412\nstatus_whiteboard                         \nvotes                                    0\nkeywords                                SR\nlastdiffed             2012-05-09 14:41:41\neverconfirmed                            1\nreporter_accessible                      1\ncclist_accessible                        1\nestimated_time                        0.00\nremaining_time                        0.00\ndeadline                              None\nalias                                 None\nfound_in_product_id                      0\nfound_in_version_id                      0\nfound_in_phase_id                        0\ncf_type                             Defect\ncf_reported_by                 Development\ncf_attempted                           NaN\ncf_failed                              NaN\ncf_public_summary                         \ncf_doc_impact                            0\ncf_security                              0\ncf_build                               NaN\ncf_branch                                 \ncf_change                              NaN\ncf_test_id                             NaN\ncf_regression                      Unknown\ncf_reviewer                              0\ncf_on_hold                               0\ncf_public_severity                     ---\ncf_i18n_impact                           0\ncf_eta                                None\ncf_bug_source                          ---\ncf_viss                               None\nName: 0, Length: 52\n</pre>\n\n<ul>\n<li>the picture of dataframe( just type 'df' in ipython notebook):</li>\n</ul>\n\n<pre>\n\nInt64Index: 924624 entries, 0 to 924623\nData columns:\nbug_id                 924624  non-null values\nassigned_to            924624  non-null values\nbug_file_loc           427318  non-null values\nbug_severity           924624  non-null values\nbug_status             924624  non-null values\ncreation_ts            924624  non-null values\ndelta_ts               924624  non-null values\nshort_desc             924624  non-null values\nhost_op_sys            924624  non-null values\nguest_op_sys           924624  non-null values\npriority               924624  non-null values\nrep_platform           924624  non-null values\nreporter               924624  non-null values\nproduct_id             924624  non-null values\ncategory_id            924624  non-null values\ncomponent_id           924624  non-null values\nresolution             924624  non-null values\ntarget_milestone       924624  non-null values\nqa_contact             924624  non-null values\nstatus_whiteboard      924624  non-null values\nvotes                  924624  non-null values\nkeywords               924624  non-null values\nlastdiffed             924509  non-null values\neverconfirmed          924624  non-null values\nreporter_accessible    924624  non-null values\ncclist_accessible      924624  non-null values\nestimated_time         924624  non-null values\nremaining_time         924624  non-null values\ndeadline               0  non-null values\nalias                  0  non-null values\nfound_in_product_id    924624  non-null values\nfound_in_version_id    924624  non-null values\nfound_in_phase_id      924624  non-null values\ncf_type                924624  non-null values\ncf_reported_by         924624  non-null values\ncf_attempted           89622  non-null values\ncf_failed              89587  non-null values\ncf_public_summary      510799  non-null values\ncf_doc_impact          924624  non-null values\ncf_security            924624  non-null values\ncf_build               327460  non-null values\ncf_branch              614929  non-null values\ncf_change              300612  non-null values\ncf_test_id             12610  non-null values\ncf_regression          924624  non-null values\ncf_reviewer            924624  non-null values\ncf_on_hold             924624  non-null values\ncf_public_severity     924624  non-null values\ncf_i18n_impact         924624  non-null values\ncf_eta                 3910  non-null values\ncf_bug_source          924624  non-null values\ncf_viss                725  non-null values\ndtypes: float64(5), int64(19), object(28)\n</pre>\n\n<ul>\n<li>after 'convert_objects()':</li>\n</ul>\n\n<pre>\ndtypes: datetime64[ns](2), float64(5), int64(19), object(26)\n</pre>\n\n<ul>\n<li>and putting the converted dataframe into hdfstore costs: <strong>749.50 s</strong> :)\n<ul>\n<li>it seems that reducing the number of 'object' dtypes is the key to decrease time costs</li>\n</ul></li>\n<li>and putting the converted dataframe into hdfstore with the param 'table' is true still returns that error</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.datetime' has no len()\n</pre>\n\n<ul>\n<li>I'm trying to put the dataframe without datetime columns</li>\n</ul>\n\n<hr>\n\n<h2>Update 4</h2>\n\n<ul>\n<li>There are 4 columns in mysql whose type is datetime:\n<ul>\n<li>creation_ts</li>\n<li>delta_ts</li>\n<li>lastdiffed</li>\n<li>deadline</li>\n</ul></li>\n</ul>\n\n<p>After calling the convert_objects():</p>\n\n<ul>\n<li>creation_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 1998-05-06 21:27:00\n</pre>\n\n<ul>\n<li>delta_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 2012-05-09 14:41:41\n</pre>\n\n<ul>\n<li>lastdiffed</li>\n</ul>\n\n<pre>\ndatetime.datetime(2012, 5, 9, 14, 41, 41)\n</pre>\n\n<ul>\n<li>deadline is always None, no matter before or after calling 'convert_objects'</li>\n</ul>\n\n<pre>\nNone\n</pre>\n\n<ul>\n<li>putting the dataframe without column 'lastdiff' costs <strong>691.75 s</strong></li>\n<li>when putting the dataframe without column 'lastdiff' and setting param 'table' equal to  True, I got an new error, :</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'Decimal' has no len()\n</pre>\n\n<ul>\n<li>the type of columns 'estimated_time', 'remaining_time', 'cf_viss' is 'decimal' in mysql</li>\n</ul>\n\n<hr>\n\n<h2>Update 5</h2>\n\n<ul>\n<li>I had transformed these 'decimal' type columns to 'float' type, by the code below:</li>\n</ul>\n\n<pre>\nno_diffed_converted_df_bugs.estimated_time = no_diffed_converted_df_bugs.estimated_time.map(float)\n</pre>\n\n<ul>\n<li>and now, the time costs is <strong>372.84 s</strong></li>\n<li>but the 'table' version putting still raised an error:</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.date' has no len()\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre><code>import pandas as pd\n\n# convert None to nan (its currently Object)\n# converts to float64 (or type of other objs)\nx = pd.Series([None])\nx = x.where(pd.notnull(x)).convert_objects()\n\n# convert datetime like with embeded nans to datetime64[ns]\ndf['foo'] = pd.Series(df['foo'].values, dtype = 'M8[ns]')\n</code></pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre><code>In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.10.1.dev'\n\nIn [3]: import tables\n\nIn [4]: tables.__version__\nOut[4]: '2.3.1'\n\nIn [4]: df = pd.DataFrame(np.random.randn(1000 * 1000, 100), index=range(int(\n   ...: 1000 * 1000)), columns=['E%03d' % i for i in xrange(100)])\n\nIn [5]: for x in range(20):\n   ...:     df['String%03d' % x] = 'string%03d' % x\n\nIn [6]: df\nOut[6]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nColumns: 120 entries, E000 to String019\ndtypes: float64(100), object(20)\n\n# storer put (cannot query) \nIn [9]: def test_put():\n   ...:     store = pd.HDFStore('test_put.h5','w')\n   ...:     store['df'] = df\n   ...:     store.close()\n\nIn [10]: %timeit test_put()\n1 loops, best of 3: 7.65 s per loop\n\n# table put (can query)\nIn [7]: def test_put():\n      ....:     store = pd.HDFStore('test_put.h5','w')\n      ....:     store.put('df',df,table=True)\n      ....:     store.close()\n\n\nIn [8]: %timeit test_put()\n1 loops, best of 3: 21.4 s per loop\n</code></pre>\n\n\n<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre><code>import pandas as pd\n\n# convert None to nan (its currently Object)\n# converts to float64 (or type of other objs)\nx = pd.Series([None])\nx = x.where(pd.notnull(x)).convert_objects()\n\n# convert datetime like with embeded nans to datetime64[ns]\ndf['foo'] = pd.Series(df['foo'].values, dtype = 'M8[ns]')\n</code></pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre><code>In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.10.1.dev'\n\nIn [3]: import tables\n\nIn [4]: tables.__version__\nOut[4]: '2.3.1'\n\nIn [4]: df = pd.DataFrame(np.random.randn(1000 * 1000, 100), index=range(int(\n   ...: 1000 * 1000)), columns=['E%03d' % i for i in xrange(100)])\n\nIn [5]: for x in range(20):\n   ...:     df['String%03d' % x] = 'string%03d' % x\n\nIn [6]: df\nOut[6]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nColumns: 120 entries, E000 to String019\ndtypes: float64(100), object(20)\n\n# storer put (cannot query) \nIn [9]: def test_put():\n   ...:     store = pd.HDFStore('test_put.h5','w')\n   ...:     store['df'] = df\n   ...:     store.close()\n\nIn [10]: %timeit test_put()\n1 loops, best of 3: 7.65 s per loop\n\n# table put (can query)\nIn [7]: def test_put():\n      ....:     store = pd.HDFStore('test_put.h5','w')\n      ....:     store.put('df',df,table=True)\n      ....:     store.close()\n\n\nIn [8]: %timeit test_put()\n1 loops, best of 3: 21.4 s per loop\n</code></pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre><code>CPU times: user 42.07 s, sys: 28.17 s, total: 70.24 s\nWall time: 98.97 s\n</code></pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre><code>CPU times: user 42.07 s, sys: 28.17 s, total: 70.24 s\nWall time: 98.97 s\n</code></pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, "answer_1": {"type": "literal", "value": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre><code>import pandas as pd\n\n# convert None to nan (its currently Object)\n# converts to float64 (or type of other objs)\nx = pd.Series([None])\nx = x.where(pd.notnull(x)).convert_objects()\n\n# convert datetime like with embeded nans to datetime64[ns]\ndf['foo'] = pd.Series(df['foo'].values, dtype = 'M8[ns]')\n</code></pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre><code>In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.10.1.dev'\n\nIn [3]: import tables\n\nIn [4]: tables.__version__\nOut[4]: '2.3.1'\n\nIn [4]: df = pd.DataFrame(np.random.randn(1000 * 1000, 100), index=range(int(\n   ...: 1000 * 1000)), columns=['E%03d' % i for i in xrange(100)])\n\nIn [5]: for x in range(20):\n   ...:     df['String%03d' % x] = 'string%03d' % x\n\nIn [6]: df\nOut[6]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nColumns: 120 entries, E000 to String019\ndtypes: float64(100), object(20)\n\n# storer put (cannot query) \nIn [9]: def test_put():\n   ...:     store = pd.HDFStore('test_put.h5','w')\n   ...:     store['df'] = df\n   ...:     store.close()\n\nIn [10]: %timeit test_put()\n1 loops, best of 3: 7.65 s per loop\n\n# table put (can query)\nIn [7]: def test_put():\n      ....:     store = pd.HDFStore('test_put.h5','w')\n      ....:     store.put('df',df,table=True)\n      ....:     store.close()\n\n\nIn [8]: %timeit test_put()\n1 loops, best of 3: 21.4 s per loop\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "answer_2": {"type": "literal", "value": "<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre><code>CPU times: user 42.07 s, sys: 28.17 s, total: 70.24 s\nWall time: 98.97 s\n</code></pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I'm trying to build a ETL toolkit with pandas, hdf5.</p>\n\n<p>My plan was  </p>\n\n<ol>\n<li>extracting a table from mysql to a DataFrame;  </li>\n<li>put this DataFrame into a HDFStore;</li>\n</ol>\n\n<p>But when i was doing the step 2, i found putting a dataframe into a *.h5 file costs too much time.</p>\n\n<ul>\n<li>the size of table in source mysql server: 498MB\n<ul>\n<li>52 columns</li>\n<li>924,624 records</li>\n</ul></li>\n<li>the size of *.h5 file after putting the dataframe inside : 513MB\n<ul>\n<li>the 'put' operation costs 849.345677137 seconds</li>\n</ul></li>\n</ul>\n\n<p>My questions are:<br>\nIs this time costs normal?<br>\nIs there any way to make it faster?</p>\n\n<hr>\n\n<h2>Update 1</h2>\n\n<p>thanks Jeff</p>\n\n<ul>\n<li><p>my codes are pretty simple:</p>\n\n<p>extract_store = HDFStore('extract_store.h5')<br>\nextract_store['df_staff'] = df_staff</p></li>\n<li>and when i trying 'ptdump -av file.h5', i got an error, but i still could load the dataframe object from this h5 file:</li>\n</ul>\n\n<blockquote>\n  <p>tables.exceptions.HDF5ExtError: HDF5 error back trace</p>\n  \n  <p>File \"../../../src/H5F.c\", line 1512, in H5Fopen<br>\n      unable to open file   File \"../../../src/H5F.c\", line 1307, in H5F_open<br>\n      unable to read superblock   File \"../../../src/H5Fsuper.c\", line 305, in H5F_super_read<br>\n      unable to find file signature   File \"../../../src/H5Fsuper.c\", line 153, in H5F_locate_signature<br>\n      unable to find a valid file signature  </p>\n  \n  <p>End of HDF5 error back trace  </p>\n  \n  <p>Unable to open/create file 'extract_store.h5'  </p>\n</blockquote>\n\n<ul>\n<li>some other infos:<br>\n<ul>\n<li>pandas version: '0.10.0'</li>\n<li>os: ubuntu server 10.04 x86_64</li>\n<li>cpu: 8 * Intel(R) Xeon(R) CPU X5670  @ 2.93GHz</li>\n<li>MemTotal: 51634016 kB</li>\n</ul></li>\n</ul>\n\n<p>I will update the pandas to 0.10.1-dev and try again.</p>\n\n<hr>\n\n<h2>Update 2</h2>\n\n<ul>\n<li>I had updated pandas to '0.10.1.dev-6e2b6ea'</li>\n<li>but the time costs wasn't decreased, it costs 884.15 s seconds this time</li>\n<li>the output of 'ptdump -av file.h5 ' is :</li>\n</ul>\n\n<pre>\n    / (RootGroup) ''  \n      /._v_attrs (AttributeSet), 4 attributes:  \n       [CLASS := 'GROUP',  \n        PYTABLES_FORMAT_VERSION := '2.0',  \n        TITLE := '',  \n        VERSION := '1.0']  \n    /df_bugs (Group) ''  \n      /df_bugs._v_attrs (AttributeSet), 12 attributes:  \n       [CLASS := 'GROUP',  \n        TITLE := '',  \n        VERSION := '1.0',  \n        axis0_variety := 'regular',  \n        axis1_variety := 'regular',  \n        block0_items_variety := 'regular',  \n        block1_items_variety := 'regular',  \n        block2_items_variety := 'regular',  \n        nblocks := 3,  \n        ndim := 2,  \n        pandas_type := 'frame',  \n        pandas_version := '0.10.1']  \n    /df_bugs/axis0 (Array(52,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/axis0._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/axis1 (Array(924624,)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/axis1._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'integer',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_items (Array(5,)) ''  \n      atom := StringAtom(itemsize=12, shape=(), dflt='')  \n      maindim := 0   \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block0_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_values (Array(924624, 5)) ''  \n      atom := Float64Atom(shape=(), dflt=0.0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block0_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block1_items (Array(19,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block1_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block1_values (Array(924624, 19)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block1_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',   \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block2_items (Array(28,)) ''  \n      atom := StringAtom(itemsize=18, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block2_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',\n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block2_values (VLArray(1,)) ''  \n      atom = ObjectAtom()  \n      byteorder = 'irrelevant'  \n      nrows = 1  \n      flavor = 'numpy'  \n      /df_bugs/block2_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'VLARRAY',  \n        PSEUDOATOM := 'object',  \n        TITLE := '',   \n        VERSION := '1.3',  \n        transposed := True]  \n</pre>\n\n<ul>\n<li>and I had tried your code below (putting the dataframe into hdfstore with the param 'table' is True) , but got an error instead, it seemed like python's datatime type was not supported :</li>\n</ul>\n\n<blockquote>\n  <p>Exception: cannot find the correct atom type -> [dtype->object] object\n  of type 'datetime.datetime' has no len()</p>\n</blockquote>\n\n<hr>\n\n<h2>Update 3</h2>\n\n<p>thanks jeff. \nSorry for the delay.</p>\n\n<ul>\n<li>tables.<strong>version</strong> : '2.4.0'.</li>\n<li>yes, the 884 seconds is only the put operation costs without the pull operation from mysql</li>\n<li>a row of dataframe (df.ix[0]):</li>\n</ul>\n\n<pre>\nbug_id                                   1\nassigned_to                            185\nbug_file_loc                          None\nbug_severity                      critical\nbug_status                          closed\ncreation_ts            1998-05-06 21:27:00\ndelta_ts               2012-05-09 14:41:41\nshort_desc                    Two cursors.\nhost_op_sys                        Unknown\nguest_op_sys                       Unknown\npriority                                P3\nrep_platform                          IA32\nreporter                                56\nproduct_id                               7\ncategory_id                            983\ncomponent_id                         12925\nresolution                           fixed\ntarget_milestone                       ws1\nqa_contact                             412\nstatus_whiteboard                         \nvotes                                    0\nkeywords                                SR\nlastdiffed             2012-05-09 14:41:41\neverconfirmed                            1\nreporter_accessible                      1\ncclist_accessible                        1\nestimated_time                        0.00\nremaining_time                        0.00\ndeadline                              None\nalias                                 None\nfound_in_product_id                      0\nfound_in_version_id                      0\nfound_in_phase_id                        0\ncf_type                             Defect\ncf_reported_by                 Development\ncf_attempted                           NaN\ncf_failed                              NaN\ncf_public_summary                         \ncf_doc_impact                            0\ncf_security                              0\ncf_build                               NaN\ncf_branch                                 \ncf_change                              NaN\ncf_test_id                             NaN\ncf_regression                      Unknown\ncf_reviewer                              0\ncf_on_hold                               0\ncf_public_severity                     ---\ncf_i18n_impact                           0\ncf_eta                                None\ncf_bug_source                          ---\ncf_viss                               None\nName: 0, Length: 52\n</pre>\n\n<ul>\n<li>the picture of dataframe( just type 'df' in ipython notebook):</li>\n</ul>\n\n<pre>\n\nInt64Index: 924624 entries, 0 to 924623\nData columns:\nbug_id                 924624  non-null values\nassigned_to            924624  non-null values\nbug_file_loc           427318  non-null values\nbug_severity           924624  non-null values\nbug_status             924624  non-null values\ncreation_ts            924624  non-null values\ndelta_ts               924624  non-null values\nshort_desc             924624  non-null values\nhost_op_sys            924624  non-null values\nguest_op_sys           924624  non-null values\npriority               924624  non-null values\nrep_platform           924624  non-null values\nreporter               924624  non-null values\nproduct_id             924624  non-null values\ncategory_id            924624  non-null values\ncomponent_id           924624  non-null values\nresolution             924624  non-null values\ntarget_milestone       924624  non-null values\nqa_contact             924624  non-null values\nstatus_whiteboard      924624  non-null values\nvotes                  924624  non-null values\nkeywords               924624  non-null values\nlastdiffed             924509  non-null values\neverconfirmed          924624  non-null values\nreporter_accessible    924624  non-null values\ncclist_accessible      924624  non-null values\nestimated_time         924624  non-null values\nremaining_time         924624  non-null values\ndeadline               0  non-null values\nalias                  0  non-null values\nfound_in_product_id    924624  non-null values\nfound_in_version_id    924624  non-null values\nfound_in_phase_id      924624  non-null values\ncf_type                924624  non-null values\ncf_reported_by         924624  non-null values\ncf_attempted           89622  non-null values\ncf_failed              89587  non-null values\ncf_public_summary      510799  non-null values\ncf_doc_impact          924624  non-null values\ncf_security            924624  non-null values\ncf_build               327460  non-null values\ncf_branch              614929  non-null values\ncf_change              300612  non-null values\ncf_test_id             12610  non-null values\ncf_regression          924624  non-null values\ncf_reviewer            924624  non-null values\ncf_on_hold             924624  non-null values\ncf_public_severity     924624  non-null values\ncf_i18n_impact         924624  non-null values\ncf_eta                 3910  non-null values\ncf_bug_source          924624  non-null values\ncf_viss                725  non-null values\ndtypes: float64(5), int64(19), object(28)\n</pre>\n\n<ul>\n<li>after 'convert_objects()':</li>\n</ul>\n\n<pre>\ndtypes: datetime64[ns](2), float64(5), int64(19), object(26)\n</pre>\n\n<ul>\n<li>and putting the converted dataframe into hdfstore costs: <strong>749.50 s</strong> :)\n<ul>\n<li>it seems that reducing the number of 'object' dtypes is the key to decrease time costs</li>\n</ul></li>\n<li>and putting the converted dataframe into hdfstore with the param 'table' is true still returns that error</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.datetime' has no len()\n</pre>\n\n<ul>\n<li>I'm trying to put the dataframe without datetime columns</li>\n</ul>\n\n<hr>\n\n<h2>Update 4</h2>\n\n<ul>\n<li>There are 4 columns in mysql whose type is datetime:\n<ul>\n<li>creation_ts</li>\n<li>delta_ts</li>\n<li>lastdiffed</li>\n<li>deadline</li>\n</ul></li>\n</ul>\n\n<p>After calling the convert_objects():</p>\n\n<ul>\n<li>creation_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 1998-05-06 21:27:00\n</pre>\n\n<ul>\n<li>delta_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 2012-05-09 14:41:41\n</pre>\n\n<ul>\n<li>lastdiffed</li>\n</ul>\n\n<pre>\ndatetime.datetime(2012, 5, 9, 14, 41, 41)\n</pre>\n\n<ul>\n<li>deadline is always None, no matter before or after calling 'convert_objects'</li>\n</ul>\n\n<pre>\nNone\n</pre>\n\n<ul>\n<li>putting the dataframe without column 'lastdiff' costs <strong>691.75 s</strong></li>\n<li>when putting the dataframe without column 'lastdiff' and setting param 'table' equal to  True, I got an new error, :</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'Decimal' has no len()\n</pre>\n\n<ul>\n<li>the type of columns 'estimated_time', 'remaining_time', 'cf_viss' is 'decimal' in mysql</li>\n</ul>\n\n<hr>\n\n<h2>Update 5</h2>\n\n<ul>\n<li>I had transformed these 'decimal' type columns to 'float' type, by the code below:</li>\n</ul>\n\n<pre>\nno_diffed_converted_df_bugs.estimated_time = no_diffed_converted_df_bugs.estimated_time.map(float)\n</pre>\n\n<ul>\n<li>and now, the time costs is <strong>372.84 s</strong></li>\n<li>but the 'table' version putting still raised an error:</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.date' has no len()\n</pre>\n", "answer_wo_code": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre> </pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre> </pre>\n\n\n<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre> </pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre> </pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/tables.exceptions.HDF5ExtError"}, "class_func_label": {"type": "literal", "value": "tables.exceptions.HDF5ExtError"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "A low level HDF5 operation failed.\n\n    This exception is raised the low level PyTables components used for\n    accessing HDF5 files.  It usually signals that something is not\n    going well in the HDF5 library or even at the Input/Output level.\n\n    Errors in the HDF5 C library may be accompanied by an extensive\n    HDF5 back trace on standard error (see also\n    :func:`tables.silence_hdf5_messages`).\n\n    .. versionchanged:: 2.4\n\n    Parameters\n    ----------\n    message\n        error message\n    h5bt\n        This parameter (keyword only) controls the HDF5 back trace\n        handling. Any keyword arguments other than h5bt is ignored.\n\n        * if set to False the HDF5 back trace is ignored and the\n          :attr:`HDF5ExtError.h5backtrace` attribute is set to None\n        * if set to True the back trace is retrieved from the HDF5\n          library and stored in the :attr:`HDF5ExtError.h5backtrace`\n          attribute as a list of tuples\n        * if set to \"VERBOSE\" (default) the HDF5 back trace is\n          stored in the :attr:`HDF5ExtError.h5backtrace` attribute\n          and also included in the string representation of the\n          exception\n        * if not set (or set to None) the default policy is used\n          (see :attr:`HDF5ExtError.DEFAULT_H5_BACKTRACE_POLICY`)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/14355151"}, "title": {"type": "literal", "value": "how to make pandas HDFStore 'put' operation faster"}, "content": {"type": "literal", "value": "<p>I'm trying to build a ETL toolkit with pandas, hdf5.</p>\n\n<p>My plan was  </p>\n\n<ol>\n<li>extracting a table from mysql to a DataFrame;  </li>\n<li>put this DataFrame into a HDFStore;</li>\n</ol>\n\n<p>But when i was doing the step 2, i found putting a dataframe into a *.h5 file costs too much time.</p>\n\n<ul>\n<li>the size of table in source mysql server: 498MB\n<ul>\n<li>52 columns</li>\n<li>924,624 records</li>\n</ul></li>\n<li>the size of *.h5 file after putting the dataframe inside : 513MB\n<ul>\n<li>the 'put' operation costs 849.345677137 seconds</li>\n</ul></li>\n</ul>\n\n<p>My questions are:<br>\nIs this time costs normal?<br>\nIs there any way to make it faster?</p>\n\n<hr>\n\n<h2>Update 1</h2>\n\n<p>thanks Jeff</p>\n\n<ul>\n<li><p>my codes are pretty simple:</p>\n\n<p>extract_store = HDFStore('extract_store.h5')<br>\nextract_store['df_staff'] = df_staff</p></li>\n<li>and when i trying 'ptdump -av file.h5', i got an error, but i still could load the dataframe object from this h5 file:</li>\n</ul>\n\n<blockquote>\n  <p>tables.exceptions.HDF5ExtError: HDF5 error back trace</p>\n  \n  <p>File \"../../../src/H5F.c\", line 1512, in H5Fopen<br>\n      unable to open file   File \"../../../src/H5F.c\", line 1307, in H5F_open<br>\n      unable to read superblock   File \"../../../src/H5Fsuper.c\", line 305, in H5F_super_read<br>\n      unable to find file signature   File \"../../../src/H5Fsuper.c\", line 153, in H5F_locate_signature<br>\n      unable to find a valid file signature  </p>\n  \n  <p>End of HDF5 error back trace  </p>\n  \n  <p>Unable to open/create file 'extract_store.h5'  </p>\n</blockquote>\n\n<ul>\n<li>some other infos:<br>\n<ul>\n<li>pandas version: '0.10.0'</li>\n<li>os: ubuntu server 10.04 x86_64</li>\n<li>cpu: 8 * Intel(R) Xeon(R) CPU X5670  @ 2.93GHz</li>\n<li>MemTotal: 51634016 kB</li>\n</ul></li>\n</ul>\n\n<p>I will update the pandas to 0.10.1-dev and try again.</p>\n\n<hr>\n\n<h2>Update 2</h2>\n\n<ul>\n<li>I had updated pandas to '0.10.1.dev-6e2b6ea'</li>\n<li>but the time costs wasn't decreased, it costs 884.15 s seconds this time</li>\n<li>the output of 'ptdump -av file.h5 ' is :</li>\n</ul>\n\n<pre>\n    / (RootGroup) ''  \n      /._v_attrs (AttributeSet), 4 attributes:  \n       [CLASS := 'GROUP',  \n        PYTABLES_FORMAT_VERSION := '2.0',  \n        TITLE := '',  \n        VERSION := '1.0']  \n    /df_bugs (Group) ''  \n      /df_bugs._v_attrs (AttributeSet), 12 attributes:  \n       [CLASS := 'GROUP',  \n        TITLE := '',  \n        VERSION := '1.0',  \n        axis0_variety := 'regular',  \n        axis1_variety := 'regular',  \n        block0_items_variety := 'regular',  \n        block1_items_variety := 'regular',  \n        block2_items_variety := 'regular',  \n        nblocks := 3,  \n        ndim := 2,  \n        pandas_type := 'frame',  \n        pandas_version := '0.10.1']  \n    /df_bugs/axis0 (Array(52,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/axis0._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/axis1 (Array(924624,)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/axis1._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'integer',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_items (Array(5,)) ''  \n      atom := StringAtom(itemsize=12, shape=(), dflt='')  \n      maindim := 0   \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block0_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_values (Array(924624, 5)) ''  \n      atom := Float64Atom(shape=(), dflt=0.0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block0_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block1_items (Array(19,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block1_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block1_values (Array(924624, 19)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block1_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',   \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block2_items (Array(28,)) ''  \n      atom := StringAtom(itemsize=18, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block2_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',\n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block2_values (VLArray(1,)) ''  \n      atom = ObjectAtom()  \n      byteorder = 'irrelevant'  \n      nrows = 1  \n      flavor = 'numpy'  \n      /df_bugs/block2_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'VLARRAY',  \n        PSEUDOATOM := 'object',  \n        TITLE := '',   \n        VERSION := '1.3',  \n        transposed := True]  \n</pre>\n\n<ul>\n<li>and I had tried your code below (putting the dataframe into hdfstore with the param 'table' is True) , but got an error instead, it seemed like python's datatime type was not supported :</li>\n</ul>\n\n<blockquote>\n  <p>Exception: cannot find the correct atom type -> [dtype->object] object\n  of type 'datetime.datetime' has no len()</p>\n</blockquote>\n\n<hr>\n\n<h2>Update 3</h2>\n\n<p>thanks jeff. \nSorry for the delay.</p>\n\n<ul>\n<li>tables.<strong>version</strong> : '2.4.0'.</li>\n<li>yes, the 884 seconds is only the put operation costs without the pull operation from mysql</li>\n<li>a row of dataframe (df.ix[0]):</li>\n</ul>\n\n<pre>\nbug_id                                   1\nassigned_to                            185\nbug_file_loc                          None\nbug_severity                      critical\nbug_status                          closed\ncreation_ts            1998-05-06 21:27:00\ndelta_ts               2012-05-09 14:41:41\nshort_desc                    Two cursors.\nhost_op_sys                        Unknown\nguest_op_sys                       Unknown\npriority                                P3\nrep_platform                          IA32\nreporter                                56\nproduct_id                               7\ncategory_id                            983\ncomponent_id                         12925\nresolution                           fixed\ntarget_milestone                       ws1\nqa_contact                             412\nstatus_whiteboard                         \nvotes                                    0\nkeywords                                SR\nlastdiffed             2012-05-09 14:41:41\neverconfirmed                            1\nreporter_accessible                      1\ncclist_accessible                        1\nestimated_time                        0.00\nremaining_time                        0.00\ndeadline                              None\nalias                                 None\nfound_in_product_id                      0\nfound_in_version_id                      0\nfound_in_phase_id                        0\ncf_type                             Defect\ncf_reported_by                 Development\ncf_attempted                           NaN\ncf_failed                              NaN\ncf_public_summary                         \ncf_doc_impact                            0\ncf_security                              0\ncf_build                               NaN\ncf_branch                                 \ncf_change                              NaN\ncf_test_id                             NaN\ncf_regression                      Unknown\ncf_reviewer                              0\ncf_on_hold                               0\ncf_public_severity                     ---\ncf_i18n_impact                           0\ncf_eta                                None\ncf_bug_source                          ---\ncf_viss                               None\nName: 0, Length: 52\n</pre>\n\n<ul>\n<li>the picture of dataframe( just type 'df' in ipython notebook):</li>\n</ul>\n\n<pre>\n\nInt64Index: 924624 entries, 0 to 924623\nData columns:\nbug_id                 924624  non-null values\nassigned_to            924624  non-null values\nbug_file_loc           427318  non-null values\nbug_severity           924624  non-null values\nbug_status             924624  non-null values\ncreation_ts            924624  non-null values\ndelta_ts               924624  non-null values\nshort_desc             924624  non-null values\nhost_op_sys            924624  non-null values\nguest_op_sys           924624  non-null values\npriority               924624  non-null values\nrep_platform           924624  non-null values\nreporter               924624  non-null values\nproduct_id             924624  non-null values\ncategory_id            924624  non-null values\ncomponent_id           924624  non-null values\nresolution             924624  non-null values\ntarget_milestone       924624  non-null values\nqa_contact             924624  non-null values\nstatus_whiteboard      924624  non-null values\nvotes                  924624  non-null values\nkeywords               924624  non-null values\nlastdiffed             924509  non-null values\neverconfirmed          924624  non-null values\nreporter_accessible    924624  non-null values\ncclist_accessible      924624  non-null values\nestimated_time         924624  non-null values\nremaining_time         924624  non-null values\ndeadline               0  non-null values\nalias                  0  non-null values\nfound_in_product_id    924624  non-null values\nfound_in_version_id    924624  non-null values\nfound_in_phase_id      924624  non-null values\ncf_type                924624  non-null values\ncf_reported_by         924624  non-null values\ncf_attempted           89622  non-null values\ncf_failed              89587  non-null values\ncf_public_summary      510799  non-null values\ncf_doc_impact          924624  non-null values\ncf_security            924624  non-null values\ncf_build               327460  non-null values\ncf_branch              614929  non-null values\ncf_change              300612  non-null values\ncf_test_id             12610  non-null values\ncf_regression          924624  non-null values\ncf_reviewer            924624  non-null values\ncf_on_hold             924624  non-null values\ncf_public_severity     924624  non-null values\ncf_i18n_impact         924624  non-null values\ncf_eta                 3910  non-null values\ncf_bug_source          924624  non-null values\ncf_viss                725  non-null values\ndtypes: float64(5), int64(19), object(28)\n</pre>\n\n<ul>\n<li>after 'convert_objects()':</li>\n</ul>\n\n<pre>\ndtypes: datetime64[ns](2), float64(5), int64(19), object(26)\n</pre>\n\n<ul>\n<li>and putting the converted dataframe into hdfstore costs: <strong>749.50 s</strong> :)\n<ul>\n<li>it seems that reducing the number of 'object' dtypes is the key to decrease time costs</li>\n</ul></li>\n<li>and putting the converted dataframe into hdfstore with the param 'table' is true still returns that error</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.datetime' has no len()\n</pre>\n\n<ul>\n<li>I'm trying to put the dataframe without datetime columns</li>\n</ul>\n\n<hr>\n\n<h2>Update 4</h2>\n\n<ul>\n<li>There are 4 columns in mysql whose type is datetime:\n<ul>\n<li>creation_ts</li>\n<li>delta_ts</li>\n<li>lastdiffed</li>\n<li>deadline</li>\n</ul></li>\n</ul>\n\n<p>After calling the convert_objects():</p>\n\n<ul>\n<li>creation_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 1998-05-06 21:27:00\n</pre>\n\n<ul>\n<li>delta_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 2012-05-09 14:41:41\n</pre>\n\n<ul>\n<li>lastdiffed</li>\n</ul>\n\n<pre>\ndatetime.datetime(2012, 5, 9, 14, 41, 41)\n</pre>\n\n<ul>\n<li>deadline is always None, no matter before or after calling 'convert_objects'</li>\n</ul>\n\n<pre>\nNone\n</pre>\n\n<ul>\n<li>putting the dataframe without column 'lastdiff' costs <strong>691.75 s</strong></li>\n<li>when putting the dataframe without column 'lastdiff' and setting param 'table' equal to  True, I got an new error, :</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'Decimal' has no len()\n</pre>\n\n<ul>\n<li>the type of columns 'estimated_time', 'remaining_time', 'cf_viss' is 'decimal' in mysql</li>\n</ul>\n\n<hr>\n\n<h2>Update 5</h2>\n\n<ul>\n<li>I had transformed these 'decimal' type columns to 'float' type, by the code below:</li>\n</ul>\n\n<pre>\nno_diffed_converted_df_bugs.estimated_time = no_diffed_converted_df_bugs.estimated_time.map(float)\n</pre>\n\n<ul>\n<li>and now, the time costs is <strong>372.84 s</strong></li>\n<li>but the 'table' version putting still raised an error:</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.date' has no len()\n</pre>\n"}, "answerContent": {"type": "literal", "value": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre><code>import pandas as pd\n\n# convert None to nan (its currently Object)\n# converts to float64 (or type of other objs)\nx = pd.Series([None])\nx = x.where(pd.notnull(x)).convert_objects()\n\n# convert datetime like with embeded nans to datetime64[ns]\ndf['foo'] = pd.Series(df['foo'].values, dtype = 'M8[ns]')\n</code></pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre><code>In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.10.1.dev'\n\nIn [3]: import tables\n\nIn [4]: tables.__version__\nOut[4]: '2.3.1'\n\nIn [4]: df = pd.DataFrame(np.random.randn(1000 * 1000, 100), index=range(int(\n   ...: 1000 * 1000)), columns=['E%03d' % i for i in xrange(100)])\n\nIn [5]: for x in range(20):\n   ...:     df['String%03d' % x] = 'string%03d' % x\n\nIn [6]: df\nOut[6]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nColumns: 120 entries, E000 to String019\ndtypes: float64(100), object(20)\n\n# storer put (cannot query) \nIn [9]: def test_put():\n   ...:     store = pd.HDFStore('test_put.h5','w')\n   ...:     store['df'] = df\n   ...:     store.close()\n\nIn [10]: %timeit test_put()\n1 loops, best of 3: 7.65 s per loop\n\n# table put (can query)\nIn [7]: def test_put():\n      ....:     store = pd.HDFStore('test_put.h5','w')\n      ....:     store.put('df',df,table=True)\n      ....:     store.close()\n\n\nIn [8]: %timeit test_put()\n1 loops, best of 3: 21.4 s per loop\n</code></pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre><code>CPU times: user 42.07 s, sys: 28.17 s, total: 70.24 s\nWall time: 98.97 s\n</code></pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, "answer_1": {"type": "literal", "value": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre><code>import pandas as pd\n\n# convert None to nan (its currently Object)\n# converts to float64 (or type of other objs)\nx = pd.Series([None])\nx = x.where(pd.notnull(x)).convert_objects()\n\n# convert datetime like with embeded nans to datetime64[ns]\ndf['foo'] = pd.Series(df['foo'].values, dtype = 'M8[ns]')\n</code></pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre><code>In [1]: import numpy as np\n\nIn [2]: import pandas as pd\n\nIn [3]: pd.__version__\nOut[3]: '0.10.1.dev'\n\nIn [3]: import tables\n\nIn [4]: tables.__version__\nOut[4]: '2.3.1'\n\nIn [4]: df = pd.DataFrame(np.random.randn(1000 * 1000, 100), index=range(int(\n   ...: 1000 * 1000)), columns=['E%03d' % i for i in xrange(100)])\n\nIn [5]: for x in range(20):\n   ...:     df['String%03d' % x] = 'string%03d' % x\n\nIn [6]: df\nOut[6]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nColumns: 120 entries, E000 to String019\ndtypes: float64(100), object(20)\n\n# storer put (cannot query) \nIn [9]: def test_put():\n   ...:     store = pd.HDFStore('test_put.h5','w')\n   ...:     store['df'] = df\n   ...:     store.close()\n\nIn [10]: %timeit test_put()\n1 loops, best of 3: 7.65 s per loop\n\n# table put (can query)\nIn [7]: def test_put():\n      ....:     store = pd.HDFStore('test_put.h5','w')\n      ....:     store.put('df',df,table=True)\n      ....:     store.close()\n\n\nIn [8]: %timeit test_put()\n1 loops, best of 3: 21.4 s per loop\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "4"}, "answer_2": {"type": "literal", "value": "<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre><code>CPU times: user 42.07 s, sys: 28.17 s, total: 70.24 s\nWall time: 98.97 s\n</code></pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I'm trying to build a ETL toolkit with pandas, hdf5.</p>\n\n<p>My plan was  </p>\n\n<ol>\n<li>extracting a table from mysql to a DataFrame;  </li>\n<li>put this DataFrame into a HDFStore;</li>\n</ol>\n\n<p>But when i was doing the step 2, i found putting a dataframe into a *.h5 file costs too much time.</p>\n\n<ul>\n<li>the size of table in source mysql server: 498MB\n<ul>\n<li>52 columns</li>\n<li>924,624 records</li>\n</ul></li>\n<li>the size of *.h5 file after putting the dataframe inside : 513MB\n<ul>\n<li>the 'put' operation costs 849.345677137 seconds</li>\n</ul></li>\n</ul>\n\n<p>My questions are:<br>\nIs this time costs normal?<br>\nIs there any way to make it faster?</p>\n\n<hr>\n\n<h2>Update 1</h2>\n\n<p>thanks Jeff</p>\n\n<ul>\n<li><p>my codes are pretty simple:</p>\n\n<p>extract_store = HDFStore('extract_store.h5')<br>\nextract_store['df_staff'] = df_staff</p></li>\n<li>and when i trying 'ptdump -av file.h5', i got an error, but i still could load the dataframe object from this h5 file:</li>\n</ul>\n\n<blockquote>\n  <p>tables.exceptions.HDF5ExtError: HDF5 error back trace</p>\n  \n  <p>File \"../../../src/H5F.c\", line 1512, in H5Fopen<br>\n      unable to open file   File \"../../../src/H5F.c\", line 1307, in H5F_open<br>\n      unable to read superblock   File \"../../../src/H5Fsuper.c\", line 305, in H5F_super_read<br>\n      unable to find file signature   File \"../../../src/H5Fsuper.c\", line 153, in H5F_locate_signature<br>\n      unable to find a valid file signature  </p>\n  \n  <p>End of HDF5 error back trace  </p>\n  \n  <p>Unable to open/create file 'extract_store.h5'  </p>\n</blockquote>\n\n<ul>\n<li>some other infos:<br>\n<ul>\n<li>pandas version: '0.10.0'</li>\n<li>os: ubuntu server 10.04 x86_64</li>\n<li>cpu: 8 * Intel(R) Xeon(R) CPU X5670  @ 2.93GHz</li>\n<li>MemTotal: 51634016 kB</li>\n</ul></li>\n</ul>\n\n<p>I will update the pandas to 0.10.1-dev and try again.</p>\n\n<hr>\n\n<h2>Update 2</h2>\n\n<ul>\n<li>I had updated pandas to '0.10.1.dev-6e2b6ea'</li>\n<li>but the time costs wasn't decreased, it costs 884.15 s seconds this time</li>\n<li>the output of 'ptdump -av file.h5 ' is :</li>\n</ul>\n\n<pre>\n    / (RootGroup) ''  \n      /._v_attrs (AttributeSet), 4 attributes:  \n       [CLASS := 'GROUP',  \n        PYTABLES_FORMAT_VERSION := '2.0',  \n        TITLE := '',  \n        VERSION := '1.0']  \n    /df_bugs (Group) ''  \n      /df_bugs._v_attrs (AttributeSet), 12 attributes:  \n       [CLASS := 'GROUP',  \n        TITLE := '',  \n        VERSION := '1.0',  \n        axis0_variety := 'regular',  \n        axis1_variety := 'regular',  \n        block0_items_variety := 'regular',  \n        block1_items_variety := 'regular',  \n        block2_items_variety := 'regular',  \n        nblocks := 3,  \n        ndim := 2,  \n        pandas_type := 'frame',  \n        pandas_version := '0.10.1']  \n    /df_bugs/axis0 (Array(52,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/axis0._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/axis1 (Array(924624,)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/axis1._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'integer',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_items (Array(5,)) ''  \n      atom := StringAtom(itemsize=12, shape=(), dflt='')  \n      maindim := 0   \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block0_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block0_values (Array(924624, 5)) ''  \n      atom := Float64Atom(shape=(), dflt=0.0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block0_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block1_items (Array(19,)) ''  \n      atom := StringAtom(itemsize=19, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block1_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',  \n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block1_values (Array(924624, 19)) ''  \n      atom := Int64Atom(shape=(), dflt=0)  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'little'  \n      chunkshape := None  \n      /df_bugs/block1_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',   \n        VERSION := '2.3',  \n        transposed := True]  \n    /df_bugs/block2_items (Array(28,)) ''  \n      atom := StringAtom(itemsize=18, shape=(), dflt='')  \n      maindim := 0  \n      flavor := 'numpy'  \n      byteorder := 'irrelevant'  \n      chunkshape := None  \n      /df_bugs/block2_items._v_attrs (AttributeSet), 7 attributes:  \n       [CLASS := 'ARRAY',  \n        FLAVOR := 'numpy',  \n        TITLE := '',  \n        VERSION := '2.3',\n        kind := 'string',  \n        name := None,  \n        transposed := True]  \n    /df_bugs/block2_values (VLArray(1,)) ''  \n      atom = ObjectAtom()  \n      byteorder = 'irrelevant'  \n      nrows = 1  \n      flavor = 'numpy'  \n      /df_bugs/block2_values._v_attrs (AttributeSet), 5 attributes:  \n       [CLASS := 'VLARRAY',  \n        PSEUDOATOM := 'object',  \n        TITLE := '',   \n        VERSION := '1.3',  \n        transposed := True]  \n</pre>\n\n<ul>\n<li>and I had tried your code below (putting the dataframe into hdfstore with the param 'table' is True) , but got an error instead, it seemed like python's datatime type was not supported :</li>\n</ul>\n\n<blockquote>\n  <p>Exception: cannot find the correct atom type -> [dtype->object] object\n  of type 'datetime.datetime' has no len()</p>\n</blockquote>\n\n<hr>\n\n<h2>Update 3</h2>\n\n<p>thanks jeff. \nSorry for the delay.</p>\n\n<ul>\n<li>tables.<strong>version</strong> : '2.4.0'.</li>\n<li>yes, the 884 seconds is only the put operation costs without the pull operation from mysql</li>\n<li>a row of dataframe (df.ix[0]):</li>\n</ul>\n\n<pre>\nbug_id                                   1\nassigned_to                            185\nbug_file_loc                          None\nbug_severity                      critical\nbug_status                          closed\ncreation_ts            1998-05-06 21:27:00\ndelta_ts               2012-05-09 14:41:41\nshort_desc                    Two cursors.\nhost_op_sys                        Unknown\nguest_op_sys                       Unknown\npriority                                P3\nrep_platform                          IA32\nreporter                                56\nproduct_id                               7\ncategory_id                            983\ncomponent_id                         12925\nresolution                           fixed\ntarget_milestone                       ws1\nqa_contact                             412\nstatus_whiteboard                         \nvotes                                    0\nkeywords                                SR\nlastdiffed             2012-05-09 14:41:41\neverconfirmed                            1\nreporter_accessible                      1\ncclist_accessible                        1\nestimated_time                        0.00\nremaining_time                        0.00\ndeadline                              None\nalias                                 None\nfound_in_product_id                      0\nfound_in_version_id                      0\nfound_in_phase_id                        0\ncf_type                             Defect\ncf_reported_by                 Development\ncf_attempted                           NaN\ncf_failed                              NaN\ncf_public_summary                         \ncf_doc_impact                            0\ncf_security                              0\ncf_build                               NaN\ncf_branch                                 \ncf_change                              NaN\ncf_test_id                             NaN\ncf_regression                      Unknown\ncf_reviewer                              0\ncf_on_hold                               0\ncf_public_severity                     ---\ncf_i18n_impact                           0\ncf_eta                                None\ncf_bug_source                          ---\ncf_viss                               None\nName: 0, Length: 52\n</pre>\n\n<ul>\n<li>the picture of dataframe( just type 'df' in ipython notebook):</li>\n</ul>\n\n<pre>\n\nInt64Index: 924624 entries, 0 to 924623\nData columns:\nbug_id                 924624  non-null values\nassigned_to            924624  non-null values\nbug_file_loc           427318  non-null values\nbug_severity           924624  non-null values\nbug_status             924624  non-null values\ncreation_ts            924624  non-null values\ndelta_ts               924624  non-null values\nshort_desc             924624  non-null values\nhost_op_sys            924624  non-null values\nguest_op_sys           924624  non-null values\npriority               924624  non-null values\nrep_platform           924624  non-null values\nreporter               924624  non-null values\nproduct_id             924624  non-null values\ncategory_id            924624  non-null values\ncomponent_id           924624  non-null values\nresolution             924624  non-null values\ntarget_milestone       924624  non-null values\nqa_contact             924624  non-null values\nstatus_whiteboard      924624  non-null values\nvotes                  924624  non-null values\nkeywords               924624  non-null values\nlastdiffed             924509  non-null values\neverconfirmed          924624  non-null values\nreporter_accessible    924624  non-null values\ncclist_accessible      924624  non-null values\nestimated_time         924624  non-null values\nremaining_time         924624  non-null values\ndeadline               0  non-null values\nalias                  0  non-null values\nfound_in_product_id    924624  non-null values\nfound_in_version_id    924624  non-null values\nfound_in_phase_id      924624  non-null values\ncf_type                924624  non-null values\ncf_reported_by         924624  non-null values\ncf_attempted           89622  non-null values\ncf_failed              89587  non-null values\ncf_public_summary      510799  non-null values\ncf_doc_impact          924624  non-null values\ncf_security            924624  non-null values\ncf_build               327460  non-null values\ncf_branch              614929  non-null values\ncf_change              300612  non-null values\ncf_test_id             12610  non-null values\ncf_regression          924624  non-null values\ncf_reviewer            924624  non-null values\ncf_on_hold             924624  non-null values\ncf_public_severity     924624  non-null values\ncf_i18n_impact         924624  non-null values\ncf_eta                 3910  non-null values\ncf_bug_source          924624  non-null values\ncf_viss                725  non-null values\ndtypes: float64(5), int64(19), object(28)\n</pre>\n\n<ul>\n<li>after 'convert_objects()':</li>\n</ul>\n\n<pre>\ndtypes: datetime64[ns](2), float64(5), int64(19), object(26)\n</pre>\n\n<ul>\n<li>and putting the converted dataframe into hdfstore costs: <strong>749.50 s</strong> :)\n<ul>\n<li>it seems that reducing the number of 'object' dtypes is the key to decrease time costs</li>\n</ul></li>\n<li>and putting the converted dataframe into hdfstore with the param 'table' is true still returns that error</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.datetime' has no len()\n</pre>\n\n<ul>\n<li>I'm trying to put the dataframe without datetime columns</li>\n</ul>\n\n<hr>\n\n<h2>Update 4</h2>\n\n<ul>\n<li>There are 4 columns in mysql whose type is datetime:\n<ul>\n<li>creation_ts</li>\n<li>delta_ts</li>\n<li>lastdiffed</li>\n<li>deadline</li>\n</ul></li>\n</ul>\n\n<p>After calling the convert_objects():</p>\n\n<ul>\n<li>creation_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 1998-05-06 21:27:00\n</pre>\n\n<ul>\n<li>delta_ts:</li>\n</ul>\n\n<pre>\nTimestamp: 2012-05-09 14:41:41\n</pre>\n\n<ul>\n<li>lastdiffed</li>\n</ul>\n\n<pre>\ndatetime.datetime(2012, 5, 9, 14, 41, 41)\n</pre>\n\n<ul>\n<li>deadline is always None, no matter before or after calling 'convert_objects'</li>\n</ul>\n\n<pre>\nNone\n</pre>\n\n<ul>\n<li>putting the dataframe without column 'lastdiff' costs <strong>691.75 s</strong></li>\n<li>when putting the dataframe without column 'lastdiff' and setting param 'table' equal to  True, I got an new error, :</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'Decimal' has no len()\n</pre>\n\n<ul>\n<li>the type of columns 'estimated_time', 'remaining_time', 'cf_viss' is 'decimal' in mysql</li>\n</ul>\n\n<hr>\n\n<h2>Update 5</h2>\n\n<ul>\n<li>I had transformed these 'decimal' type columns to 'float' type, by the code below:</li>\n</ul>\n\n<pre>\nno_diffed_converted_df_bugs.estimated_time = no_diffed_converted_df_bugs.estimated_time.map(float)\n</pre>\n\n<ul>\n<li>and now, the time costs is <strong>372.84 s</strong></li>\n<li>but the 'table' version putting still raised an error:</li>\n</ul>\n\n<pre>\n/usr/local/lib/python2.6/dist-packages/pandas-0.10.1.dev_6e2b6ea-py2.6-linux-x86_64.egg/pandas/io/pytables.pyc in create_axes(self, axes, obj, validate, nan_rep, data_columns, min_itemsize, **kwargs)\n   2203                 raise\n   2204             except (Exception), detail:\n-> 2205                 raise Exception(\"cannot find the correct atom type -> [dtype->%s] %s\" % (b.dtype.name, str(detail)))\n   2206             j += 1\n   2207 \n\nException: cannot find the correct atom type -> [dtype->object] object of type 'datetime.date' has no len()\n</pre>\n", "answer_wo_code": "<p>I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables.</p>\n\n<ul>\n<li>Simple types (floats/ints/bools) that have a fixed represenation, these are mapped to fixed c-types</li>\n<li>Datetimes are handled if they can properly be converted (e.g. they have a dtype of 'datetime64[ns]', notably datetimes.date are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)</li>\n<li>Strings are mapped (in Storer objects to Object type, Table maps them to String types)</li>\n<li>Unicode are not handled </li>\n<li>all other types are handled as Object in Storers or an Exception is throw for Tables</li>\n</ul>\n\n<p>What this means is that if you are doing a <em>put</em> to a Storer (a fixed-representation), then all of the non-mappable types will become Object, see this. <strong>PyTables pickles these columns</strong>. See the below reference for ObjectAtom</p>\n\n<p><a href=\"http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants\" rel=\"nofollow\">http://pytables.github.com/usersguide/libref/declarative_classes.html#the-atom-class-and-its-descendants</a></p>\n\n<p>Table will raise on an invalid type (I should provide a better error message here). I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons).</p>\n\n<p>To force some types try some of these:</p>\n\n<pre> </pre>\n\n<p>Heres a sample on 64-bit linux (file is 1M rows, about 1 GB in size on disk)</p>\n\n<pre> </pre>\n\n\n<h2>How to make this faster?</h2>\n\n<ol>\n<li>use 'io.sql.read_frame' to load data from a sql db to a dataframe. Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float.</li>\n<li>fill the missing data for each columns.</li>\n<li>call the function 'DataFrame.convert_objects' before putting operation</li>\n<li>if having string type columns in dateframe, use 'table' instead of 'storer'</li>\n</ol>\n\n<p>store.put('key', df, table=True)</p>\n\n<p>After doing these jobs, the performance of putting operation has a big improvement with the same data set:</p>\n\n<pre> </pre>\n\n<hr>\n\n<p>Profile logs of the second test:</p>\n\n<pre>\n95984 function calls (95958 primitive calls) in 68.688 CPU seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n      445   16.757    0.038   16.757    0.038 {numpy.core.multiarray.array}\n       19   16.250    0.855   16.250    0.855 {method '_append_records' of 'tables.tableExtension.Table' objects}\n       16    7.958    0.497    7.958    0.497 {method 'astype' of 'numpy.ndarray' objects}\n       19    6.533    0.344    6.533    0.344 {pandas.lib.create_hdf_rows_2d}\n        4    6.284    1.571    6.388    1.597 {method '_fillCol' of 'tables.tableExtension.Row' objects}\n       20    2.640    0.132    2.641    0.132 {pandas.lib.maybe_convert_objects}\n        1    1.785    1.785    1.785    1.785 {pandas.lib.isnullobj}\n        7    1.619    0.231    1.619    0.231 {method 'flatten' of 'numpy.ndarray' objects}\n       11    1.059    0.096    1.059    0.096 {pandas.lib.infer_dtype}\n        1    0.997    0.997   41.952   41.952 pytables.py:2468(write_data)\n       19    0.985    0.052   40.590    2.136 pytables.py:2504(write_data_chunk)\n        1    0.827    0.827   60.617   60.617 pytables.py:2433(write)\n     1504    0.592    0.000    0.592    0.000 {method '_g_readSlice' of 'tables.hdf5Extension.Array' objects}\n        4    0.534    0.133   13.676    3.419 pytables.py:1038(set_atom)\n        1    0.528    0.528    0.528    0.528 {pandas.lib.max_len_string_array}\n        4    0.441    0.110    0.571    0.143 internals.py:1409(_stack_arrays)\n       35    0.358    0.010    0.358    0.010 {method 'copy' of 'numpy.ndarray' objects}\n        1    0.276    0.276    3.135    3.135 internals.py:208(fillna)\n        5    0.263    0.053    2.054    0.411 common.py:128(_isnull_ndarraylike)\n       48    0.253    0.005    0.253    0.005 {method '_append' of 'tables.hdf5Extension.Array' objects}\n        4    0.240    0.060    1.500    0.375 internals.py:1400(_simple_blockify)\n        1    0.234    0.234   12.145   12.145 pytables.py:1066(set_atom_string)\n       28    0.225    0.008    0.225    0.008 {method '_createCArray' of 'tables.hdf5Extension.Array' objects}\n       36    0.218    0.006    0.218    0.006 {method '_g_writeSlice' of 'tables.hdf5Extension.Array' objects}\n     6110    0.155    0.000    0.155    0.000 {numpy.core.multiarray.empty}\n        4    0.097    0.024    0.097    0.024 {method 'all' of 'numpy.ndarray' objects}\n        6    0.084    0.014    0.084    0.014 {tables.indexesExtension.keysort}\n       18    0.084    0.005    0.084    0.005 {method '_g_close' of 'tables.hdf5Extension.Leaf' objects}\n    11816    0.064    0.000    0.108    0.000 file.py:1036(_getNode)\n       19    0.053    0.003    0.053    0.003 {method '_g_flush' of 'tables.hdf5Extension.Leaf' objects}\n     1528    0.045    0.000    0.098    0.000 array.py:342(_interpret_indexing)\n    11709    0.040    0.000    0.042    0.000 file.py:248(__getitem__)\n        2    0.027    0.013    0.383    0.192 index.py:1099(get_neworder)\n        1    0.018    0.018    0.018    0.018 {numpy.core.multiarray.putmask}\n        4    0.013    0.003    0.017    0.004 index.py:607(final_idx32)\n</pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/joblib.dump"}, "class_func_label": {"type": "literal", "value": "joblib.dump"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nPersist an arbitrary Python object into one file.\n\nRead more in the :ref:`User Guide <persistence>`.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/56126062"}, "title": {"type": "literal", "value": "How to destroy Python objects and free up memory"}, "content": {"type": "literal", "value": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre><code>#file_list_1 contains 100,000 images\nfile_list_chunks = list(divide_chunks(file_list_1,20000))\nfor count,f in enumerate(file_list_chunks):\n    # make the Pool of workers\n    pool = ThreadPool(64) \n    results = pool.map(get_image_features,f)\n    # close the pool and wait for the work to finish \n    list_a, list_b = zip(*results)\n    df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n    df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n    del list_a\n    del list_b\n    del df\n    gc.collect()\n    pool.close() \n    pool.join()\n    print(\"pool closed\")\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "7"}, "answer_2": {"type": "literal", "value": "<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n"}, "answer_3_votes": {"type": "literal", "value": "1"}, "answer_4": {"type": "literal", "value": "<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n"}, "answer_4_votes": {"type": "literal", "value": "1"}, "answer_5": {"type": "literal", "value": "<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n"}, "answer_5_votes": {"type": "literal", "value": ""}, "answer_6": {"type": "literal", "value": "<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n"}, "answer_6_votes": {"type": "literal", "value": ""}, "answer_7": {"type": "literal", "value": "<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "1"}, "answer_8": {"type": "literal", "value": "<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n"}, "answer_8_votes": {"type": "literal", "value": ""}, "answer_9": {"type": "literal", "value": "<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_9_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre> </pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit   constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre> </pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like   might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre> </pre>\n\n<p>The explicit  s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre> </pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre> </pre>\n\n<p>and use context manager:</p>\n\n<pre> </pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre> </pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre> </pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using    ? That isn't really well documented (in  ) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on   to return a list of all declared  , together with their memory footprint. </li>\n<li>also call   (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre> </pre>\n\n<p>and then I would change the function   by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre> </pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p>  may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even   might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of  :</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing  </p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About  : in python, the command does not actually delete an object. It only decreases its reference counter. When you run  , the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/joblib.load"}, "class_func_label": {"type": "literal", "value": "joblib.load"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nReconstruct a Python object from a file persisted with joblib.dump.\n\nRead more in the :ref:`User Guide <persistence>`.\n\nWARNING: joblib.load relies on the pickle module and can therefore\nexecute arbitrary Python code. It should therefore never be used\nto load files from untrusted sources.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/56126062"}, "title": {"type": "literal", "value": "How to destroy Python objects and free up memory"}, "content": {"type": "literal", "value": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre><code>#file_list_1 contains 100,000 images\nfile_list_chunks = list(divide_chunks(file_list_1,20000))\nfor count,f in enumerate(file_list_chunks):\n    # make the Pool of workers\n    pool = ThreadPool(64) \n    results = pool.map(get_image_features,f)\n    # close the pool and wait for the work to finish \n    list_a, list_b = zip(*results)\n    df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n    df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n    del list_a\n    del list_b\n    del df\n    gc.collect()\n    pool.close() \n    pool.join()\n    print(\"pool closed\")\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "7"}, "answer_2": {"type": "literal", "value": "<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n"}, "answer_3_votes": {"type": "literal", "value": "1"}, "answer_4": {"type": "literal", "value": "<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n"}, "answer_4_votes": {"type": "literal", "value": "1"}, "answer_5": {"type": "literal", "value": "<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n"}, "answer_5_votes": {"type": "literal", "value": ""}, "answer_6": {"type": "literal", "value": "<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n"}, "answer_6_votes": {"type": "literal", "value": ""}, "answer_7": {"type": "literal", "value": "<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "1"}, "answer_8": {"type": "literal", "value": "<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n"}, "answer_8_votes": {"type": "literal", "value": ""}, "answer_9": {"type": "literal", "value": "<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_9_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre> </pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit   constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre> </pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like   might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre> </pre>\n\n<p>The explicit  s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre> </pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre> </pre>\n\n<p>and use context manager:</p>\n\n<pre> </pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre> </pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre> </pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using    ? That isn't really well documented (in  ) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on   to return a list of all declared  , together with their memory footprint. </li>\n<li>also call   (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre> </pre>\n\n<p>and then I would change the function   by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre> </pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p>  may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even   might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of  :</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing  </p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About  : in python, the command does not actually delete an object. It only decreases its reference counter. When you run  , the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/joblib.Parallel"}, "class_func_label": {"type": "literal", "value": "joblib.Parallel"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": " Helper class for readable parallel mapping.\n\n        Read more in the :ref:`User Guide <parallel>`.\n\n        Parameters\n        -----------\n        n_jobs: int, default: None\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n            None is a marker for 'unset' that will be interpreted as n_jobs=1\n            (sequential execution) unless the call is performed under a\n            parallel_backend context manager that sets another value for\n            n_jobs.\n        backend: str, ParallelBackendBase instance or None, default: 'loky'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"loky\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes.\n            - \"multiprocessing\" previous process-based backend based on\n              `multiprocessing.Pool`. Less robust than `loky`.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              register_parallel_backend. This will allow you to implement\n              a backend of your liking.\n\n            It is not recommended to hard-code the backend name in a call to\n            Parallel in a library. Instead it is recommended to set soft hints\n            (prefer) or hard constraints (require) so as to make it possible\n            for library users to change the backend from the outside using the\n            parallel_backend context manager.\n        prefer: str in {'processes', 'threads'} or None, default: None\n            Soft hint to choose the default backend if no specific backend\n            was selected with the parallel_backend context manager. The\n            default process-based backend is 'loky' and the default\n            thread-based backend is 'threading'. Ignored if the ``backend``\n            parameter is specified.\n        require: 'sharedmem' or None, default None\n            Hard constraint to select the backend. If set to 'sharedmem',\n            the selected backend will be single-host and thread-based even\n            if the user asked for a non-thread based backend with\n            parallel_backend.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the workers should never starve.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, dispatching\n            calls to workers can be slower than sequential computation because\n            of the overhead. Batching fast computations together can mitigate\n            this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmapping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAM disk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmapping of large arrays.\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n            Memmapping mode for numpy arrays passed to workers.\n            See 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses workers to compute in parallel the application of a\n        function to many different arguments. The main functionality it brings\n        in addition to using the raw multiprocessing or concurrent.futures API\n        are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/56126062"}, "title": {"type": "literal", "value": "How to destroy Python objects and free up memory"}, "content": {"type": "literal", "value": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre><code>#file_list_1 contains 100,000 images\nfile_list_chunks = list(divide_chunks(file_list_1,20000))\nfor count,f in enumerate(file_list_chunks):\n    # make the Pool of workers\n    pool = ThreadPool(64) \n    results = pool.map(get_image_features,f)\n    # close the pool and wait for the work to finish \n    list_a, list_b = zip(*results)\n    df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n    df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n    del list_a\n    del list_b\n    del df\n    gc.collect()\n    pool.close() \n    pool.join()\n    print(\"pool closed\")\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "7"}, "answer_2": {"type": "literal", "value": "<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n"}, "answer_3_votes": {"type": "literal", "value": "1"}, "answer_4": {"type": "literal", "value": "<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n"}, "answer_4_votes": {"type": "literal", "value": "1"}, "answer_5": {"type": "literal", "value": "<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n"}, "answer_5_votes": {"type": "literal", "value": ""}, "answer_6": {"type": "literal", "value": "<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n"}, "answer_6_votes": {"type": "literal", "value": ""}, "answer_7": {"type": "literal", "value": "<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "1"}, "answer_8": {"type": "literal", "value": "<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n"}, "answer_8_votes": {"type": "literal", "value": ""}, "answer_9": {"type": "literal", "value": "<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_9_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre> </pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit   constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre> </pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like   might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre> </pre>\n\n<p>The explicit  s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre> </pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre> </pre>\n\n<p>and use context manager:</p>\n\n<pre> </pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre> </pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre> </pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using    ? That isn't really well documented (in  ) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on   to return a list of all declared  , together with their memory footprint. </li>\n<li>also call   (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre> </pre>\n\n<p>and then I would change the function   by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre> </pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p>  may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even   might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of  :</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing  </p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About  : in python, the command does not actually delete an object. It only decreases its reference counter. When you run  , the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/56126062"}, "title": {"type": "literal", "value": "How to destroy Python objects and free up memory"}, "content": {"type": "literal", "value": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre><code>#file_list_1 contains 100,000 images\nfile_list_chunks = list(divide_chunks(file_list_1,20000))\nfor count,f in enumerate(file_list_chunks):\n    # make the Pool of workers\n    pool = ThreadPool(64) \n    results = pool.map(get_image_features,f)\n    # close the pool and wait for the work to finish \n    list_a, list_b = zip(*results)\n    df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n    df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n    del list_a\n    del list_b\n    del df\n    gc.collect()\n    pool.close() \n    pool.join()\n    print(\"pool closed\")\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))[30000:]\n</code></pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit <code>list</code> constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre><code>file_list_chunks = list(divide_chunks(file_list_1,20000))\n# becomes\nfile_list_chunks = divide_chunks(file_list_1,20000)\n</code></pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like <code>close</code> might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre><code>with ThreadPool(64) as pool: \n    results = pool.map(get_image_features,f)\n    # etc.\n</code></pre>\n\n<p>The explicit <code>del</code>s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre><code>with ThreadPool(..):\n    ...\n    pool.join()\ngc.collect()\n</code></pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre><code>import sqlite3\nconn = sqlite3.connect(':memory:', check_same_thread=False)  # or, use a file e.g. 'image-features.db'\n</code></pre>\n\n<p>and use context manager:</p>\n\n<pre><code>with conn:\n    conn.execute('''CREATE TABLE images\n                    (filename text, features text)''')\n\nwith conn:\n    # Insert a row of data\n    conn.execute(\"INSERT INTO images VALUES ('my-image.png','feature1,feature2')\")\n</code></pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre><code>results = pool.map(get_image_features, zip(itertools.repeat(conn), f))\n</code></pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre><code># main.py\n# a for loop to iterate over this\nsubprocess.check_call([\"python\", \"chunk.py\", \"0\", \"20000\"])\n\n# chunk.py a b\nfor count,f in enumerate(file_list_chunks):\n    if count &lt; int(sys.argv[1]) or count &gt; int(sys.argv[2]):\n         pass\n    # do stuff\n</code></pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "7"}, "answer_2": {"type": "literal", "value": "<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using <code>ThreadPool()</code> <code>from multiprocessing.pool</code>? That isn't really well documented (in <code>python3</code>) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on <code>sys.getsizeof()</code> to return a list of all declared <code>globals()</code>, together with their memory footprint. </li>\n<li>also call <code>del results</code> (although that shouldn't be to large, I guess)</li>\n</ul>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre><code>from multiprocessing import Pool\n\nif __name__ == '__main__':\n    cpus = multiprocessing.cpu_count()        \n    with Pool(cpus-1) as p:\n        p.map(get_image_features, file_list_1)\n</code></pre>\n\n<p>and then I would change the function <code>get_image_features</code> by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre><code>df = pd.DataFrame({'filename':list_a,'image_features':list_b})\ndf.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n</code></pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n"}, "answer_3_votes": {"type": "literal", "value": "1"}, "answer_4": {"type": "literal", "value": "<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n"}, "answer_4_votes": {"type": "literal", "value": "1"}, "answer_5": {"type": "literal", "value": "<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n"}, "answer_5_votes": {"type": "literal", "value": ""}, "answer_6": {"type": "literal", "value": "<p><code>pd.DataFrame(...)</code> may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even <code>del df</code> might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of <code>pd.DataFrame.__del__</code>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from ctypes import cdll, CDLL\ntry:\n    cdll.LoadLibrary(\"libc.so.6\")\n    libc = CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\nexcept (OSError, AttributeError):\n    libc = None\n\n\nif no libc:\n    print(\"Sorry, but pandas.DataFrame may leak over time even if it's instances are deleted...\")\n\n\nCHUNK_SIZE = 20000\n\n\n#file_list_1 contains 100,000 images\nwith ThreadPool(64) as pool:\n    for count,f in enumerate(divide_chunks(file_list_1, CHUNK_SIZE)):\n        # make the Pool of workers\n        results = pool.map(get_image_features,f)\n        # close the pool and wait for the work to finish \n        list_a, list_b = zip(*results)\n        df = pd.DataFrame({'filename':list_a,'image_features':list_b})\n        df.to_pickle(\"PATH_TO_FILE\"+str(count)+\".pickle\")\n\n        del df\n\n        # 2 new lines of code:\n        if libc:  # Fix leaking of pd.DataFrame(...)\n            libc.malloc_trim(0)\n\nprint(\"pool closed\")\n</code></pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing <code>CHUNK_SIZE</code></p>\n"}, "answer_6_votes": {"type": "literal", "value": ""}, "answer_7": {"type": "literal", "value": "<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About <code>del object</code>: in python, the command does not actually delete an object. It only decreases its reference counter. When you run <code>import gc; gc.collect()</code>, the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n"}, "answer_7_votes": {"type": "literal", "value": "1"}, "answer_8": {"type": "literal", "value": "<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n"}, "answer_8_votes": {"type": "literal", "value": ""}, "answer_9": {"type": "literal", "value": "<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, "answer_9_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I am trying to iterate over 100,000 images and capture some image features and store the resulting dataFrame on disk as a pickle file. </p>\n\n<p>Unfortunately due to RAM constraints, i am forced to split the images into chunks of 20,000 and perform operations on them before saving the results onto disk.</p>\n\n<p>The code written below is supposed to save the dataframe of results for 20,000 images before starting the loop to process the next 20,000 images. </p>\n\n<p>However - This does not seem to be solving my problem as the memory is not getting released from RAM at the end of the first for loop</p>\n\n<p>So somewhere while processing the 50,000th record, the program crashes due to Out of Memory Error.</p>\n\n<p>I tried deleting the objects after saving them to disk and invoking the garbage collector, however the RAM usage does not seem to be going down.</p>\n\n<p>What am i missing? </p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Now, it could be that something in the 50,000th is very large, and that's causing the OOM, so to test this I'd first try:</p>\n\n<pre> </pre>\n\n<p>If it fails at 10,000 this will confirm whether 20k is too big a chunksize, or if it fails at 50,000 again, there is an issue with the code...</p>\n\n<hr>\n\n<p>Okay, onto the code...</p>\n\n<p>Firstly, you don't need the explicit   constructor, it's much better in python to iterate rather than generate the entire the list into memory.</p>\n\n<pre> </pre>\n\n<p>I think you might be misusing ThreadPool here:</p>\n\n<blockquote>\n  <p>Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.</p>\n</blockquote>\n\n<p>This reads like   might have some thinks still running, although I guess this is safe it feels a little un-pythonic, it's better to use the context manager for ThreadPool:</p>\n\n<pre> </pre>\n\n<p>The explicit  s in python <a href=\"https://stackoverflow.com/a/1316788/1240268\">aren't actually guaranteed to free memory</a>.</p>\n\n<p>You should collect <em>after</em> the join/after the with:</p>\n\n<pre> </pre>\n\n<p>You could also try chunk this into smaller pieces e.g. 10,000 or even smaller!</p>\n\n<hr>\n\n<h3>Hammer 1</h3>\n\n<p>One thing, I would consider doing here, instead of using pandas DataFrames and large lists is to use a SQL database, you can do this locally with <a href=\"https://docs.python.org/3/library/sqlite3.html\" rel=\"noreferrer\">sqlite3</a>:</p>\n\n<pre> </pre>\n\n<p>and use context manager:</p>\n\n<pre> </pre>\n\n<p>That way, we won't have to handle the large list objects or DataFrame.</p>\n\n<p>You can pass the connection to each of the threads... you might have to something a little weird like:</p>\n\n<pre> </pre>\n\n<p>Then, after the calculation is complete you can select all from the database, into which ever format you like. E.g. using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>.</p>\n\n<hr>\n\n<h3>Hammer 2</h3>\n\n<p>Use a subprocess here, rather than running this in the same instance of python \"shell out\" to another.</p>\n\n<p>Since you can pass start and end to python as sys.args, you can slice these:</p>\n\n<pre> </pre>\n\n<p>That way, the subprocess will properly clean up python (there's no way there'll be memory leaks, since the process will be terminated).</p>\n\n<hr>\n\n<p>My bet is that Hammer 1 is the way to go, it feels like you're gluing up a lot of data, and reading it into python lists unnecessarily, and using sqlite3 (or some other database) completely avoids that.</p>\n\n\n<p><strong>Note: this is not an answer, rather a quick list of questions &amp; suggestions</strong></p>\n\n<ul>\n<li>Are you using    ? That isn't really well documented (in  ) and I'd rather use <a href=\"https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor\" rel=\"nofollow noreferrer\">ThreadPoolExecutor</a>, (also see <a href=\"https://stackoverflow.com/a/11529742/565489\">here</a>)</li>\n<li>try to debug which objects are held in memory at the very end of each loop, e.g. using <a href=\"https://stackoverflow.com/a/40997868/565489\">this solution</a> which relies on   to return a list of all declared  , together with their memory footprint. </li>\n<li>also call   (although that shouldn't be to large, I guess)</li>\n</ul>\n\n\n<p>Your problem is that you are using threading where multiprocessing should be used (CPU bound vs IO bound).</p>\n\n<p>I would refactor your code a bit like this:</p>\n\n<pre> </pre>\n\n<p>and then I would change the function   by appending (something like) these two lines to the end of it. I can't tell how exactly you are processing those images but the idea is to do every image inside each process and then immediately also save it to disk:</p>\n\n<pre> </pre>\n\n<p>So the dataframe will be pickled and saved inside of each process, instead after it exits. Processes get cleaned out of memory as soon as they exit, so this should work to keep the memory footprint low. </p>\n\n\n<p>Do NOT call list(), it is creating an in-memory\nlist of whatever is being returned from divide_chunks().\nThat is where your memory issue is probably happening.</p>\n\n<p>You don\u2019t need all of that data in memeory at once. \nJust iterate over the filenames one at a time, that way all of the data is not in memory at once.</p>\n\n<p>Please post the stack trace so we have more information </p>\n\n\n<p>You can try to divide your file into multiple files of 20k images each, and then loop over the different files. This will free the memory used at the end of every file.<br>\nAlso the dataframe is stored in RAM and keeps growing when you put images in it. That may be the reason of your RAM issue. You can check here <a href=\"https://stackoverflow.com/a/39377643/11463544\">https://stackoverflow.com/a/39377643/11463544</a></p>\n\n\n<p>  may leak on some linux builds (see github <a href=\"https://github.com/pandas-dev/pandas/issues/2659#issuecomment-415177442\" rel=\"nofollow noreferrer\">issue and \"workaround\"</a>), so even   might not help.</p>\n\n<p>In your case solution from github can be used without monkey-patching of  :</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>P.S. This solution will not help if any single dataframe is too big. This can only be helped by reducing  </p>\n\n\n<p>My solution to this kind of problems is to use some parallel processing tool. I prefer <a href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">joblib</a> since it allows to parallelize even locally created functions (which are \"details of implementation\" and so it is better to avoid making them global in a module). My other advise: do not use threads (and thread pools) in python, use processes (and process pools) instead - this is almost always a better idea! Just make sure to create a pool of at least 2 processes in joblib, otherwise it would run everything in the original python process and so RAM would not be released in the end. Once the joblib worker processes are closed automatically, RAM which they allocated will be fully released by the OS. My favorite weapon of choice is <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\" rel=\"nofollow noreferrer\">joblib.Parallel</a>. If you need to transfer to workers large data (i.e. larger than 2GB), use <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html\" rel=\"nofollow noreferrer\">joblib.dump</a> (to write a python object into a file in the main process) and <a href=\"https://joblib.readthedocs.io/en/latest/generated/joblib.load.html\" rel=\"nofollow noreferrer\">joblib.load</a> (to read it in a worker process).</p>\n\n<p>About  : in python, the command does not actually delete an object. It only decreases its reference counter. When you run  , the garbage collector decides for itself which memory to free and which to leave allocated, and I am not aware of a way to force it to free all the memory possible. Even worse, if some memory was actually allocated not by python but, instead, for example, in some external C/C++/Cython/etc code and the code did not associate a python reference counter with the memory, there would be absolutely nothing you could do to free it from within python, except what I wrote above, i.e. by terminating the python process which allocated the RAM, in which case it would be guaranteed to be freed by the OS. That is why <strong>the only 100% reliable way to free some memory in python, is to run the code which allocates it in a parallel process and then to terminate the process</strong>.</p>\n\n\n<p>In short you cant release memory back in the Python interpreter. Your best bet would be to use multiprocessing as each process can handle memory on its own.</p>\n\n<p>The garbage collector will \"free\" memory, but not in the context you may expect. The handling of pages and pools can be explored in the CPython source. There is also a high level article here: <a href=\"https://realpython.com/python-memory-management/\" rel=\"nofollow noreferrer\">https://realpython.com/python-memory-management/</a></p>\n\n\n<p>I think it will be possible with <a href=\"http://docs.celeryproject.org/en/latest/index.html\" rel=\"nofollow noreferrer\">celery</a>, thanks to celery you can use concurrency and parallelism easily with python.</p>\n\n<p>Processing images seems are idempotent and atomic so it can be a <a href=\"http://docs.celeryproject.org/en/latest/userguide/tasks.html#basics\" rel=\"nofollow noreferrer\">celery task</a>.</p>\n\n<p>You can run <a href=\"http://docs.celeryproject.org/en/latest/userguide/workers.html#starting-the-worker\" rel=\"nofollow noreferrer\">a few workers</a> that will process tasks - work with image.</p>\n\n<p>Additionally it have <a href=\"https://docs.celeryproject.org/en/latest/userguide/workers.html#max-tasks-per-child-setting\" rel=\"nofollow noreferrer\">configuration</a> for memory leaks.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/csv.writer"}, "class_func_label": {"type": "literal", "value": "csv.writer"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    for row in sequence:\n        csv_writer.writerow(row)\n\n    [or]\n\n    csv_writer = csv.writer(fileobj [, dialect='excel']\n                            [optional keyword args])\n    csv_writer.writerows(rows)\n\nThe \"fileobj\" argument can be any object that supports the file API."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/54816169"}, "title": {"type": "literal", "value": "How to keep null values when writing to csv"}, "content": {"type": "literal", "value": "<p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command. The issue is that Python's csv writer automatically converts Nulls into an empty string \"\" and it fails my job when the column is an int or float datatype and it tries to insert this \"\" when it should be a None or null value.</p>\n\n<blockquote>\n  <p>To make it as easy as possible to interface with modules which\n  implement the DB API, the value None is written as the empty string.</p>\n  \n  <p><a href=\"https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer\" rel=\"nofollow noreferrer\">https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer</a></p>\n</blockquote>\n\n<p>What is the best way to keep the null value? Is there a better way to write csvs in Python? I'm open to all suggestions.</p>\n\n<p>Example:</p>\n\n<p>I have lat and long values:</p>\n\n<pre><code>42.313270000    -71.116240000\n42.377010000    -71.064770000\nNULL    NULL\n</code></pre>\n\n<p>When writing to csv it converts nulls to \"\":</p>\n\n<pre><code>with file_path.open(mode='w', newline='') as outfile:\n    csv_writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)\n    if include_headers:\n        csv_writer.writerow(col[0] for col in self.cursor.description)\n    for row in self.cursor:\n        csv_writer.writerow(row)\n</code></pre>\n\n<p>.</p>\n\n<pre><code>42.313270000,-71.116240000\n42.377010000,-71.064770000\n\"\",\"\"\n</code></pre>\n\n<blockquote>\n  <p>NULL</p>\n  \n  <p>Specifies the string that represents a null value. The default is \\N\n  (backslash-N) in text format, and an unquoted empty string in CSV\n  format. You might prefer an empty string even in text format for cases\n  where you don't want to distinguish nulls from empty strings. This\n  option is not allowed when using binary format.</p>\n  \n  <p><a href=\"https://www.postgresql.org/docs/9.2/sql-copy.html\" rel=\"nofollow noreferrer\">https://www.postgresql.org/docs/9.2/sql-copy.html</a></p>\n</blockquote>\n\n<p><strong>ANSWER:</strong></p>\n\n<p>What solved the problem for me was changing the quoting to csv.QUOTE_MINIMAL.</p>\n\n<blockquote>\n  <p>csv.QUOTE_MINIMAL Instructs writer objects to only quote those fields\n  which contain special characters such as delimiter, quotechar or any\n  of the characters in lineterminator.</p>\n</blockquote>\n\n<p>Related questions:\n- <a href=\"https://stackoverflow.com/questions/45892420/postgresql-copy-empty-string-as-null-not-work\">Postgresql COPY empty string as NULL not work</a></p>\n"}, "answerContent": {"type": "literal", "value": "<p>You are asking for <code>csv.QUOTE_NONNUMERIC</code>.  This will turn everything that is not a number into a string.  You should consider using <code>csv.QUOTE_MINIMAL</code> as it might be more what you are after:</p>\n\n<h3>Test Code:</h3>\n\n<pre><code>import csv\n\ntest_data = (None, 0, '', 'data')\nfor name, quotes in (('test1.csv', csv.QUOTE_NONNUMERIC),\n                     ('test2.csv', csv.QUOTE_MINIMAL)):\n\n    with open(name, mode='w') as outfile:\n        csv_writer = csv.writer(outfile, delimiter=',', quoting=quotes)\n        csv_writer.writerow(test_data))\n</code></pre>\n\n<h3>Results:</h3>\n\n<p><strong>test1.csv:</strong></p>\n\n<pre><code>\"\",0,\"\",\"data\"\n</code></pre>\n\n<p><strong>test2.csv:</strong></p>\n\n<pre><code>,0,,data\n</code></pre>\n\n\n<p>your code</p>\n\n<pre><code>for row in self.cursor:\n    csv_writer.writerow(row)\n</code></pre>\n\n<p>uses writer as-is, but you don't have to do that. You can filter the values to change some particular values with a generator comprehension and a ternary expression</p>\n\n<pre><code>for row in self.cursor:\n    csv_writer.writerow(\"null\" if x is None else x for x in row)\n</code></pre>\n\n\n<p>I would use pandas,psycopg2,and sqlalchemy. Make sure  are installed. Coming from your current workflow and avoiding writing to csv</p>\n\n<pre><code>#no need to import psycopg2\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\n#create connection to postgres\nengine = create_engine('postgres://.....')\n\n#get column names from cursor.description\ncolumns = [col[0] for col in self.cursor.description]\n\n#convert data into dataframe\ndf = pd.DataFrame(cursor.fetchall(),columns=columns)\n\n#send dataframe to postgres\ndf.to_sql('name_of_table',engine,if_exists='append',index=False)\n\n#if you still need to write to csv\ndf.to_csv('your_file.csv')\n</code></pre>\n\n\n<p>You have two options here: change the <code>csv.writing</code> quoting option in Python, or tell PostgreSQL to accept quoted strings as possible NULLs (requires PostgreSQL 9.4 or newer)</p>\n\n<h2>Python <code>csv.writer()</code> and quoting</h2>\n\n<p>On the Python side, you are telling the <code>csv.writer()</code> object to add quotes, because you configured it to use <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONNUMERIC\" rel=\"noreferrer\"><code>csv.QUOTE_NONNUMERIC</code></a>:</p>\n\n<blockquote>\n  <p>Instructs <code>writer</code> objects to quote all non-numeric fields.</p>\n</blockquote>\n\n<p><code>None</code> values are non-numeric, so result in <code>\"\"</code> being written.</p>\n\n<p>Switch to using <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_MINIMAL\" rel=\"noreferrer\"><code>csv.QUOTE_MINIMAL</code></a> or <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONE\" rel=\"noreferrer\"><code>csv.QUOTE_NONE</code></a>:</p>\n\n<blockquote>\n  <p><code>csv.QUOTE_MINIMAL</code><br>\n  Instructs <code>writer</code> objects to only quote those fields which contain special characters such as <em>delimiter</em>, <em>quotechar</em> or any of the characters in <em>lineterminator</em>.</p>\n  \n  <p><code>csv.QUOTE_NONE</code><br>\n  Instructs <code>writer</code> objects to never quote fields. When the current <em>delimiter</em> occurs in output data it is preceded by the current <em>escapechar</em> character.</p>\n</blockquote>\n\n<p>Since all you are writing is longitude and latitude values, you don't need any quoting here, there are no delimiters or quotecharacters present in your data.</p>\n\n<p>With either option, the CSV output for <code>None</code> values is simple an empty string:</p>\n\n<pre><code>&gt;&gt;&gt; import csv\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; def test_csv_writing(rows, quoting):\n...     outfile = StringIO()\n...     csv_writer = csv.writer(outfile, delimiter=',', quoting=quoting)\n...     csv_writer.writerows(rows)\n...     return outfile.getvalue()\n...\n&gt;&gt;&gt; rows = [\n...     [42.313270000, -71.116240000],\n...     [42.377010000, -71.064770000],\n...     [None, None],\n... ]\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONNUMERIC))\n42.31327,-71.11624\n42.37701,-71.06477\n\"\",\"\"\n\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_MINIMAL))\n42.31327,-71.11624\n42.37701,-71.06477\n,\n\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONE))\n42.31327,-71.11624\n42.37701,-71.06477\n,\n</code></pre>\n\n<h2>PostgreSQL 9.4 <code>COPY FROM</code>, <code>NULL</code> values and <code>FORCE_NULL</code></h2>\n\n<p>As of PostgreSQL 9.4, you can also force PostgreSQL to accept quoted empty strings as <code>NULL</code>s, when you use the <code>FORCE_NULL</code> option. From the <a href=\"https://www.postgresql.org/docs/10/sql-copy.html\" rel=\"noreferrer\"><code>COPY FROM</code> documentation</a>:</p>\n\n<blockquote>\n  <p><code>FORCE_NULL</code></p>\n  \n  <p>Match the specified columns' values against the null string, even if it has been quoted, and if a match is found set the value to <code>NULL</code>. In the default case where the null string is empty, this converts a quoted empty string into <code>NULL</code>. This option is allowed only in <code>COPY FROM</code>, and only when using CSV format.</p>\n</blockquote>\n\n<p>Naming the columns in a <code>FORCE_NULL</code> option lets PostgreSQL accept both the empty column and <code>\"\"</code> as <code>NULL</code> values for those columns, e.g.:</p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>COPY position (\n    lon, \n    lat\n) \nFROM \"filename\"\nWITH (\n    FORMAT csv,\n    NULL '',\n    DELIMITER ',',\n    FORCE_NULL(lon, lat)\n);\n</code></pre>\n\n<p>at which point it doesn't matter anymore what quoting options you used on the Python side.</p>\n\n<h2>Other options to consider</h2>\n\n<h3>For simple data transformation tasks from other databases, don't use Python</h3>\n\n<p>If you already querying databases to collate data to go into PostgreSQL, consider <em>directly inserting into Postgres</em>. If the data comes from other sources, using the <a href=\"https://www.postgresql.org/docs/9.4/postgres-fdw.html\" rel=\"noreferrer\">foreign data wrapper (<em>fdw</em>) module</a> lets you cut out the middle-man and directly pull data into PostgreSQL from other sources.</p>\n\n<h3>Numpy data? Consider using COPY FROM as binary, directly from Python</h3>\n\n<p>Numpy data can more efficiently be inserted via <a href=\"https://stackoverflow.com/questions/8144002/use-binary-copy-table-from-with-psycopg2/8150329#8150329\">binary <code>COPY FROM</code></a>; the linked answer augments a numpy structured array with the required extra metadata and byte ordering, then efficiently creates a binary copy of the data and inserts it into PostgreSQL using <code>COPY FROM STDIN WITH BINARY</code> and the <a href=\"http://initd.org/psycopg/docs/cursor.html#cursor.copy_expert\" rel=\"noreferrer\"><code>psycopg2.copy_expert()</code> method</a>. This neatly avoids number -> text -> number conversions.</p>\n\n<h3>Persisting data to handle large datasets in a pipeline?</h3>\n\n<p>Don't re-invent the data pipeline wheels. Consider using existing projects such as <a href=\"http://spark.apache.org/\" rel=\"noreferrer\">Apache Spark</a>, which have already solved the efficiency problems. Spark lets you <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model\" rel=\"noreferrer\">treat data as a structured stream</a>, and includes the infrastructure to <a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html\" rel=\"noreferrer\">run data analysis steps in parallel</a>, and you can treat <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" rel=\"noreferrer\">distributed, structured data as Pandas dataframes</a>.</p>\n\n<p>Another option might be to look at <a href=\"https://distributed.readthedocs.io/en/latest/\" rel=\"noreferrer\">Dask</a> to help share datasets between distributed tasks to process large amounts of data.</p>\n\n<p>Even if converting an already running project to Spark might be a step too far, at least consider using <a href=\"https://arrow.apache.org/\" rel=\"noreferrer\">Apache Arrow</a>, the data exchange platform Spark builds on top of. The <a href=\"https://arrow.apache.org/docs/python/\" rel=\"noreferrer\"><code>pyarrow</code> project</a> would let you exchange data via Parquet files, or <a href=\"https://arrow.apache.org/docs/python/ipc.html\" rel=\"noreferrer\">exchange data over IPC</a>.</p>\n\n<p>The Pandas and Numpy teams are quite heavily invested in supporting the needs of Arrow and Dask (there is considerable overlap in core members between these projects) and are actively working to make Python data exchange as efficient as possible, including <a href=\"https://www.python.org/dev/peps/pep-0574/\" rel=\"noreferrer\">extending Python's <code>pickle</code> module to allow for out-of-band data streams</a> to avoid unnecessary memory copying when sharing data. </p>\n\n\n<blockquote>\n  <p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command.</p>\n</blockquote>\n\n<p>I believe your true requirement is you need to hop data rows through the filesystem, and as both the sentence above and the question title make clear, you are <strong>currently</strong> doing that with a csv file.\nTrouble is that csv format offers poor support for the RDBMS notion of NULL.\nLet me solve your problem for you by changing the question slightly.\nI'd like to introduce you to parquet format.\nGiven a set of table rows in memory, it allows you to <em>very quickly</em> persist them to a compressed binary file, and recover them, with metadata and NULLs intact, no text quoting hassles.\nHere is an example, using the <a href=\"https://pypi.org/project/pyarrow/\" rel=\"nofollow noreferrer\">pyarrow 0.12.1</a> parquet engine:</p>\n\n<pre><code>import pandas as pd\nimport pyarrow\n\n\ndef round_trip(fspec='/tmp/locations.parquet'):\n    rows = [\n        dict(lat=42.313, lng=-71.116),\n        dict(lat=42.377, lng=-71.065),\n        dict(lat=None, lng=None),\n    ]\n\n    df = pd.DataFrame(rows)\n    df.to_parquet(fspec)\n    del(df)\n\n    df2 = pd.read_parquet(fspec)\n    print(df2)\n\n\nif __name__ == '__main__':\n    round_trip()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>      lat     lng\n0  42.313 -71.116\n1  42.377 -71.065\n2     NaN     NaN\n</code></pre>\n\n<p>Once you've recovered the rows in a dataframe you're free to call <code>df2.to_sql()</code> or use some other favorite technique to put numbers and NULLs into a DB table.</p>\n\n<p>EDIT:</p>\n\n<p>If you're able to run <code>.to_sql()</code> on the PG server, or on same LAN, then do that.\nOtherwise your favorite technique will likely involve <code>.copy_expert()</code>.\nWhy?\nThe summary is that with psycopg2, \"bulk INSERT is slow\".\nMiddle layers like sqlalchemy and pandas, and well-written apps that care about insert performance, will use <a href=\"http://mysql-python.sourceforge.net/MySQLdb.html#some-examples\" rel=\"nofollow noreferrer\"><code>.executemany()</code></a>.\nThe idea is to send lots of rows all at once, without waiting for individual result status, because we're not worried about unique index violations.\nSo TCP gets a giant buffer of SQL text and sends it all at once, saturating the end-to-end channel's bandwidth,\nmuch as copy_expert sends a big buffer to TCP to achieve high bandwidth.</p>\n\n<p>In contrast the psycopg2 driver lacks support for high performance executemany.\nAs of 2.7.4 it just executes items one at a time, sending a SQL command across the WAN and waiting a round trip time for the result before sending next command.\nPing your server;\nif ping times suggest you could get a dozen round trips per second,\nthen plan on only inserting about a dozen rows per second.\nMost of the time is spent waiting for a reply packet, rather than spent processing DB rows.\nIt would be lovely if at some future date psycopg2 would offer better support for this.</p>\n"}, "answer_1": {"type": "literal", "value": "<p>You are asking for <code>csv.QUOTE_NONNUMERIC</code>.  This will turn everything that is not a number into a string.  You should consider using <code>csv.QUOTE_MINIMAL</code> as it might be more what you are after:</p>\n\n<h3>Test Code:</h3>\n\n<pre><code>import csv\n\ntest_data = (None, 0, '', 'data')\nfor name, quotes in (('test1.csv', csv.QUOTE_NONNUMERIC),\n                     ('test2.csv', csv.QUOTE_MINIMAL)):\n\n    with open(name, mode='w') as outfile:\n        csv_writer = csv.writer(outfile, delimiter=',', quoting=quotes)\n        csv_writer.writerow(test_data))\n</code></pre>\n\n<h3>Results:</h3>\n\n<p><strong>test1.csv:</strong></p>\n\n<pre><code>\"\",0,\"\",\"data\"\n</code></pre>\n\n<p><strong>test2.csv:</strong></p>\n\n<pre><code>,0,,data\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "answer_2": {"type": "literal", "value": "<p>your code</p>\n\n<pre><code>for row in self.cursor:\n    csv_writer.writerow(row)\n</code></pre>\n\n<p>uses writer as-is, but you don't have to do that. You can filter the values to change some particular values with a generator comprehension and a ternary expression</p>\n\n<pre><code>for row in self.cursor:\n    csv_writer.writerow(\"null\" if x is None else x for x in row)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "2"}, "answer_3": {"type": "literal", "value": "<p>I would use pandas,psycopg2,and sqlalchemy. Make sure  are installed. Coming from your current workflow and avoiding writing to csv</p>\n\n<pre><code>#no need to import psycopg2\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\n#create connection to postgres\nengine = create_engine('postgres://.....')\n\n#get column names from cursor.description\ncolumns = [col[0] for col in self.cursor.description]\n\n#convert data into dataframe\ndf = pd.DataFrame(cursor.fetchall(),columns=columns)\n\n#send dataframe to postgres\ndf.to_sql('name_of_table',engine,if_exists='append',index=False)\n\n#if you still need to write to csv\ndf.to_csv('your_file.csv')\n</code></pre>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "answer_4": {"type": "literal", "value": "<p>You have two options here: change the <code>csv.writing</code> quoting option in Python, or tell PostgreSQL to accept quoted strings as possible NULLs (requires PostgreSQL 9.4 or newer)</p>\n\n<h2>Python <code>csv.writer()</code> and quoting</h2>\n\n<p>On the Python side, you are telling the <code>csv.writer()</code> object to add quotes, because you configured it to use <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONNUMERIC\" rel=\"noreferrer\"><code>csv.QUOTE_NONNUMERIC</code></a>:</p>\n\n<blockquote>\n  <p>Instructs <code>writer</code> objects to quote all non-numeric fields.</p>\n</blockquote>\n\n<p><code>None</code> values are non-numeric, so result in <code>\"\"</code> being written.</p>\n\n<p>Switch to using <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_MINIMAL\" rel=\"noreferrer\"><code>csv.QUOTE_MINIMAL</code></a> or <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONE\" rel=\"noreferrer\"><code>csv.QUOTE_NONE</code></a>:</p>\n\n<blockquote>\n  <p><code>csv.QUOTE_MINIMAL</code><br>\n  Instructs <code>writer</code> objects to only quote those fields which contain special characters such as <em>delimiter</em>, <em>quotechar</em> or any of the characters in <em>lineterminator</em>.</p>\n  \n  <p><code>csv.QUOTE_NONE</code><br>\n  Instructs <code>writer</code> objects to never quote fields. When the current <em>delimiter</em> occurs in output data it is preceded by the current <em>escapechar</em> character.</p>\n</blockquote>\n\n<p>Since all you are writing is longitude and latitude values, you don't need any quoting here, there are no delimiters or quotecharacters present in your data.</p>\n\n<p>With either option, the CSV output for <code>None</code> values is simple an empty string:</p>\n\n<pre><code>&gt;&gt;&gt; import csv\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; def test_csv_writing(rows, quoting):\n...     outfile = StringIO()\n...     csv_writer = csv.writer(outfile, delimiter=',', quoting=quoting)\n...     csv_writer.writerows(rows)\n...     return outfile.getvalue()\n...\n&gt;&gt;&gt; rows = [\n...     [42.313270000, -71.116240000],\n...     [42.377010000, -71.064770000],\n...     [None, None],\n... ]\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONNUMERIC))\n42.31327,-71.11624\n42.37701,-71.06477\n\"\",\"\"\n\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_MINIMAL))\n42.31327,-71.11624\n42.37701,-71.06477\n,\n\n&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONE))\n42.31327,-71.11624\n42.37701,-71.06477\n,\n</code></pre>\n\n<h2>PostgreSQL 9.4 <code>COPY FROM</code>, <code>NULL</code> values and <code>FORCE_NULL</code></h2>\n\n<p>As of PostgreSQL 9.4, you can also force PostgreSQL to accept quoted empty strings as <code>NULL</code>s, when you use the <code>FORCE_NULL</code> option. From the <a href=\"https://www.postgresql.org/docs/10/sql-copy.html\" rel=\"noreferrer\"><code>COPY FROM</code> documentation</a>:</p>\n\n<blockquote>\n  <p><code>FORCE_NULL</code></p>\n  \n  <p>Match the specified columns' values against the null string, even if it has been quoted, and if a match is found set the value to <code>NULL</code>. In the default case where the null string is empty, this converts a quoted empty string into <code>NULL</code>. This option is allowed only in <code>COPY FROM</code>, and only when using CSV format.</p>\n</blockquote>\n\n<p>Naming the columns in a <code>FORCE_NULL</code> option lets PostgreSQL accept both the empty column and <code>\"\"</code> as <code>NULL</code> values for those columns, e.g.:</p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>COPY position (\n    lon, \n    lat\n) \nFROM \"filename\"\nWITH (\n    FORMAT csv,\n    NULL '',\n    DELIMITER ',',\n    FORCE_NULL(lon, lat)\n);\n</code></pre>\n\n<p>at which point it doesn't matter anymore what quoting options you used on the Python side.</p>\n\n<h2>Other options to consider</h2>\n\n<h3>For simple data transformation tasks from other databases, don't use Python</h3>\n\n<p>If you already querying databases to collate data to go into PostgreSQL, consider <em>directly inserting into Postgres</em>. If the data comes from other sources, using the <a href=\"https://www.postgresql.org/docs/9.4/postgres-fdw.html\" rel=\"noreferrer\">foreign data wrapper (<em>fdw</em>) module</a> lets you cut out the middle-man and directly pull data into PostgreSQL from other sources.</p>\n\n<h3>Numpy data? Consider using COPY FROM as binary, directly from Python</h3>\n\n<p>Numpy data can more efficiently be inserted via <a href=\"https://stackoverflow.com/questions/8144002/use-binary-copy-table-from-with-psycopg2/8150329#8150329\">binary <code>COPY FROM</code></a>; the linked answer augments a numpy structured array with the required extra metadata and byte ordering, then efficiently creates a binary copy of the data and inserts it into PostgreSQL using <code>COPY FROM STDIN WITH BINARY</code> and the <a href=\"http://initd.org/psycopg/docs/cursor.html#cursor.copy_expert\" rel=\"noreferrer\"><code>psycopg2.copy_expert()</code> method</a>. This neatly avoids number -> text -> number conversions.</p>\n\n<h3>Persisting data to handle large datasets in a pipeline?</h3>\n\n<p>Don't re-invent the data pipeline wheels. Consider using existing projects such as <a href=\"http://spark.apache.org/\" rel=\"noreferrer\">Apache Spark</a>, which have already solved the efficiency problems. Spark lets you <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model\" rel=\"noreferrer\">treat data as a structured stream</a>, and includes the infrastructure to <a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html\" rel=\"noreferrer\">run data analysis steps in parallel</a>, and you can treat <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" rel=\"noreferrer\">distributed, structured data as Pandas dataframes</a>.</p>\n\n<p>Another option might be to look at <a href=\"https://distributed.readthedocs.io/en/latest/\" rel=\"noreferrer\">Dask</a> to help share datasets between distributed tasks to process large amounts of data.</p>\n\n<p>Even if converting an already running project to Spark might be a step too far, at least consider using <a href=\"https://arrow.apache.org/\" rel=\"noreferrer\">Apache Arrow</a>, the data exchange platform Spark builds on top of. The <a href=\"https://arrow.apache.org/docs/python/\" rel=\"noreferrer\"><code>pyarrow</code> project</a> would let you exchange data via Parquet files, or <a href=\"https://arrow.apache.org/docs/python/ipc.html\" rel=\"noreferrer\">exchange data over IPC</a>.</p>\n\n<p>The Pandas and Numpy teams are quite heavily invested in supporting the needs of Arrow and Dask (there is considerable overlap in core members between these projects) and are actively working to make Python data exchange as efficient as possible, including <a href=\"https://www.python.org/dev/peps/pep-0574/\" rel=\"noreferrer\">extending Python's <code>pickle</code> module to allow for out-of-band data streams</a> to avoid unnecessary memory copying when sharing data. </p>\n"}, "answer_4_votes": {"type": "literal", "value": "9"}, "answer_5": {"type": "literal", "value": "<blockquote>\n  <p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command.</p>\n</blockquote>\n\n<p>I believe your true requirement is you need to hop data rows through the filesystem, and as both the sentence above and the question title make clear, you are <strong>currently</strong> doing that with a csv file.\nTrouble is that csv format offers poor support for the RDBMS notion of NULL.\nLet me solve your problem for you by changing the question slightly.\nI'd like to introduce you to parquet format.\nGiven a set of table rows in memory, it allows you to <em>very quickly</em> persist them to a compressed binary file, and recover them, with metadata and NULLs intact, no text quoting hassles.\nHere is an example, using the <a href=\"https://pypi.org/project/pyarrow/\" rel=\"nofollow noreferrer\">pyarrow 0.12.1</a> parquet engine:</p>\n\n<pre><code>import pandas as pd\nimport pyarrow\n\n\ndef round_trip(fspec='/tmp/locations.parquet'):\n    rows = [\n        dict(lat=42.313, lng=-71.116),\n        dict(lat=42.377, lng=-71.065),\n        dict(lat=None, lng=None),\n    ]\n\n    df = pd.DataFrame(rows)\n    df.to_parquet(fspec)\n    del(df)\n\n    df2 = pd.read_parquet(fspec)\n    print(df2)\n\n\nif __name__ == '__main__':\n    round_trip()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>      lat     lng\n0  42.313 -71.116\n1  42.377 -71.065\n2     NaN     NaN\n</code></pre>\n\n<p>Once you've recovered the rows in a dataframe you're free to call <code>df2.to_sql()</code> or use some other favorite technique to put numbers and NULLs into a DB table.</p>\n\n<p>EDIT:</p>\n\n<p>If you're able to run <code>.to_sql()</code> on the PG server, or on same LAN, then do that.\nOtherwise your favorite technique will likely involve <code>.copy_expert()</code>.\nWhy?\nThe summary is that with psycopg2, \"bulk INSERT is slow\".\nMiddle layers like sqlalchemy and pandas, and well-written apps that care about insert performance, will use <a href=\"http://mysql-python.sourceforge.net/MySQLdb.html#some-examples\" rel=\"nofollow noreferrer\"><code>.executemany()</code></a>.\nThe idea is to send lots of rows all at once, without waiting for individual result status, because we're not worried about unique index violations.\nSo TCP gets a giant buffer of SQL text and sends it all at once, saturating the end-to-end channel's bandwidth,\nmuch as copy_expert sends a big buffer to TCP to achieve high bandwidth.</p>\n\n<p>In contrast the psycopg2 driver lacks support for high performance executemany.\nAs of 2.7.4 it just executes items one at a time, sending a SQL command across the WAN and waiting a round trip time for the result before sending next command.\nPing your server;\nif ping times suggest you could get a dozen round trips per second,\nthen plan on only inserting about a dozen rows per second.\nMost of the time is spent waiting for a reply packet, rather than spent processing DB rows.\nIt would be lovely if at some future date psycopg2 would offer better support for this.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "2"}, "content_wo_code": "<p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command. The issue is that Python's csv writer automatically converts Nulls into an empty string \"\" and it fails my job when the column is an int or float datatype and it tries to insert this \"\" when it should be a None or null value.</p>\n\n<blockquote>\n  <p>To make it as easy as possible to interface with modules which\n  implement the DB API, the value None is written as the empty string.</p>\n  \n  <p><a href=\"https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer\" rel=\"nofollow noreferrer\">https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer</a></p>\n</blockquote>\n\n<p>What is the best way to keep the null value? Is there a better way to write csvs in Python? I'm open to all suggestions.</p>\n\n<p>Example:</p>\n\n<p>I have lat and long values:</p>\n\n<pre> </pre>\n\n<p>When writing to csv it converts nulls to \"\":</p>\n\n<pre> </pre>\n\n<p>.</p>\n\n<pre> </pre>\n\n<blockquote>\n  <p>NULL</p>\n  \n  <p>Specifies the string that represents a null value. The default is \\N\n  (backslash-N) in text format, and an unquoted empty string in CSV\n  format. You might prefer an empty string even in text format for cases\n  where you don't want to distinguish nulls from empty strings. This\n  option is not allowed when using binary format.</p>\n  \n  <p><a href=\"https://www.postgresql.org/docs/9.2/sql-copy.html\" rel=\"nofollow noreferrer\">https://www.postgresql.org/docs/9.2/sql-copy.html</a></p>\n</blockquote>\n\n<p><strong>ANSWER:</strong></p>\n\n<p>What solved the problem for me was changing the quoting to csv.QUOTE_MINIMAL.</p>\n\n<blockquote>\n  <p>csv.QUOTE_MINIMAL Instructs writer objects to only quote those fields\n  which contain special characters such as delimiter, quotechar or any\n  of the characters in lineterminator.</p>\n</blockquote>\n\n<p>Related questions:\n- <a href=\"https://stackoverflow.com/questions/45892420/postgresql-copy-empty-string-as-null-not-work\">Postgresql COPY empty string as NULL not work</a></p>\n", "answer_wo_code": "<p>You are asking for  .  This will turn everything that is not a number into a string.  You should consider using   as it might be more what you are after:</p>\n\n<h3>Test Code:</h3>\n\n<pre> </pre>\n\n<h3>Results:</h3>\n\n<p><strong>test1.csv:</strong></p>\n\n<pre> </pre>\n\n<p><strong>test2.csv:</strong></p>\n\n<pre> </pre>\n\n\n<p>your code</p>\n\n<pre> </pre>\n\n<p>uses writer as-is, but you don't have to do that. You can filter the values to change some particular values with a generator comprehension and a ternary expression</p>\n\n<pre> </pre>\n\n\n<p>I would use pandas,psycopg2,and sqlalchemy. Make sure  are installed. Coming from your current workflow and avoiding writing to csv</p>\n\n<pre> </pre>\n\n\n<p>You have two options here: change the   quoting option in Python, or tell PostgreSQL to accept quoted strings as possible NULLs (requires PostgreSQL 9.4 or newer)</p>\n\n<h2>Python   and quoting</h2>\n\n<p>On the Python side, you are telling the   object to add quotes, because you configured it to use <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONNUMERIC\" rel=\"noreferrer\"> </a>:</p>\n\n<blockquote>\n  <p>Instructs   objects to quote all non-numeric fields.</p>\n</blockquote>\n\n<p>  values are non-numeric, so result in   being written.</p>\n\n<p>Switch to using <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_MINIMAL\" rel=\"noreferrer\"> </a> or <a href=\"https://docs.python.org/3/library/csv.html#csv.QUOTE_NONE\" rel=\"noreferrer\"> </a>:</p>\n\n<blockquote>\n  <p> <br>\n  Instructs   objects to only quote those fields which contain special characters such as <em>delimiter</em>, <em>quotechar</em> or any of the characters in <em>lineterminator</em>.</p>\n  \n  <p> <br>\n  Instructs   objects to never quote fields. When the current <em>delimiter</em> occurs in output data it is preceded by the current <em>escapechar</em> character.</p>\n</blockquote>\n\n<p>Since all you are writing is longitude and latitude values, you don't need any quoting here, there are no delimiters or quotecharacters present in your data.</p>\n\n<p>With either option, the CSV output for   values is simple an empty string:</p>\n\n<pre> </pre>\n\n<h2>PostgreSQL 9.4  ,   values and  </h2>\n\n<p>As of PostgreSQL 9.4, you can also force PostgreSQL to accept quoted empty strings as  s, when you use the   option. From the <a href=\"https://www.postgresql.org/docs/10/sql-copy.html\" rel=\"noreferrer\">  documentation</a>:</p>\n\n<blockquote>\n  <p> </p>\n  \n  <p>Match the specified columns' values against the null string, even if it has been quoted, and if a match is found set the value to  . In the default case where the null string is empty, this converts a quoted empty string into  . This option is allowed only in  , and only when using CSV format.</p>\n</blockquote>\n\n<p>Naming the columns in a   option lets PostgreSQL accept both the empty column and   as   values for those columns, e.g.:</p>\n\n<pre class=\"lang-sql prettyprint-override\"> </pre>\n\n<p>at which point it doesn't matter anymore what quoting options you used on the Python side.</p>\n\n<h2>Other options to consider</h2>\n\n<h3>For simple data transformation tasks from other databases, don't use Python</h3>\n\n<p>If you already querying databases to collate data to go into PostgreSQL, consider <em>directly inserting into Postgres</em>. If the data comes from other sources, using the <a href=\"https://www.postgresql.org/docs/9.4/postgres-fdw.html\" rel=\"noreferrer\">foreign data wrapper (<em>fdw</em>) module</a> lets you cut out the middle-man and directly pull data into PostgreSQL from other sources.</p>\n\n<h3>Numpy data? Consider using COPY FROM as binary, directly from Python</h3>\n\n<p>Numpy data can more efficiently be inserted via <a href=\"https://stackoverflow.com/questions/8144002/use-binary-copy-table-from-with-psycopg2/8150329#8150329\">binary  </a>; the linked answer augments a numpy structured array with the required extra metadata and byte ordering, then efficiently creates a binary copy of the data and inserts it into PostgreSQL using   and the <a href=\"http://initd.org/psycopg/docs/cursor.html#cursor.copy_expert\" rel=\"noreferrer\">  method</a>. This neatly avoids number -> text -> number conversions.</p>\n\n<h3>Persisting data to handle large datasets in a pipeline?</h3>\n\n<p>Don't re-invent the data pipeline wheels. Consider using existing projects such as <a href=\"http://spark.apache.org/\" rel=\"noreferrer\">Apache Spark</a>, which have already solved the efficiency problems. Spark lets you <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model\" rel=\"noreferrer\">treat data as a structured stream</a>, and includes the infrastructure to <a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html\" rel=\"noreferrer\">run data analysis steps in parallel</a>, and you can treat <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" rel=\"noreferrer\">distributed, structured data as Pandas dataframes</a>.</p>\n\n<p>Another option might be to look at <a href=\"https://distributed.readthedocs.io/en/latest/\" rel=\"noreferrer\">Dask</a> to help share datasets between distributed tasks to process large amounts of data.</p>\n\n<p>Even if converting an already running project to Spark might be a step too far, at least consider using <a href=\"https://arrow.apache.org/\" rel=\"noreferrer\">Apache Arrow</a>, the data exchange platform Spark builds on top of. The <a href=\"https://arrow.apache.org/docs/python/\" rel=\"noreferrer\">  project</a> would let you exchange data via Parquet files, or <a href=\"https://arrow.apache.org/docs/python/ipc.html\" rel=\"noreferrer\">exchange data over IPC</a>.</p>\n\n<p>The Pandas and Numpy teams are quite heavily invested in supporting the needs of Arrow and Dask (there is considerable overlap in core members between these projects) and are actively working to make Python data exchange as efficient as possible, including <a href=\"https://www.python.org/dev/peps/pep-0574/\" rel=\"noreferrer\">extending Python's   module to allow for out-of-band data streams</a> to avoid unnecessary memory copying when sharing data. </p>\n\n\n<blockquote>\n  <p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command.</p>\n</blockquote>\n\n<p>I believe your true requirement is you need to hop data rows through the filesystem, and as both the sentence above and the question title make clear, you are <strong>currently</strong> doing that with a csv file.\nTrouble is that csv format offers poor support for the RDBMS notion of NULL.\nLet me solve your problem for you by changing the question slightly.\nI'd like to introduce you to parquet format.\nGiven a set of table rows in memory, it allows you to <em>very quickly</em> persist them to a compressed binary file, and recover them, with metadata and NULLs intact, no text quoting hassles.\nHere is an example, using the <a href=\"https://pypi.org/project/pyarrow/\" rel=\"nofollow noreferrer\">pyarrow 0.12.1</a> parquet engine:</p>\n\n<pre> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n<p>Once you've recovered the rows in a dataframe you're free to call   or use some other favorite technique to put numbers and NULLs into a DB table.</p>\n\n<p>EDIT:</p>\n\n<p>If you're able to run   on the PG server, or on same LAN, then do that.\nOtherwise your favorite technique will likely involve  .\nWhy?\nThe summary is that with psycopg2, \"bulk INSERT is slow\".\nMiddle layers like sqlalchemy and pandas, and well-written apps that care about insert performance, will use <a href=\"http://mysql-python.sourceforge.net/MySQLdb.html#some-examples\" rel=\"nofollow noreferrer\"> </a>.\nThe idea is to send lots of rows all at once, without waiting for individual result status, because we're not worried about unique index violations.\nSo TCP gets a giant buffer of SQL text and sends it all at once, saturating the end-to-end channel's bandwidth,\nmuch as copy_expert sends a big buffer to TCP to achieve high bandwidth.</p>\n\n<p>In contrast the psycopg2 driver lacks support for high performance executemany.\nAs of 2.7.4 it just executes items one at a time, sending a SQL command across the WAN and waiting a round trip time for the result before sending next command.\nPing your server;\nif ping times suggest you could get a dozen round trips per second,\nthen plan on only inserting about a dozen rows per second.\nMost of the time is spent waiting for a reply packet, rather than spent processing DB rows.\nIt would be lovely if at some future date psycopg2 would offer better support for this.</p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.DataFrame"}, "class_func_label": {"type": "literal", "value": "pandas.DataFrame"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/27987323"}, "title": {"type": "literal", "value": "Importing a large csv into DB using pandas"}, "content": {"type": "literal", "value": "<p>I have a csv file with ~ 3 million records, that I want to migrate to sql server through my laptop (4GB ram).</p>\n\n<p><code>pandas</code> successfully reads the file to DataFrame (<code>pd.read_csv()</code>), but when I try to migrate (<code>.to_sql()</code>) I receive <code>Memory Error</code>:</p>\n\n<pre><code>---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n&lt;ipython-input-12-94c146c2b7b7&gt; in &lt;module&gt;()\n----&gt; 1 csv.to_sql(name='stats', con=engine, if_exists='append')\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in to_sql(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n    964             self, name, con, flavor=flavor, schema=schema, if_exists=if_exists,\n    965             index=index, index_label=index_label, chunksize=chunksize,\n--&gt; 966             dtype=dtype)\n    967 \n    968     def to_pickle(self, path):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in to_sql(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n    536     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n    537                       index_label=index_label, schema=schema,\n--&gt; 538                       chunksize=chunksize, dtype=dtype)\n    539 \n    540 \n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\n   1170                          schema=schema, dtype=dtype)\n   1171         table.create()\n-&gt; 1172         table.insert(chunksize)\n   1173         # check for potentially case sensitivity issues (GH7815)\n   1174         if name not in self.engine.table_names(schema=schema or self.meta.schema):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in insert(self, chunksize)\n    715 \n    716                 chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n--&gt; 717                 self._execute_insert(conn, keys, chunk_iter)\n    718 \n    719     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in _execute_insert(self, conn, keys, data_iter)\n    689 \n    690     def _execute_insert(self, conn, keys, data_iter):\n--&gt; 691         data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n    692         conn.execute(self.insert_statement(), data)\n    693 \n\nMemoryError:\n</code></pre>\n\n<p>Is there some other way that would let me successfully do migration?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for <code>read_csv</code> there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"><code>chunksize</code></a> param.</p>\n\n<p>Equally there is also <code>chunksize</code> param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"><code>to_sql</code></a></p>\n"}, "answer_1": {"type": "literal", "value": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for <code>read_csv</code> there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"><code>chunksize</code></a> param.</p>\n\n<p>Equally there is also <code>chunksize</code> param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"><code>to_sql</code></a></p>\n"}, "answer_1_votes": {"type": "literal", "value": "8"}, "content_wo_code": "<p>I have a csv file with ~ 3 million records, that I want to migrate to sql server through my laptop (4GB ram).</p>\n\n<p>  successfully reads the file to DataFrame ( ), but when I try to migrate ( ) I receive  :</p>\n\n<pre> </pre>\n\n<p>Is there some other way that would let me successfully do migration?</p>\n", "answer_wo_code": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for   there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"> </a> param.</p>\n\n<p>Equally there is also   param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"> </a></p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_csv"}, "class_func_label": {"type": "literal", "value": "pandas.read_csv"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/27987323"}, "title": {"type": "literal", "value": "Importing a large csv into DB using pandas"}, "content": {"type": "literal", "value": "<p>I have a csv file with ~ 3 million records, that I want to migrate to sql server through my laptop (4GB ram).</p>\n\n<p><code>pandas</code> successfully reads the file to DataFrame (<code>pd.read_csv()</code>), but when I try to migrate (<code>.to_sql()</code>) I receive <code>Memory Error</code>:</p>\n\n<pre><code>---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n&lt;ipython-input-12-94c146c2b7b7&gt; in &lt;module&gt;()\n----&gt; 1 csv.to_sql(name='stats', con=engine, if_exists='append')\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\generic.pyc in to_sql(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n    964             self, name, con, flavor=flavor, schema=schema, if_exists=if_exists,\n    965             index=index, index_label=index_label, chunksize=chunksize,\n--&gt; 966             dtype=dtype)\n    967 \n    968     def to_pickle(self, path):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in to_sql(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n    536     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n    537                       index_label=index_label, schema=schema,\n--&gt; 538                       chunksize=chunksize, dtype=dtype)\n    539 \n    540 \n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\n   1170                          schema=schema, dtype=dtype)\n   1171         table.create()\n-&gt; 1172         table.insert(chunksize)\n   1173         # check for potentially case sensitivity issues (GH7815)\n   1174         if name not in self.engine.table_names(schema=schema or self.meta.schema):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in insert(self, chunksize)\n    715 \n    716                 chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n--&gt; 717                 self._execute_insert(conn, keys, chunk_iter)\n    718 \n    719     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\sql.pyc in _execute_insert(self, conn, keys, data_iter)\n    689 \n    690     def _execute_insert(self, conn, keys, data_iter):\n--&gt; 691         data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n    692         conn.execute(self.insert_statement(), data)\n    693 \n\nMemoryError:\n</code></pre>\n\n<p>Is there some other way that would let me successfully do migration?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for <code>read_csv</code> there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"><code>chunksize</code></a> param.</p>\n\n<p>Equally there is also <code>chunksize</code> param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"><code>to_sql</code></a></p>\n"}, "answer_1": {"type": "literal", "value": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for <code>read_csv</code> there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"><code>chunksize</code></a> param.</p>\n\n<p>Equally there is also <code>chunksize</code> param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"><code>to_sql</code></a></p>\n"}, "answer_1_votes": {"type": "literal", "value": "8"}, "content_wo_code": "<p>I have a csv file with ~ 3 million records, that I want to migrate to sql server through my laptop (4GB ram).</p>\n\n<p>  successfully reads the file to DataFrame ( ), but when I try to migrate ( ) I receive  :</p>\n\n<pre> </pre>\n\n<p>Is there some other way that would let me successfully do migration?</p>\n", "answer_wo_code": "<p>I think you have 2 approaches:</p>\n\n<ol>\n<li>Read the csv in chunks and then write to SQL DB and repeat</li>\n<li>OR you can write in chunks to the DB</li>\n</ol>\n\n<p>So for   there is a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\"> </a> param.</p>\n\n<p>Equally there is also   param for <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql\"> </a></p>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/psycopg2.ProgrammingError"}, "class_func_label": {"type": "literal", "value": "psycopg2.ProgrammingError"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "Error related to database programming (SQL error, table not found etc)."}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/31997859"}, "title": {"type": "literal", "value": "Bulk Insert A Pandas DataFrame Using SQLAlchemy"}, "content": {"type": "literal", "value": "<p>I have some rather large pandas DataFrames and I'd like to use the new bulk SQL mappings to upload them to a Microsoft SQL Server via SQL Alchemy. The pandas.to_sql method, while nice, is slow. </p>\n\n<p>I'm having trouble writing the code...</p>\n\n<p>I'd like to be able to pass this function a pandas DataFrame which I'm calling <code>table</code>, a schema name I'm calling <code>schema</code>, and a table name I'm calling <code>name</code>. Ideally, the function will 1.) delete the table if it already exists. 2.) create a new table 3.) create a mapper and 4.) bulk insert using the mapper and pandas data. I'm stuck on part 3.</p>\n\n<p>Here's my (admittedly rough) code. I'm struggling with how to get the mapper function to work with my primary keys. I don't really need primary keys but the mapper function requires it. </p>\n\n<p>Thanks for the insights.</p>\n\n<pre><code>from sqlalchemy import create_engine Table, Column, MetaData\nfrom sqlalchemy.orm import mapper, create_session\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom pandas.io.sql import SQLTable, SQLDatabase\n\ndef bulk_upload(table, schema, name):\n    e = create_engine('mssql+pyodbc://MYDB')\n    s = create_session(bind=e)\n    m = MetaData(bind=e,reflect=True,schema=schema)\n    Base = declarative_base(bind=e,metadata=m)\n    t = Table(name,m)\n    m.remove(t)\n    t.drop(checkfirst=True)\n    sqld = SQLDatabase(e, schema=schema,meta=m)\n    sqlt = SQLTable(name, sqld, table).table\n    sqlt.metadata = m\n    m.create_all(bind=e,tables=[sqlt])    \n    class MyClass(Base):\n        return\n    mapper(MyClass, sqlt)    \n\n    s.bulk_insert_mappings(MyClass, table.to_dict(orient='records'))\n    return\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Based on @ansonw answers:</p>\n\n<pre><code>def to_sql(engine, df, table, if_exists='fail', sep='\\t', encoding='utf8'):\n    # Create Table\n    df[:0].to_sql(table, engine, if_exists=if_exists)\n\n    # Prepare data\n    output = cStringIO.StringIO()\n    df.to_csv(output, sep=sep, header=False, encoding=encoding)\n    output.seek(0)\n\n    # Insert data\n    connection = engine.raw_connection()\n    cursor = connection.cursor()\n    cursor.copy_from(output, table, sep=sep, null='')\n    connection.commit()\n    cursor.close()\n</code></pre>\n\n<p>I insert 200000 lines in 5 seconds instead of 4 minutes</p>\n\n\n<p>for people like me who are trying to implement the aforementioned solutions:</p>\n\n<p>Pandas 0.24.0 has now to_sql with chunksize and method='multi' option that inserts in bulk...</p>\n\n\n<p>For anyone facing this problem and having the destination DB as Redshift, note that Redshift does not implement the full set of Postgres commands, and so some of the answers using either Postgres' <code>COPY FROM</code> or <code>copy_from()</code> will not work.\n<a href=\"https://stackoverflow.com/questions/40904489/psycopg2-programmingerror-syntax-error-at-or-near-stdin-error-when-trying-to\">psycopg2.ProgrammingError: syntax error at or near &quot;stdin&quot; error when trying to copy_from redshift</a></p>\n\n<p>Solution for speeding up the INSERTs to Redshift is to use a file ingest or Odo.</p>\n\n<p>Reference:<br>\nAbout Odo\n<a href=\"http://odo.pydata.org/en/latest/perf.html\" rel=\"nofollow noreferrer\">http://odo.pydata.org/en/latest/perf.html</a><br>\nOdo with Redshift<br>\n<a href=\"https://github.com/blaze/odo/blob/master/docs/source/aws.rst\" rel=\"nofollow noreferrer\">https://github.com/blaze/odo/blob/master/docs/source/aws.rst</a><br>\nRedshift COPY (from S3 file)<br>\n<a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\" rel=\"nofollow noreferrer\">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html</a>  </p>\n\n\n<p>This might have been answered by then, but I found the solution by collating different answers on this site and aligning with SQLAlchemy's doc.</p>\n\n<ol>\n<li>The table needs to already exist in db1; with an index set up with auto_increment on.</li>\n<li>The Class <i>Current</i> needs to align with the dataframe imported in the CSV and the table in the db1.</li>\n</ol>\n\n<p>Hope this helps whoever comes here and wants to mix Panda and SQLAlchemy in a quick way.</p>\n\n<pre><code>from urllib import quote_plus as urlquote\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String, Numeric\nfrom sqlalchemy.orm import sessionmaker\nimport pandas as pd\n\n\n# Set up of the engine to connect to the database\n# the urlquote is used for passing the password which might contain special characters such as \"/\"\nengine = create_engine('mysql://root:%s@localhost/db1' % urlquote('weirdPassword*withsp\u20accialcharacters'), echo=False)\nconn = engine.connect()\nBase = declarative_base()\n\n#Declaration of the class in order to write into the database. This structure is standard and should align with SQLAlchemy's doc.\nclass Current(Base):\n    __tablename__ = 'tableName'\n\n    id = Column(Integer, primary_key=True)\n    Date = Column(String(500))\n    Type = Column(String(500))\n    Value = Column(Numeric())\n\n    def __repr__(self):\n        return \"(id='%s', Date='%s', Type='%s', Value='%s')\" % (self.id, self.Date, self.Type, self.Value)\n\n# Set up of the table in db and the file to import\nfileToRead = 'file.csv'\ntableToWriteTo = 'tableName'\n\n# Panda to create a lovely dataframe\ndf_to_be_written = pd.read_csv(fileToRead)\n# The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\nlistToWrite = df_to_be_written.to_dict(orient='records')\n\nmetadata = sqlalchemy.schema.MetaData(bind=engine,reflect=True)\ntable = sqlalchemy.Table(tableToWriteTo, metadata, autoload=True)\n\n# Open the session\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Inser the dataframe into the database in one bulk\nconn.execute(table.insert(), listToWrite)\n\n# Commit the changes\nsession.commit()\n\n# Close the session\nsession.close()\n</code></pre>\n\n\n<p>I ran into a similar issue with pd.to_sql taking hours to upload data.  The below code bulk inserted the same data in a few seconds.  </p>\n\n<pre><code>from sqlalchemy import create_engine\nimport psycopg2 as pg\n#load python script that batch loads pandas df to sql\nimport cStringIO\n\naddress = 'postgresql://&lt;username&gt;:&lt;pswd&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;'\nengine = create_engine(address)\nconnection = engine.raw_connection()\ncursor = connection.cursor()\n\n#df is the dataframe containing an index and the columns \"Event\" and \"Day\"\n#create Index column to use as primary key\ndf.reset_index(inplace=True)\ndf.rename(columns={'index':'Index'}, inplace =True)\n\n#create the table but first drop if it already exists\ncommand = '''DROP TABLE IF EXISTS localytics_app2;\nCREATE TABLE localytics_app2\n(\n\"Index\" serial primary key,\n\"Event\" text,\n\"Day\" timestamp without time zone,\n);'''\ncursor.execute(command)\nconnection.commit()\n\n#stream the data using 'to_csv' and StringIO(); then use sql's 'copy_from' function\noutput = cStringIO.StringIO()\n#ignore the index\ndf.to_csv(output, sep='\\t', header=False, index=False)\n#jump to start of stream\noutput.seek(0)\ncontents = output.getvalue()\ncur = connection.cursor()\n#null values become ''\ncur.copy_from(output, 'localytics_app2', null=\"\")    \nconnection.commit()\ncur.close()\n</code></pre>\n\n\n<p>My postgres specific solution below auto-creates the database table using your pandas dataframe, and performs a fast bulk insert using the postgres <code>COPY my_table FROM ...</code></p>\n\n<pre><code>import io\n\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef write_to_table(df, db_engine, schema, table_name, if_exists='fail'):\n    string_data_io = io.StringIO()\n    df.to_csv(string_data_io, sep='|', index=False)\n    pd_sql_engine = pd.io.sql.pandasSQL_builder(db_engine, schema=schema)\n    table = pd.io.sql.SQLTable(table_name, pd_sql_engine, frame=df,\n                               index=False, if_exists=if_exists, schema=schema)\n    table.create()\n    string_data_io.seek(0)\n    string_data_io.readline()  # remove header\n    with db_engine.connect() as connection:\n        with connection.connection.cursor() as cursor:\n            copy_cmd = \"COPY %s.%s FROM STDIN HEADER DELIMITER '|' CSV\" % (schema, table_name)\n            cursor.copy_expert(copy_cmd, string_data_io)\n        connection.connection.commit()\n</code></pre>\n\n\n<p>This worked for me to connect to Oracle Database using cx_Oracle and SQLALchemy</p>\n\n<pre><code>import sqlalchemy\nimport cx_Oracle\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, String\nfrom sqlalchemy.orm import sessionmaker\nimport pandas as pd\n\n# credentials\nusername = \"username\"\npassword = \"password\"\nconnectStr = \"connection:/string\"\ntableName = \"tablename\"\n\nt0 = time.time()\n\n# connection\ndsn = cx_Oracle.makedsn('host','port',service_name='servicename')\n\nBase = declarative_base()\n\nclass LANDMANMINERAL(Base):\n    __tablename__ = 'tablename'\n\n    DOCUMENTNUM = Column(String(500), primary_key=True)\n    DOCUMENTTYPE = Column(String(500))\n    FILENUM = Column(String(500))\n    LEASEPAYOR = Column(String(500))\n    LEASESTATUS = Column(String(500))\n    PROSPECT = Column(String(500))\n    SPLIT = Column(String(500))\n    SPLITSTATUS = Column(String(500))\n\nengine = create_engine('oracle+cx_oracle://%s:%s@%s' % (username, password, dsn))\nconn = engine.connect()  \n\nBase.metadata.bind = engine\n\n# Creating the session\n\nDBSession = sessionmaker(bind=engine)\n\nsession = DBSession()\n\n# Bulk insertion\ndata = pd.read_csv('data.csv')\nlists = data.to_dict(orient='records')\n\n\ntable = sqlalchemy.Table('landmanmineral', Base.metadata, autoreload=True)\nconn.execute(table.insert(), lists)\n\nsession.commit()\n\nsession.close() \n\nprint(\"time taken %8.8f seconds\" % (time.time() - t0) )\n</code></pre>\n\n\n<p>As this is an I/O heavy workload you can also use the python threading module through <a href=\"https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy\" rel=\"nofollow noreferrer\">multiprocessing.dummy</a>. This sped things up for me:</p>\n\n<pre><code>import math\nfrom multiprocessing.dummy import Pool as ThreadPool\n\n...\n\ndef insert_df(df, *args, **kwargs):\n    nworkers = 4\n\n    chunksize = math.floor(df.shape[0] / nworkers)\n    chunks = [(chunksize * i, (chunksize * i) + chunksize) for i in range(nworkers)]\n    chunks.append((chunksize * nworkers, df.shape[0]))\n    pool = ThreadPool(nworkers)\n\n    def worker(chunk):\n        i, j = chunk\n        df.iloc[i:j, :].to_sql(*args, **kwargs)\n\n    pool.map(worker, chunks)\n    pool.close()\n    pool.join()\n\n\n....\n\ninsert_df(df, \"foo_bar\", engine, if_exists='append')\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Based on @ansonw answers:</p>\n\n<pre><code>def to_sql(engine, df, table, if_exists='fail', sep='\\t', encoding='utf8'):\n    # Create Table\n    df[:0].to_sql(table, engine, if_exists=if_exists)\n\n    # Prepare data\n    output = cStringIO.StringIO()\n    df.to_csv(output, sep=sep, header=False, encoding=encoding)\n    output.seek(0)\n\n    # Insert data\n    connection = engine.raw_connection()\n    cursor = connection.cursor()\n    cursor.copy_from(output, table, sep=sep, null='')\n    connection.commit()\n    cursor.close()\n</code></pre>\n\n<p>I insert 200000 lines in 5 seconds instead of 4 minutes</p>\n"}, "answer_1_votes": {"type": "literal", "value": "15"}, "answer_2": {"type": "literal", "value": "<p>for people like me who are trying to implement the aforementioned solutions:</p>\n\n<p>Pandas 0.24.0 has now to_sql with chunksize and method='multi' option that inserts in bulk...</p>\n"}, "answer_2_votes": {"type": "literal", "value": "2"}, "answer_3": {"type": "literal", "value": "<p>For anyone facing this problem and having the destination DB as Redshift, note that Redshift does not implement the full set of Postgres commands, and so some of the answers using either Postgres' <code>COPY FROM</code> or <code>copy_from()</code> will not work.\n<a href=\"https://stackoverflow.com/questions/40904489/psycopg2-programmingerror-syntax-error-at-or-near-stdin-error-when-trying-to\">psycopg2.ProgrammingError: syntax error at or near &quot;stdin&quot; error when trying to copy_from redshift</a></p>\n\n<p>Solution for speeding up the INSERTs to Redshift is to use a file ingest or Odo.</p>\n\n<p>Reference:<br>\nAbout Odo\n<a href=\"http://odo.pydata.org/en/latest/perf.html\" rel=\"nofollow noreferrer\">http://odo.pydata.org/en/latest/perf.html</a><br>\nOdo with Redshift<br>\n<a href=\"https://github.com/blaze/odo/blob/master/docs/source/aws.rst\" rel=\"nofollow noreferrer\">https://github.com/blaze/odo/blob/master/docs/source/aws.rst</a><br>\nRedshift COPY (from S3 file)<br>\n<a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\" rel=\"nofollow noreferrer\">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html</a>  </p>\n"}, "answer_3_votes": {"type": "literal", "value": ""}, "answer_4": {"type": "literal", "value": "<p>This might have been answered by then, but I found the solution by collating different answers on this site and aligning with SQLAlchemy's doc.</p>\n\n<ol>\n<li>The table needs to already exist in db1; with an index set up with auto_increment on.</li>\n<li>The Class <i>Current</i> needs to align with the dataframe imported in the CSV and the table in the db1.</li>\n</ol>\n\n<p>Hope this helps whoever comes here and wants to mix Panda and SQLAlchemy in a quick way.</p>\n\n<pre><code>from urllib import quote_plus as urlquote\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String, Numeric\nfrom sqlalchemy.orm import sessionmaker\nimport pandas as pd\n\n\n# Set up of the engine to connect to the database\n# the urlquote is used for passing the password which might contain special characters such as \"/\"\nengine = create_engine('mysql://root:%s@localhost/db1' % urlquote('weirdPassword*withsp\u20accialcharacters'), echo=False)\nconn = engine.connect()\nBase = declarative_base()\n\n#Declaration of the class in order to write into the database. This structure is standard and should align with SQLAlchemy's doc.\nclass Current(Base):\n    __tablename__ = 'tableName'\n\n    id = Column(Integer, primary_key=True)\n    Date = Column(String(500))\n    Type = Column(String(500))\n    Value = Column(Numeric())\n\n    def __repr__(self):\n        return \"(id='%s', Date='%s', Type='%s', Value='%s')\" % (self.id, self.Date, self.Type, self.Value)\n\n# Set up of the table in db and the file to import\nfileToRead = 'file.csv'\ntableToWriteTo = 'tableName'\n\n# Panda to create a lovely dataframe\ndf_to_be_written = pd.read_csv(fileToRead)\n# The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\nlistToWrite = df_to_be_written.to_dict(orient='records')\n\nmetadata = sqlalchemy.schema.MetaData(bind=engine,reflect=True)\ntable = sqlalchemy.Table(tableToWriteTo, metadata, autoload=True)\n\n# Open the session\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Inser the dataframe into the database in one bulk\nconn.execute(table.insert(), listToWrite)\n\n# Commit the changes\nsession.commit()\n\n# Close the session\nsession.close()\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "16"}, "answer_5": {"type": "literal", "value": "<p>I ran into a similar issue with pd.to_sql taking hours to upload data.  The below code bulk inserted the same data in a few seconds.  </p>\n\n<pre><code>from sqlalchemy import create_engine\nimport psycopg2 as pg\n#load python script that batch loads pandas df to sql\nimport cStringIO\n\naddress = 'postgresql://&lt;username&gt;:&lt;pswd&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;'\nengine = create_engine(address)\nconnection = engine.raw_connection()\ncursor = connection.cursor()\n\n#df is the dataframe containing an index and the columns \"Event\" and \"Day\"\n#create Index column to use as primary key\ndf.reset_index(inplace=True)\ndf.rename(columns={'index':'Index'}, inplace =True)\n\n#create the table but first drop if it already exists\ncommand = '''DROP TABLE IF EXISTS localytics_app2;\nCREATE TABLE localytics_app2\n(\n\"Index\" serial primary key,\n\"Event\" text,\n\"Day\" timestamp without time zone,\n);'''\ncursor.execute(command)\nconnection.commit()\n\n#stream the data using 'to_csv' and StringIO(); then use sql's 'copy_from' function\noutput = cStringIO.StringIO()\n#ignore the index\ndf.to_csv(output, sep='\\t', header=False, index=False)\n#jump to start of stream\noutput.seek(0)\ncontents = output.getvalue()\ncur = connection.cursor()\n#null values become ''\ncur.copy_from(output, 'localytics_app2', null=\"\")    \nconnection.commit()\ncur.close()\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "26"}, "answer_6": {"type": "literal", "value": "<p>My postgres specific solution below auto-creates the database table using your pandas dataframe, and performs a fast bulk insert using the postgres <code>COPY my_table FROM ...</code></p>\n\n<pre><code>import io\n\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef write_to_table(df, db_engine, schema, table_name, if_exists='fail'):\n    string_data_io = io.StringIO()\n    df.to_csv(string_data_io, sep='|', index=False)\n    pd_sql_engine = pd.io.sql.pandasSQL_builder(db_engine, schema=schema)\n    table = pd.io.sql.SQLTable(table_name, pd_sql_engine, frame=df,\n                               index=False, if_exists=if_exists, schema=schema)\n    table.create()\n    string_data_io.seek(0)\n    string_data_io.readline()  # remove header\n    with db_engine.connect() as connection:\n        with connection.connection.cursor() as cursor:\n            copy_cmd = \"COPY %s.%s FROM STDIN HEADER DELIMITER '|' CSV\" % (schema, table_name)\n            cursor.copy_expert(copy_cmd, string_data_io)\n        connection.connection.commit()\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "3"}, "answer_7": {"type": "literal", "value": "<p>This worked for me to connect to Oracle Database using cx_Oracle and SQLALchemy</p>\n\n<pre><code>import sqlalchemy\nimport cx_Oracle\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, String\nfrom sqlalchemy.orm import sessionmaker\nimport pandas as pd\n\n# credentials\nusername = \"username\"\npassword = \"password\"\nconnectStr = \"connection:/string\"\ntableName = \"tablename\"\n\nt0 = time.time()\n\n# connection\ndsn = cx_Oracle.makedsn('host','port',service_name='servicename')\n\nBase = declarative_base()\n\nclass LANDMANMINERAL(Base):\n    __tablename__ = 'tablename'\n\n    DOCUMENTNUM = Column(String(500), primary_key=True)\n    DOCUMENTTYPE = Column(String(500))\n    FILENUM = Column(String(500))\n    LEASEPAYOR = Column(String(500))\n    LEASESTATUS = Column(String(500))\n    PROSPECT = Column(String(500))\n    SPLIT = Column(String(500))\n    SPLITSTATUS = Column(String(500))\n\nengine = create_engine('oracle+cx_oracle://%s:%s@%s' % (username, password, dsn))\nconn = engine.connect()  \n\nBase.metadata.bind = engine\n\n# Creating the session\n\nDBSession = sessionmaker(bind=engine)\n\nsession = DBSession()\n\n# Bulk insertion\ndata = pd.read_csv('data.csv')\nlists = data.to_dict(orient='records')\n\n\ntable = sqlalchemy.Table('landmanmineral', Base.metadata, autoreload=True)\nconn.execute(table.insert(), lists)\n\nsession.commit()\n\nsession.close() \n\nprint(\"time taken %8.8f seconds\" % (time.time() - t0) )\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": ""}, "answer_8": {"type": "literal", "value": "<p>As this is an I/O heavy workload you can also use the python threading module through <a href=\"https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy\" rel=\"nofollow noreferrer\">multiprocessing.dummy</a>. This sped things up for me:</p>\n\n<pre><code>import math\nfrom multiprocessing.dummy import Pool as ThreadPool\n\n...\n\ndef insert_df(df, *args, **kwargs):\n    nworkers = 4\n\n    chunksize = math.floor(df.shape[0] / nworkers)\n    chunks = [(chunksize * i, (chunksize * i) + chunksize) for i in range(nworkers)]\n    chunks.append((chunksize * nworkers, df.shape[0]))\n    pool = ThreadPool(nworkers)\n\n    def worker(chunk):\n        i, j = chunk\n        df.iloc[i:j, :].to_sql(*args, **kwargs)\n\n    pool.map(worker, chunks)\n    pool.close()\n    pool.join()\n\n\n....\n\ninsert_df(df, \"foo_bar\", engine, if_exists='append')\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have some rather large pandas DataFrames and I'd like to use the new bulk SQL mappings to upload them to a Microsoft SQL Server via SQL Alchemy. The pandas.to_sql method, while nice, is slow. </p>\n\n<p>I'm having trouble writing the code...</p>\n\n<p>I'd like to be able to pass this function a pandas DataFrame which I'm calling  , a schema name I'm calling  , and a table name I'm calling  . Ideally, the function will 1.) delete the table if it already exists. 2.) create a new table 3.) create a mapper and 4.) bulk insert using the mapper and pandas data. I'm stuck on part 3.</p>\n\n<p>Here's my (admittedly rough) code. I'm struggling with how to get the mapper function to work with my primary keys. I don't really need primary keys but the mapper function requires it. </p>\n\n<p>Thanks for the insights.</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Based on @ansonw answers:</p>\n\n<pre> </pre>\n\n<p>I insert 200000 lines in 5 seconds instead of 4 minutes</p>\n\n\n<p>for people like me who are trying to implement the aforementioned solutions:</p>\n\n<p>Pandas 0.24.0 has now to_sql with chunksize and method='multi' option that inserts in bulk...</p>\n\n\n<p>For anyone facing this problem and having the destination DB as Redshift, note that Redshift does not implement the full set of Postgres commands, and so some of the answers using either Postgres'   or   will not work.\n<a href=\"https://stackoverflow.com/questions/40904489/psycopg2-programmingerror-syntax-error-at-or-near-stdin-error-when-trying-to\">psycopg2.ProgrammingError: syntax error at or near &quot;stdin&quot; error when trying to copy_from redshift</a></p>\n\n<p>Solution for speeding up the INSERTs to Redshift is to use a file ingest or Odo.</p>\n\n<p>Reference:<br>\nAbout Odo\n<a href=\"http://odo.pydata.org/en/latest/perf.html\" rel=\"nofollow noreferrer\">http://odo.pydata.org/en/latest/perf.html</a><br>\nOdo with Redshift<br>\n<a href=\"https://github.com/blaze/odo/blob/master/docs/source/aws.rst\" rel=\"nofollow noreferrer\">https://github.com/blaze/odo/blob/master/docs/source/aws.rst</a><br>\nRedshift COPY (from S3 file)<br>\n<a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\" rel=\"nofollow noreferrer\">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html</a>  </p>\n\n\n<p>This might have been answered by then, but I found the solution by collating different answers on this site and aligning with SQLAlchemy's doc.</p>\n\n<ol>\n<li>The table needs to already exist in db1; with an index set up with auto_increment on.</li>\n<li>The Class <i>Current</i> needs to align with the dataframe imported in the CSV and the table in the db1.</li>\n</ol>\n\n<p>Hope this helps whoever comes here and wants to mix Panda and SQLAlchemy in a quick way.</p>\n\n<pre> </pre>\n\n\n<p>I ran into a similar issue with pd.to_sql taking hours to upload data.  The below code bulk inserted the same data in a few seconds.  </p>\n\n<pre> </pre>\n\n\n<p>My postgres specific solution below auto-creates the database table using your pandas dataframe, and performs a fast bulk insert using the postgres  </p>\n\n<pre> </pre>\n\n\n<p>This worked for me to connect to Oracle Database using cx_Oracle and SQLALchemy</p>\n\n<pre> </pre>\n\n\n<p>As this is an I/O heavy workload you can also use the python threading module through <a href=\"https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy\" rel=\"nofollow noreferrer\">multiprocessing.dummy</a>. This sped things up for me:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/numpy.genfromtxt"}, "class_func_label": {"type": "literal", "value": "numpy.genfromtxt"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nLoad data from a text file, with missing values handled as specified.\n\nEach line past the first `skip_header` lines is split at the `delimiter`\ncharacter, and characters following the `comments` character are discarded.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/3518778"}, "title": {"type": "literal", "value": "How do I read CSV data into a record array in NumPy?"}, "content": {"type": "literal", "value": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's <code>read.table()</code>, <code>read.delim()</code>, and <code>read.csv()</code> family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like <code>numpy.core.records.fromrecords()</code>?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n\n\n<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n\n\n<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n\n\n<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "12"}, "answer_2": {"type": "literal", "value": "<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "answer_3": {"type": "literal", "value": "<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "63"}, "answer_4": {"type": "literal", "value": "<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "65"}, "answer_5": {"type": "literal", "value": "<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "549"}, "answer_6": {"type": "literal", "value": "<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "158"}, "answer_7": {"type": "literal", "value": "<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "2"}, "answer_10": {"type": "literal", "value": "<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's  ,  , and   family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like  ?</p>\n", "answer_wo_code": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre> </pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre> </pre>\n\n<h3>test_pandas.py</h3>\n\n<pre> </pre>\n\n<h3>Data file:</h3>\n\n<pre> </pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre> </pre>\n\n\n<p>This is the easiest way:</p>\n\n<p> </p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre> </pre>\n\n<p>versus</p>\n\n<pre> </pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try   which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's   method to do so, by setting the   kwarg to a comma.</p>\n\n<pre> </pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"> </a> function from the   library:</p>\n\n<pre> </pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend  . However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the   parameter needs to be added to the   call:</p>\n\n<p>Given an input file,  :</p>\n\n<pre> </pre>\n\n<p>gives an array:</p>\n\n<pre> </pre>\n\n<p>and </p>\n\n<pre> </pre>\n\n<p>gives a record array:</p>\n\n<pre> </pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre> </pre>\n\n\n<p>I would suggest using tables ( ). You can save your   file to   using pandas ( ),</p>\n\n<pre> </pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre> </pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"> </a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre> </pre>\n\n\n<p>I tried this:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/numpy.load"}, "class_func_label": {"type": "literal", "value": "numpy.load"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nLoad arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n\n.. warning:: Loading files that contain object arrays uses the ``pickle``\n             module, which is not secure against erroneous or maliciously\n             constructed data. Consider passing ``allow_pickle=False`` to\n             load data that is known not to contain object arrays for the\n             safer handling of untrusted sources.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/3518778"}, "title": {"type": "literal", "value": "How do I read CSV data into a record array in NumPy?"}, "content": {"type": "literal", "value": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's <code>read.table()</code>, <code>read.delim()</code>, and <code>read.csv()</code> family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like <code>numpy.core.records.fromrecords()</code>?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n\n\n<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n\n\n<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n\n\n<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "12"}, "answer_2": {"type": "literal", "value": "<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "answer_3": {"type": "literal", "value": "<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "63"}, "answer_4": {"type": "literal", "value": "<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "65"}, "answer_5": {"type": "literal", "value": "<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "549"}, "answer_6": {"type": "literal", "value": "<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "158"}, "answer_7": {"type": "literal", "value": "<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "2"}, "answer_10": {"type": "literal", "value": "<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's  ,  , and   family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like  ?</p>\n", "answer_wo_code": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre> </pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre> </pre>\n\n<h3>test_pandas.py</h3>\n\n<pre> </pre>\n\n<h3>Data file:</h3>\n\n<pre> </pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre> </pre>\n\n\n<p>This is the easiest way:</p>\n\n<p> </p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre> </pre>\n\n<p>versus</p>\n\n<pre> </pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try   which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's   method to do so, by setting the   kwarg to a comma.</p>\n\n<pre> </pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"> </a> function from the   library:</p>\n\n<pre> </pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend  . However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the   parameter needs to be added to the   call:</p>\n\n<p>Given an input file,  :</p>\n\n<pre> </pre>\n\n<p>gives an array:</p>\n\n<pre> </pre>\n\n<p>and </p>\n\n<pre> </pre>\n\n<p>gives a record array:</p>\n\n<pre> </pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre> </pre>\n\n\n<p>I would suggest using tables ( ). You can save your   file to   using pandas ( ),</p>\n\n<pre> </pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre> </pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"> </a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre> </pre>\n\n\n<p>I tried this:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/numpy.loadtxt"}, "class_func_label": {"type": "literal", "value": "numpy.loadtxt"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nLoad data from a text file.\n\nEach row in the text file must have the same number of values.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/3518778"}, "title": {"type": "literal", "value": "How do I read CSV data into a record array in NumPy?"}, "content": {"type": "literal", "value": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's <code>read.table()</code>, <code>read.delim()</code>, and <code>read.csv()</code> family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like <code>numpy.core.records.fromrecords()</code>?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n\n\n<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n\n\n<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n\n\n<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "12"}, "answer_2": {"type": "literal", "value": "<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "answer_3": {"type": "literal", "value": "<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "63"}, "answer_4": {"type": "literal", "value": "<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "65"}, "answer_5": {"type": "literal", "value": "<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "549"}, "answer_6": {"type": "literal", "value": "<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "158"}, "answer_7": {"type": "literal", "value": "<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "2"}, "answer_10": {"type": "literal", "value": "<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's  ,  , and   family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like  ?</p>\n", "answer_wo_code": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre> </pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre> </pre>\n\n<h3>test_pandas.py</h3>\n\n<pre> </pre>\n\n<h3>Data file:</h3>\n\n<pre> </pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre> </pre>\n\n\n<p>This is the easiest way:</p>\n\n<p> </p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre> </pre>\n\n<p>versus</p>\n\n<pre> </pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try   which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's   method to do so, by setting the   kwarg to a comma.</p>\n\n<pre> </pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"> </a> function from the   library:</p>\n\n<pre> </pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend  . However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the   parameter needs to be added to the   call:</p>\n\n<p>Given an input file,  :</p>\n\n<pre> </pre>\n\n<p>gives an array:</p>\n\n<pre> </pre>\n\n<p>and </p>\n\n<pre> </pre>\n\n<p>gives a record array:</p>\n\n<pre> </pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre> </pre>\n\n\n<p>I would suggest using tables ( ). You can save your   file to   using pandas ( ),</p>\n\n<pre> </pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre> </pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"> </a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre> </pre>\n\n\n<p>I tried this:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.io.parsers.read_csv"}, "class_func_label": {"type": "literal", "value": "pandas.io.parsers.read_csv"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/3518778"}, "title": {"type": "literal", "value": "How do I read CSV data into a record array in NumPy?"}, "content": {"type": "literal", "value": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's <code>read.table()</code>, <code>read.delim()</code>, and <code>read.csv()</code> family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like <code>numpy.core.records.fromrecords()</code>?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n\n\n<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n\n\n<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n\n\n<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre><code>$ for f in test_pandas.py test_numpy_csv.py ; do  /usr/bin/time python $f; done\n2.94user 0.41system 0:03.05elapsed 109%CPU (0avgtext+0avgdata 502068maxresident)k\n0inputs+24outputs (0major+107147minor)pagefaults 0swaps\n\n23.29user 0.72system 0:23.72elapsed 101%CPU (0avgtext+0avgdata 1680888maxresident)k\n0inputs+0outputs (0major+416145minor)pagefaults 0swaps\n</code></pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre><code>from numpy import genfromtxt\ntrain = genfromtxt('/home/hvn/me/notebook/train.csv', delimiter=',')\n</code></pre>\n\n<h3>test_pandas.py</h3>\n\n<pre><code>from pandas import read_csv\ndf = read_csv('/home/hvn/me/notebook/train.csv')\n</code></pre>\n\n<h3>Data file:</h3>\n\n<pre><code>du -h ~/me/notebook/train.csv\n 59M    /home/hvn/me/notebook/train.csv\n</code></pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre><code>$ pip freeze | egrep -i 'pandas|numpy'\nnumpy==1.13.3\npandas==0.20.2\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "12"}, "answer_2": {"type": "literal", "value": "<p>This is the easiest way:</p>\n\n<p><code>import csv\n with open('testfile.csv', newline='') as csvfile:\n     data = list(csv.reader(csvfile))</code></p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n"}, "answer_2_votes": {"type": "literal", "value": "3"}, "answer_3": {"type": "literal", "value": "<p>I timed the</p>\n\n<pre><code>from numpy import genfromtxt\ngenfromtxt(fname = dest_file, dtype = (&lt;whatever options&gt;))\n</code></pre>\n\n<p>versus</p>\n\n<pre><code>import csv\nimport numpy as np\nwith open(dest_file,'r') as dest_f:\n    data_iter = csv.reader(dest_f,\n                           delimiter = delimiter,\n                           quotechar = '\"')\n    data = [data for data in data_iter]\ndata_array = np.asarray(data, dtype = &lt;whatever options&gt;)\n</code></pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "63"}, "answer_4": {"type": "literal", "value": "<p>You can also try <code>recfromcsv()</code> which can guess data types and return a properly formatted record array.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "65"}, "answer_5": {"type": "literal", "value": "<p>You can use Numpy's <code>genfromtxt()</code> method to do so, by setting the <code>delimiter</code> kwarg to a comma.</p>\n\n<pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "549"}, "answer_6": {"type": "literal", "value": "<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"><code>read_csv</code></a> function from the <code>pandas</code> library:</p>\n\n<pre><code>import pandas as pd\ndf=pd.read_csv('myfile.csv', sep=',',header=None)\ndf.values\narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend <code>genfromtxt</code>. However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the <code>dtype=None</code> parameter needs to be added to the <code>genfromtxt</code> call:</p>\n\n<p>Given an input file, <code>myfile.csv</code>:</p>\n\n<pre><code>1.0, 2, 3\n4, 5.5, 6\n\nimport numpy as np\nnp.genfromtxt('myfile.csv',delimiter=',')\n</code></pre>\n\n<p>gives an array:</p>\n\n<pre><code>array([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5.5,  6. ]])\n</code></pre>\n\n<p>and </p>\n\n<pre><code>np.genfromtxt('myfile.csv',delimiter=',',dtype=None)\n</code></pre>\n\n<p>gives a record array:</p>\n\n<pre><code>array([(1.0, 2.0, 3), (4.0, 5.5, 6)], \n      dtype=[('f0', '&lt;f8'), ('f1', '&lt;f8'), ('f2', '&lt;i4')])\n</code></pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n"}, "answer_6_votes": {"type": "literal", "value": "158"}, "answer_7": {"type": "literal", "value": "<p>You can use this code to send CSV file data into an array:</p>\n\n<pre><code>import numpy as np\ncsv = np.genfromtxt('test.csv', delimiter=\",\")\nprint(csv)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>I would suggest using tables (<code>pip3 install tables</code>). You can save your <code>.csv</code> file to <code>.h5</code> using pandas (<code>pip3 install pandas</code>),</p>\n\n<pre><code>import pandas as pd\ndata = pd.read_csv(\"dataset.csv\")\nstore = pd.HDFStore('dataset.h5')\nstore['mydata'] = data\nstore.close()\n</code></pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre><code>import pandas as pd\nstore = pd.HDFStore('dataset.h5')\ndata = store['mydata']\nstore.close()\n\n# Data in NumPy format\ndata = data.values\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"><code>numpy.loadtxt</code></a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre><code>import numpy as np \ndata = np.loadtxt('c:\\\\1.csv',delimiter=',',skiprows=0)  \n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "2"}, "answer_10": {"type": "literal", "value": "<p>I tried this:</p>\n\n<pre><code>import pandas as p\nimport numpy as n\n\nclosingValue = p.read_csv(\"&lt;FILENAME&gt;\", usecols=[4], dtype=float)\nprint(closingValue)\n</code></pre>\n"}, "answer_10_votes": {"type": "literal", "value": "3"}, "content_wo_code": "<p>I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's  ,  , and   family imports data to R's data frame?</p>\n\n<p>Or is the best way to use <a href=\"https://stackoverflow.com/questions/2859404/reading-csv-files-in-scipy-numpy-in-python\">csv.reader()</a> and then apply something like  ?</p>\n", "answer_wo_code": "<p>As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:</p>\n\n<ul>\n<li>Faster</li>\n<li>Less CPU usage</li>\n<li>1/3 RAM usage compared to NumPy genfromtxt</li>\n</ul>\n\n<p>This is my test code:</p>\n\n<pre> </pre>\n\n<h3>test_numpy_csv.py</h3>\n\n<pre> </pre>\n\n<h3>test_pandas.py</h3>\n\n<pre> </pre>\n\n<h3>Data file:</h3>\n\n<pre> </pre>\n\n<p>With NumPy and pandas at versions:</p>\n\n<pre> </pre>\n\n\n<p>This is the easiest way:</p>\n\n<p> </p>\n\n<p>Now each entry in data is a record, represented as an array. So you have a 2D array. It saved me so much time.</p>\n\n\n<p>I timed the</p>\n\n<pre> </pre>\n\n<p>versus</p>\n\n<pre> </pre>\n\n<p>on 4.6 million rows with about 70 columns and found that the NumPy path took 2 min 16 secs and the csv-list comprehension method took 13 seconds.</p>\n\n<p>I would recommend the csv-list comprehension method as it is most likely relies on pre-compiled libraries and not the interpreter as much as NumPy. I suspect the pandas method would have similar interpreter overhead.</p>\n\n\n<p>You can also try   which can guess data types and return a properly formatted record array.</p>\n\n\n<p>You can use Numpy's   method to do so, by setting the   kwarg to a comma.</p>\n\n<pre> </pre>\n\n<p>More information on the function can be found at its respective <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\" rel=\"noreferrer\">documentation</a>.</p>\n\n\n<p>I would recommend the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\"> </a> function from the   library:</p>\n\n<pre> </pre>\n\n<p>This gives a pandas <a href=\"http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\" rel=\"noreferrer\">DataFrame</a> - allowing <a href=\"https://stackoverflow.com/a/11077215/1461850\">many useful data manipulation functions which are not directly available with numpy record arrays</a>.</p>\n\n<blockquote>\n  <p>DataFrame is a 2-dimensional labeled data structure with columns of\n  potentially different types. You can think of it like a spreadsheet or\n  SQL table...</p>\n</blockquote>\n\n<hr>\n\n<p>I would also recommend  . However, since the question asks for a <a href=\"http://docs.scipy.org/doc/numpy/user/basics.rec.html\" rel=\"noreferrer\">record array</a>, as opposed to a normal array, the   parameter needs to be added to the   call:</p>\n\n<p>Given an input file,  :</p>\n\n<pre> </pre>\n\n<p>gives an array:</p>\n\n<pre> </pre>\n\n<p>and </p>\n\n<pre> </pre>\n\n<p>gives a record array:</p>\n\n<pre> </pre>\n\n<p>This has the advantage that file with <a href=\"https://stackoverflow.com/a/15481761\">multiple data types (including strings) can be easily imported</a>.</p>\n\n\n<p>You can use this code to send CSV file data into an array:</p>\n\n<pre> </pre>\n\n\n<p>I would suggest using tables ( ). You can save your   file to   using pandas ( ),</p>\n\n<pre> </pre>\n\n<p>You can then easily, and with less time even for huge amount of data, load your data in a <em>NumPy array</em>.</p>\n\n<pre> </pre>\n\n\n<p>Using <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\" rel=\"nofollow noreferrer\"> </a></p>\n\n<p>A quite simple method. But it requires all the elements being float (int and so on)</p>\n\n<pre> </pre>\n\n\n<p>I tried this:</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.DataFrame"}, "class_func_label": {"type": "literal", "value": "pandas.DataFrame"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Class"}, "docstr": {"type": "literal", "value": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/12047193"}, "title": {"type": "literal", "value": "How to convert SQL Query result to PANDAS Data Structure?"}, "content": {"type": "literal", "value": "<p>Any help on this problem will be greatly appreciated.</p>\n\n<p>So basically I want to run a query to my SQL database and store the returned data as Pandas data structure.</p>\n\n<p>I have attached code for query.</p>\n\n<p>I am reading the documentation on Pandas, but I have problem to identify the return type of my query.</p>\n\n<p>I tried to print the query result, but it doesn't give any useful information.</p>\n\n<p>Thanks!!!! </p>\n\n<pre><code>from sqlalchemy import create_engine\n\nengine2 = create_engine('mysql://THE DATABASE I AM ACCESSING')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(\"\n  SELECT \n      sum(BLABLA) AS BLA,\n      sum(BLABLABLA2) AS BLABLABLA2,\n      sum(SOME_INT) AS SOME_INT,\n      sum(SOME_INT2) AS SOME_INT2,\n      100*sum(SOME_INT2)/sum(SOME_INT) AS ctr,\n      sum(SOME_INT2)/sum(SOME_INT) AS cpc\n   FROM daily_report_cooked\n   WHERE campaign_id = '%s'\", %dataid)\n</code></pre>\n\n<p>So I sort of want to understand what's the format/datatype of my variable \"resoverall\" and how to put it with PANDAS data structure.</p>\n"}, "answerContent": {"type": "literal", "value": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre><code>import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = \"SELECT * FROM myTable\"\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    '''\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    '''\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], 'U%d' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], 'S%d' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], 'f4'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], 'O4'))\n        elif col[1] == int:\n            datatypes.append((col[0], 'i4'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n</code></pre>\n\n\n<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre><code>my_dic = session.query(query.all())\nmy_df = pandas.DataFrame.from_dict(my_dic)\n</code></pre>\n\n\n<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre><code>import MySQLdb as mdb\nimport pandas.io.sql as sql\nfrom pandas import *\n\nconn = mdb.connect('&lt;server&gt;','&lt;user&gt;','&lt;pass&gt;','&lt;db&gt;');\ndf = sql.read_frame('&lt;query&gt;', conn)\n</code></pre>\n\n<p>For example,</p>\n\n<pre><code>conn = mdb.connect('localhost','myname','mypass','testdb');\ndf = sql.read_frame('select * from testTable', conn)\n</code></pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n\n\n<p><code>resoverall</code> is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that <code>resoverall</code> is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n\n\n<p>Here's the shortest code that will do the job:</p>\n\n<pre><code>from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n</code></pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n\n\n<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's <code>read_sql()</code> method. E.g., starting with a Query object called <code>query</code>:</p>\n\n<pre><code>df = pd.read_sql(query.statement, query.session.bind)\n</code></pre>\n\n\n<p>This is a short and crisp answer to your problem:</p>\n\n<pre><code>from __future__ import print_function\nimport MySQLdb\nimport numpy as np\nimport pandas as pd\nimport xlrd\n\n# Connecting to MySQL Database\nconnection = MySQLdb.connect(\n             host=\"hostname\",\n             port=0000,\n             user=\"userID\",\n             passwd=\"password\",\n             db=\"table_documents\",\n             charset='utf8'\n           )\nprint(connection)\n#getting data from database into a dataframe\nsql_for_df = 'select * from tabledata'\ndf_from_database = pd.read_sql(sql_for_df , connection)\n</code></pre>\n\n\n<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre><code>import pymysql\nfrom pandas import DataFrame\n\nhost   = 'localhost'\nport   = 3306\nuser   = 'yourUserName'\npasswd = 'yourPassword'\ndb     = 'yourDatabase'\n\ncnx    = pymysql.connect(host=host, port=port, user=user, passwd=passwd, db=db)\ncur    = cnx.cursor()\n\nquery  = \"\"\" SELECT * FROM yourTable LIMIT 10\"\"\"\ncur.execute(query)\n\nfield_names = [i[0] for i in cur.description]\nget_data = [xx for xx in cur]\n\ncur.close()\ncnx.close()\n\ndf = DataFrame(get_data)\ndf.columns = field_names\n</code></pre>\n\n\n<p>Here's the code I use. Hope this helps.</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\ndef getData():\n  # Parameters\n  ServerName = \"my_server\"\n  Database = \"my_db\"\n  UserPwd = \"user:pwd\"\n  Driver = \"driver=SQL Server Native Client 11.0\"\n\n  # Create the connection\n  engine = create_engine('mssql+pyodbc://' + UserPwd + '@' + ServerName + '/' + Database + \"?\" + Driver)\n\n  sql = \"select * from mytable\"\n  df = pd.read_sql(sql, engine)\n  return df\n\ndf2 = getData()\nprint(df2)\n</code></pre>\n\n\n<p>Simply use <code>pandas</code> and <code>pyodbc</code> together. You'll have to modify your connection string (<code>connstr</code>) according to your database specifications.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n# MSSQL Connection String Example\nconnstr = \"Server=myServerAddress;Database=myDB;User Id=myUsername;Password=myPass;\"\n\n# Query Database and Create DataFrame Using Results\ndf = pd.read_sql(\"select * from myTable\", pyodbc.connect(connstr))\n</code></pre>\n\n<p>I've used <code>pyodbc</code> with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n\n\n<p>best way I do this</p>\n\n<pre><code>db.execute(query) where db=db_class() #database class\n    mydata=[x for x in db.fetchall()]\n    df=pd.DataFrame(data=mydata)\n</code></pre>\n\n\n<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre><code>query = session.query(tbl.Field1, tbl.Field2)\nDataFrame(query.all(), columns=[column['name'] for column in query.column_descriptions])\n</code></pre>\n\n\n<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre><code>df = pd.DataFrame([dict(r) for r in resoverall])\n</code></pre>\n\n\n<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre><code>import pandas as pd\nimport mysql.connector\n\n# Setup MySQL connection\ndb = mysql.connector.connect(\n    host=\"&lt;IP&gt;\",              # your host, usually localhost\n    user=\"&lt;USER&gt;\",            # your username\n    password=\"&lt;PASS&gt;\",        # your password\n    database=\"&lt;DATABASE&gt;\"     # name of the data base\n)   \n\n# You must create a Cursor object. It will let you execute all the queries you need\ncur = db.cursor()\n\n# Use all the SQL you like\ncur.execute(\"SELECT * FROM &lt;TABLE&gt;\")\n\n# Put it all to a data frame\nsql_data = pd.DataFrame(cur.fetchall())\nsql_data.columns = cur.column_names\n\n# Close the session\ndb.close()\n\n# Show the data\nprint(sql_data.head())\n</code></pre>\n\n\n<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n</code></pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre><code>import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = \"SELECT * FROM TABLE\"\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre><code>import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = \"SELECT * FROM myTable\"\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    '''\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    '''\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], 'U%d' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], 'S%d' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], 'f4'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], 'O4'))\n        elif col[1] == int:\n            datatypes.append((col[0], 'i4'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "23"}, "answer_2": {"type": "literal", "value": "<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre><code>my_dic = session.query(query.all())\nmy_df = pandas.DataFrame.from_dict(my_dic)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre><code>import MySQLdb as mdb\nimport pandas.io.sql as sql\nfrom pandas import *\n\nconn = mdb.connect('&lt;server&gt;','&lt;user&gt;','&lt;pass&gt;','&lt;db&gt;');\ndf = sql.read_frame('&lt;query&gt;', conn)\n</code></pre>\n\n<p>For example,</p>\n\n<pre><code>conn = mdb.connect('localhost','myname','mypass','testdb');\ndf = sql.read_frame('select * from testTable', conn)\n</code></pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "3"}, "answer_4": {"type": "literal", "value": "<p><code>resoverall</code> is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that <code>resoverall</code> is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "4"}, "answer_5": {"type": "literal", "value": "<p>Here's the shortest code that will do the job:</p>\n\n<pre><code>from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n</code></pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "98"}, "answer_6": {"type": "literal", "value": "<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's <code>read_sql()</code> method. E.g., starting with a Query object called <code>query</code>:</p>\n\n<pre><code>df = pd.read_sql(query.statement, query.session.bind)\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "31"}, "answer_7": {"type": "literal", "value": "<p>This is a short and crisp answer to your problem:</p>\n\n<pre><code>from __future__ import print_function\nimport MySQLdb\nimport numpy as np\nimport pandas as pd\nimport xlrd\n\n# Connecting to MySQL Database\nconnection = MySQLdb.connect(\n             host=\"hostname\",\n             port=0000,\n             user=\"userID\",\n             passwd=\"password\",\n             db=\"table_documents\",\n             charset='utf8'\n           )\nprint(connection)\n#getting data from database into a dataframe\nsql_for_df = 'select * from tabledata'\ndf_from_database = pd.read_sql(sql_for_df , connection)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre><code>import pymysql\nfrom pandas import DataFrame\n\nhost   = 'localhost'\nport   = 3306\nuser   = 'yourUserName'\npasswd = 'yourPassword'\ndb     = 'yourDatabase'\n\ncnx    = pymysql.connect(host=host, port=port, user=user, passwd=passwd, db=db)\ncur    = cnx.cursor()\n\nquery  = \"\"\" SELECT * FROM yourTable LIMIT 10\"\"\"\ncur.execute(query)\n\nfield_names = [i[0] for i in cur.description]\nget_data = [xx for xx in cur]\n\ncur.close()\ncnx.close()\n\ndf = DataFrame(get_data)\ndf.columns = field_names\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Here's the code I use. Hope this helps.</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\ndef getData():\n  # Parameters\n  ServerName = \"my_server\"\n  Database = \"my_db\"\n  UserPwd = \"user:pwd\"\n  Driver = \"driver=SQL Server Native Client 11.0\"\n\n  # Create the connection\n  engine = create_engine('mssql+pyodbc://' + UserPwd + '@' + ServerName + '/' + Database + \"?\" + Driver)\n\n  sql = \"select * from mytable\"\n  df = pd.read_sql(sql, engine)\n  return df\n\ndf2 = getData()\nprint(df2)\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "9"}, "answer_10": {"type": "literal", "value": "<p>Simply use <code>pandas</code> and <code>pyodbc</code> together. You'll have to modify your connection string (<code>connstr</code>) according to your database specifications.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n# MSSQL Connection String Example\nconnstr = \"Server=myServerAddress;Database=myDB;User Id=myUsername;Password=myPass;\"\n\n# Query Database and Create DataFrame Using Results\ndf = pd.read_sql(\"select * from myTable\", pyodbc.connect(connstr))\n</code></pre>\n\n<p>I've used <code>pyodbc</code> with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n"}, "answer_10_votes": {"type": "literal", "value": "5"}, "answer_11": {"type": "literal", "value": "<p>best way I do this</p>\n\n<pre><code>db.execute(query) where db=db_class() #database class\n    mydata=[x for x in db.fetchall()]\n    df=pd.DataFrame(data=mydata)\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "1"}, "answer_12": {"type": "literal", "value": "<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre><code>query = session.query(tbl.Field1, tbl.Field2)\nDataFrame(query.all(), columns=[column['name'] for column in query.column_descriptions])\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "4"}, "answer_13": {"type": "literal", "value": "<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre><code>df = pd.DataFrame([dict(r) for r in resoverall])\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": ""}, "answer_14": {"type": "literal", "value": "<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre><code>import pandas as pd\nimport mysql.connector\n\n# Setup MySQL connection\ndb = mysql.connector.connect(\n    host=\"&lt;IP&gt;\",              # your host, usually localhost\n    user=\"&lt;USER&gt;\",            # your username\n    password=\"&lt;PASS&gt;\",        # your password\n    database=\"&lt;DATABASE&gt;\"     # name of the data base\n)   \n\n# You must create a Cursor object. It will let you execute all the queries you need\ncur = db.cursor()\n\n# Use all the SQL you like\ncur.execute(\"SELECT * FROM &lt;TABLE&gt;\")\n\n# Put it all to a data frame\nsql_data = pd.DataFrame(cur.fetchall())\nsql_data.columns = cur.column_names\n\n# Close the session\ndb.close()\n\n# Show the data\nprint(sql_data.head())\n</code></pre>\n"}, "answer_14_votes": {"type": "literal", "value": "12"}, "answer_15": {"type": "literal", "value": "<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n</code></pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre><code>import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = \"SELECT * FROM TABLE\"\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n</code></pre>\n"}, "answer_15_votes": {"type": "literal", "value": "116"}, "content_wo_code": "<p>Any help on this problem will be greatly appreciated.</p>\n\n<p>So basically I want to run a query to my SQL database and store the returned data as Pandas data structure.</p>\n\n<p>I have attached code for query.</p>\n\n<p>I am reading the documentation on Pandas, but I have problem to identify the return type of my query.</p>\n\n<p>I tried to print the query result, but it doesn't give any useful information.</p>\n\n<p>Thanks!!!! </p>\n\n<pre> </pre>\n\n<p>So I sort of want to understand what's the format/datatype of my variable \"resoverall\" and how to put it with PANDAS data structure.</p>\n", "answer_wo_code": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a   function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre> </pre>\n\n\n<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre> </pre>\n\n\n<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre> </pre>\n\n<p>For example,</p>\n\n<pre> </pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n\n\n<p>  is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that   is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n\n\n<p>Here's the shortest code that will do the job:</p>\n\n<pre> </pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n\n\n<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type   to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's   method. E.g., starting with a Query object called  :</p>\n\n<pre> </pre>\n\n\n<p>This is a short and crisp answer to your problem:</p>\n\n<pre> </pre>\n\n\n<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre> </pre>\n\n\n<p>Here's the code I use. Hope this helps.</p>\n\n<pre> </pre>\n\n\n<p>Simply use   and   together. You'll have to modify your connection string ( ) according to your database specifications.</p>\n\n<pre> </pre>\n\n<p>I've used   with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n\n\n<p>best way I do this</p>\n\n<pre> </pre>\n\n\n<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre> </pre>\n\n\n<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre> </pre>\n\n\n<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre> </pre>\n\n\n<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre> </pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/12047193"}, "title": {"type": "literal", "value": "How to convert SQL Query result to PANDAS Data Structure?"}, "content": {"type": "literal", "value": "<p>Any help on this problem will be greatly appreciated.</p>\n\n<p>So basically I want to run a query to my SQL database and store the returned data as Pandas data structure.</p>\n\n<p>I have attached code for query.</p>\n\n<p>I am reading the documentation on Pandas, but I have problem to identify the return type of my query.</p>\n\n<p>I tried to print the query result, but it doesn't give any useful information.</p>\n\n<p>Thanks!!!! </p>\n\n<pre><code>from sqlalchemy import create_engine\n\nengine2 = create_engine('mysql://THE DATABASE I AM ACCESSING')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute(\"\n  SELECT \n      sum(BLABLA) AS BLA,\n      sum(BLABLABLA2) AS BLABLABLA2,\n      sum(SOME_INT) AS SOME_INT,\n      sum(SOME_INT2) AS SOME_INT2,\n      100*sum(SOME_INT2)/sum(SOME_INT) AS ctr,\n      sum(SOME_INT2)/sum(SOME_INT) AS cpc\n   FROM daily_report_cooked\n   WHERE campaign_id = '%s'\", %dataid)\n</code></pre>\n\n<p>So I sort of want to understand what's the format/datatype of my variable \"resoverall\" and how to put it with PANDAS data structure.</p>\n"}, "answerContent": {"type": "literal", "value": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre><code>import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = \"SELECT * FROM myTable\"\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    '''\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    '''\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], 'U%d' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], 'S%d' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], 'f4'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], 'O4'))\n        elif col[1] == int:\n            datatypes.append((col[0], 'i4'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n</code></pre>\n\n\n<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre><code>my_dic = session.query(query.all())\nmy_df = pandas.DataFrame.from_dict(my_dic)\n</code></pre>\n\n\n<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre><code>import MySQLdb as mdb\nimport pandas.io.sql as sql\nfrom pandas import *\n\nconn = mdb.connect('&lt;server&gt;','&lt;user&gt;','&lt;pass&gt;','&lt;db&gt;');\ndf = sql.read_frame('&lt;query&gt;', conn)\n</code></pre>\n\n<p>For example,</p>\n\n<pre><code>conn = mdb.connect('localhost','myname','mypass','testdb');\ndf = sql.read_frame('select * from testTable', conn)\n</code></pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n\n\n<p><code>resoverall</code> is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that <code>resoverall</code> is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n\n\n<p>Here's the shortest code that will do the job:</p>\n\n<pre><code>from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n</code></pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n\n\n<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's <code>read_sql()</code> method. E.g., starting with a Query object called <code>query</code>:</p>\n\n<pre><code>df = pd.read_sql(query.statement, query.session.bind)\n</code></pre>\n\n\n<p>This is a short and crisp answer to your problem:</p>\n\n<pre><code>from __future__ import print_function\nimport MySQLdb\nimport numpy as np\nimport pandas as pd\nimport xlrd\n\n# Connecting to MySQL Database\nconnection = MySQLdb.connect(\n             host=\"hostname\",\n             port=0000,\n             user=\"userID\",\n             passwd=\"password\",\n             db=\"table_documents\",\n             charset='utf8'\n           )\nprint(connection)\n#getting data from database into a dataframe\nsql_for_df = 'select * from tabledata'\ndf_from_database = pd.read_sql(sql_for_df , connection)\n</code></pre>\n\n\n<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre><code>import pymysql\nfrom pandas import DataFrame\n\nhost   = 'localhost'\nport   = 3306\nuser   = 'yourUserName'\npasswd = 'yourPassword'\ndb     = 'yourDatabase'\n\ncnx    = pymysql.connect(host=host, port=port, user=user, passwd=passwd, db=db)\ncur    = cnx.cursor()\n\nquery  = \"\"\" SELECT * FROM yourTable LIMIT 10\"\"\"\ncur.execute(query)\n\nfield_names = [i[0] for i in cur.description]\nget_data = [xx for xx in cur]\n\ncur.close()\ncnx.close()\n\ndf = DataFrame(get_data)\ndf.columns = field_names\n</code></pre>\n\n\n<p>Here's the code I use. Hope this helps.</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\ndef getData():\n  # Parameters\n  ServerName = \"my_server\"\n  Database = \"my_db\"\n  UserPwd = \"user:pwd\"\n  Driver = \"driver=SQL Server Native Client 11.0\"\n\n  # Create the connection\n  engine = create_engine('mssql+pyodbc://' + UserPwd + '@' + ServerName + '/' + Database + \"?\" + Driver)\n\n  sql = \"select * from mytable\"\n  df = pd.read_sql(sql, engine)\n  return df\n\ndf2 = getData()\nprint(df2)\n</code></pre>\n\n\n<p>Simply use <code>pandas</code> and <code>pyodbc</code> together. You'll have to modify your connection string (<code>connstr</code>) according to your database specifications.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n# MSSQL Connection String Example\nconnstr = \"Server=myServerAddress;Database=myDB;User Id=myUsername;Password=myPass;\"\n\n# Query Database and Create DataFrame Using Results\ndf = pd.read_sql(\"select * from myTable\", pyodbc.connect(connstr))\n</code></pre>\n\n<p>I've used <code>pyodbc</code> with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n\n\n<p>best way I do this</p>\n\n<pre><code>db.execute(query) where db=db_class() #database class\n    mydata=[x for x in db.fetchall()]\n    df=pd.DataFrame(data=mydata)\n</code></pre>\n\n\n<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre><code>query = session.query(tbl.Field1, tbl.Field2)\nDataFrame(query.all(), columns=[column['name'] for column in query.column_descriptions])\n</code></pre>\n\n\n<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre><code>df = pd.DataFrame([dict(r) for r in resoverall])\n</code></pre>\n\n\n<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre><code>import pandas as pd\nimport mysql.connector\n\n# Setup MySQL connection\ndb = mysql.connector.connect(\n    host=\"&lt;IP&gt;\",              # your host, usually localhost\n    user=\"&lt;USER&gt;\",            # your username\n    password=\"&lt;PASS&gt;\",        # your password\n    database=\"&lt;DATABASE&gt;\"     # name of the data base\n)   \n\n# You must create a Cursor object. It will let you execute all the queries you need\ncur = db.cursor()\n\n# Use all the SQL you like\ncur.execute(\"SELECT * FROM &lt;TABLE&gt;\")\n\n# Put it all to a data frame\nsql_data = pd.DataFrame(cur.fetchall())\nsql_data.columns = cur.column_names\n\n# Close the session\ndb.close()\n\n# Show the data\nprint(sql_data.head())\n</code></pre>\n\n\n<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n</code></pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre><code>import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = \"SELECT * FROM TABLE\"\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a <code>read_sql</code> function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre><code>import decimal\n\nimport pydobc\nimport numpy as np\nimport pandas\n\ncnn, cur = myConnectToDBfunction()\ncmd = \"SELECT * FROM myTable\"\ncur.execute(cmd)\ndataframe = __processCursor(cur, dataframe=True)\n\ndef __processCursor(cur, dataframe=False, index=None):\n    '''\n    Processes a database cursor with data on it into either\n    a structured numpy array or a pandas dataframe.\n\n    input:\n    cur - a pyodbc cursor that has just received data\n    dataframe - bool. if false, a numpy record array is returned\n                if true, return a pandas dataframe\n    index - list of column(s) to use as index in a pandas dataframe\n    '''\n    datatypes = []\n    colinfo = cur.description\n    for col in colinfo:\n        if col[1] == unicode:\n            datatypes.append((col[0], 'U%d' % col[3]))\n        elif col[1] == str:\n            datatypes.append((col[0], 'S%d' % col[3]))\n        elif col[1] in [float, decimal.Decimal]:\n            datatypes.append((col[0], 'f4'))\n        elif col[1] == datetime.datetime:\n            datatypes.append((col[0], 'O4'))\n        elif col[1] == int:\n            datatypes.append((col[0], 'i4'))\n\n    data = []\n    for row in cur:\n        data.append(tuple(row))\n\n    array = np.array(data, dtype=datatypes)\n    if dataframe:\n        output = pandas.DataFrame.from_records(array)\n\n        if index is not None:\n            output = output.set_index(index)\n\n    else:\n        output = array\n\n    return output\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "23"}, "answer_2": {"type": "literal", "value": "<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre><code>my_dic = session.query(query.all())\nmy_df = pandas.DataFrame.from_dict(my_dic)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "1"}, "answer_3": {"type": "literal", "value": "<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre><code>import MySQLdb as mdb\nimport pandas.io.sql as sql\nfrom pandas import *\n\nconn = mdb.connect('&lt;server&gt;','&lt;user&gt;','&lt;pass&gt;','&lt;db&gt;');\ndf = sql.read_frame('&lt;query&gt;', conn)\n</code></pre>\n\n<p>For example,</p>\n\n<pre><code>conn = mdb.connect('localhost','myname','mypass','testdb');\ndf = sql.read_frame('select * from testTable', conn)\n</code></pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "3"}, "answer_4": {"type": "literal", "value": "<p><code>resoverall</code> is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that <code>resoverall</code> is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n"}, "answer_4_votes": {"type": "literal", "value": "4"}, "answer_5": {"type": "literal", "value": "<p>Here's the shortest code that will do the job:</p>\n\n<pre><code>from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n</code></pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n"}, "answer_5_votes": {"type": "literal", "value": "98"}, "answer_6": {"type": "literal", "value": "<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type <code>sqlalchemy.orm.query.Query</code> to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's <code>read_sql()</code> method. E.g., starting with a Query object called <code>query</code>:</p>\n\n<pre><code>df = pd.read_sql(query.statement, query.session.bind)\n</code></pre>\n"}, "answer_6_votes": {"type": "literal", "value": "31"}, "answer_7": {"type": "literal", "value": "<p>This is a short and crisp answer to your problem:</p>\n\n<pre><code>from __future__ import print_function\nimport MySQLdb\nimport numpy as np\nimport pandas as pd\nimport xlrd\n\n# Connecting to MySQL Database\nconnection = MySQLdb.connect(\n             host=\"hostname\",\n             port=0000,\n             user=\"userID\",\n             passwd=\"password\",\n             db=\"table_documents\",\n             charset='utf8'\n           )\nprint(connection)\n#getting data from database into a dataframe\nsql_for_df = 'select * from tabledata'\ndf_from_database = pd.read_sql(sql_for_df , connection)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "5"}, "answer_8": {"type": "literal", "value": "<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre><code>import pymysql\nfrom pandas import DataFrame\n\nhost   = 'localhost'\nport   = 3306\nuser   = 'yourUserName'\npasswd = 'yourPassword'\ndb     = 'yourDatabase'\n\ncnx    = pymysql.connect(host=host, port=port, user=user, passwd=passwd, db=db)\ncur    = cnx.cursor()\n\nquery  = \"\"\" SELECT * FROM yourTable LIMIT 10\"\"\"\ncur.execute(query)\n\nfield_names = [i[0] for i in cur.description]\nget_data = [xx for xx in cur]\n\ncur.close()\ncnx.close()\n\ndf = DataFrame(get_data)\ndf.columns = field_names\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": "1"}, "answer_9": {"type": "literal", "value": "<p>Here's the code I use. Hope this helps.</p>\n\n<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\ndef getData():\n  # Parameters\n  ServerName = \"my_server\"\n  Database = \"my_db\"\n  UserPwd = \"user:pwd\"\n  Driver = \"driver=SQL Server Native Client 11.0\"\n\n  # Create the connection\n  engine = create_engine('mssql+pyodbc://' + UserPwd + '@' + ServerName + '/' + Database + \"?\" + Driver)\n\n  sql = \"select * from mytable\"\n  df = pd.read_sql(sql, engine)\n  return df\n\ndf2 = getData()\nprint(df2)\n</code></pre>\n"}, "answer_9_votes": {"type": "literal", "value": "9"}, "answer_10": {"type": "literal", "value": "<p>Simply use <code>pandas</code> and <code>pyodbc</code> together. You'll have to modify your connection string (<code>connstr</code>) according to your database specifications.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n# MSSQL Connection String Example\nconnstr = \"Server=myServerAddress;Database=myDB;User Id=myUsername;Password=myPass;\"\n\n# Query Database and Create DataFrame Using Results\ndf = pd.read_sql(\"select * from myTable\", pyodbc.connect(connstr))\n</code></pre>\n\n<p>I've used <code>pyodbc</code> with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n"}, "answer_10_votes": {"type": "literal", "value": "5"}, "answer_11": {"type": "literal", "value": "<p>best way I do this</p>\n\n<pre><code>db.execute(query) where db=db_class() #database class\n    mydata=[x for x in db.fetchall()]\n    df=pd.DataFrame(data=mydata)\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "1"}, "answer_12": {"type": "literal", "value": "<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre><code>query = session.query(tbl.Field1, tbl.Field2)\nDataFrame(query.all(), columns=[column['name'] for column in query.column_descriptions])\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": "4"}, "answer_13": {"type": "literal", "value": "<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre><code>df = pd.DataFrame([dict(r) for r in resoverall])\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": ""}, "answer_14": {"type": "literal", "value": "<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre><code>import pandas as pd\nimport mysql.connector\n\n# Setup MySQL connection\ndb = mysql.connector.connect(\n    host=\"&lt;IP&gt;\",              # your host, usually localhost\n    user=\"&lt;USER&gt;\",            # your username\n    password=\"&lt;PASS&gt;\",        # your password\n    database=\"&lt;DATABASE&gt;\"     # name of the data base\n)   \n\n# You must create a Cursor object. It will let you execute all the queries you need\ncur = db.cursor()\n\n# Use all the SQL you like\ncur.execute(\"SELECT * FROM &lt;TABLE&gt;\")\n\n# Put it all to a data frame\nsql_data = pd.DataFrame(cur.fetchall())\nsql_data.columns = cur.column_names\n\n# Close the session\ndb.close()\n\n# Show the data\nprint(sql_data.head())\n</code></pre>\n"}, "answer_14_votes": {"type": "literal", "value": "12"}, "answer_15": {"type": "literal", "value": "<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre><code>import pandas as pd\n\ndf = pd.read_sql(sql, cnxn)\n</code></pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre><code>import pyodbc\nimport pandas.io.sql as psql\n\ncnxn = pyodbc.connect(connection_info) \ncursor = cnxn.cursor()\nsql = \"SELECT * FROM TABLE\"\n\ndf = psql.frame_query(sql, cnxn)\ncnxn.close()\n</code></pre>\n"}, "answer_15_votes": {"type": "literal", "value": "116"}, "content_wo_code": "<p>Any help on this problem will be greatly appreciated.</p>\n\n<p>So basically I want to run a query to my SQL database and store the returned data as Pandas data structure.</p>\n\n<p>I have attached code for query.</p>\n\n<p>I am reading the documentation on Pandas, but I have problem to identify the return type of my query.</p>\n\n<p>I tried to print the query result, but it doesn't give any useful information.</p>\n\n<p>Thanks!!!! </p>\n\n<pre> </pre>\n\n<p>So I sort of want to understand what's the format/datatype of my variable \"resoverall\" and how to put it with PANDAS data structure.</p>\n", "answer_wo_code": "<h3>Edit 2014-09-30:</h3>\n\n<p>pandas now has a   function. You definitely want to use that instead.</p>\n\n<h3>Original answer:</h3>\n\n<p>I can't help you with SQLAlchemy -- I always use pyodbc, MySQLdb, or psychopg2 as needed. But when doing so, a function as simple as the one below tends to suit my needs:</p>\n\n<pre> </pre>\n\n\n<p>Long time from last post but maybe it helps someone...</p>\n\n<p>Shorted way than Paul H:</p>\n\n<pre> </pre>\n\n\n<p>This question is old, but I wanted to add my two-cents. I read the question as \" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].\"</p>\n\n<p>From the code it looks like you mean mysql database and assume you mean pandas DataFrame.</p>\n\n<pre> </pre>\n\n<p>For example,</p>\n\n<pre> </pre>\n\n<p>This will import all rows of testTable into a DataFrame.</p>\n\n\n<p>  is a sqlalchemy ResultProxy object. You can read more about it in the <a href=\"http://docs.sqlalchemy.org/en/rel_0_7/core/connections.html?highlight=execute%20examples#basic-usage\" rel=\"nofollow\">sqlalchemy docs</a>, the latter explains basic usage of working with Engines and Connections. Important here is that   is dict like.</p>\n\n<p>Pandas likes dict like objects to create its data structures, see the <a href=\"http://pandas.pydata.org/pandas-docs/stable/\" rel=\"nofollow\">online docs</a></p>\n\n<p>Good luck with sqlalchemy and pandas.</p>\n\n\n<p>Here's the shortest code that will do the job:</p>\n\n<pre> </pre>\n\n<p>You can go fancier and parse the types as in Paul's answer.</p>\n\n\n<p>If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type   to a Pandas data frame. </p>\n\n<p>The cleanest approach is to get the generated SQL from the query's statement attribute, and then execute it with pandas's   method. E.g., starting with a Query object called  :</p>\n\n<pre> </pre>\n\n\n<p>This is a short and crisp answer to your problem:</p>\n\n<pre> </pre>\n\n\n<p>Here is mine. Just in case if you are using \"pymysql\":</p>\n\n<pre> </pre>\n\n\n<p>Here's the code I use. Hope this helps.</p>\n\n<pre> </pre>\n\n\n<p>Simply use   and   together. You'll have to modify your connection string ( ) according to your database specifications.</p>\n\n<pre> </pre>\n\n<p>I've used   with several enterprise databases (e.g. SQL Server, MySQL, MariaDB, IBM).</p>\n\n\n<p>best way I do this</p>\n\n<pre> </pre>\n\n\n<p>Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:</p>\n\n<pre> </pre>\n\n\n<p>If the result type is <strong>ResultSet</strong>, you should convert it to dictionary first. Then the <strong>DataFrame columns</strong> will be collected automatically.</p>\n\n<p>This works on my case:</p>\n\n<pre> </pre>\n\n\n<h1>MySQL Connector</h1>\n\n<p>For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)</p>\n\n<p>Used refs:</p>\n\n<ul>\n<li><a href=\"https://dev.mysql.com/doc/connector-python/en/connector-python-example-cursor-select.html\" rel=\"noreferrer\">Querying Data Using Connector/Python</a></li>\n<li><a href=\"https://stackoverflow.com/questions/372885/how-do-i-connect-to-a-mysql-database-in-python\">Connecting to MYSQL with Python in 3 steps</a></li>\n</ul>\n\n<hr>\n\n<pre> </pre>\n\n\n<p><strong>Edit: Mar. 2015</strong></p>\n\n<p>As noted below, pandas now uses <a href=\"http://www.sqlalchemy.org/\" rel=\"noreferrer\">SQLAlchemy</a> to both read from (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.read_sql.html\" rel=\"noreferrer\">read_sql</a>) and insert into (<a href=\"http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html\" rel=\"noreferrer\">to_sql</a>) a database. The following should work</p>\n\n<pre> </pre>\n\n<p><strong>Previous answer:</strong>\nVia mikebmassey from a <a href=\"https://stackoverflow.com/a/13570851/386279\">similar question</a></p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL query or database table into a DataFrame.\n\nThis function is a convenience wrapper around ``read_sql_table`` and\n``read_sql_query`` (for backward compatibility). It will delegate\nto the specific function depending on the provided input. A SQL query\nwill be routed to ``read_sql_query``, while a database table name will\nbe routed to ``read_sql_table``. Note that the delegated function might\nhave more specific notes about their functionality not listed here.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/46166179"}, "title": {"type": "literal", "value": "errors trying to read Access Database Tables into Pandas with PYODBC"}, "content": {"type": "literal", "value": "<p>I would like to be performing a simple task of bringing table data from a MS Access database into Pandas in the form of a dataframe. I had this working great recently and now I can not figure out why it is no longer working. I remember when initially troubleshooting the connection there was work that I needed to do around installing a new microsoft database driver with the correct bitness so I have revisited that and gone through a reinstallation of the driver. Below is what I am using for a setup.</p>\n\n<p>Record of install on Laptop:</p>\n\n<ul>\n<li>OS: Windows 7 Professional 64-bit (verified 9/6/2017)</li>\n<li>Access version: Access 2016 32bit (verified 9/6/2017)</li>\n<li>Python version: Python 3.6.1 (64-bit) found using >Python -V (verified 9/11/2017)</li>\n<li>the AccessDatabaseEngine needed will be based on the Python bitness above</li>\n<li>Windows database engine driver installed with AccessDatabaseEngine_X64.exe from 2010 release using >AccessDatabaseEngine_X64.exe /passive (verified 9/11/2017)</li>\n</ul>\n\n<p>I am running the following simple test code to try out the connection to a test database.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n[x for x in pyodbc.drivers() if x.startswith('Microsoft Access Driver')]\n</code></pre>\n\n<p>returns:</p>\n\n<pre><code>['Microsoft Access Driver (*.mdb, *.accdb)']\n</code></pre>\n\n<p>Setting the connection string.</p>\n\n<pre><code>dbpath = r'Z:\\1Users\\myfiles\\software\\JupyterNotebookFiles\\testDB.accdb'\nconn_str = (r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};''DBQ=%s;' %(dbpath))\ncnxn = pyodbc.connect(conn_str)\ncrsr = cnxn.cursor()\n</code></pre>\n\n<p>Verifying that the I am connected to the db...</p>\n\n<pre><code>for table_info in crsr.tables(tableType='TABLE'):\n    print(table_info.table_name)\n</code></pre>\n\n<p>returns:</p>\n\n<pre><code>TestTable1\n</code></pre>\n\n<p>Trying to connect to TestTable1 gives the error below.</p>\n\n<pre><code>dfTable = pd.read_sql_table(TestTable1, cnxn)\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-14-a24de1550834&gt; in &lt;module&gt;()\n----&gt; 1 dfTable = pd.read_sql_table(TestTable1, cnxn)\n      2 #dfQuery = pd.read_sql_query(\"SELECT FROM [TestQuery1]\", cnxn)\n\nNameError: name 'TestTable1' is not defined\n</code></pre>\n\n<p>Trying again with single quotes gives the error below.</p>\n\n<pre><code>dfTable = pd.read_sql_table('TestTable1', cnxn)\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n&lt;ipython-input-15-1f89f9725f0a&gt; in &lt;module&gt;()\n----&gt; 1 dfTable = pd.read_sql_table('TestTable1', cnxn)\n      2 #dfQuery = pd.read_sql_query(\"SELECT FROM [TestQuery1]\", cnxn)\n\nC:\\Users\\myfiles\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py in read_sql_table(table_name, con, schema, index_col, coerce_float, parse_dates, columns, chunksize)\n    250     con = _engine_builder(con)\n    251     if not _is_sqlalchemy_connectable(con):\n--&gt; 252         raise NotImplementedError(\"read_sql_table only supported for \"\n    253                                   \"SQLAlchemy connectable.\")\n    254     import sqlalchemy\n\nNotImplementedError: read_sql_table only supported for SQLAlchemy connectable.\n</code></pre>\n\n<p>I have tried going back to the driver issue and reinstalling a 32bit version without any luck.</p>\n\n<p>Anybody have any ideas?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"><code>pandas.read_sql_table</code></a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"><code>pandas.read_sql</code></a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre><code>dfTable = pd.read_sql(\"SELECT * FROM TestTable1\", cnxn)\n</code></pre>\n\n\n<p>Reading db table with just table_name</p>\n\n<pre><code>import pandas\nfrom sqlalchemy import create_engine\n\nengine=create_engine('postgresql+psycopg2://user:password@localhost/db_name')\ndf=pandas.read_sql_table(\"table_name\",engine)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"><code>pandas.read_sql_table</code></a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"><code>pandas.read_sql</code></a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre><code>dfTable = pd.read_sql(\"SELECT * FROM TestTable1\", cnxn)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "answer_2": {"type": "literal", "value": "<p>Reading db table with just table_name</p>\n\n<pre><code>import pandas\nfrom sqlalchemy import create_engine\n\nengine=create_engine('postgresql+psycopg2://user:password@localhost/db_name')\ndf=pandas.read_sql_table(\"table_name\",engine)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>I would like to be performing a simple task of bringing table data from a MS Access database into Pandas in the form of a dataframe. I had this working great recently and now I can not figure out why it is no longer working. I remember when initially troubleshooting the connection there was work that I needed to do around installing a new microsoft database driver with the correct bitness so I have revisited that and gone through a reinstallation of the driver. Below is what I am using for a setup.</p>\n\n<p>Record of install on Laptop:</p>\n\n<ul>\n<li>OS: Windows 7 Professional 64-bit (verified 9/6/2017)</li>\n<li>Access version: Access 2016 32bit (verified 9/6/2017)</li>\n<li>Python version: Python 3.6.1 (64-bit) found using >Python -V (verified 9/11/2017)</li>\n<li>the AccessDatabaseEngine needed will be based on the Python bitness above</li>\n<li>Windows database engine driver installed with AccessDatabaseEngine_X64.exe from 2010 release using >AccessDatabaseEngine_X64.exe /passive (verified 9/11/2017)</li>\n</ul>\n\n<p>I am running the following simple test code to try out the connection to a test database.</p>\n\n<pre> </pre>\n\n<p>returns:</p>\n\n<pre> </pre>\n\n<p>Setting the connection string.</p>\n\n<pre> </pre>\n\n<p>Verifying that the I am connected to the db...</p>\n\n<pre> </pre>\n\n<p>returns:</p>\n\n<pre> </pre>\n\n<p>Trying to connect to TestTable1 gives the error below.</p>\n\n<pre> </pre>\n\n<p>Trying again with single quotes gives the error below.</p>\n\n<pre> </pre>\n\n<p>I have tried going back to the driver issue and reinstalling a 32bit version without any luck.</p>\n\n<p>Anybody have any ideas?</p>\n", "answer_wo_code": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"> </a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"> </a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre> </pre>\n\n\n<p>Reading db table with just table_name</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_sql_table"}, "class_func_label": {"type": "literal", "value": "pandas.read_sql_table"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead SQL database table into a DataFrame.\n\nGiven a table name and a SQLAlchemy connectable, returns a DataFrame.\nThis function does not support DBAPI connections.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/46166179"}, "title": {"type": "literal", "value": "errors trying to read Access Database Tables into Pandas with PYODBC"}, "content": {"type": "literal", "value": "<p>I would like to be performing a simple task of bringing table data from a MS Access database into Pandas in the form of a dataframe. I had this working great recently and now I can not figure out why it is no longer working. I remember when initially troubleshooting the connection there was work that I needed to do around installing a new microsoft database driver with the correct bitness so I have revisited that and gone through a reinstallation of the driver. Below is what I am using for a setup.</p>\n\n<p>Record of install on Laptop:</p>\n\n<ul>\n<li>OS: Windows 7 Professional 64-bit (verified 9/6/2017)</li>\n<li>Access version: Access 2016 32bit (verified 9/6/2017)</li>\n<li>Python version: Python 3.6.1 (64-bit) found using >Python -V (verified 9/11/2017)</li>\n<li>the AccessDatabaseEngine needed will be based on the Python bitness above</li>\n<li>Windows database engine driver installed with AccessDatabaseEngine_X64.exe from 2010 release using >AccessDatabaseEngine_X64.exe /passive (verified 9/11/2017)</li>\n</ul>\n\n<p>I am running the following simple test code to try out the connection to a test database.</p>\n\n<pre><code>import pyodbc\nimport pandas as pd\n\n[x for x in pyodbc.drivers() if x.startswith('Microsoft Access Driver')]\n</code></pre>\n\n<p>returns:</p>\n\n<pre><code>['Microsoft Access Driver (*.mdb, *.accdb)']\n</code></pre>\n\n<p>Setting the connection string.</p>\n\n<pre><code>dbpath = r'Z:\\1Users\\myfiles\\software\\JupyterNotebookFiles\\testDB.accdb'\nconn_str = (r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};''DBQ=%s;' %(dbpath))\ncnxn = pyodbc.connect(conn_str)\ncrsr = cnxn.cursor()\n</code></pre>\n\n<p>Verifying that the I am connected to the db...</p>\n\n<pre><code>for table_info in crsr.tables(tableType='TABLE'):\n    print(table_info.table_name)\n</code></pre>\n\n<p>returns:</p>\n\n<pre><code>TestTable1\n</code></pre>\n\n<p>Trying to connect to TestTable1 gives the error below.</p>\n\n<pre><code>dfTable = pd.read_sql_table(TestTable1, cnxn)\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-14-a24de1550834&gt; in &lt;module&gt;()\n----&gt; 1 dfTable = pd.read_sql_table(TestTable1, cnxn)\n      2 #dfQuery = pd.read_sql_query(\"SELECT FROM [TestQuery1]\", cnxn)\n\nNameError: name 'TestTable1' is not defined\n</code></pre>\n\n<p>Trying again with single quotes gives the error below.</p>\n\n<pre><code>dfTable = pd.read_sql_table('TestTable1', cnxn)\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n&lt;ipython-input-15-1f89f9725f0a&gt; in &lt;module&gt;()\n----&gt; 1 dfTable = pd.read_sql_table('TestTable1', cnxn)\n      2 #dfQuery = pd.read_sql_query(\"SELECT FROM [TestQuery1]\", cnxn)\n\nC:\\Users\\myfiles\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py in read_sql_table(table_name, con, schema, index_col, coerce_float, parse_dates, columns, chunksize)\n    250     con = _engine_builder(con)\n    251     if not _is_sqlalchemy_connectable(con):\n--&gt; 252         raise NotImplementedError(\"read_sql_table only supported for \"\n    253                                   \"SQLAlchemy connectable.\")\n    254     import sqlalchemy\n\nNotImplementedError: read_sql_table only supported for SQLAlchemy connectable.\n</code></pre>\n\n<p>I have tried going back to the driver issue and reinstalling a 32bit version without any luck.</p>\n\n<p>Anybody have any ideas?</p>\n"}, "answerContent": {"type": "literal", "value": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"><code>pandas.read_sql_table</code></a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"><code>pandas.read_sql</code></a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre><code>dfTable = pd.read_sql(\"SELECT * FROM TestTable1\", cnxn)\n</code></pre>\n\n\n<p>Reading db table with just table_name</p>\n\n<pre><code>import pandas\nfrom sqlalchemy import create_engine\n\nengine=create_engine('postgresql+psycopg2://user:password@localhost/db_name')\ndf=pandas.read_sql_table(\"table_name\",engine)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"><code>pandas.read_sql_table</code></a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"><code>pandas.read_sql</code></a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre><code>dfTable = pd.read_sql(\"SELECT * FROM TestTable1\", cnxn)\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "answer_2": {"type": "literal", "value": "<p>Reading db table with just table_name</p>\n\n<pre><code>import pandas\nfrom sqlalchemy import create_engine\n\nengine=create_engine('postgresql+psycopg2://user:password@localhost/db_name')\ndf=pandas.read_sql_table(\"table_name\",engine)\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "content_wo_code": "<p>I would like to be performing a simple task of bringing table data from a MS Access database into Pandas in the form of a dataframe. I had this working great recently and now I can not figure out why it is no longer working. I remember when initially troubleshooting the connection there was work that I needed to do around installing a new microsoft database driver with the correct bitness so I have revisited that and gone through a reinstallation of the driver. Below is what I am using for a setup.</p>\n\n<p>Record of install on Laptop:</p>\n\n<ul>\n<li>OS: Windows 7 Professional 64-bit (verified 9/6/2017)</li>\n<li>Access version: Access 2016 32bit (verified 9/6/2017)</li>\n<li>Python version: Python 3.6.1 (64-bit) found using >Python -V (verified 9/11/2017)</li>\n<li>the AccessDatabaseEngine needed will be based on the Python bitness above</li>\n<li>Windows database engine driver installed with AccessDatabaseEngine_X64.exe from 2010 release using >AccessDatabaseEngine_X64.exe /passive (verified 9/11/2017)</li>\n</ul>\n\n<p>I am running the following simple test code to try out the connection to a test database.</p>\n\n<pre> </pre>\n\n<p>returns:</p>\n\n<pre> </pre>\n\n<p>Setting the connection string.</p>\n\n<pre> </pre>\n\n<p>Verifying that the I am connected to the db...</p>\n\n<pre> </pre>\n\n<p>returns:</p>\n\n<pre> </pre>\n\n<p>Trying to connect to TestTable1 gives the error below.</p>\n\n<pre> </pre>\n\n<p>Trying again with single quotes gives the error below.</p>\n\n<pre> </pre>\n\n<p>I have tried going back to the driver issue and reinstalling a 32bit version without any luck.</p>\n\n<p>Anybody have any ideas?</p>\n", "answer_wo_code": "<p>Per the docs of <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html\" rel=\"nofollow noreferrer\"> </a>:</p>\n\n<blockquote>\n  <p>Given a table name and an SQLAlchemy connectable, returns a DataFrame.\n  This function does not support DBAPI connections.</p>\n</blockquote>\n\n<p>Since pyodbc is a DBAPI, use the query method, <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html#pandas.read_sql\" rel=\"nofollow noreferrer\"> </a> which the <em>con</em> argument does support DBAPI:</p>\n\n<pre> </pre>\n\n\n<p>Reading db table with just table_name</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/Nor"}, "class_func_label": {"type": "literal", "value": "Nor"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Module"}, "docstr": {"type": "literal", "value": "\n    Logical NOR function.\n\n    It evaluates its arguments in order, giving False immediately if any\n    of them are True, and True if they are all False.\n\n    Returns False if any argument is True\n    Returns True if all arguments are False\n\n    Examples\n    ========\n\n    >>> from sympy.logic.boolalg import Nor\n    >>> from sympy import symbols\n    >>> x, y = symbols('x y')\n\n    >>> Nor(True, False)\n    False\n    >>> Nor(True, True)\n    False\n    >>> Nor(False, True)\n    False\n    >>> Nor(False, False)\n    True\n    >>> Nor(x, y)\n    ~(x | y)\n\n    "}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/33181846"}, "title": {"type": "literal", "value": "Programmatically convert pandas dataframe to markdown table"}, "content": {"type": "literal", "value": "<p>I have a Pandas Dataframe generated from a database, which has data with mixed encodings. For example:</p>\n\n<pre><code>+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| ID | path                    | language | date       | longest_sentence                               | shortest_sentence                                      | number_words | readability_consensus |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| 0  | data/Eng/Sagitarius.txt | Eng      | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...  | 306          | 11th and 12th grade   |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| 31 | data/Nor/H\u00f8ylandet.txt  | Nor      | 2015-07-22 | H\u00f8gskolen i \u00d8stfold er et eksempel...          | Som skuespiller har jeg b\u00e5de...                        | 253          | 15th and 16th grade   |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n</code></pre>\n\n<p>As seen there is a mix of English and Norwegian (encoded as ISO-8859-1 in the database I think). I need to get the contents of this Dataframe output as a Markdown table, but without getting problems with encoding. I followed <a href=\"https://stackoverflow.com/a/15445930/603387\">this answer</a> (from the question <a href=\"https://stackoverflow.com/questions/13394140/generate-markdown-tables\">Generate Markdown tables?</a>) and got the following:</p>\n\n<pre><code>import sys, sqlite3\n\ndb = sqlite3.connect(\"Applications.db\")\ndf = pd.read_sql_query(\"SELECT path, language, date, longest_sentence, shortest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC\", db)\ndb.close()\n\nrows = []\nfor index, row in df.iterrows():\n    items = (row['date'], \n             row['path'], \n             row['language'], \n             row['shortest_sentence'],\n             row['longest_sentence'], \n             row['number_words'], \n             row['readability_consensus'])\n    rows.append(items)\n\nheadings = ['Date', \n            'Path', \n            'Language',\n            'Shortest Sentence', \n            'Longest Sentence since', \n            'Words',\n            'Grade level']\n\nfields = [0, 1, 2, 3, 4, 5, 6]\nalign = [('^', '&lt;'), ('^', '^'), ('^', '&lt;'), ('^', '^'), ('^', '&gt;'),\n         ('^','^'), ('^','^')]\n\ntable(sys.stdout, rows, fields, headings, align)\n</code></pre>\n\n<p>However, this yields an <code>UnicodeEncodeError: 'ascii' codec can't encode character u'\\xe5' in position 72: ordinal not in range(128)</code> error. How can I output the Dataframe as a Markdown table? That is, for the purpose of storing this code in a file for use in writing a Markdown document. I need the output to look like this:</p>\n\n<pre><code>| ID | path                    | language | date       | longest_sentence                               | shortest_sentence                                      | number_words | readability_consensus |\n|----|-------------------------|----------|------------|------------------------------------------------|--------------------------------------------------------|--------------|-----------------------|\n| 0  | data/Eng/Sagitarius.txt | Eng      | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...  | 306          | 11th and 12th grade   |\n| 31 | data/Nor/H\u00f8ylandet.txt  | Nor      | 2015-07-22 | H\u00f8gskolen i \u00d8stfold er et eksempel...          | Som skuespiller har jeg b\u00e5de...                        | 253          | 15th and 16th grade   |\n</code></pre>\n"}, "answerContent": {"type": "literal", "value": "<p>Right, so I've taken a leaf from a question suggested by <a href=\"https://stackoverflow.com/users/1625098/rohit\">Rohit</a> (<a href=\"https://stackoverflow.com/questions/7315629/python-encoding-string-swedish-letters\">Python - Encoding string - Swedish Letters</a>), extended <a href=\"https://stackoverflow.com/a/33187706/603387\">his answer</a>, and came up with the following:</p>\n\n<pre><code># Enforce UTF-8 encoding\nimport sys\nstdin, stdout = sys.stdin, sys.stdout\nreload(sys)\nsys.stdin, sys.stdout = stdin, stdout\nsys.setdefaultencoding('UTF-8')\n\n# SQLite3 database\nimport sqlite3\n# Pandas: Data structures and data analysis tools\nimport pandas as pd\n\n# Read database, attach as Pandas dataframe\ndb = sqlite3.connect(\"Applications.db\")\ndf = pd.read_sql_query(\"SELECT path, language, date, shortest_sentence, longest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC\", db)\ndb.close()\ndf.columns = ['Path', 'Language', 'Date', 'Shortest Sentence', 'Longest Sentence', 'Words', 'Readability Consensus']\n\n# Parse Dataframe and apply Markdown, then save as 'table.md'\ncols = df.columns\ndf2 = pd.DataFrame([['---','---','---','---','---','---','---']], columns=cols)\ndf3 = pd.concat([df2, df])\ndf3.to_csv(\"table.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p>An important precursor to this is that the <code>shortest_sentence</code> and <code>longest_sentence</code> columns do not contain unnecessary line breaks, as removed by applying <code>.replace('\\n', ' ').replace('\\r', '')</code> to them before submitting into the SQLite database. It appears that the solution is not to enforce the language-specific encoding (<code>ISO-8859-1</code> for Norwegian), but rather that <code>UTF-8</code> is used instead of the default <code>ASCII</code>.</p>\n\n<p>I ran this through my IPython notebook (Python 2.7.10) and got a table like the following (fixed spacing for appearance here):</p>\n\n<pre><code>| Path                    | Language | Date       | Shortest Sentence                                                                            | Longest Sentence                                                                                                                                                                                                                                         | Words | Readability Consensus |\n|-------------------------|----------|------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-----------------------|\n| data/Eng/Something1.txt | Eng      | 2015-09-17 | I am able to relocate to London on short notice.                                             | With my administrative experience in the preparation of the structure and content of seminars in various courses, and critiquing academic papers on various levels, I am confident that I can execute the work required as an editorial assistant.       | 306   | 11th and 12th grade   |\n| data/Nor/NoeNorr\u00f8nt.txt | Nor      | 2015-09-17 | Jeg har grundig kjennskap til Microsoft Office og Adobe.                                     | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 205   | 18th and 19th grade   |\n| data/Nor/\u00d8rret.txt.txt  | Nor      | 2015-09-17 | Jeg h\u00e5per p\u00e5 positiv tilbakemelding, og m\u00f8ter naturligvis til intervju hvis det er \u00f8nskelig. | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 160   | 18th and 19th grade   |\n</code></pre>\n\n<p>Thus, a Markdown table without problems with encoding.</p>\n\n\n<p>Right, so I've taken a leaf from a question suggested by <a href=\"https://stackoverflow.com/users/1625098/rohit\">Rohit</a> (<a href=\"https://stackoverflow.com/questions/7315629/python-encoding-string-swedish-letters\">Python - Encoding string - Swedish Letters</a>), extended <a href=\"https://stackoverflow.com/a/33187706/603387\">his answer</a>, and came up with the following:</p>\n\n<pre><code># Enforce UTF-8 encoding\nimport sys\nstdin, stdout = sys.stdin, sys.stdout\nreload(sys)\nsys.stdin, sys.stdout = stdin, stdout\nsys.setdefaultencoding('UTF-8')\n\n# SQLite3 database\nimport sqlite3\n# Pandas: Data structures and data analysis tools\nimport pandas as pd\n\n# Read database, attach as Pandas dataframe\ndb = sqlite3.connect(\"Applications.db\")\ndf = pd.read_sql_query(\"SELECT path, language, date, shortest_sentence, longest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC\", db)\ndb.close()\ndf.columns = ['Path', 'Language', 'Date', 'Shortest Sentence', 'Longest Sentence', 'Words', 'Readability Consensus']\n\n# Parse Dataframe and apply Markdown, then save as 'table.md'\ncols = df.columns\ndf2 = pd.DataFrame([['---','---','---','---','---','---','---']], columns=cols)\ndf3 = pd.concat([df2, df])\ndf3.to_csv(\"table.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p>An important precursor to this is that the <code>shortest_sentence</code> and <code>longest_sentence</code> columns do not contain unnecessary line breaks, as removed by applying <code>.replace('\\n', ' ').replace('\\r', '')</code> to them before submitting into the SQLite database. It appears that the solution is not to enforce the language-specific encoding (<code>ISO-8859-1</code> for Norwegian), but rather that <code>UTF-8</code> is used instead of the default <code>ASCII</code>.</p>\n\n<p>I ran this through my IPython notebook (Python 2.7.10) and got a table like the following (fixed spacing for appearance here):</p>\n\n<pre><code>| Path                    | Language | Date       | Shortest Sentence                                                                            | Longest Sentence                                                                                                                                                                                                                                         | Words | Readability Consensus |\n|-------------------------|----------|------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-----------------------|\n| data/Eng/Something1.txt | Eng      | 2015-09-17 | I am able to relocate to London on short notice.                                             | With my administrative experience in the preparation of the structure and content of seminars in various courses, and critiquing academic papers on various levels, I am confident that I can execute the work required as an editorial assistant.       | 306   | 11th and 12th grade   |\n| data/Nor/NoeNorr\u00f8nt.txt | Nor      | 2015-09-17 | Jeg har grundig kjennskap til Microsoft Office og Adobe.                                     | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 205   | 18th and 19th grade   |\n| data/Nor/\u00d8rret.txt.txt  | Nor      | 2015-09-17 | Jeg h\u00e5per p\u00e5 positiv tilbakemelding, og m\u00f8ter naturligvis til intervju hvis det er \u00f8nskelig. | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 160   | 18th and 19th grade   |\n</code></pre>\n\n<p>Thus, a Markdown table without problems with encoding.</p>\n\n\n<p>sqlite3 returns Unicodes by default for TEXT fields. Everything was set up to work before you introduced the <code>table()</code> function from an external source (that you did not provide in your question).</p>\n\n<p>The <code>table()</code> function has <code>str()</code> calls which do not provide an encoding, so ASCII is used to protect you. </p>\n\n<p>You need to re-write <code>table()</code> not to do this, especially as you've got Unicode objects. You may have some success by simply replacing <code>str()</code> with <code>unicode()</code></p>\n\n\n<p>sqlite3 returns Unicodes by default for TEXT fields. Everything was set up to work before you introduced the <code>table()</code> function from an external source (that you did not provide in your question).</p>\n\n<p>The <code>table()</code> function has <code>str()</code> calls which do not provide an encoding, so ASCII is used to protect you. </p>\n\n<p>You need to re-write <code>table()</code> not to do this, especially as you've got Unicode objects. You may have some success by simply replacing <code>str()</code> with <code>unicode()</code></p>\n\n\n<h1>Export a DataFrame to markdown</h1>\n\n<p>I created the following function for exporting a pandas.DataFrame to markdown in Python:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>def df_to_markdown(df, float_format='%.2g'):\n    \"\"\"\n    Export a pandas.DataFrame to markdown-formatted text.\n    DataFrame should not contain any `|` characters.\n    \"\"\"\n    from os import linesep\n    return linesep.join([\n        '|'.join(df.columns),\n        '|'.join(4 * '-' for i in df.columns),\n        df.to_csv(sep='|', index=False, header=False, float_format=float_format)\n    ]).replace('|', ' | ')\n</code></pre>\n\n<p>This function may not automatically fix the encoding issues of the OP, but that is a different issue than converting from pandas to markdown.</p>\n\n\n<h1>Export a DataFrame to markdown</h1>\n\n<p>I created the following function for exporting a pandas.DataFrame to markdown in Python:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>def df_to_markdown(df, float_format='%.2g'):\n    \"\"\"\n    Export a pandas.DataFrame to markdown-formatted text.\n    DataFrame should not contain any `|` characters.\n    \"\"\"\n    from os import linesep\n    return linesep.join([\n        '|'.join(df.columns),\n        '|'.join(4 * '-' for i in df.columns),\n        df.to_csv(sep='|', index=False, header=False, float_format=float_format)\n    ]).replace('|', ' | ')\n</code></pre>\n\n<p>This function may not automatically fix the encoding issues of the OP, but that is a different issue than converting from pandas to markdown.</p>\n\n\n<p>I recommend <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"nofollow noreferrer\" title=\"python-tabulate\">python-tabulate</a> library for generating ascii-tables. The library supports <code>pandas.DataFrame</code> as well.</p>\n\n<p>Here is how to use it:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pandas import DataFrame\nfrom tabulate import tabulate\n\ndf = DataFrame({\n    \"weekday\": [\"monday\", \"thursday\", \"wednesday\"],\n    \"temperature\": [20, 30, 25],\n    \"precipitation\": [100, 200, 150],\n}).set_index(\"weekday\")\n\nprint(tabulate(df, tablefmt=\"pipe\", headers=\"keys\"))\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>| weekday   |   temperature |   precipitation |\n|:----------|--------------:|----------------:|\n| monday    |            20 |             100 |\n| thursday  |            30 |             200 |\n| wednesday |            25 |             150 |\n</code></pre>\n\n\n<p>I recommend <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"nofollow noreferrer\" title=\"python-tabulate\">python-tabulate</a> library for generating ascii-tables. The library supports <code>pandas.DataFrame</code> as well.</p>\n\n<p>Here is how to use it:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pandas import DataFrame\nfrom tabulate import tabulate\n\ndf = DataFrame({\n    \"weekday\": [\"monday\", \"thursday\", \"wednesday\"],\n    \"temperature\": [20, 30, 25],\n    \"precipitation\": [100, 200, 150],\n}).set_index(\"weekday\")\n\nprint(tabulate(df, tablefmt=\"pipe\", headers=\"keys\"))\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>| weekday   |   temperature |   precipitation |\n|:----------|--------------:|----------------:|\n| monday    |            20 |             100 |\n| thursday  |            30 |             200 |\n| wednesday |            25 |             150 |\n</code></pre>\n\n\n<p>Yet another solution. This time via thin wrapper around tabulate: <a href=\"https://github.com/kiwi0fruit/tabulatehelper\" rel=\"nofollow noreferrer\">tabulatehelper</a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\nimport tabulatehelper as th\n\ndf = pd.DataFrame(np.random.random(16).reshape(4, 4), columns=('a', 'b', 'c', 'd'))\nprint(th.md_table(df, formats={-1: 'c'}))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>|        a |        b |        c |        d |\n|---------:|---------:|---------:|:--------:|\n| 0.413284 | 0.932373 | 0.277797 | 0.646333 |\n| 0.552731 | 0.381826 | 0.141727 | 0.2483   |\n| 0.779889 | 0.012458 | 0.308352 | 0.650859 |\n| 0.301109 | 0.982111 | 0.994024 | 0.43551  |\n</code></pre>\n\n\n<p>Yet another solution. This time via thin wrapper around tabulate: <a href=\"https://github.com/kiwi0fruit/tabulatehelper\" rel=\"nofollow noreferrer\">tabulatehelper</a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\nimport tabulatehelper as th\n\ndf = pd.DataFrame(np.random.random(16).reshape(4, 4), columns=('a', 'b', 'c', 'd'))\nprint(th.md_table(df, formats={-1: 'c'}))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>|        a |        b |        c |        d |\n|---------:|---------:|---------:|:--------:|\n| 0.413284 | 0.932373 | 0.277797 | 0.646333 |\n| 0.552731 | 0.381826 | 0.141727 | 0.2483   |\n| 0.779889 | 0.012458 | 0.308352 | 0.650859 |\n| 0.301109 | 0.982111 | 0.994024 | 0.43551  |\n</code></pre>\n\n\n<p>Try this out. I got it to work. </p>\n\n<p>See the screenshot of my markdown file converted to HTML at the end of this answer.      </p>\n\n<pre><code>import pandas as pd\n\n# You don't need these two lines\n# as you already have your DataFrame in memory\ndf = pd.read_csv(\"nor.txt\", sep=\"|\")\ndf.drop(df.columns[-1], axis=1)\n\n# Get column names\ncols = df.columns\n\n# Create a new DataFrame with just the markdown\n# strings\ndf2 = pd.DataFrame([['---',]*len(cols)], columns=cols)\n\n#Create a new concatenated DataFrame\ndf3 = pd.concat([df2, df])\n\n#Save as markdown\ndf3.to_csv(\"nor.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/j1E1K.jpg\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/j1E1K.jpg\" alt=\"My output in HTML format by converting HTML to Markdown\"></a></p>\n\n\n<p>Try this out. I got it to work. </p>\n\n<p>See the screenshot of my markdown file converted to HTML at the end of this answer.      </p>\n\n<pre><code>import pandas as pd\n\n# You don't need these two lines\n# as you already have your DataFrame in memory\ndf = pd.read_csv(\"nor.txt\", sep=\"|\")\ndf.drop(df.columns[-1], axis=1)\n\n# Get column names\ncols = df.columns\n\n# Create a new DataFrame with just the markdown\n# strings\ndf2 = pd.DataFrame([['---',]*len(cols)], columns=cols)\n\n#Create a new concatenated DataFrame\ndf3 = pd.concat([df2, df])\n\n#Save as markdown\ndf3.to_csv(\"nor.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/j1E1K.jpg\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/j1E1K.jpg\" alt=\"My output in HTML format by converting HTML to Markdown\"></a></p>\n\n\n<p>Using external tool <code>pandoc</code> and pipe:</p>\n\n<pre><code>def to_markdown(df):\n    from subprocess import Popen, PIPE\n    s = df.to_latex()\n    p = Popen('pandoc -f latex -t markdown',\n              stdin=PIPE, stdout=PIPE, shell=True)\n    stdoutdata, _ = p.communicate(input=s.encode(\"utf-8\"))\n    return stdoutdata.decode(\"utf-8\")\n</code></pre>\n\n\n<p>Using external tool <code>pandoc</code> and pipe:</p>\n\n<pre><code>def to_markdown(df):\n    from subprocess import Popen, PIPE\n    s = df.to_latex()\n    p = Popen('pandoc -f latex -t markdown',\n              stdin=PIPE, stdout=PIPE, shell=True)\n    stdoutdata, _ = p.communicate(input=s.encode(\"utf-8\"))\n    return stdoutdata.decode(\"utf-8\")\n</code></pre>\n\n\n<p>I have tried several of the above solutions in this post and found this worked most consistently.  </p>\n\n<p>To convert a pandas data frame to a markdown table I suggest using <a href=\"http://pytablewriter.readthedocs.io/en/latest/\" rel=\"noreferrer\">pytablewriter</a>.\nUsing the data provided in this post:</p>\n\n<pre><code>import pandas as pd\nimport pytablewriter\nfrom StringIO import StringIO\n\nc = StringIO(\"\"\"ID, path,language, date,longest_sentence, shortest_sentence, number_words , readability_consensus \n0, data/Eng/Sagitarius.txt , Eng, 2015-09-17 , With administrative experience in the prepa... , I am able to relocate internationally on short not..., 306, 11th and 12th grade\n31 , data/Nor/H\u00f8ylandet.txt  , Nor, 2015-07-22 , H\u00f8gskolen i \u00d8stfold er et eksempel..., Som skuespiller har jeg b\u00e5de..., 253, 15th and 16th grade\n\"\"\")\ndf = pd.read_csv(c,sep=',',index_col=['ID'])\n\nwriter = pytablewriter.MarkdownTableWriter()\nwriter.table_name = \"example_table\"\nwriter.header_list = list(df.columns.values)\nwriter.value_matrix = df.values.tolist()\nwriter.write_table()\n</code></pre>\n\n<p>This results in:</p>\n\n<pre><code># example_table\nID |           path           |language|    date    |                longest_sentence                |                   shortest_sentence                  | number_words | readability_consensus \n--:|--------------------------|--------|------------|------------------------------------------------|------------------------------------------------------|-------------:|-----------------------\n  0| data/Eng/Sagitarius.txt  | Eng    | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...|           306| 11th and 12th grade   \n 31| data/Nor/H\u00f8ylandet.txt  | Nor    | 2015-07-22 | H\u00f8gskolen i \u00d8stfold er et eksempel...        | Som skuespiller har jeg b\u00e5de...                      |           253| 15th and 16th grade   \n</code></pre>\n\n<p>Here is a markdown rendered screenshot.</p>\n\n<p><a href=\"https://i.stack.imgur.com/Huojg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Huojg.png\" alt=\"enter image description here\"></a></p>\n\n\n<p>I have tried several of the above solutions in this post and found this worked most consistently.  </p>\n\n<p>To convert a pandas data frame to a markdown table I suggest using <a href=\"http://pytablewriter.readthedocs.io/en/latest/\" rel=\"noreferrer\">pytablewriter</a>.\nUsing the data provided in this post:</p>\n\n<pre><code>import pandas as pd\nimport pytablewriter\nfrom StringIO import StringIO\n\nc = StringIO(\"\"\"ID, path,language, date,longest_sentence, shortest_sentence, number_words , readability_consensus \n0, data/Eng/Sagitarius.txt , Eng, 2015-09-17 , With administrative experience in the prepa... , I am able to relocate internationally on short not..., 306, 11th and 12th grade\n31 , data/Nor/H\u00f8ylandet.txt  , Nor, 2015-07-22 , H\u00f8gskolen i \u00d8stfold er et eksempel..., Som skuespiller har jeg b\u00e5de..., 253, 15th and 16th grade\n\"\"\")\ndf = pd.read_csv(c,sep=',',index_col=['ID'])\n\nwriter = pytablewriter.MarkdownTableWriter()\nwriter.table_name = \"example_table\"\nwriter.header_list = list(df.columns.values)\nwriter.value_matrix = df.values.tolist()\nwriter.write_table()\n</code></pre>\n\n<p>This results in:</p>\n\n<pre><code># example_table\nID |           path           |language|    date    |                longest_sentence                |                   shortest_sentence                  | number_words | readability_consensus \n--:|--------------------------|--------|------------|------------------------------------------------|------------------------------------------------------|-------------:|-----------------------\n  0| data/Eng/Sagitarius.txt  | Eng    | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...|           306| 11th and 12th grade   \n 31| data/Nor/H\u00f8ylandet.txt  | Nor    | 2015-07-22 | H\u00f8gskolen i \u00d8stfold er et eksempel...        | Som skuespiller har jeg b\u00e5de...                      |           253| 15th and 16th grade   \n</code></pre>\n\n<p>Here is a markdown rendered screenshot.</p>\n\n<p><a href=\"https://i.stack.imgur.com/Huojg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Huojg.png\" alt=\"enter image description here\"></a></p>\n\n\n<p>For those looking for how to do this using <code>tabulate</code>, I thought I'd put this here to save you some time:</p>\n\n<p><code>print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", showindex=False))\n</code></p>\n\n\n<p>For those looking for how to do this using <code>tabulate</code>, I thought I'd put this here to save you some time:</p>\n\n<p><code>print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", showindex=False))\n</code></p>\n\n\n<p>Improving the answer further, for use in IPython Notebook:</p>\n\n<pre><code>def pandas_df_to_markdown_table(df):\n    from IPython.display import Markdown, display\n    fmt = ['---' for i in range(len(df.columns))]\n    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n    df_formatted = pd.concat([df_fmt, df])\n    display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n\npandas_df_to_markdown_table(infodf)\n</code></pre>\n\n<p>Or use <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"noreferrer\">tabulate</a>: </p>\n\n<pre><code>pip install tabulate\n</code></pre>\n\n<p>Examples of use are in the documentation.</p>\n\n\n<p>Improving the answer further, for use in IPython Notebook:</p>\n\n<pre><code>def pandas_df_to_markdown_table(df):\n    from IPython.display import Markdown, display\n    fmt = ['---' for i in range(len(df.columns))]\n    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n    df_formatted = pd.concat([df_fmt, df])\n    display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n\npandas_df_to_markdown_table(infodf)\n</code></pre>\n\n<p>Or use <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"noreferrer\">tabulate</a>: </p>\n\n<pre><code>pip install tabulate\n</code></pre>\n\n<p>Examples of use are in the documentation.</p>\n\n\n<p>Here's an example function using <code>pytablewriter</code> and some regular expressions to make the markdown table more similar to how a dataframe looks on Jupyter (with the row headers bold).</p>\n\n<pre><code>import io\nimport re\nimport pandas as pd\nimport pytablewriter\n\ndef df_to_markdown(df):\n    \"\"\"\n    Converts Pandas DataFrame to markdown table,\n    making the index bold (as in Jupyter) unless it's a\n    pd.RangeIndex, in which case the index is completely dropped.\n    Returns a string containing markdown table.\n    \"\"\"\n    isRangeIndex = isinstance(df.index, pd.RangeIndex)\n    if not isRangeIndex:\n        df = df.reset_index()\n    writer = pytablewriter.MarkdownTableWriter()\n    writer.stream = io.StringIO()\n    writer.header_list = df.columns\n    writer.value_matrix = df.values\n    writer.write_table()\n    writer.stream.seek(0)\n    table = writer.stream.readlines()\n\n    if isRangeIndex:\n        return ''.join(table)\n    else:\n        # Make the indexes bold\n        new_table = table[:2]\n        for line in table[2:]:\n            new_table.append(re.sub('^(.*?)\\|', r'**\\1**|', line))    \n\n        return ''.join(new_table)\n</code></pre>\n\n\n<p>Here's an example function using <code>pytablewriter</code> and some regular expressions to make the markdown table more similar to how a dataframe looks on Jupyter (with the row headers bold).</p>\n\n<pre><code>import io\nimport re\nimport pandas as pd\nimport pytablewriter\n\ndef df_to_markdown(df):\n    \"\"\"\n    Converts Pandas DataFrame to markdown table,\n    making the index bold (as in Jupyter) unless it's a\n    pd.RangeIndex, in which case the index is completely dropped.\n    Returns a string containing markdown table.\n    \"\"\"\n    isRangeIndex = isinstance(df.index, pd.RangeIndex)\n    if not isRangeIndex:\n        df = df.reset_index()\n    writer = pytablewriter.MarkdownTableWriter()\n    writer.stream = io.StringIO()\n    writer.header_list = df.columns\n    writer.value_matrix = df.values\n    writer.write_table()\n    writer.stream.seek(0)\n    table = writer.stream.readlines()\n\n    if isRangeIndex:\n        return ''.join(table)\n    else:\n        # Make the indexes bold\n        new_table = table[:2]\n        for line in table[2:]:\n            new_table.append(re.sub('^(.*?)\\|', r'**\\1**|', line))    \n\n        return ''.join(new_table)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>Right, so I've taken a leaf from a question suggested by <a href=\"https://stackoverflow.com/users/1625098/rohit\">Rohit</a> (<a href=\"https://stackoverflow.com/questions/7315629/python-encoding-string-swedish-letters\">Python - Encoding string - Swedish Letters</a>), extended <a href=\"https://stackoverflow.com/a/33187706/603387\">his answer</a>, and came up with the following:</p>\n\n<pre><code># Enforce UTF-8 encoding\nimport sys\nstdin, stdout = sys.stdin, sys.stdout\nreload(sys)\nsys.stdin, sys.stdout = stdin, stdout\nsys.setdefaultencoding('UTF-8')\n\n# SQLite3 database\nimport sqlite3\n# Pandas: Data structures and data analysis tools\nimport pandas as pd\n\n# Read database, attach as Pandas dataframe\ndb = sqlite3.connect(\"Applications.db\")\ndf = pd.read_sql_query(\"SELECT path, language, date, shortest_sentence, longest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC\", db)\ndb.close()\ndf.columns = ['Path', 'Language', 'Date', 'Shortest Sentence', 'Longest Sentence', 'Words', 'Readability Consensus']\n\n# Parse Dataframe and apply Markdown, then save as 'table.md'\ncols = df.columns\ndf2 = pd.DataFrame([['---','---','---','---','---','---','---']], columns=cols)\ndf3 = pd.concat([df2, df])\ndf3.to_csv(\"table.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p>An important precursor to this is that the <code>shortest_sentence</code> and <code>longest_sentence</code> columns do not contain unnecessary line breaks, as removed by applying <code>.replace('\\n', ' ').replace('\\r', '')</code> to them before submitting into the SQLite database. It appears that the solution is not to enforce the language-specific encoding (<code>ISO-8859-1</code> for Norwegian), but rather that <code>UTF-8</code> is used instead of the default <code>ASCII</code>.</p>\n\n<p>I ran this through my IPython notebook (Python 2.7.10) and got a table like the following (fixed spacing for appearance here):</p>\n\n<pre><code>| Path                    | Language | Date       | Shortest Sentence                                                                            | Longest Sentence                                                                                                                                                                                                                                         | Words | Readability Consensus |\n|-------------------------|----------|------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-----------------------|\n| data/Eng/Something1.txt | Eng      | 2015-09-17 | I am able to relocate to London on short notice.                                             | With my administrative experience in the preparation of the structure and content of seminars in various courses, and critiquing academic papers on various levels, I am confident that I can execute the work required as an editorial assistant.       | 306   | 11th and 12th grade   |\n| data/Nor/NoeNorr\u00f8nt.txt | Nor      | 2015-09-17 | Jeg har grundig kjennskap til Microsoft Office og Adobe.                                     | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 205   | 18th and 19th grade   |\n| data/Nor/\u00d8rret.txt.txt  | Nor      | 2015-09-17 | Jeg h\u00e5per p\u00e5 positiv tilbakemelding, og m\u00f8ter naturligvis til intervju hvis det er \u00f8nskelig. | I l\u00f8pet av studiene har jeg v\u00e6rt salgsmedarbeider for et st\u00f8rre konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Tr\u00f8nderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 160   | 18th and 19th grade   |\n</code></pre>\n\n<p>Thus, a Markdown table without problems with encoding.</p>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "answer_2": {"type": "literal", "value": "<p>sqlite3 returns Unicodes by default for TEXT fields. Everything was set up to work before you introduced the <code>table()</code> function from an external source (that you did not provide in your question).</p>\n\n<p>The <code>table()</code> function has <code>str()</code> calls which do not provide an encoding, so ASCII is used to protect you. </p>\n\n<p>You need to re-write <code>table()</code> not to do this, especially as you've got Unicode objects. You may have some success by simply replacing <code>str()</code> with <code>unicode()</code></p>\n"}, "answer_2_votes": {"type": "literal", "value": ""}, "answer_3": {"type": "literal", "value": "<h1>Export a DataFrame to markdown</h1>\n\n<p>I created the following function for exporting a pandas.DataFrame to markdown in Python:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>def df_to_markdown(df, float_format='%.2g'):\n    \"\"\"\n    Export a pandas.DataFrame to markdown-formatted text.\n    DataFrame should not contain any `|` characters.\n    \"\"\"\n    from os import linesep\n    return linesep.join([\n        '|'.join(df.columns),\n        '|'.join(4 * '-' for i in df.columns),\n        df.to_csv(sep='|', index=False, header=False, float_format=float_format)\n    ]).replace('|', ' | ')\n</code></pre>\n\n<p>This function may not automatically fix the encoding issues of the OP, but that is a different issue than converting from pandas to markdown.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "4"}, "answer_4": {"type": "literal", "value": "<p>I recommend <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"nofollow noreferrer\" title=\"python-tabulate\">python-tabulate</a> library for generating ascii-tables. The library supports <code>pandas.DataFrame</code> as well.</p>\n\n<p>Here is how to use it:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pandas import DataFrame\nfrom tabulate import tabulate\n\ndf = DataFrame({\n    \"weekday\": [\"monday\", \"thursday\", \"wednesday\"],\n    \"temperature\": [20, 30, 25],\n    \"precipitation\": [100, 200, 150],\n}).set_index(\"weekday\")\n\nprint(tabulate(df, tablefmt=\"pipe\", headers=\"keys\"))\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>| weekday   |   temperature |   precipitation |\n|:----------|--------------:|----------------:|\n| monday    |            20 |             100 |\n| thursday  |            30 |             200 |\n| wednesday |            25 |             150 |\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "18"}, "answer_5": {"type": "literal", "value": "<p>Yet another solution. This time via thin wrapper around tabulate: <a href=\"https://github.com/kiwi0fruit/tabulatehelper\" rel=\"nofollow noreferrer\">tabulatehelper</a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport pandas as pd\nimport tabulatehelper as th\n\ndf = pd.DataFrame(np.random.random(16).reshape(4, 4), columns=('a', 'b', 'c', 'd'))\nprint(th.md_table(df, formats={-1: 'c'}))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>|        a |        b |        c |        d |\n|---------:|---------:|---------:|:--------:|\n| 0.413284 | 0.932373 | 0.277797 | 0.646333 |\n| 0.552731 | 0.381826 | 0.141727 | 0.2483   |\n| 0.779889 | 0.012458 | 0.308352 | 0.650859 |\n| 0.301109 | 0.982111 | 0.994024 | 0.43551  |\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "1"}, "answer_6": {"type": "literal", "value": "<p>Try this out. I got it to work. </p>\n\n<p>See the screenshot of my markdown file converted to HTML at the end of this answer.      </p>\n\n<pre><code>import pandas as pd\n\n# You don't need these two lines\n# as you already have your DataFrame in memory\ndf = pd.read_csv(\"nor.txt\", sep=\"|\")\ndf.drop(df.columns[-1], axis=1)\n\n# Get column names\ncols = df.columns\n\n# Create a new DataFrame with just the markdown\n# strings\ndf2 = pd.DataFrame([['---',]*len(cols)], columns=cols)\n\n#Create a new concatenated DataFrame\ndf3 = pd.concat([df2, df])\n\n#Save as markdown\ndf3.to_csv(\"nor.md\", sep=\"|\", index=False)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/j1E1K.jpg\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/j1E1K.jpg\" alt=\"My output in HTML format by converting HTML to Markdown\"></a></p>\n"}, "answer_6_votes": {"type": "literal", "value": "8"}, "answer_7": {"type": "literal", "value": "<p>Using external tool <code>pandoc</code> and pipe:</p>\n\n<pre><code>def to_markdown(df):\n    from subprocess import Popen, PIPE\n    s = df.to_latex()\n    p = Popen('pandoc -f latex -t markdown',\n              stdin=PIPE, stdout=PIPE, shell=True)\n    stdoutdata, _ = p.communicate(input=s.encode(\"utf-8\"))\n    return stdoutdata.decode(\"utf-8\")\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "1"}, "answer_8": {"type": "literal", "value": "<p>I have tried several of the above solutions in this post and found this worked most consistently.  </p>\n\n<p>To convert a pandas data frame to a markdown table I suggest using <a href=\"http://pytablewriter.readthedocs.io/en/latest/\" rel=\"noreferrer\">pytablewriter</a>.\nUsing the data provided in this post:</p>\n\n<pre><code>import pandas as pd\nimport pytablewriter\nfrom StringIO import StringIO\n\nc = StringIO(\"\"\"ID, path,language, date,longest_sentence, shortest_sentence, number_words , readability_consensus \n0, data/Eng/Sagitarius.txt , Eng, 2015-09-17 , With administrative experience in the prepa... , I am able to relocate internationally on short not..., 306, 11th and 12th grade\n31 , data/Nor/H\u00f8ylandet.txt  , Nor, 2015-07-22 , H\u00f8gskolen i \u00d8stfold er et eksempel..., Som skuespiller har jeg b\u00e5de..., 253, 15th and 16th grade\n\"\"\")\ndf = pd.read_csv(c,sep=',',index_col=['ID'])\n\nwriter = pytablewriter.MarkdownTableWriter()\nwriter.table_name = \"example_table\"\nwriter.header_list = list(df.columns.values)\nwriter.value_matrix = df.values.tolist()\nwriter.write_table()\n</code></pre>\n\n<p>This results in:</p>\n\n<pre><code># example_table\nID |           path           |language|    date    |                longest_sentence                |                   shortest_sentence                  | number_words | readability_consensus \n--:|--------------------------|--------|------------|------------------------------------------------|------------------------------------------------------|-------------:|-----------------------\n  0| data/Eng/Sagitarius.txt  | Eng    | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...|           306| 11th and 12th grade   \n 31| data/Nor/H\u00f8ylandet.txt  | Nor    | 2015-07-22 | H\u00f8gskolen i \u00d8stfold er et eksempel...        | Som skuespiller har jeg b\u00e5de...                      |           253| 15th and 16th grade   \n</code></pre>\n\n<p>Here is a markdown rendered screenshot.</p>\n\n<p><a href=\"https://i.stack.imgur.com/Huojg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Huojg.png\" alt=\"enter image description here\"></a></p>\n"}, "answer_8_votes": {"type": "literal", "value": "5"}, "answer_9": {"type": "literal", "value": "<p>For those looking for how to do this using <code>tabulate</code>, I thought I'd put this here to save you some time:</p>\n\n<p><code>print(tabulate(df, tablefmt=\"pipe\", headers=\"keys\", showindex=False))\n</code></p>\n"}, "answer_9_votes": {"type": "literal", "value": "1"}, "answer_10": {"type": "literal", "value": "<p>Improving the answer further, for use in IPython Notebook:</p>\n\n<pre><code>def pandas_df_to_markdown_table(df):\n    from IPython.display import Markdown, display\n    fmt = ['---' for i in range(len(df.columns))]\n    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n    df_formatted = pd.concat([df_fmt, df])\n    display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n\npandas_df_to_markdown_table(infodf)\n</code></pre>\n\n<p>Or use <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"noreferrer\">tabulate</a>: </p>\n\n<pre><code>pip install tabulate\n</code></pre>\n\n<p>Examples of use are in the documentation.</p>\n"}, "answer_10_votes": {"type": "literal", "value": "29"}, "answer_11": {"type": "literal", "value": "<p>Here's an example function using <code>pytablewriter</code> and some regular expressions to make the markdown table more similar to how a dataframe looks on Jupyter (with the row headers bold).</p>\n\n<pre><code>import io\nimport re\nimport pandas as pd\nimport pytablewriter\n\ndef df_to_markdown(df):\n    \"\"\"\n    Converts Pandas DataFrame to markdown table,\n    making the index bold (as in Jupyter) unless it's a\n    pd.RangeIndex, in which case the index is completely dropped.\n    Returns a string containing markdown table.\n    \"\"\"\n    isRangeIndex = isinstance(df.index, pd.RangeIndex)\n    if not isRangeIndex:\n        df = df.reset_index()\n    writer = pytablewriter.MarkdownTableWriter()\n    writer.stream = io.StringIO()\n    writer.header_list = df.columns\n    writer.value_matrix = df.values\n    writer.write_table()\n    writer.stream.seek(0)\n    table = writer.stream.readlines()\n\n    if isRangeIndex:\n        return ''.join(table)\n    else:\n        # Make the indexes bold\n        new_table = table[:2]\n        for line in table[2:]:\n            new_table.append(re.sub('^(.*?)\\|', r'**\\1**|', line))    \n\n        return ''.join(new_table)\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "1"}, "content_wo_code": "<p>I have a Pandas Dataframe generated from a database, which has data with mixed encodings. For example:</p>\n\n<pre> </pre>\n\n<p>As seen there is a mix of English and Norwegian (encoded as ISO-8859-1 in the database I think). I need to get the contents of this Dataframe output as a Markdown table, but without getting problems with encoding. I followed <a href=\"https://stackoverflow.com/a/15445930/603387\">this answer</a> (from the question <a href=\"https://stackoverflow.com/questions/13394140/generate-markdown-tables\">Generate Markdown tables?</a>) and got the following:</p>\n\n<pre> </pre>\n\n<p>However, this yields an   error. How can I output the Dataframe as a Markdown table? That is, for the purpose of storing this code in a file for use in writing a Markdown document. I need the output to look like this:</p>\n\n<pre> </pre>\n", "answer_wo_code": "<p>Right, so I've taken a leaf from a question suggested by <a href=\"https://stackoverflow.com/users/1625098/rohit\">Rohit</a> (<a href=\"https://stackoverflow.com/questions/7315629/python-encoding-string-swedish-letters\">Python - Encoding string - Swedish Letters</a>), extended <a href=\"https://stackoverflow.com/a/33187706/603387\">his answer</a>, and came up with the following:</p>\n\n<pre> </pre>\n\n<p>An important precursor to this is that the   and   columns do not contain unnecessary line breaks, as removed by applying   to them before submitting into the SQLite database. It appears that the solution is not to enforce the language-specific encoding (  for Norwegian), but rather that   is used instead of the default  .</p>\n\n<p>I ran this through my IPython notebook (Python 2.7.10) and got a table like the following (fixed spacing for appearance here):</p>\n\n<pre> </pre>\n\n<p>Thus, a Markdown table without problems with encoding.</p>\n\n\n<p>Right, so I've taken a leaf from a question suggested by <a href=\"https://stackoverflow.com/users/1625098/rohit\">Rohit</a> (<a href=\"https://stackoverflow.com/questions/7315629/python-encoding-string-swedish-letters\">Python - Encoding string - Swedish Letters</a>), extended <a href=\"https://stackoverflow.com/a/33187706/603387\">his answer</a>, and came up with the following:</p>\n\n<pre> </pre>\n\n<p>An important precursor to this is that the   and   columns do not contain unnecessary line breaks, as removed by applying   to them before submitting into the SQLite database. It appears that the solution is not to enforce the language-specific encoding (  for Norwegian), but rather that   is used instead of the default  .</p>\n\n<p>I ran this through my IPython notebook (Python 2.7.10) and got a table like the following (fixed spacing for appearance here):</p>\n\n<pre> </pre>\n\n<p>Thus, a Markdown table without problems with encoding.</p>\n\n\n<p>sqlite3 returns Unicodes by default for TEXT fields. Everything was set up to work before you introduced the   function from an external source (that you did not provide in your question).</p>\n\n<p>The   function has   calls which do not provide an encoding, so ASCII is used to protect you. </p>\n\n<p>You need to re-write   not to do this, especially as you've got Unicode objects. You may have some success by simply replacing   with  </p>\n\n\n<p>sqlite3 returns Unicodes by default for TEXT fields. Everything was set up to work before you introduced the   function from an external source (that you did not provide in your question).</p>\n\n<p>The   function has   calls which do not provide an encoding, so ASCII is used to protect you. </p>\n\n<p>You need to re-write   not to do this, especially as you've got Unicode objects. You may have some success by simply replacing   with  </p>\n\n\n<h1>Export a DataFrame to markdown</h1>\n\n<p>I created the following function for exporting a pandas.DataFrame to markdown in Python:</p>\n\n<pre class=\"lang-python prettyprint-override\"> </pre>\n\n<p>This function may not automatically fix the encoding issues of the OP, but that is a different issue than converting from pandas to markdown.</p>\n\n\n<h1>Export a DataFrame to markdown</h1>\n\n<p>I created the following function for exporting a pandas.DataFrame to markdown in Python:</p>\n\n<pre class=\"lang-python prettyprint-override\"> </pre>\n\n<p>This function may not automatically fix the encoding issues of the OP, but that is a different issue than converting from pandas to markdown.</p>\n\n\n<p>I recommend <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"nofollow noreferrer\" title=\"python-tabulate\">python-tabulate</a> library for generating ascii-tables. The library supports   as well.</p>\n\n<p>Here is how to use it:</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-none prettyprint-override\"> </pre>\n\n\n<p>I recommend <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"nofollow noreferrer\" title=\"python-tabulate\">python-tabulate</a> library for generating ascii-tables. The library supports   as well.</p>\n\n<p>Here is how to use it:</p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>Output:</p>\n\n<pre class=\"lang-none prettyprint-override\"> </pre>\n\n\n<p>Yet another solution. This time via thin wrapper around tabulate: <a href=\"https://github.com/kiwi0fruit/tabulatehelper\" rel=\"nofollow noreferrer\">tabulatehelper</a></p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<p>Yet another solution. This time via thin wrapper around tabulate: <a href=\"https://github.com/kiwi0fruit/tabulatehelper\" rel=\"nofollow noreferrer\">tabulatehelper</a></p>\n\n<pre class=\"lang-py prettyprint-override\"> </pre>\n\n<p>Output:</p>\n\n<pre> </pre>\n\n\n<p>Try this out. I got it to work. </p>\n\n<p>See the screenshot of my markdown file converted to HTML at the end of this answer.      </p>\n\n<pre> </pre>\n\n<p><a href=\"https://i.stack.imgur.com/j1E1K.jpg\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/j1E1K.jpg\" alt=\"My output in HTML format by converting HTML to Markdown\"></a></p>\n\n\n<p>Try this out. I got it to work. </p>\n\n<p>See the screenshot of my markdown file converted to HTML at the end of this answer.      </p>\n\n<pre> </pre>\n\n<p><a href=\"https://i.stack.imgur.com/j1E1K.jpg\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/j1E1K.jpg\" alt=\"My output in HTML format by converting HTML to Markdown\"></a></p>\n\n\n<p>Using external tool   and pipe:</p>\n\n<pre> </pre>\n\n\n<p>Using external tool   and pipe:</p>\n\n<pre> </pre>\n\n\n<p>I have tried several of the above solutions in this post and found this worked most consistently.  </p>\n\n<p>To convert a pandas data frame to a markdown table I suggest using <a href=\"http://pytablewriter.readthedocs.io/en/latest/\" rel=\"noreferrer\">pytablewriter</a>.\nUsing the data provided in this post:</p>\n\n<pre> </pre>\n\n<p>This results in:</p>\n\n<pre> </pre>\n\n<p>Here is a markdown rendered screenshot.</p>\n\n<p><a href=\"https://i.stack.imgur.com/Huojg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Huojg.png\" alt=\"enter image description here\"></a></p>\n\n\n<p>I have tried several of the above solutions in this post and found this worked most consistently.  </p>\n\n<p>To convert a pandas data frame to a markdown table I suggest using <a href=\"http://pytablewriter.readthedocs.io/en/latest/\" rel=\"noreferrer\">pytablewriter</a>.\nUsing the data provided in this post:</p>\n\n<pre> </pre>\n\n<p>This results in:</p>\n\n<pre> </pre>\n\n<p>Here is a markdown rendered screenshot.</p>\n\n<p><a href=\"https://i.stack.imgur.com/Huojg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/Huojg.png\" alt=\"enter image description here\"></a></p>\n\n\n<p>For those looking for how to do this using  , I thought I'd put this here to save you some time:</p>\n\n<p> </p>\n\n\n<p>For those looking for how to do this using  , I thought I'd put this here to save you some time:</p>\n\n<p> </p>\n\n\n<p>Improving the answer further, for use in IPython Notebook:</p>\n\n<pre> </pre>\n\n<p>Or use <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"noreferrer\">tabulate</a>: </p>\n\n<pre> </pre>\n\n<p>Examples of use are in the documentation.</p>\n\n\n<p>Improving the answer further, for use in IPython Notebook:</p>\n\n<pre> </pre>\n\n<p>Or use <a href=\"https://pypi.python.org/pypi/tabulate\" rel=\"noreferrer\">tabulate</a>: </p>\n\n<pre> </pre>\n\n<p>Examples of use are in the documentation.</p>\n\n\n<p>Here's an example function using   and some regular expressions to make the markdown table more similar to how a dataframe looks on Jupyter (with the row headers bold).</p>\n\n<pre> </pre>\n\n\n<p>Here's an example function using   and some regular expressions to make the markdown table more similar to how a dataframe looks on Jupyter (with the row headers bold).</p>\n\n<pre> </pre>\n"}, {"class_func": {"type": "uri", "value": "http://purl.org/twc/graph4code/python/pandas.read_csv"}, "class_func_label": {"type": "literal", "value": "pandas.read_csv"}, "class_func_type": {"type": "uri", "value": "http://purl.org/twc/graph4code/ontology/Function"}, "docstr": {"type": "literal", "value": "\nRead a comma-separated values (csv) file into DataFrame.\n\nAlso supports optionally iterating or breaking of the file\ninto chunks.\n\nAdditional help can be found in the online docs for\n`IO Tools <http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n"}, "q": {"type": "uri", "value": "https://stackoverflow.com/questions/25962114"}, "title": {"type": "literal", "value": "How to read a 6 GB csv file with pandas"}, "content": {"type": "literal", "value": "<p>I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:</p>\n\n<pre><code>MemoryError                               Traceback (most recent call last)\n&lt;ipython-input-58-67a72687871b&gt; in &lt;module&gt;()\n----&gt; 1 data=pd.read_csv('aphro.csv',sep=';')\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format)\n    450                     infer_datetime_format=infer_datetime_format)\n    451 \n--&gt; 452         return _read(filepath_or_buffer, kwds)\n    453 \n    454     parser_f.__name__ = name\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    242         return parser\n    243 \n--&gt; 244     return parser.read()\n    245 \n    246 _parser_defaults = {\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    693                 raise ValueError('skip_footer not supported for iteration')\n    694 \n--&gt; 695         ret = self._engine.read(nrows)\n    696 \n    697         if self.options.get('as_recarray'):\n\nC:\\Python27\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1137 \n   1138         try:\n-&gt; 1139             data = self._reader.read(nrows)\n   1140         except StopIteration:\n   1141             if nrows is None:\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader.read (pandas\\parser.c:7145)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7369)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._read_rows (pandas\\parser.c:8194)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:9402)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:10057)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10361)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\parser.pyd in pandas.parser._try_int64 (pandas\\parser.c:17806)()\n\nMemoryError: \n</code></pre>\n\n<p>Any help on this?? </p>\n"}, "answerContent": {"type": "literal", "value": "<p>If you use pandas read large file into chunk and then yield row by row, here is what I have done</p>\n\n<pre><code>import pandas as pd\n\ndef chunck_generator(filename, header=False,chunk_size = 10 ** 5):\n   for chunk in pd.read_csv(filename,delimiter=',', iterator=True, chunksize=chunk_size, parse_dates=[1] ): \n        yield (chunk)\n\ndef _generator( filename, header=False,chunk_size = 10 ** 5):\n    chunk = chunck_generator(filename, header=False,chunk_size = 10 ** 5)\n    for row in chunk:\n        yield row\n\nif __name__ == \"__main__\":\nfilename = r'file.csv'\n        generator = generator(filename=filename)\n        while True:\n           print(next(generator))\n</code></pre>\n\n\n<p>For large data l recommend you use the library \"dask\" <br> e.g: </p>\n\n<pre><code># Dataframes implement the Pandas API\nimport dask.dataframe as dd\ndf = dd.read_csv('s3://.../2018-*-*.csv')\n</code></pre>\n\n\n<p>Chunking shouldn't always be the first port of call for this problem.</p>\n\n<p><strong>1. Is the file large due to repeated non-numeric data or unwanted columns?</strong>  </p>\n\n<p>If so, you can sometimes see massive memory savings by <a href=\"https://pandas.pydata.org/pandas-docs/stable/categorical.html#categoricaldtype\" rel=\"noreferrer\">reading in columns as categories</a> and selecting required columns via <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"noreferrer\">pd.read_csv</a> <code>usecols</code> parameter.</p>\n\n<p><strong>2. Does your workflow require slicing, manipulating, exporting?</strong></p>\n\n<p>If so, you can use <a href=\"http://dask.pydata.org/en/latest/dataframe.html\" rel=\"noreferrer\">dask.dataframe</a> to slice, perform your calculations and export iteratively. Chunking is performed silently by dask, which also supports a subset of pandas API.</p>\n\n<p><strong>3. If all else fails, read line by line via chunks.</strong></p>\n\n<p>Chunk <a href=\"https://stackoverflow.com/a/25962187/9209546\">via pandas</a> or via <a href=\"https://stackoverflow.com/a/4957046/9209546\">csv library</a> as a last resort.</p>\n\n\n<p>You can read in the data as chunks and save each chunk as pickle. </p>\n\n<pre><code>import pandas as pd \nimport pickle\n\nin_path = \"\" #Path where the large file is\nout_path = \"\" #Path to save the pickle files to\nchunk_size = 400000 #size of chunks relies on your available memory\nseparator = \"~\"\n\nreader = pd.read_csv(in_path,sep=separator,chunksize=chunk_size, \n                    low_memory=False)    \n\ni=1\nfor chunk in reader:\n    out_file = out_path + \"/data_{}.pkl\".format(i)\n    with open(out_file, \"wb\") as f:\n        pickle.dump(chunk,f,pickle.HIGHEST_PROTOCOL)\n    i+=1\n</code></pre>\n\n<p>In the next step you read in the pickles and append each pickle to your desired dataframe.</p>\n\n<pre><code>import glob\npickle_path = \"\" #Same Path as out_path i.e. where the pickle files are\n\ndata_p_files=[]\nfor name in glob.glob(pickle_path + \"/data_*.pkl\"):\n   data_p_files.append(name)\n\n\ndf = pd.DataFrame([])\nfor i in range(len(data_p_files)):\n    df = df.append(pd.read_pickle(data_p_files[i]),ignore_index=True)\n</code></pre>\n\n\n<p>The function read_csv and read_table is almost the same. But you must assign the delimiter \u201c\uff0c\u201d when you use the function read_table in your program.</p>\n\n<pre><code>def get_from_action_data(fname, chunk_size=100000):\n    reader = pd.read_csv(fname, header=0, iterator=True)\n    chunks = []\n    loop = True\n    while loop:\n        try:\n            chunk = reader.get_chunk(chunk_size)[[\"user_id\", \"type\"]]\n            chunks.append(chunk)\n        except StopIteration:\n            loop = False\n            print(\"Iteration is stopped\")\n\n    df_ac = pd.concat(chunks, ignore_index=True)\n</code></pre>\n\n\n<p>The error shows that the machine does not have enough memory to read the entire\nCSV into a DataFrame at one time. Assuming you do not need the entire dataset in\nmemory all at one time, one way to avoid the problem would be to <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\">process the CSV in\nchunks</a> (by specifying the <code>chunksize</code> parameter):</p>\n\n<pre><code>chunksize = 10 ** 6\nfor chunk in pd.read_csv(filename, chunksize=chunksize):\n    process(chunk)\n</code></pre>\n\n<p>The <code>chucksize</code> parameter specifies the number of rows per chunk.\n(The last chunk may contain fewer than <code>chunksize</code> rows, of course.)</p>\n\n\n<p>Solution 1: </p>\n\n<p><a href=\"https://www.dataquest.io/blog/pandas-big-data/\" rel=\"nofollow noreferrer\">Using pandas with large data</a></p>\n\n<p>Solution 2:</p>\n\n<pre><code>TextFileReader = pd.read_csv(path, chunksize=1000)  # the number of rows per chunk\n\ndfList = []\nfor df in TextFileReader:\n    dfList.append(df)\n\ndf = pd.concat(dfList,sort=False)\n</code></pre>\n\n\n<p>In case someone is still looking for something like this, I found that this new library called <a href=\"https://github.com/modin-project/modin\" rel=\"nofollow noreferrer\">modin</a> can help. It uses distributed computing that can help with the read. Here's a nice <a href=\"https://towardsdatascience.com/get-faster-pandas-with-modin-even-on-your-laptops-b527a2eeda74\" rel=\"nofollow noreferrer\">article</a> comparing its functionality with pandas. It essentially uses the same functions as pandas.</p>\n\n<pre><code>import modin.pandas as pd\npd.read_csv(CSV_FILE_NAME)\n</code></pre>\n\n\n<p>The above answer is already satisfying the topic. Anyway, if you need all the data in memory - have a look at <a href=\"https://github.com/Blosc/bcolz\" rel=\"nofollow noreferrer\">bcolz</a>. Its compressing the data in memory. I have had really good experience with it. But its missing a lot of pandas features</p>\n\n<p>Edit: I got compression rates at around 1/10 or orig size i think, of course depending of the kind of data. Important features missing were aggregates. </p>\n\n\n<p>You can try sframe, that have the same syntax as pandas but allows you to manipulate files that are bigger than your RAM.</p>\n\n\n<p>Here follows an example:</p>\n\n<pre><code>chunkTemp = []\nqueryTemp = []\nquery = pd.DataFrame()\n\nfor chunk in pd.read_csv(file, header=0, chunksize=&lt;your_chunksize&gt;, iterator=True, low_memory=False):\n\n    #REPLACING BLANK SPACES AT COLUMNS' NAMES FOR SQL OPTIMIZATION\n    chunk = chunk.rename(columns = {c: c.replace(' ', '') for c in chunk.columns})\n\n    #YOU CAN EITHER: \n    #1)BUFFER THE CHUNKS IN ORDER TO LOAD YOUR WHOLE DATASET \n    chunkTemp.append(chunk)\n\n    #2)DO YOUR PROCESSING OVER A CHUNK AND STORE THE RESULT OF IT\n    query = chunk[chunk[&lt;column_name&gt;].str.startswith(&lt;some_pattern&gt;)]   \n    #BUFFERING PROCESSED DATA\n    queryTemp.append(query)\n\n#!  NEVER DO pd.concat OR pd.DataFrame() INSIDE A LOOP\nprint(\"Database: CONCATENATING CHUNKS INTO A SINGLE DATAFRAME\")\nchunk = pd.concat(chunkTemp)\nprint(\"Database: LOADED\")\n\n#CONCATENATING PROCESSED DATA\nquery = pd.concat(queryTemp)\nprint(query)\n</code></pre>\n\n\n<p>In addition to the answers above, for those who want to process CSV and then export to csv, parquet or SQL, <a href=\"https://github.com/d6t/d6tstack\" rel=\"nofollow noreferrer\">d6tstack</a> is another good option. You can load multiple files and it deals with data schema changes (added/removed columns). Chunked out of core support is already built in.</p>\n\n<pre><code>def apply(dfg):\n    # do stuff\n    return dfg\n\nc = d6tstack.combine_csv.CombinerCSV([bigfile.csv], apply_after_read=apply, sep=',', chunksize=1e6)\n\n# or\nc = d6tstack.combine_csv.CombinerCSV(glob.glob('*.csv'), apply_after_read=apply, chunksize=1e6)\n\n# output to various formats, automatically chunked to reduce memory consumption\nc.to_csv_combine(filename='out.csv')\nc.to_parquet_combine(filename='out.pq')\nc.to_psql_combine('postgresql+psycopg2://usr:pwd@localhost/db', 'tablename') # fast for postgres\nc.to_mysql_combine('mysql+mysqlconnector://usr:pwd@localhost/db', 'tablename') # fast for mysql\nc.to_sql_combine('postgresql+psycopg2://usr:pwd@localhost/db', 'tablename') # slow but flexible\n</code></pre>\n\n\n<p>I proceeded like this:</p>\n\n<pre><code>chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\\\n       names=['lat','long','rf','date','slno'],index_col='slno',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)\n</code></pre>\n"}, "answer_1": {"type": "literal", "value": "<p>If you use pandas read large file into chunk and then yield row by row, here is what I have done</p>\n\n<pre><code>import pandas as pd\n\ndef chunck_generator(filename, header=False,chunk_size = 10 ** 5):\n   for chunk in pd.read_csv(filename,delimiter=',', iterator=True, chunksize=chunk_size, parse_dates=[1] ): \n        yield (chunk)\n\ndef _generator( filename, header=False,chunk_size = 10 ** 5):\n    chunk = chunck_generator(filename, header=False,chunk_size = 10 ** 5)\n    for row in chunk:\n        yield row\n\nif __name__ == \"__main__\":\nfilename = r'file.csv'\n        generator = generator(filename=filename)\n        while True:\n           print(next(generator))\n</code></pre>\n"}, "answer_1_votes": {"type": "literal", "value": "2"}, "answer_2": {"type": "literal", "value": "<p>For large data l recommend you use the library \"dask\" <br> e.g: </p>\n\n<pre><code># Dataframes implement the Pandas API\nimport dask.dataframe as dd\ndf = dd.read_csv('s3://.../2018-*-*.csv')\n</code></pre>\n"}, "answer_2_votes": {"type": "literal", "value": "25"}, "answer_3": {"type": "literal", "value": "<p>Chunking shouldn't always be the first port of call for this problem.</p>\n\n<p><strong>1. Is the file large due to repeated non-numeric data or unwanted columns?</strong>  </p>\n\n<p>If so, you can sometimes see massive memory savings by <a href=\"https://pandas.pydata.org/pandas-docs/stable/categorical.html#categoricaldtype\" rel=\"noreferrer\">reading in columns as categories</a> and selecting required columns via <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"noreferrer\">pd.read_csv</a> <code>usecols</code> parameter.</p>\n\n<p><strong>2. Does your workflow require slicing, manipulating, exporting?</strong></p>\n\n<p>If so, you can use <a href=\"http://dask.pydata.org/en/latest/dataframe.html\" rel=\"noreferrer\">dask.dataframe</a> to slice, perform your calculations and export iteratively. Chunking is performed silently by dask, which also supports a subset of pandas API.</p>\n\n<p><strong>3. If all else fails, read line by line via chunks.</strong></p>\n\n<p>Chunk <a href=\"https://stackoverflow.com/a/25962187/9209546\">via pandas</a> or via <a href=\"https://stackoverflow.com/a/4957046/9209546\">csv library</a> as a last resort.</p>\n"}, "answer_3_votes": {"type": "literal", "value": "54"}, "answer_4": {"type": "literal", "value": "<p>You can read in the data as chunks and save each chunk as pickle. </p>\n\n<pre><code>import pandas as pd \nimport pickle\n\nin_path = \"\" #Path where the large file is\nout_path = \"\" #Path to save the pickle files to\nchunk_size = 400000 #size of chunks relies on your available memory\nseparator = \"~\"\n\nreader = pd.read_csv(in_path,sep=separator,chunksize=chunk_size, \n                    low_memory=False)    \n\ni=1\nfor chunk in reader:\n    out_file = out_path + \"/data_{}.pkl\".format(i)\n    with open(out_file, \"wb\") as f:\n        pickle.dump(chunk,f,pickle.HIGHEST_PROTOCOL)\n    i+=1\n</code></pre>\n\n<p>In the next step you read in the pickles and append each pickle to your desired dataframe.</p>\n\n<pre><code>import glob\npickle_path = \"\" #Same Path as out_path i.e. where the pickle files are\n\ndata_p_files=[]\nfor name in glob.glob(pickle_path + \"/data_*.pkl\"):\n   data_p_files.append(name)\n\n\ndf = pd.DataFrame([])\nfor i in range(len(data_p_files)):\n    df = df.append(pd.read_pickle(data_p_files[i]),ignore_index=True)\n</code></pre>\n"}, "answer_4_votes": {"type": "literal", "value": "6"}, "answer_5": {"type": "literal", "value": "<p>The function read_csv and read_table is almost the same. But you must assign the delimiter \u201c\uff0c\u201d when you use the function read_table in your program.</p>\n\n<pre><code>def get_from_action_data(fname, chunk_size=100000):\n    reader = pd.read_csv(fname, header=0, iterator=True)\n    chunks = []\n    loop = True\n    while loop:\n        try:\n            chunk = reader.get_chunk(chunk_size)[[\"user_id\", \"type\"]]\n            chunks.append(chunk)\n        except StopIteration:\n            loop = False\n            print(\"Iteration is stopped\")\n\n    df_ac = pd.concat(chunks, ignore_index=True)\n</code></pre>\n"}, "answer_5_votes": {"type": "literal", "value": "4"}, "answer_6": {"type": "literal", "value": "<p>The error shows that the machine does not have enough memory to read the entire\nCSV into a DataFrame at one time. Assuming you do not need the entire dataset in\nmemory all at one time, one way to avoid the problem would be to <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\">process the CSV in\nchunks</a> (by specifying the <code>chunksize</code> parameter):</p>\n\n<pre><code>chunksize = 10 ** 6\nfor chunk in pd.read_csv(filename, chunksize=chunksize):\n    process(chunk)\n</code></pre>\n\n<p>The <code>chucksize</code> parameter specifies the number of rows per chunk.\n(The last chunk may contain fewer than <code>chunksize</code> rows, of course.)</p>\n"}, "answer_6_votes": {"type": "literal", "value": "195"}, "answer_7": {"type": "literal", "value": "<p>Solution 1: </p>\n\n<p><a href=\"https://www.dataquest.io/blog/pandas-big-data/\" rel=\"nofollow noreferrer\">Using pandas with large data</a></p>\n\n<p>Solution 2:</p>\n\n<pre><code>TextFileReader = pd.read_csv(path, chunksize=1000)  # the number of rows per chunk\n\ndfList = []\nfor df in TextFileReader:\n    dfList.append(df)\n\ndf = pd.concat(dfList,sort=False)\n</code></pre>\n"}, "answer_7_votes": {"type": "literal", "value": "6"}, "answer_8": {"type": "literal", "value": "<p>In case someone is still looking for something like this, I found that this new library called <a href=\"https://github.com/modin-project/modin\" rel=\"nofollow noreferrer\">modin</a> can help. It uses distributed computing that can help with the read. Here's a nice <a href=\"https://towardsdatascience.com/get-faster-pandas-with-modin-even-on-your-laptops-b527a2eeda74\" rel=\"nofollow noreferrer\">article</a> comparing its functionality with pandas. It essentially uses the same functions as pandas.</p>\n\n<pre><code>import modin.pandas as pd\npd.read_csv(CSV_FILE_NAME)\n</code></pre>\n"}, "answer_8_votes": {"type": "literal", "value": ""}, "answer_9": {"type": "literal", "value": "<p>The above answer is already satisfying the topic. Anyway, if you need all the data in memory - have a look at <a href=\"https://github.com/Blosc/bcolz\" rel=\"nofollow noreferrer\">bcolz</a>. Its compressing the data in memory. I have had really good experience with it. But its missing a lot of pandas features</p>\n\n<p>Edit: I got compression rates at around 1/10 or orig size i think, of course depending of the kind of data. Important features missing were aggregates. </p>\n"}, "answer_9_votes": {"type": "literal", "value": "10"}, "answer_10": {"type": "literal", "value": "<p>You can try sframe, that have the same syntax as pandas but allows you to manipulate files that are bigger than your RAM.</p>\n"}, "answer_10_votes": {"type": "literal", "value": "4"}, "answer_11": {"type": "literal", "value": "<p>Here follows an example:</p>\n\n<pre><code>chunkTemp = []\nqueryTemp = []\nquery = pd.DataFrame()\n\nfor chunk in pd.read_csv(file, header=0, chunksize=&lt;your_chunksize&gt;, iterator=True, low_memory=False):\n\n    #REPLACING BLANK SPACES AT COLUMNS' NAMES FOR SQL OPTIMIZATION\n    chunk = chunk.rename(columns = {c: c.replace(' ', '') for c in chunk.columns})\n\n    #YOU CAN EITHER: \n    #1)BUFFER THE CHUNKS IN ORDER TO LOAD YOUR WHOLE DATASET \n    chunkTemp.append(chunk)\n\n    #2)DO YOUR PROCESSING OVER A CHUNK AND STORE THE RESULT OF IT\n    query = chunk[chunk[&lt;column_name&gt;].str.startswith(&lt;some_pattern&gt;)]   \n    #BUFFERING PROCESSED DATA\n    queryTemp.append(query)\n\n#!  NEVER DO pd.concat OR pd.DataFrame() INSIDE A LOOP\nprint(\"Database: CONCATENATING CHUNKS INTO A SINGLE DATAFRAME\")\nchunk = pd.concat(chunkTemp)\nprint(\"Database: LOADED\")\n\n#CONCATENATING PROCESSED DATA\nquery = pd.concat(queryTemp)\nprint(query)\n</code></pre>\n"}, "answer_11_votes": {"type": "literal", "value": "2"}, "answer_12": {"type": "literal", "value": "<p>In addition to the answers above, for those who want to process CSV and then export to csv, parquet or SQL, <a href=\"https://github.com/d6t/d6tstack\" rel=\"nofollow noreferrer\">d6tstack</a> is another good option. You can load multiple files and it deals with data schema changes (added/removed columns). Chunked out of core support is already built in.</p>\n\n<pre><code>def apply(dfg):\n    # do stuff\n    return dfg\n\nc = d6tstack.combine_csv.CombinerCSV([bigfile.csv], apply_after_read=apply, sep=',', chunksize=1e6)\n\n# or\nc = d6tstack.combine_csv.CombinerCSV(glob.glob('*.csv'), apply_after_read=apply, chunksize=1e6)\n\n# output to various formats, automatically chunked to reduce memory consumption\nc.to_csv_combine(filename='out.csv')\nc.to_parquet_combine(filename='out.pq')\nc.to_psql_combine('postgresql+psycopg2://usr:pwd@localhost/db', 'tablename') # fast for postgres\nc.to_mysql_combine('mysql+mysqlconnector://usr:pwd@localhost/db', 'tablename') # fast for mysql\nc.to_sql_combine('postgresql+psycopg2://usr:pwd@localhost/db', 'tablename') # slow but flexible\n</code></pre>\n"}, "answer_12_votes": {"type": "literal", "value": ""}, "answer_13": {"type": "literal", "value": "<p>I proceeded like this:</p>\n\n<pre><code>chunks=pd.read_table('aphro.csv',chunksize=1000000,sep=';',\\\n       names=['lat','long','rf','date','slno'],index_col='slno',\\\n       header=None,parse_dates=['date'])\n\ndf=pd.DataFrame()\n%time df=pd.concat(chunk.groupby(['lat','long',chunk['date'].map(lambda x: x.year)])['rf'].agg(['sum']) for chunk in chunks)\n</code></pre>\n"}, "answer_13_votes": {"type": "literal", "value": "34"}, "content_wo_code": "<p>I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting the following memory error:</p>\n\n<pre> </pre>\n\n<p>Any help on this?? </p>\n", "answer_wo_code": "<p>If you use pandas read large file into chunk and then yield row by row, here is what I have done</p>\n\n<pre> </pre>\n\n\n<p>For large data l recommend you use the library \"dask\" <br> e.g: </p>\n\n<pre> </pre>\n\n\n<p>Chunking shouldn't always be the first port of call for this problem.</p>\n\n<p><strong>1. Is the file large due to repeated non-numeric data or unwanted columns?</strong>  </p>\n\n<p>If so, you can sometimes see massive memory savings by <a href=\"https://pandas.pydata.org/pandas-docs/stable/categorical.html#categoricaldtype\" rel=\"noreferrer\">reading in columns as categories</a> and selecting required columns via <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" rel=\"noreferrer\">pd.read_csv</a>   parameter.</p>\n\n<p><strong>2. Does your workflow require slicing, manipulating, exporting?</strong></p>\n\n<p>If so, you can use <a href=\"http://dask.pydata.org/en/latest/dataframe.html\" rel=\"noreferrer\">dask.dataframe</a> to slice, perform your calculations and export iteratively. Chunking is performed silently by dask, which also supports a subset of pandas API.</p>\n\n<p><strong>3. If all else fails, read line by line via chunks.</strong></p>\n\n<p>Chunk <a href=\"https://stackoverflow.com/a/25962187/9209546\">via pandas</a> or via <a href=\"https://stackoverflow.com/a/4957046/9209546\">csv library</a> as a last resort.</p>\n\n\n<p>You can read in the data as chunks and save each chunk as pickle. </p>\n\n<pre> </pre>\n\n<p>In the next step you read in the pickles and append each pickle to your desired dataframe.</p>\n\n<pre> </pre>\n\n\n<p>The function read_csv and read_table is almost the same. But you must assign the delimiter \u201c\uff0c\u201d when you use the function read_table in your program.</p>\n\n<pre> </pre>\n\n\n<p>The error shows that the machine does not have enough memory to read the entire\nCSV into a DataFrame at one time. Assuming you do not need the entire dataset in\nmemory all at one time, one way to avoid the problem would be to <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html\" rel=\"noreferrer\">process the CSV in\nchunks</a> (by specifying the   parameter):</p>\n\n<pre> </pre>\n\n<p>The   parameter specifies the number of rows per chunk.\n(The last chunk may contain fewer than   rows, of course.)</p>\n\n\n<p>Solution 1: </p>\n\n<p><a href=\"https://www.dataquest.io/blog/pandas-big-data/\" rel=\"nofollow noreferrer\">Using pandas with large data</a></p>\n\n<p>Solution 2:</p>\n\n<pre> </pre>\n\n\n<p>In case someone is still looking for something like this, I found that this new library called <a href=\"https://github.com/modin-project/modin\" rel=\"nofollow noreferrer\">modin</a> can help. It uses distributed computing that can help with the read. Here's a nice <a href=\"https://towardsdatascience.com/get-faster-pandas-with-modin-even-on-your-laptops-b527a2eeda74\" rel=\"nofollow noreferrer\">article</a> comparing its functionality with pandas. It essentially uses the same functions as pandas.</p>\n\n<pre> </pre>\n\n\n<p>The above answer is already satisfying the topic. Anyway, if you need all the data in memory - have a look at <a href=\"https://github.com/Blosc/bcolz\" rel=\"nofollow noreferrer\">bcolz</a>. Its compressing the data in memory. I have had really good experience with it. But its missing a lot of pandas features</p>\n\n<p>Edit: I got compression rates at around 1/10 or orig size i think, of course depending of the kind of data. Important features missing were aggregates. </p>\n\n\n<p>You can try sframe, that have the same syntax as pandas but allows you to manipulate files that are bigger than your RAM.</p>\n\n\n<p>Here follows an example:</p>\n\n<pre> </pre>\n\n\n<p>In addition to the answers above, for those who want to process CSV and then export to csv, parquet or SQL, <a href=\"https://github.com/d6t/d6tstack\" rel=\"nofollow noreferrer\">d6tstack</a> is another good option. You can load multiple files and it deals with data schema changes (added/removed columns). Chunked out of core support is already built in.</p>\n\n<pre> </pre>\n\n\n<p>I proceeded like this:</p>\n\n<pre> </pre>\n"}]}}