[
    {
        "base_classes": [
            "datetime.date"
        ],
        "class_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "datetime.datetime",
        "module": "datetime"
    },
    {
        "base_classes": [
            "_socket.socket"
        ],
        "class_docstring": "A subclass of _socket.socket adding the makefile() method.",
        "klass": "socket.socket",
        "module": "socket"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n",
        "klass": "functools.partial",
        "module": "functools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Difference between two datetime values.\n\ntimedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)\n\nAll arguments are optional and default to 0.\nArguments may be integers or floats, and may be positive or negative.",
        "klass": "datetime.timedelta",
        "module": "datetime"
    },
    {
        "base_classes": [
            "requests_mock.mocker.MockerCore"
        ],
        "class_docstring": "The standard entry point for mock Adapter loading.\n    ",
        "klass": "requests_mock.Mocker",
        "module": "requests_mock"
    },
    {
        "base_classes": [
            "requests.adapters.BaseAdapter"
        ],
        "class_docstring": "The built-in HTTP Adapter for urllib3.\n\n    Provides a general-case interface for Requests sessions to contact HTTP and\n    HTTPS urls by implementing the Transport Adapter interface. This class will\n    usually be created by the :class:`Session <Session>` class under the\n    covers.\n\n    :param pool_connections: The number of urllib3 connection pools to cache.\n    :param pool_maxsize: The maximum number of connections to save in the pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups, socket\n        connections and connection timeouts, never to requests where data has\n        made it to the server. By default, Requests does not retry failed\n        connections. If you need granular control over the conditions under\n        which we retry a request, import urllib3's ``Retry`` class and pass\n        that instead.\n    :param pool_block: Whether the connection pool should block for connections.\n\n    Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> a = requests.adapters.HTTPAdapter(max_retries=3)\n      >>> s.mount('http://', a)\n    ",
        "klass": "requests.adapters.HTTPAdapter",
        "module": "requests"
    },
    {
        "base_classes": [
            "http.cookiejar.CookieJar",
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "Compatibility class; is a cookielib.CookieJar, but exposes a dict\n    interface.\n\n    This is the CookieJar we create by default for requests and sessions that\n    don't specify one, since some clients may expect response.cookies and\n    session.cookies to support dict operations.\n\n    Requests does not use the dict interface internally; it's just for\n    compatibility with external client code. All requests code should work\n    out of the box with externally provided instances of ``CookieJar``, e.g.\n    ``LWPCookieJar`` and ``FileCookieJar``.\n\n    Unlike a regular CookieJar, this class is pickleable.\n\n    .. warning:: dictionary operations that are normally O(1) may be O(n).\n    ",
        "klass": "requests.cookies.RequestsCookieJar",
        "module": "requests"
    },
    {
        "base_classes": [
            "urllib.request.URLopener"
        ],
        "class_docstring": "Derived class with handlers for errors we can handle (perhaps).",
        "klass": "urllib.request.FancyURLopener",
        "module": "urllib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to open URLs.\n    This is a class rather than just a subroutine because we may need\n    more than one set of global protocol-specific options.\n    Note -- this is a base class for those who don't want the\n    automatic handling of errors type 302 (relocated) and 401\n    (authorization needed).",
        "klass": "urllib.request.URLopener",
        "module": "urllib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a (possibly partial) specification for a TensorFlow device.\n\n  `DeviceSpec`s are used throughout TensorFlow to describe where state is stored\n  and computations occur. Using `DeviceSpec` allows you to parse device spec\n  strings to verify their validity, merge them or compose them programmatically.\n\n  Example:\n\n  ```python\n  # Place the operations on device \"GPU:0\" in the \"ps\" job.\n  device_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n  with tf.device(device_spec):\n    # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n    my_var = tf.Variable(..., name=\"my_variable\")\n    squared_var = tf.square(my_var)\n  ```\n\n  If a `DeviceSpec` is partially specified, it will be merged with other\n  `DeviceSpec`s according to the scope in which it is defined. `DeviceSpec`\n  components defined in inner scopes take precedence over those defined in\n  outer scopes.\n\n  ```python\n  with tf.device(DeviceSpec(job=\"train\", )):\n    with tf.device(DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0):\n      # Nodes created here will be assigned to /job:ps/device:GPU:0.\n    with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1):\n      # Nodes created here will be assigned to /job:train/device:GPU:1.\n  ```\n\n  A `DeviceSpec` consists of 5 components -- each of\n  which is optionally specified:\n\n  * Job: The job name.\n  * Replica: The replica index.\n  * Task: The task index.\n  * Device type: The device type string (e.g. \"CPU\" or \"GPU\").\n  * Device index: The device index.\n  ",
        "klass": "tensorflow.python.framework.device.DeviceSpec",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A queue implementation that dequeues elements in first-in first-out order.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.FIFOQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A queue implementation that dequeues elements in first-in first-out order.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.FIFOQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Record operations for automatic differentiation.\n\n  Operations are recorded if they are executed within this context manager and\n  at least one of their inputs is being \"watched\".\n\n  Trainable variables (created by `tf.Variable` or `tf.get_variable`, where\n  `trainable=True` is default in both cases) are automatically watched. Tensors\n  can be manually watched by invoking the `watch` method on this context\n  manager.\n\n  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n  be computed as:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    y = x * x\n  dy_dx = g.gradient(y, x) # Will compute to 6.0\n  ```\n\n  GradientTapes can be nested to compute higher-order derivatives. For example,\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    with tf.GradientTape() as gg:\n      gg.watch(x)\n      y = x * x\n    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n  ```\n\n  By default, the resources held by a GradientTape are released as soon as\n  GradientTape.gradient() method is called. To compute multiple gradients over\n  the same computation, create a persistent gradient tape. This allows multiple\n  calls to the gradient() method as resources are released when the tape object\n  is garbage collected. For example:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape(persistent=True) as g:\n    g.watch(x)\n    y = x * x\n    z = y * y\n  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n  dy_dx = g.gradient(y, x)  # 6.0\n  del g  # Drop the reference to the tape\n  ```\n\n  By default GradientTape will automatically watch any trainable variables that\n  are accessed inside the context. If you want fine grained control over which\n  variables are watched you can disable automatic tracking by passing\n  `watch_accessed_variables=False` to the tape constructor:\n\n  ```python\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(variable_a)\n    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n    z = variable_b ** 3  # No gradients will be avaialble since `variable_b` is\n                         # not being watched.\n  ```\n\n  Note that when using models you should ensure that your variables exist when\n  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n  first iteration not have any gradients:\n\n  ```python\n  a = tf.keras.layers.Dense(32)\n  b = tf.keras.layers.Dense(32)\n\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                             # `a.variables` will return an empty list and the\n                             # tape will not be watching anything.\n    result = b(a(inputs))\n    tape.gradient(result, a.variables)  # The result of this computation will be\n                                        # a list of `None`s since a's variables\n                                        # are not being watched.\n  ```\n\n  Note that only tensors with real or complex dtypes are differentiable.\n  ",
        "klass": "tensorflow.python.eager.backprop.GradientTape",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Record operations for automatic differentiation.\n\n  Operations are recorded if they are executed within this context manager and\n  at least one of their inputs is being \"watched\".\n\n  Trainable variables (created by `tf.Variable` or `tf.get_variable`, where\n  `trainable=True` is default in both cases) are automatically watched. Tensors\n  can be manually watched by invoking the `watch` method on this context\n  manager.\n\n  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n  be computed as:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    y = x * x\n  dy_dx = g.gradient(y, x) # Will compute to 6.0\n  ```\n\n  GradientTapes can be nested to compute higher-order derivatives. For example,\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    with tf.GradientTape() as gg:\n      gg.watch(x)\n      y = x * x\n    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n  ```\n\n  By default, the resources held by a GradientTape are released as soon as\n  GradientTape.gradient() method is called. To compute multiple gradients over\n  the same computation, create a persistent gradient tape. This allows multiple\n  calls to the gradient() method as resources are released when the tape object\n  is garbage collected. For example:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape(persistent=True) as g:\n    g.watch(x)\n    y = x * x\n    z = y * y\n  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n  dy_dx = g.gradient(y, x)  # 6.0\n  del g  # Drop the reference to the tape\n  ```\n\n  By default GradientTape will automatically watch any trainable variables that\n  are accessed inside the context. If you want fine grained control over which\n  variables are watched you can disable automatic tracking by passing\n  `watch_accessed_variables=False` to the tape constructor:\n\n  ```python\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(variable_a)\n    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n    z = variable_b ** 3  # No gradients will be avaialble since `variable_b` is\n                         # not being watched.\n  ```\n\n  Note that when using models you should ensure that your variables exist when\n  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n  first iteration not have any gradients:\n\n  ```python\n  a = tf.keras.layers.Dense(32)\n  b = tf.keras.layers.Dense(32)\n\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                             # `a.variables` will return an empty list and the\n                             # tape will not be watching anything.\n    result = b(a(inputs))\n    tape.gradient(result, a.variables)  # The result of this computation will be\n                                        # a list of `None`s since a's variables\n                                        # are not being watched.\n  ```\n\n  Note that only tensors with real or complex dtypes are differentiable.\n  ",
        "klass": "tensorflow.GradientTape",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A TensorFlow computation, represented as a dataflow graph.\n\n  A `Graph` contains a set of\n  `tf.Operation` objects,\n  which represent units of computation; and\n  `tf.Tensor` objects, which represent\n  the units of data that flow between operations.\n\n  A default `Graph` is always registered, and accessible by calling\n  `tf.get_default_graph`.\n  To add an operation to the default graph, simply call one of the functions\n  that defines a new `Operation`:\n\n  ```python\n  c = tf.constant(4.0)\n  assert c.graph is tf.get_default_graph()\n  ```\n\n  Another typical usage involves the\n  `tf.Graph.as_default`\n  context manager, which overrides the current default graph for the\n  lifetime of the context:\n\n  ```python\n  g = tf.Graph()\n  with g.as_default():\n    # Define operations and tensors in `g`.\n    c = tf.constant(30.0)\n    assert c.graph is g\n  ```\n\n  Important note: This class *is not* thread-safe for graph construction. All\n  operations should be created from a single thread, or external\n  synchronization must be provided. Unless otherwise specified, all methods\n  are not thread-safe.\n\n  A `Graph` instance supports an arbitrary number of \"collections\"\n  that are identified by name. For convenience when building a large\n  graph, collections can store groups of related objects: for\n  example, the `tf.Variable` uses a collection (named\n  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n  all variables that are created during the construction of a graph. The caller\n  may define additional collections by specifying a new name.\n  ",
        "klass": "tensorflow.python.framework.ops.Graph",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A TensorFlow computation, represented as a dataflow graph.\n\n  A `Graph` contains a set of\n  `tf.Operation` objects,\n  which represent units of computation; and\n  `tf.Tensor` objects, which represent\n  the units of data that flow between operations.\n\n  A default `Graph` is always registered, and accessible by calling\n  `tf.get_default_graph`.\n  To add an operation to the default graph, simply call one of the functions\n  that defines a new `Operation`:\n\n  ```python\n  c = tf.constant(4.0)\n  assert c.graph is tf.get_default_graph()\n  ```\n\n  Another typical usage involves the\n  `tf.Graph.as_default`\n  context manager, which overrides the current default graph for the\n  lifetime of the context:\n\n  ```python\n  g = tf.Graph()\n  with g.as_default():\n    # Define operations and tensors in `g`.\n    c = tf.constant(30.0)\n    assert c.graph is g\n  ```\n\n  Important note: This class *is not* thread-safe for graph construction. All\n  operations should be created from a single thread, or external\n  synchronization must be provided. Unless otherwise specified, all methods\n  are not thread-safe.\n\n  A `Graph` instance supports an arbitrary number of \"collections\"\n  that are identified by name. For convenience when building a large\n  graph, collections can store groups of related objects: for\n  example, the `tf.Variable` uses a collection (named\n  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n  all variables that are created during the construction of a graph. The caller\n  may define additional collections by specifying a new name.\n  ",
        "klass": "tensorflow.Graph",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.client.session.BaseSession"
        ],
        "class_docstring": "A TensorFlow `Session` for use in interactive contexts, such as a shell.\n\n  The only difference with a regular `Session` is that an `InteractiveSession`\n  installs itself as the default session on construction.\n  The methods `tf.Tensor.eval`\n  and `tf.Operation.run`\n  will use that session to run ops.\n\n  This is convenient in interactive shells and [IPython\n  notebooks](http://ipython.org), as it avoids having to pass an explicit\n  `Session` object to run ops.\n\n  For example:\n\n  ```python\n  sess = tf.InteractiveSession()\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n  # We can just use 'c.eval()' without passing 'sess'\n  print(c.eval())\n  sess.close()\n  ```\n\n  Note that a regular session installs itself as the default session when it\n  is created in a `with` statement.  The common usage in non-interactive\n  programs is to follow that pattern:\n\n  ```python\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n  with tf.Session():\n    # We can also use 'c.eval()' here.\n    print(c.eval())\n  ```\n  ",
        "klass": "tensorflow.InteractiveSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A FIFOQueue that supports batching variable-sized tensors by padding.\n\n  A `PaddingFIFOQueue` may contain components with dynamic shape, while also\n  supporting `dequeue_many`.  See the constructor for more details.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.PaddingFIFOQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A FIFOQueue that supports batching variable-sized tensors by padding.\n\n  A `PaddingFIFOQueue` may contain components with dynamic shape, while also\n  supporting `dequeue_many`.  See the constructor for more details.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.PaddingFIFOQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A queue implementation that dequeues elements in prioritized order.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.PriorityQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A queue implementation that dequeues elements in a random order.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.RandomShuffleQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.QueueBase"
        ],
        "class_docstring": "A queue implementation that dequeues elements in a random order.\n\n  See `tf.QueueBase` for a description of the methods on\n  this class.\n  ",
        "klass": "tensorflow.RandomShuffleQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.client.session.BaseSession"
        ],
        "class_docstring": "A class for running TensorFlow operations.\n\n  A `Session` object encapsulates the environment in which `Operation`\n  objects are executed, and `Tensor` objects are evaluated. For\n  example:\n\n  ```python\n  # Build a graph.\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n\n  # Launch the graph in a session.\n  sess = tf.Session()\n\n  # Evaluate the tensor `c`.\n  print(sess.run(c))\n  ```\n\n  A session may own resources, such as\n  `tf.Variable`, `tf.QueueBase`,\n  and `tf.ReaderBase`. It is important to release\n  these resources when they are no longer required. To do this, either\n  invoke the `tf.Session.close` method on the session, or use\n  the session as a context manager. The following two examples are\n  equivalent:\n\n  ```python\n  # Using the `close()` method.\n  sess = tf.Session()\n  sess.run(...)\n  sess.close()\n\n  # Using the context manager.\n  with tf.Session() as sess:\n    sess.run(...)\n  ```\n\n  The\n  [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto)\n  protocol buffer exposes various configuration options for a\n  session. For example, to create a session that uses soft constraints\n  for device placement, and log the resulting placement decisions,\n  create a session as follows:\n\n  ```python\n  # Launch the graph in a session that allows soft device placement and\n  # logs the placement decisions.\n  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                          log_device_placement=True))\n  ```\n  ",
        "klass": "tensorflow.python.client.session.Session",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.client.session.BaseSession"
        ],
        "class_docstring": "A class for running TensorFlow operations.\n\n  A `Session` object encapsulates the environment in which `Operation`\n  objects are executed, and `Tensor` objects are evaluated. For\n  example:\n\n  ```python\n  # Build a graph.\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n\n  # Launch the graph in a session.\n  sess = tf.Session()\n\n  # Evaluate the tensor `c`.\n  print(sess.run(c))\n  ```\n\n  A session may own resources, such as\n  `tf.Variable`, `tf.QueueBase`,\n  and `tf.ReaderBase`. It is important to release\n  these resources when they are no longer required. To do this, either\n  invoke the `tf.Session.close` method on the session, or use\n  the session as a context manager. The following two examples are\n  equivalent:\n\n  ```python\n  # Using the `close()` method.\n  sess = tf.Session()\n  sess.run(...)\n  sess.close()\n\n  # Using the context manager.\n  with tf.Session() as sess:\n    sess.run(...)\n  ```\n\n  The\n  [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto)\n  protocol buffer exposes various configuration options for a\n  session. For example, to create a session that uses soft constraints\n  for device placement, and log the resulting placement decisions,\n  create a session as follows:\n\n  ```python\n  # Launch the graph in a session that allows soft device placement and\n  # logs the placement decisions.\n  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                          log_device_placement=True))\n  ```\n  ",
        "klass": "tensorflow.Session",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.io_ops.ReaderBase"
        ],
        "class_docstring": "A Reader that outputs the records from a TFRecords file.\n\n  See ReaderBase for supported methods.\n\n  @compatibility(eager)\n  Readers are not compatible with eager execution. Instead, please\n  use `tf.data` to get data into your model.\n  @end_compatibility\n  ",
        "klass": "tensorflow.TFRecordReader",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.\n\n  This class is meant to be used with dynamic iteration primitives such as\n  `while_loop` and `map_fn`.  It supports gradient back-propagation via special\n  \"flow\" control flow dependencies.\n  ",
        "klass": "tensorflow.python.ops.tensor_array_ops.TensorArray",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.\n\n  This class is meant to be used with dynamic iteration primitives such as\n  `while_loop` and `map_fn`.  It supports gradient back-propagation via special\n  \"flow\" control flow dependencies.\n  ",
        "klass": "tensorflow.TensorArray",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Describes a tf.Tensor.\n\n  Metadata for describing the `tf.Tensor` objects accepted or returned\n  by some TensorFlow APIs.\n  ",
        "klass": "tensorflow.python.framework.tensor_spec.TensorSpec",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.io_ops.ReaderBase"
        ],
        "class_docstring": "A Reader that outputs the lines of a file delimited by newlines.\n\n  Newlines are stripped from the output.\n  See ReaderBase for supported methods.\n\n  @compatibility(eager)\n  Readers are not compatible with eager execution. Instead, please\n  use `tf.data` to get data into your model.\n  @end_compatibility\n  ",
        "klass": "tensorflow.TextLineReader",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.variables.Variable"
        ],
        "class_docstring": "See the [Variables Guide](https://tensorflow.org/guide/variables).\n\n  A variable maintains state in the graph across calls to `run()`. You add a\n  variable to the graph by constructing an instance of the class `Variable`.\n\n  The `Variable()` constructor requires an initial value for the variable,\n  which can be a `Tensor` of any type and shape. The initial value defines the\n  type and shape of the variable. After construction, the type and shape of\n  the variable are fixed. The value can be changed using one of the assign\n  methods.\n\n  If you want to change the shape of a variable later you have to use an\n  `assign` Op with `validate_shape=False`.\n\n  Just like any `Tensor`, variables created with `Variable()` can be used as\n  inputs for other Ops in the graph. Additionally, all the operators\n  overloaded for the `Tensor` class are carried over to variables, so you can\n  also add nodes to the graph by just doing arithmetic on variables.\n\n  ```python\n  import tensorflow as tf\n\n  # Create a variable.\n  w = tf.Variable(<initial-value>, name=<optional-name>)\n\n  # Use the variable in the graph like any Tensor.\n  y = tf.matmul(w, ...another variable or tensor...)\n\n  # The overloaded operators are available too.\n  z = tf.sigmoid(w + y)\n\n  # Assign a new value to the variable with `assign()` or a related method.\n  w.assign(w + 1.0)\n  w.assign_add(1.0)\n  ```\n\n  When you launch the graph, variables have to be explicitly initialized before\n  you can run Ops that use their value. You can initialize a variable by\n  running its *initializer op*, restoring the variable from a save file, or\n  simply running an `assign` Op that assigns a value to the variable. In fact,\n  the variable *initializer op* is just an `assign` Op that assigns the\n  variable's initial value to the variable itself.\n\n  ```python\n  # Launch the graph in a session.\n  with tf.Session() as sess:\n      # Run the variable initializer.\n      sess.run(w.initializer)\n      # ...you now can run ops that use the value of 'w'...\n  ```\n\n  The most common initialization pattern is to use the convenience function\n  `global_variables_initializer()` to add an Op to the graph that initializes\n  all the variables. You then run that Op after launching the graph.\n\n  ```python\n  # Add an Op to initialize global variables.\n  init_op = tf.global_variables_initializer()\n\n  # Launch the graph in a session.\n  with tf.Session() as sess:\n      # Run the Op that initializes global variables.\n      sess.run(init_op)\n      # ...you can now run any Op that uses variable values...\n  ```\n\n  If you need to create a variable with an initial value dependent on another\n  variable, use the other variable's `initialized_value()`. This ensures that\n  variables are initialized in the right order.\n\n  All variables are automatically collected in the graph where they are\n  created. By default, the constructor adds the new variable to the graph\n  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n  `global_variables()` returns the contents of that collection.\n\n  When building a machine learning model it is often convenient to distinguish\n  between variables holding the trainable model parameters and other variables\n  such as a `global step` variable used to count training steps. To make this\n  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n  `True`, the new variable is also added to the graph collection\n  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n  `trainable_variables()` returns the contents of this collection. The\n  various `Optimizer` classes use this collection as the default list of\n  variables to optimize.\n\n  WARNING: tf.Variable objects by default have a non-intuitive memory model. A\n  Variable is represented internally as a mutable Tensor which can\n  non-deterministically alias other Tensors in a graph. The set of operations\n  which consume a Variable and can lead to aliasing is undetermined and can\n  change across TensorFlow versions. Avoid writing code which relies on the\n  value of a Variable either changing or not changing as other operations\n  happen. For example, using Variable objects or simple functions thereof as\n  predicates in a `tf.cond` is dangerous and error-prone:\n\n  ```\n  v = tf.Variable(True)\n  tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.\n  ```\n\n  Here replacing adding `use_resource=True` when constructing the variable will\n  fix any nondeterminism issues:\n  ```\n  v = tf.Variable(True, use_resource=True)\n  tf.cond(v, lambda: v.assign(False), my_false_fn)\n  ```\n\n  To use the replacement for variables which does\n  not have these issues:\n\n  * Add `use_resource=True` when constructing `tf.Variable`;\n  * Call `tf.get_variable_scope().set_use_resource(True)` inside a\n    `tf.variable_scope` before the `tf.get_variable()` call.\n  ",
        "klass": "tensorflow.python.ops.variables.VariableV1",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for use when defining a Python op.\n\n  This context manager validates that the given `values` are from the\n  same graph, makes that graph the default graph, and pushes a\n  name scope in that graph (see\n  `tf.Graph.name_scope`\n  for more details on that).\n\n  For example, to define a new Python op called `my_op`:\n\n  ```python\n  def my_op(a, b, c, name=None):\n    with tf.name_scope(name, \"MyOp\", [a, b, c]) as scope:\n      a = tf.convert_to_tensor(a, name=\"a\")\n      b = tf.convert_to_tensor(b, name=\"b\")\n      c = tf.convert_to_tensor(c, name=\"c\")\n      # Define some computation that uses `a`, `b`, and `c`.\n      return foo_op(..., name=scope)\n  ```\n  ",
        "klass": "tensorflow.python.framework.ops.name_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for use when defining a Python op.\n\n  This context manager validates that the given `values` are from the\n  same graph, makes that graph the default graph, and pushes a\n  name scope in that graph (see\n  `tf.Graph.name_scope`\n  for more details on that).\n\n  For example, to define a new Python op called `my_op`:\n\n  ```python\n  def my_op(a, b, c, name=None):\n    with tf.name_scope(name, \"MyOp\", [a, b, c]) as scope:\n      a = tf.convert_to_tensor(a, name=\"a\")\n      b = tf.convert_to_tensor(b, name=\"b\")\n      c = tf.convert_to_tensor(c, name=\"c\")\n      # Define some computation that uses `a`, `b`, and `c`.\n      return foo_op(..., name=scope)\n  ```\n  ",
        "klass": "tensorflow.name_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for defining ops that creates variables (layers).\n\n  This context manager validates that the (optional) `values` are from the same\n  graph, ensures that graph is the default graph, and pushes a name scope and a\n  variable scope.\n\n  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,\n  then `default_name` is used.  In that case, if the same name has been\n  previously used in the same scope, it will be made unique by appending `_N`\n  to it.\n\n  Variable scope allows you to create new variables and to share already created\n  ones while providing checks to not create or share by accident. For details,\n  see the [Variable Scope How To](https://tensorflow.org/guide/variables), here\n  we present only a few basic examples.\n\n  Simple example of how to create a new variable:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      with tf.variable_scope(\"bar\"):\n          v = tf.get_variable(\"v\", [1])\n          assert v.name == \"foo/bar/v:0\"\n  ```\n\n  Simple example of how to reenter a premade variable scope safely:\n\n  ```python\n  with tf.variable_scope(\"foo\") as vs:\n    pass\n\n  # Re-enter the variable scope.\n  with tf.variable_scope(vs,\n                         auxiliary_name_scope=False) as vs1:\n    # Restore the original name_scope.\n    with tf.name_scope(vs1.original_name_scope):\n        v = tf.get_variable(\"v\", [1])\n        assert v.name == \"foo/v:0\"\n        c = tf.constant([1], name=\"c\")\n        assert c.name == \"foo/c:0\"\n  ```\n\n  Basic example of sharing a variable AUTO_REUSE:\n\n  ```python\n  def foo():\n    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n      v = tf.get_variable(\"v\", [1])\n    return v\n\n  v1 = foo()  # Creates v.\n  v2 = foo()  # Gets the same, existing v.\n  assert v1 == v2\n  ```\n\n  Basic example of sharing a variable with reuse=True:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n  with tf.variable_scope(\"foo\", reuse=True):\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  Sharing a variable by capturing a scope and setting reuse:\n\n  ```python\n  with tf.variable_scope(\"foo\") as scope:\n      v = tf.get_variable(\"v\", [1])\n      scope.reuse_variables()\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  To prevent accidental sharing of variables, we raise an exception when getting\n  an existing variable in a non-reusing scope.\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n      v1 = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v already exists ...\").\n  ```\n\n  Similarly, we raise an exception when trying to get a variable that does not\n  exist in reuse mode.\n\n  ```python\n  with tf.variable_scope(\"foo\", reuse=True):\n      v = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v does not exists ...\").\n  ```\n\n  Note that the `reuse` flag is inherited: if we open a reusing scope, then all\n  its sub-scopes become reusing as well.\n\n  A note about name scoping: Setting `reuse` does not impact the naming of other\n  ops such as mult. See related discussion on\n  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n\n  Note that up to and including version 1.0, it was allowed (though explicitly\n  discouraged) to pass False to the reuse argument, yielding undocumented\n  behaviour slightly different from None. Starting at 1.1.0 passing None and\n  False as reuse has exactly the same effect.\n\n  A note about using variable scopes in multi-threaded environment: Variable\n  scopes are thread local, so one thread will not see another thread's current\n  scope. Also, when using `default_name`, unique scopes names are also generated\n  only on a per thread basis. If the same name was used within a different\n  thread, that doesn't prevent a new thread from creating the same scope.\n  However, the underlying variable store is shared across threads (within the\n  same graph). As such, if another thread tries to create a new variable with\n  the same name as a variable created by a previous thread, it will fail unless\n  reuse is True.\n\n  Further, each thread starts with an empty variable scope. So if you wish to\n  preserve name prefixes from a scope from the main thread, you should capture\n  the main thread's scope and re-enter it in each thread. For e.g.\n\n  ```\n  main_thread_scope = variable_scope.get_variable_scope()\n\n  # Thread's target function:\n  def thread_target_fn(captured_scope):\n    with variable_scope.variable_scope(captured_scope):\n      # .... regular code for this thread\n\n\n  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))\n  ```\n  ",
        "klass": "tensorflow.python.ops.variable_scope.variable_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for defining ops that creates variables (layers).\n\n  This context manager validates that the (optional) `values` are from the same\n  graph, ensures that graph is the default graph, and pushes a name scope and a\n  variable scope.\n\n  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,\n  then `default_name` is used.  In that case, if the same name has been\n  previously used in the same scope, it will be made unique by appending `_N`\n  to it.\n\n  Variable scope allows you to create new variables and to share already created\n  ones while providing checks to not create or share by accident. For details,\n  see the [Variable Scope How To](https://tensorflow.org/guide/variables), here\n  we present only a few basic examples.\n\n  Simple example of how to create a new variable:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      with tf.variable_scope(\"bar\"):\n          v = tf.get_variable(\"v\", [1])\n          assert v.name == \"foo/bar/v:0\"\n  ```\n\n  Simple example of how to reenter a premade variable scope safely:\n\n  ```python\n  with tf.variable_scope(\"foo\") as vs:\n    pass\n\n  # Re-enter the variable scope.\n  with tf.variable_scope(vs,\n                         auxiliary_name_scope=False) as vs1:\n    # Restore the original name_scope.\n    with tf.name_scope(vs1.original_name_scope):\n        v = tf.get_variable(\"v\", [1])\n        assert v.name == \"foo/v:0\"\n        c = tf.constant([1], name=\"c\")\n        assert c.name == \"foo/c:0\"\n  ```\n\n  Basic example of sharing a variable AUTO_REUSE:\n\n  ```python\n  def foo():\n    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n      v = tf.get_variable(\"v\", [1])\n    return v\n\n  v1 = foo()  # Creates v.\n  v2 = foo()  # Gets the same, existing v.\n  assert v1 == v2\n  ```\n\n  Basic example of sharing a variable with reuse=True:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n  with tf.variable_scope(\"foo\", reuse=True):\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  Sharing a variable by capturing a scope and setting reuse:\n\n  ```python\n  with tf.variable_scope(\"foo\") as scope:\n      v = tf.get_variable(\"v\", [1])\n      scope.reuse_variables()\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  To prevent accidental sharing of variables, we raise an exception when getting\n  an existing variable in a non-reusing scope.\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n      v1 = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v already exists ...\").\n  ```\n\n  Similarly, we raise an exception when trying to get a variable that does not\n  exist in reuse mode.\n\n  ```python\n  with tf.variable_scope(\"foo\", reuse=True):\n      v = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v does not exists ...\").\n  ```\n\n  Note that the `reuse` flag is inherited: if we open a reusing scope, then all\n  its sub-scopes become reusing as well.\n\n  A note about name scoping: Setting `reuse` does not impact the naming of other\n  ops such as mult. See related discussion on\n  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n\n  Note that up to and including version 1.0, it was allowed (though explicitly\n  discouraged) to pass False to the reuse argument, yielding undocumented\n  behaviour slightly different from None. Starting at 1.1.0 passing None and\n  False as reuse has exactly the same effect.\n\n  A note about using variable scopes in multi-threaded environment: Variable\n  scopes are thread local, so one thread will not see another thread's current\n  scope. Also, when using `default_name`, unique scopes names are also generated\n  only on a per thread basis. If the same name was used within a different\n  thread, that doesn't prevent a new thread from creating the same scope.\n  However, the underlying variable store is shared across threads (within the\n  same graph). As such, if another thread tries to create a new variable with\n  the same name as a variable created by a previous thread, it will fail unless\n  reuse is True.\n\n  Further, each thread starts with an empty variable scope. So if you wish to\n  preserve name prefixes from a scope from the main thread, you should capture\n  the main thread's scope and re-enter it in each thread. For e.g.\n\n  ```\n  main_thread_scope = variable_scope.get_variable_scope()\n\n  # Thread's target function:\n  def thread_target_fn(captured_scope):\n    with variable_scope.variable_scope(captured_scope):\n      # .... regular code for this thread\n\n\n  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))\n  ```\n  ",
        "klass": "tensorflow.variable_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow DNN models with user-specified head.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = DNNEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=lambda: tf.AdamOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = DNNEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss and predicted output are determined by the specified head.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow_estimator.python.estimator.canned.dnn.DNNEstimator",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow Linear and DNN joined models with custom head.\n\n  Note: This estimator is also known as wide-n-deep.\n\n  Example:\n\n  ```python\n  numeric_feature = numeric_column(...)\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = DNNLinearCombinedEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      # wide settings\n      linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n      linear_optimizer=tf.train.FtrlOptimizer(...),\n      # deep settings\n      dnn_feature_columns=[\n          categorical_feature_a_emb, categorical_feature_b_emb,\n          numeric_feature],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.train.ProximalAdagradOptimizer(...))\n\n  # To apply L1 and L2 regularization, you can set dnn_optimizer to:\n  tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001,\n      l2_regularization_strength=0.001)\n  # To apply learning rate decay, you can set dnn_optimizer to a callable:\n  lambda: tf.AdamOptimizer(\n      learning_rate=tf.exponential_decay(\n          learning_rate=0.1,\n          global_step=tf.get_global_step(),\n          decay_steps=10000,\n          decay_rate=0.96)\n  # It is the same for linear_optimizer.\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow_estimator.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedEstimator",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow linear models with user-specified head.\n\n  Example:\n\n  ```python\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = LinearEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=lambda: tf.train.FtrlOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearEstimator(\n      head=tf.contrib.estimator.multi_label_head(n_classes=3),\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n      optimizer=tf.train.FtrlOptimizer(\n          learning_rate=0.1,\n          l1_regularization_strength=0.001\n      ))\n\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss and predicted output are determined by the specified head.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow_estimator.python.estimator.canned.linear.LinearEstimator",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "A classifier for TensorFlow DNN models.\n\n  Example:\n\n  ```python\n  categorical_feature_a = categorical_column_with_hash_bucket(...)\n  categorical_feature_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = DNNClassifier(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNClassifier(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = DNNClassifier(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=lambda: tf.AdamOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = DNNClassifier(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using softmax cross entropy.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.estimator.DNNClassifier",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "A regressor for TensorFlow DNN models.\n\n  Example:\n\n  ```python\n  categorical_feature_a = categorical_column_with_hash_bucket(...)\n  categorical_feature_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = DNNRegressor(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNRegressor(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = DNNRegressor(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=lambda: tf.AdamOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = DNNRegressor(\n      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.estimator.DNNRegressor",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.EstimatorV2"
        ],
        "class_docstring": "Estimator class to train and evaluate TensorFlow models.\n\n  The `Estimator` object wraps a model which is specified by a `model_fn`,\n  which, given inputs and a number of other parameters, returns the ops\n  necessary to perform training, evaluation, or predictions.\n\n  All outputs (checkpoints, event files, etc.) are written to `model_dir`, or a\n  subdirectory thereof. If `model_dir` is not set, a temporary directory is\n  used.\n\n  The `config` argument can be passed `tf.estimator.RunConfig` object containing\n  information about the execution environment. It is passed on to the\n  `model_fn`, if the `model_fn` has a parameter named \"config\" (and input\n  functions in the same manner). If the `config` parameter is not passed, it is\n  instantiated by the `Estimator`. Not passing config means that defaults useful\n  for local execution are used. `Estimator` makes config available to the model\n  (for instance, to allow specialization based on the number of workers\n  available), and also uses some of its fields to control internals, especially\n  regarding checkpointing.\n\n  The `params` argument contains hyperparameters. It is passed to the\n  `model_fn`, if the `model_fn` has a parameter named \"params\", and to the input\n  functions in the same manner. `Estimator` only passes params along, it does\n  not inspect it. The structure of `params` is therefore entirely up to the\n  developer.\n\n  None of `Estimator`'s methods can be overridden in subclasses (its\n  constructor enforces this). Subclasses should use `model_fn` to configure\n  the base class, and may add methods implementing specialized functionality.\n\n  @compatibility(eager)\n  Calling methods of `Estimator` will work while eager execution is enabled.\n  However, the `model_fn` and `input_fn` is not executed eagerly, `Estimator`\n  will switch to graph mode before calling all user-provided functions (incl.\n  hooks), so their code has to be compatible with graph mode execution. Note\n  that `input_fn` code using `tf.data` generally works in both graph and eager\n  modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.estimator.Estimator",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "Linear classifier model.\n\n  Train a linear model to classify instances into one of multiple possible\n  classes. When number of possible classes is 2, this is binary classification.\n\n  Example:\n\n  ```python\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=lambda: tf.train.FtrlOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using softmax cross entropy.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.estimator.LinearClassifier",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow Linear regression problems.\n\n  Train a linear regression model to predict label value given observation of\n  feature values.\n\n  Example:\n\n  ```python\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=lambda: tf.train.FtrlOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n    otherwise there will be a KeyError:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.estimator.LinearRegressor",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class specifies the configurations for an `Estimator` run.",
        "klass": "tensorflow.estimator.RunConfig",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "Mirrors vars to distribute across multiple devices and machines.\n\n  This strategy uses one replica per device and sync replication for its\n  multi-GPU version.\n\n  The multi-worker version will be added in the fture.\n\n  Args:\n    devices: a list of device strings.\n    cross_device_ops: optional, a descedant of `CrossDeviceOps`. If this is not\n      set, nccl will be use by default.\n  ",
        "klass": "tensorflow.python.distribute.mirrored_strategy.MirroredStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Bernoulli distribution.\n\n  The Bernoulli distribution with `probs` parameter, i.e., the probability of a\n  `1` outcome (vs a `0` outcome).\n  ",
        "klass": "tensorflow.python.ops.distributions.bernoulli.Bernoulli",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Beta distribution.\n\n  The Beta distribution is defined over the `(0, 1)` interval using parameters\n  `concentration1` (aka \"alpha\") and `concentration0` (aka \"beta\").\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; alpha, beta) = x**(alpha - 1) (1 - x)**(beta - 1) / Z\n  Z = Gamma(alpha) Gamma(beta) / Gamma(alpha + beta)\n  ```\n\n  where:\n\n  * `concentration1 = alpha`,\n  * `concentration0 = beta`,\n  * `Z` is the normalization constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The concentration parameters represent mean total counts of a `1` or a `0`,\n  i.e.,\n\n  ```none\n  concentration1 = alpha = mean * total_concentration\n  concentration0 = beta  = (1. - mean) * total_concentration\n  ```\n\n  where `mean` in `(0, 1)` and `total_concentration` is a positive real number\n  representing a mean `total_count = concentration1 + concentration0`.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  Warning: The samples can be zero due to finite precision.\n  This happens more often when some of the concentrations are very small.\n  Make sure to round the samples to `np.finfo(dtype).tiny` before computing the\n  density.\n\n  Samples of this distribution are reparameterized (pathwise differentiable).\n  The derivatives are computed using the approach described in the paper\n\n  [Michael Figurnov, Shakir Mohamed, Andriy Mnih.\n  Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Create a batch of three Beta distributions.\n  alpha = [1, 2, 3]\n  beta = [1, 2, 3]\n  dist = tfd.Beta(alpha, beta)\n\n  dist.sample([4, 5])  # Shape [4, 5, 3]\n\n  # `x` has three batch entries, each with two samples.\n  x = [[.1, .4, .5],\n       [.2, .3, .5]]\n  # Calculate the probability of each pair of samples under the corresponding\n  # distribution in `dist`.\n  dist.prob(x)         # Shape [2, 3]\n  ```\n\n  ```python\n  # Create batch_shape=[2, 3] via parameter broadcast:\n  alpha = [[1.], [2]]      # Shape [2, 1]\n  beta = [3., 4, 5]        # Shape [3]\n  dist = tfd.Beta(alpha, beta)\n\n  # alpha broadcast as: [[1., 1, 1,],\n  #                      [2, 2, 2]]\n  # beta broadcast as:  [[3., 4, 5],\n  #                      [3, 4, 5]]\n  # batch_Shape [2, 3]\n  dist.sample([4, 5])  # Shape [4, 5, 2, 3]\n\n  x = [.2, .3, .5]\n  # x will be broadcast as [[.2, .3, .5],\n  #                         [.2, .3, .5]],\n  # thus matching batch_shape [2, 3].\n  dist.prob(x)         # Shape [2, 3]\n  ```\n\n  Compute the gradients of samples w.r.t. the parameters:\n\n  ```python\n  alpha = tf.constant(1.0)\n  beta = tf.constant(2.0)\n  dist = tfd.Beta(alpha, beta)\n  samples = dist.sample(5)  # Shape [5]\n  loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n  # Unbiased stochastic gradients of the loss function\n  grads = tf.gradients(loss, [alpha, beta])\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.beta.Beta",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Categorical distribution.\n\n  The Categorical distribution is parameterized by either probabilities or\n  log-probabilities of a set of `K` classes. It is defined over the integers\n  `{0, 1, ..., K}`.\n\n  The Categorical distribution is closely related to the `OneHotCategorical` and\n  `Multinomial` distributions.  The Categorical distribution can be intuited as\n  generating samples according to `argmax{ OneHotCategorical(probs) }` itself\n  being identical to `argmax{ Multinomial(probs, total_count=1) }`.\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; pi) = prod_j pi_j**[k == j]\n  ```\n\n  #### Pitfalls\n\n  The number of classes, `K`, must not exceed:\n  - the largest integer representable by `self.dtype`, i.e.,\n    `2**(mantissa_bits+1)` (IEEE 754),\n  - the maximum `Tensor` index, i.e., `2**31-1`.\n\n  In other words,\n\n  ```python\n  K <= min(2**31-1, {\n    tf.float16: 2**11,\n    tf.float32: 2**24,\n    tf.float64: 2**53 }[param.dtype])\n  ```\n\n  Note: This condition is validated only when `self.validate_args = True`.\n\n  #### Examples\n\n  Creates a 3-class distribution with the 2nd class being most likely.\n\n  ```python\n  dist = Categorical(probs=[0.1, 0.5, 0.4])\n  n = 1e4\n  empirical_prob = tf.cast(\n      tf.histogram_fixed_width(\n        dist.sample(int(n)),\n        [0., 2],\n        nbins=3),\n      dtype=tf.float32) / n\n  # ==> array([ 0.1005,  0.5037,  0.3958], dtype=float32)\n  ```\n\n  Creates a 3-class distribution with the 2nd class being most likely.\n  Parameterized by [logits](https://en.wikipedia.org/wiki/Logit) rather than\n  probabilities.\n\n  ```python\n  dist = Categorical(logits=np.log([0.1, 0.5, 0.4])\n  n = 1e4\n  empirical_prob = tf.cast(\n      tf.histogram_fixed_width(\n        dist.sample(int(n)),\n        [0., 2],\n        nbins=3),\n      dtype=tf.float32) / n\n  # ==> array([0.1045,  0.5047, 0.3908], dtype=float32)\n  ```\n\n  Creates a 3-class distribution with the 3rd class being most likely.\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts is a scalar.\n  p = [0.1, 0.4, 0.5]\n  dist = Categorical(probs=p)\n  dist.prob(0)  # Shape []\n\n  # p will be broadcast to [[0.1, 0.4, 0.5], [0.1, 0.4, 0.5]] to match counts.\n  counts = [1, 0]\n  dist.prob(counts)  # Shape [2]\n\n  # p will be broadcast to shape [3, 5, 7, 3] to match counts.\n  counts = [[...]] # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7, 3]\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.categorical.Categorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Dirichlet distribution.\n\n  The Dirichlet distribution is defined over the\n  [`(k-1)`-simplex](https://en.wikipedia.org/wiki/Simplex) using a positive,\n  length-`k` vector `concentration` (`k > 1`). The Dirichlet is identically the\n  Beta distribution when `k = 2`.\n\n  #### Mathematical Details\n\n  The Dirichlet is a distribution over the open `(k-1)`-simplex, i.e.,\n\n  ```none\n  S^{k-1} = { (x_0, ..., x_{k-1}) in R^k : sum_j x_j = 1 and all_j x_j > 0 }.\n  ```\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; alpha) = prod_j x_j**(alpha_j - 1) / Z\n  Z = prod_j Gamma(alpha_j) / Gamma(sum_j alpha_j)\n  ```\n\n  where:\n\n  * `x in S^{k-1}`, i.e., the `(k-1)`-simplex,\n  * `concentration = alpha = [alpha_0, ..., alpha_{k-1}]`, `alpha_j > 0`,\n  * `Z` is the normalization constant aka the [multivariate beta function](\n    https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function),\n    and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The `concentration` represents mean total counts of class occurrence, i.e.,\n\n  ```none\n  concentration = alpha = mean * total_concentration\n  ```\n\n  where `mean` in `S^{k-1}` and `total_concentration` is a positive real number\n  representing a mean total count.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  Warning: Some components of the samples can be zero due to finite precision.\n  This happens more often when some of the concentrations are very small.\n  Make sure to round the samples to `np.finfo(dtype).tiny` before computing the\n  density.\n\n  Samples of this distribution are reparameterized (pathwise differentiable).\n  The derivatives are computed using the approach described in the paper\n\n  [Michael Figurnov, Shakir Mohamed, Andriy Mnih.\n  Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Create a single trivariate Dirichlet, with the 3rd class being three times\n  # more frequent than the first. I.e., batch_shape=[], event_shape=[3].\n  alpha = [1., 2, 3]\n  dist = tfd.Dirichlet(alpha)\n\n  dist.sample([4, 5])  # shape: [4, 5, 3]\n\n  # x has one sample, one batch, three classes:\n  x = [.2, .3, .5]   # shape: [3]\n  dist.prob(x)       # shape: []\n\n  # x has two samples from one batch:\n  x = [[.1, .4, .5],\n       [.2, .3, .5]]\n  dist.prob(x)         # shape: [2]\n\n  # alpha will be broadcast to shape [5, 7, 3] to match x.\n  x = [[...]]   # shape: [5, 7, 3]\n  dist.prob(x)  # shape: [5, 7]\n  ```\n\n  ```python\n  # Create batch_shape=[2], event_shape=[3]:\n  alpha = [[1., 2, 3],\n           [4, 5, 6]]   # shape: [2, 3]\n  dist = tfd.Dirichlet(alpha)\n\n  dist.sample([4, 5])  # shape: [4, 5, 2, 3]\n\n  x = [.2, .3, .5]\n  # x will be broadcast as [[.2, .3, .5],\n  #                         [.2, .3, .5]],\n  # thus matching batch_shape [2, 3].\n  dist.prob(x)         # shape: [2]\n  ```\n\n  Compute the gradients of samples w.r.t. the parameters:\n\n  ```python\n  alpha = tf.constant([1.0, 2.0, 3.0])\n  dist = tfd.Dirichlet(alpha)\n  samples = dist.sample(5)  # Shape [5, 3]\n  loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n  # Unbiased stochastic gradients of the loss function\n  grads = tf.gradients(loss, alpha)\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.dirichlet.Dirichlet",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.gamma.Gamma"
        ],
        "class_docstring": "Exponential distribution.\n\n  The Exponential distribution is parameterized by an event `rate` parameter.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; lambda, x > 0) = exp(-lambda x) / Z\n  Z = 1 / lambda\n  ```\n\n  where `rate = lambda` and `Z` is the normalizaing constant.\n\n  The Exponential distribution is a special case of the Gamma distribution,\n  i.e.,\n\n  ```python\n  Exponential(rate) = Gamma(concentration=1., rate)\n  ```\n\n  The Exponential distribution uses a `rate` parameter, or \"inverse scale\",\n  which can be intuited as,\n\n  ```none\n  X ~ Exponential(rate=1)\n  Y = X / rate\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.exponential.Exponential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Multinomial distribution.\n\n  This Multinomial distribution is parameterized by `probs`, a (batch of)\n  length-`K` `prob` (probability) vectors (`K > 1`) such that\n  `tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\n  the number of trials per draw from the Multinomial. It is defined over a\n  (batch of) length-`K` vector `counts` such that\n  `tf.reduce_sum(counts, -1) = total_count`. The Multinomial is identically the\n  Binomial distribution when `K = 2`.\n\n  #### Mathematical Details\n\n  The Multinomial is a distribution over `K`-class counts, i.e., a length-`K`\n  vector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(n; pi, N) = prod_j (pi_j)**n_j / Z\n  Z = (prod_j n_j!) / N!\n  ```\n\n  where:\n  * `probs = pi = [pi_0, ..., pi_{K-1}]`, `pi_j > 0`, `sum_j pi_j = 1`,\n  * `total_count = N`, `N` a positive integer,\n  * `Z` is the normalization constant, and,\n  * `N!` denotes `N` factorial.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  #### Pitfalls\n\n  The number of classes, `K`, must not exceed:\n  - the largest integer representable by `self.dtype`, i.e.,\n    `2**(mantissa_bits+1)` (IEE754),\n  - the maximum `Tensor` index, i.e., `2**31-1`.\n\n  In other words,\n\n  ```python\n  K <= min(2**31-1, {\n    tf.float16: 2**11,\n    tf.float32: 2**24,\n    tf.float64: 2**53 }[param.dtype])\n  ```\n\n  Note: This condition is validated only when `self.validate_args = True`.\n\n  #### Examples\n\n  Create a 3-class distribution, with the 3rd class is most likely to be drawn,\n  using logits.\n\n  ```python\n  logits = [-50., -43, 0]\n  dist = Multinomial(total_count=4., logits=logits)\n  ```\n\n  Create a 3-class distribution, with the 3rd class is most likely to be drawn.\n\n  ```python\n  p = [.2, .3, .5]\n  dist = Multinomial(total_count=4., probs=p)\n  ```\n\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts same shape as p.\n  counts = [1., 0, 3]\n  dist.prob(counts)  # Shape []\n\n  # p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\n  counts = [[1., 2, 1], [2, 2, 0]]\n  dist.prob(counts)  # Shape [2]\n\n  # p will be broadcast to shape [5, 7, 3] to match counts.\n  counts = [[...]]  # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7]\n  ```\n\n  Create a 2-batch of 3-class distributions.\n\n  ```python\n  p = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\n  dist = Multinomial(total_count=[4., 5], probs=p)\n\n  counts = [[2., 1, 1], [3, 1, 1]]\n  dist.prob(counts)  # Shape [2]\n\n  dist.sample(5) # Shape [5, 2, 3]\n  ```\n  ",
        "klass": "tensorflow.python.ops.distributions.multinomial.Multinomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "The Normal distribution with location `loc` and `scale` parameters.\n\n  #### Mathematical details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; mu, sigma) = exp(-0.5 (x - mu)**2 / sigma**2) / Z\n  Z = (2 pi sigma**2)**0.5\n  ```\n\n  where `loc = mu` is the mean, `scale = sigma` is the std. deviation, and, `Z`\n  is the normalization constant.\n\n  The Normal distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ Normal(loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  #### Examples\n\n  Examples of initialization of one or a batch of distributions.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Define a single scalar Normal distribution.\n  dist = tfd.Normal(loc=0., scale=3.)\n\n  # Evaluate the cdf at 1, returning a scalar.\n  dist.cdf(1.)\n\n  # Define a batch of two scalar valued Normals.\n  # The first has mean 1 and standard deviation 11, the second 2 and 22.\n  dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n\n  # Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n  # returning a length two tensor.\n  dist.prob([0, 1.5])\n\n  # Get 3 samples, returning a 3 x 2 tensor.\n  dist.sample([3])\n  ```\n\n  Arguments are broadcast when possible.\n\n  ```python\n  # Define a batch of two scalar valued Normals.\n  # Both have mean 1, but different standard deviations.\n  dist = tfd.Normal(loc=1., scale=[11, 22.])\n\n  # Evaluate the pdf of both distributions on the same point, 3.0,\n  # returning a length 2 tensor.\n  dist.prob(3.0)\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.normal.Normal",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Uniform distribution with `low` and `high` parameters.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; a, b) = I[a <= x < b] / Z\n  Z = b - a\n  ```\n\n  where\n\n  - `low = a`,\n  - `high = b`,\n  - `Z` is the normalizing constant, and\n  - `I[predicate]` is the [indicator function](\n    https://en.wikipedia.org/wiki/Indicator_function) for `predicate`.\n\n  The parameters `low` and `high` must be shaped in a way that supports\n  broadcasting (e.g., `high - low` is a valid operation).\n\n  #### Examples\n\n  ```python\n  # Without broadcasting:\n  u1 = Uniform(low=3.0, high=4.0)  # a single uniform distribution [3, 4]\n  u2 = Uniform(low=[1.0, 2.0],\n               high=[3.0, 4.0])  # 2 distributions [1, 3], [2, 4]\n  u3 = Uniform(low=[[1.0, 2.0],\n                    [3.0, 4.0]],\n               high=[[1.5, 2.5],\n                     [3.5, 4.5]])  # 4 distributions\n  ```\n\n  ```python\n  # With broadcasting:\n  u1 = Uniform(low=3.0, high=[5.0, 6.0, 7.0])  # 3 distributions\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.uniform.Uniform",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.lib.io.file_io.FileIO"
        ],
        "class_docstring": "File I/O wrappers without thread locking.\n\n  Note, that this  is somewhat like builtin Python  file I/O, but\n  there are  semantic differences to  make it more  efficient for\n  some backing filesystems.  For example, a write  mode file will\n  not  be opened  until the  first  write call  (to minimize  RPC\n  invocations in network filesystems).\n  ",
        "klass": "tensorflow.python.platform.gfile.FastGFile",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.lib.io.file_io.FileIO"
        ],
        "class_docstring": "File I/O wrappers without thread locking.\n\n  Note, that this  is somewhat like builtin Python  file I/O, but\n  there are  semantic differences to  make it more  efficient for\n  some backing filesystems.  For example, a write  mode file will\n  not  be opened  until the  first  write call  (to minimize  RPC\n  invocations in network filesystems).\n  ",
        "klass": "tensorflow.python.platform.gfile.GFile",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to write records to a TFRecords file.\n\n  This class implements `__enter__` and `__exit__`, and can be used\n  in `with` blocks like a normal file.\n  ",
        "klass": "tensorflow.python.lib.io.tf_record.TFRecordWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.network.Network"
        ],
        "class_docstring": "`Model` groups layers into an object with training and inference features.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n  ",
        "klass": "tensorflow.python.keras.engine.training.Model",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Linear stack of layers.\n\n  Arguments:\n      layers: list of layers to add to the model.\n\n  Example:\n\n  ```python\n  # Optionally, the first layer can receive an `input_shape` argument:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  # Afterwards, we do automatic shape inference:\n  model.add(Dense(32))\n\n  # This is identical to the following:\n  model = Sequential()\n  model.add(Dense(32, input_dim=500))\n\n  # And to the following:\n  model = Sequential()\n  model.add(Dense(32, batch_input_shape=(None, 500)))\n\n  # Note that you can also omit the `input_shape` argument:\n  # In that case the model gets built the first time you call `fit` (or other\n  # training and evaluation methods).\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.compile(optimizer=optimizer, loss=loss)\n  # This builds the model for the first time:\n  model.fit(x, y, batch_size=32, epochs=10)\n\n  # Note that when using this delayed-build pattern (no input shape specified),\n  # the model doesn't have any weights until the first call\n  # to a training/evaluation method (since it isn't yet built):\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.weights  # returns []\n\n  # Whereas if you specify the input shape, the model gets built continuously\n  # as you are adding layers:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  model.add(Dense(32))\n  model.weights  # returns list of length 4\n\n  When using the delayed-build pattern (no input shape specified), you can\n  choose to manually build your model by calling `build(batch_input_shape)`:\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.build((None, 500))\n  model.weights  # returns list of length 4\n  ```\n  ",
        "klass": "tensorflow.python.keras.engine.sequential.Sequential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.MeanMetricWrapper"
        ],
        "class_docstring": "Calculates how often predictions matches labels.\n\n  For example, if `y_true` is [1, 2, 3, 4] and `y_pred` is [0, 2, 3, 4]\n  then the accuracy is 3/4 or .75.  If the weights were specified as\n  [1, 1, 0, 0] then the accuracy would be 1/2 or .5.\n\n  This metric creates two local variables, `total` and `count` that are used to\n  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n  ultimately returned as `binary accuracy`: an idempotent operation that simply\n  divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.Accuracy()\n  m.update_state([1, 2, 3, 4], [0, 2, 3, 4])\n  print('Final result: ', m.result().numpy())  # Final result: 0.75\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.Accuracy()])\n  ```\n  ",
        "klass": "tensorflow.python.keras.metrics.Accuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n\n  Code within a `with` statement will be able to access custom objects\n  by name. Changes to global custom objects persist\n  within the enclosing `with` statement. At end of the `with` statement,\n  global custom objects are reverted to state\n  at beginning of the `with` statement.\n\n  Example:\n\n  Consider a custom object `MyObject` (e.g. a class):\n\n  ```python\n      with CustomObjectScope({'MyObject':MyObject}):\n          layer = Dense(..., kernel_regularizer='MyObject')\n          # save, load, etc. will recognize custom object by name\n  ```\n  ",
        "klass": "tensorflow.python.keras.utils.generic_utils.CustomObjectScope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.utils.data_utils.SequenceEnqueuer"
        ],
        "class_docstring": "Builds a queue out of a data generator.\n\n  The provided generator can be finite in which case the class will throw\n  a `StopIteration` exception.\n\n  Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n\n  Arguments:\n      generator: a generator function which yields data\n      use_multiprocessing: use multiprocessing if True, otherwise threading\n      wait_time: time to sleep in-between calls to `put()`\n      random_seed: Initial seed for workers,\n          will be incremented by one for each worker.\n  ",
        "klass": "tensorflow.python.keras.utils.data_utils.GeneratorEnqueuer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.utils.data_utils.SequenceEnqueuer"
        ],
        "class_docstring": "Builds a Enqueuer from a Sequence.\n\n  Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n\n  Arguments:\n      sequence: A `tf.keras.utils.data_utils.Sequence` object.\n      use_multiprocessing: use multiprocessing if True, otherwise threading\n      shuffle: whether to shuffle the data at the beginning of each epoch\n  ",
        "klass": "tensorflow.python.keras.utils.data_utils.OrderedEnqueuer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.wrappers.scikit_learn.BaseWrapper"
        ],
        "class_docstring": "Implementation of the scikit-learn classifier API for Keras.\n  ",
        "klass": "tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.wrappers.scikit_learn.BaseWrapper"
        ],
        "class_docstring": "Implementation of the scikit-learn regressor API for Keras.\n  ",
        "klass": "tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.AveragePooling1D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Average Pooling layer for 1D inputs.\n\n  Arguments:\n    pool_size: An integer or tuple/list of a single integer,\n      representing the size of the pooling window.\n    strides: An integer or tuple/list of a single integer, specifying the\n      strides of the pooling operation.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs.\n      `channels_last` corresponds to inputs with shape\n      `(batch, length, channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, length)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.AveragePooling1D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.AveragePooling2D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Average pooling layer for 2D inputs (e.g. images).\n\n  Arguments:\n    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n      specifying the size of the pooling window.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n      specifying the strides of the pooling operation.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string. The ordering of the dimensions in the inputs.\n      `channels_last` (default) and `channels_first` are supported.\n      `channels_last` corresponds to inputs with shape\n      `(batch, height, width, channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, height, width)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.AveragePooling2D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.AveragePooling3D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Average pooling layer for 3D inputs (e.g. volumes).\n\n  Arguments:\n    pool_size: An integer or tuple/list of 3 integers:\n      (pool_depth, pool_height, pool_width)\n      specifying the size of the pooling window.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n      specifying the strides of the pooling operation.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string. The ordering of the dimensions in the inputs.\n      `channels_last` (default) and `channels_first` are supported.\n      `channels_last` corresponds to inputs with shape\n      `(batch, depth, height, width, channels)` while `channels_first`\n      corresponds to inputs with shape\n      `(batch, channels, depth, height, width)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.AveragePooling3D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.core.Dense",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Densely-connected layer class.\n\n  This layer implements the operation:\n  `outputs = activation(inputs * kernel + bias)`\n  Where `activation` is the activation function passed as the `activation`\n  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n  and `bias` is a bias vector created by the layer\n  (only if `use_bias` is `True`).\n\n  Arguments:\n    units: Integer or Long, dimensionality of the output space.\n    activation: Activation function (callable). Set it to None to maintain a\n      linear activation.\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the default\n      initializer used by `tf.get_variable`.\n    bias_initializer: Initializer function for the bias.\n    kernel_regularizer: Regularizer function for the weight matrix.\n    bias_regularizer: Regularizer function for the bias.\n    activity_regularizer: Regularizer function for the output.\n    kernel_constraint: An optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The function\n        must take as input the unprojected variable and must return the\n        projected variable (which must have the same shape). Constraints are\n        not safe to use when doing asynchronous distributed training.\n    bias_constraint: An optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such cases.\n    reuse: Boolean, whether to reuse the weights of a previous layer\n      by the same name.\n\n  Properties:\n    units: Python integer, dimensionality of the output space.\n    activation: Activation function (callable).\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer instance (or name) for the kernel matrix.\n    bias_initializer: Initializer instance (or name) for the bias.\n    kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n    bias_regularizer: Regularizer instance for the bias (callable).\n    activity_regularizer: Regularizer instance for the output (callable)\n    kernel_constraint: Constraint function for the kernel matrix.\n    bias_constraint: Constraint function for the bias.\n    kernel: Weight matrix (TensorFlow variable or tensor).\n    bias: Bias vector, if applicable (TensorFlow variable or tensor).\n  ",
        "klass": "tensorflow.python.layers.core.Dense",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.MaxPooling1D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Max Pooling layer for 1D inputs.\n\n  Arguments:\n    pool_size: An integer or tuple/list of a single integer,\n      representing the size of the pooling window.\n    strides: An integer or tuple/list of a single integer, specifying the\n      strides of the pooling operation.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs.\n      `channels_last` corresponds to inputs with shape\n      `(batch, length, channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, length)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.MaxPooling1D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.MaxPooling2D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Max pooling layer for 2D inputs (e.g. images).\n\n  Arguments:\n    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n      specifying the size of the pooling window.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    strides: An integer or tuple/list of 2 integers,\n      specifying the strides of the pooling operation.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string. The ordering of the dimensions in the inputs.\n      `channels_last` (default) and `channels_first` are supported.\n      `channels_last` corresponds to inputs with shape\n      `(batch, height, width, channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, height, width)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.MaxPooling2D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.pooling.MaxPooling3D",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Max pooling layer for 3D inputs (e.g. volumes).\n\n  Arguments:\n    pool_size: An integer or tuple/list of 3 integers:\n      (pool_depth, pool_height, pool_width)\n      specifying the size of the pooling window.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    strides: An integer or tuple/list of 3 integers,\n      specifying the strides of the pooling operation.\n      Can be a single integer to specify the same value for\n      all spatial dimensions.\n    padding: A string. The padding method, either 'valid' or 'same'.\n      Case-insensitive.\n    data_format: A string. The ordering of the dimensions in the inputs.\n      `channels_last` (default) and `channels_first` are supported.\n      `channels_last` corresponds to inputs with shape\n      `(batch, depth, height, width, channels)` while `channels_first`\n      corresponds to inputs with shape\n      `(batch, channels, depth, height, width)`.\n    name: A string, the name of the layer.\n  ",
        "klass": "tensorflow.python.layers.pooling.MaxPooling3D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.summary.writer.writer.SummaryToEventTransformer"
        ],
        "class_docstring": "Writes `Summary` protocol buffers to event files.\n\n  The `FileWriter` class provides a mechanism to create an event file in a\n  given directory and add summaries and events to it. The class updates the\n  file contents asynchronously. This allows a training program to call methods\n  to add data to the file directly from the training loop, without slowing down\n  training.\n\n  When constructed with a `tf.Session` parameter, a `FileWriter` instead forms\n  a compatibility layer over new graph-based summaries (`tf.contrib.summary`)\n  to facilitate the use of new summary writing with pre-existing code that\n  expects a `FileWriter` instance.\n  ",
        "klass": "tensorflow.python.summary.writer.writer.FileWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adadelta algorithm.\n\n  See [M. D. Zeiler](http://arxiv.org/abs/1212.5701)\n  ([pdf](http://arxiv.org/pdf/1212.5701v1.pdf))\n  ",
        "klass": "tensorflow.python.training.adadelta.AdadeltaOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Adagrad Dual Averaging algorithm for sparse linear models.\n\n  See this [paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).\n\n  This optimizer takes care of regularization of unseen features in a mini batch\n  by updating them when they are seen with a closed form update rule that is\n  equivalent to having updated them on every mini-batch.\n\n  AdagradDA is typically used when there is a need for large sparsity in the\n  trained model. This optimizer only guarantees sparsity for linear models. Be\n  careful when using AdagradDA for deep networks as it will require careful\n  initialization of the gradient accumulators for it to train.\n  ",
        "klass": "tensorflow.python.training.adagrad_da.AdagradDAOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adagrad algorithm.\n\n  See this [paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n  or this\n  [intro](https://ppasupat.github.io/a9online/uploads/proximal_notes.pdf).\n  ",
        "klass": "tensorflow.python.training.adagrad.AdagradOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adam algorithm.\n\n  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n  ",
        "klass": "tensorflow.python.training.adam.AdamOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.checkpointable.tracking.Checkpointable"
        ],
        "class_docstring": "Groups checkpointable objects, saving and restoring them.\n\n  `Checkpoint`'s constructor accepts keyword arguments whose values are types\n  that contain checkpointable state, such as `tf.train.Optimizer`\n  implementations, `tf.Variable`, `tf.keras.Layer` implementations, or\n  `tf.keras.Model` implementations. It saves these values with a checkpoint, and\n  maintains a `save_counter` for numbering checkpoints.\n\n  Example usage when graph building:\n\n  ```python\n  import tensorflow as tf\n  import os\n\n  checkpoint_directory = \"/tmp/training_checkpoints\"\n  checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n  status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n  train_op = optimizer.minimize( ... )\n  status.assert_consumed()  # Optional sanity checks.\n  with tf.Session() as session:\n    # Use the Session to restore variables, or initialize them if\n    # tf.train.latest_checkpoint returned None.\n    status.initialize_or_restore(session)\n    for _ in range(num_training_steps):\n      session.run(train_op)\n    checkpoint.save(file_prefix=checkpoint_prefix)\n  ```\n\n  Example usage with eager execution enabled:\n\n  ```python\n  import tensorflow as tf\n  import os\n\n  tf.enable_eager_execution()\n\n  checkpoint_directory = \"/tmp/training_checkpoints\"\n  checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n  status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n  for _ in range(num_training_steps):\n    optimizer.minimize( ... )  # Variables will be restored on creation.\n  status.assert_consumed()  # Optional sanity checks.\n  checkpoint.save(file_prefix=checkpoint_prefix)\n  ```\n\n  `Checkpoint.save` and `Checkpoint.restore` write and read object-based\n  checkpoints, in contrast to `tf.train.Saver` which writes and reads\n  `variable.name` based checkpoints. Object-based checkpointing saves a graph of\n  dependencies between Python objects (`Layer`s, `Optimizer`s, `Variable`s,\n  etc.) with named edges, and this graph is used to match variables when\n  restoring a checkpoint. It can be more robust to changes in the Python\n  program, and helps to support restore-on-create for variables when executing\n  eagerly. Prefer `tf.train.Checkpoint` over `tf.train.Saver` for new code.\n\n  `Checkpoint` objects have dependencies on the objects passed as keyword\n  arguments to their constructors, and each dependency is given a name that is\n  identical to the name of the keyword argument for which it was created.\n  TensorFlow classes like `Layer`s and `Optimizer`s will automatically add\n  dependencies on their variables (e.g. \"kernel\" and \"bias\" for\n  `tf.keras.layers.Dense`). Inheriting from `tf.keras.Model` makes managing\n  dependencies easy in user-defined classes, since `Model` hooks into attribute\n  assignment. For example:\n\n  ```python\n  class Regress(tf.keras.Model):\n\n    def __init__(self):\n      super(Regress, self).__init__()\n      self.input_transform = tf.keras.layers.Dense(10)\n      # ...\n\n    def call(self, inputs):\n      x = self.input_transform(inputs)\n      # ...\n  ```\n\n  This `Model` has a dependency named \"input_transform\" on its `Dense` layer,\n  which in turn depends on its variables. As a result, saving an instance of\n  `Regress` using `tf.train.Checkpoint` will also save all the variables created\n  by the `Dense` layer.\n\n  Attributes:\n    save_counter: Incremented when `save()` is called. Used to number\n      checkpoints.\n  ",
        "klass": "tensorflow.python.training.checkpointable.util.Checkpoint",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a cluster as a set of \"tasks\", organized into \"jobs\".\n\n  A `tf.train.ClusterSpec` represents the set of processes that\n  participate in a distributed TensorFlow computation. Every\n  `tf.train.Server` is constructed in a particular cluster.\n\n  To create a cluster with two jobs and five tasks, you specify the\n  mapping from job names to lists of network addresses (typically\n  hostname-port pairs).\n\n  ```python\n  cluster = tf.train.ClusterSpec({\"worker\": [\"worker0.example.com:2222\",\n                                             \"worker1.example.com:2222\",\n                                             \"worker2.example.com:2222\"],\n                                  \"ps\": [\"ps0.example.com:2222\",\n                                         \"ps1.example.com:2222\"]})\n  ```\n\n  Each job may also be specified as a sparse mapping from task indices\n  to network addresses. This enables a server to be configured without\n  needing to know the identity of (for example) all other worker\n  tasks:\n\n  ```python\n  cluster = tf.train.ClusterSpec({\"worker\": {1: \"worker1.example.com:2222\"},\n                                  \"ps\": [\"ps0.example.com:2222\",\n                                         \"ps1.example.com:2222\"]})\n  ```\n  ",
        "klass": "tensorflow.python.training.server_lib.ClusterSpec",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A coordinator for threads.\n\n  This class implements a simple mechanism to coordinate the termination of a\n  set of threads.\n\n  #### Usage:\n\n  ```python\n  # Create a coordinator.\n  coord = Coordinator()\n  # Start a number of threads, passing the coordinator to each of them.\n  ...start thread 1...(coord, ...)\n  ...start thread N...(coord, ...)\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  ```\n\n  Any of the threads can call `coord.request_stop()` to ask for all the threads\n  to stop.  To cooperate with the requests, each thread must check for\n  `coord.should_stop()` on a regular basis.  `coord.should_stop()` returns\n  `True` as soon as `coord.request_stop()` has been called.\n\n  A typical thread running with a coordinator will do something like:\n\n  ```python\n  while not coord.should_stop():\n    ...do some work...\n  ```\n\n  #### Exception handling:\n\n  A thread can report an exception to the coordinator as part of the\n  `request_stop()` call.  The exception will be re-raised from the\n  `coord.join()` call.\n\n  Thread code:\n\n  ```python\n  try:\n    while not coord.should_stop():\n      ...do some work...\n  except Exception as e:\n    coord.request_stop(e)\n  ```\n\n  Main code:\n\n  ```python\n  try:\n    ...\n    coord = Coordinator()\n    # Start a number of threads, passing the coordinator to each of them.\n    ...start thread 1...(coord, ...)\n    ...start thread N...(coord, ...)\n    # Wait for all the threads to terminate.\n    coord.join(threads)\n  except Exception as e:\n    ...exception that was passed to coord.request_stop()\n  ```\n\n  To simplify the thread implementation, the Coordinator provides a\n  context handler `stop_on_exception()` that automatically requests a stop if\n  an exception is raised.  Using the context handler the thread code above\n  can be written as:\n\n  ```python\n  with coord.stop_on_exception():\n    while not coord.should_stop():\n      ...do some work...\n  ```\n\n  #### Grace period for stopping:\n\n  After a thread has called `coord.request_stop()` the other threads have a\n  fixed time to stop, this is called the 'stop grace period' and defaults to 2\n  minutes.  If any of the threads is still alive after the grace period expires\n  `coord.join()` raises a RuntimeError reporting the laggards.\n\n  ```python\n  try:\n    ...\n    coord = Coordinator()\n    # Start a number of threads, passing the coordinator to each of them.\n    ...start thread 1...(coord, ...)\n    ...start thread N...(coord, ...)\n    # Wait for all the threads to terminate, give them 10s grace period\n    coord.join(threads, stop_grace_period_secs=10)\n  except RuntimeError:\n    ...one of the threads took more than 10s to stop after request_stop()\n    ...was called.\n  except Exception:\n    ...exception that was passed to coord.request_stop()\n  ```\n  ",
        "klass": "tensorflow.python.training.coordinator.Coordinator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Maintains moving averages of variables by employing an exponential decay.\n\n  When training a model, it is often beneficial to maintain moving averages of\n  the trained parameters.  Evaluations that use averaged parameters sometimes\n  produce significantly better results than the final trained values.\n\n  The `apply()` method adds shadow copies of trained variables and add ops that\n  maintain a moving average of the trained variables in their shadow copies.\n  It is used when building the training model.  The ops that maintain moving\n  averages are typically run after each training step.\n  The `average()` and `average_name()` methods give access to the shadow\n  variables and their names.  They are useful when building an evaluation\n  model, or when restoring a model from a checkpoint file.  They help use the\n  moving averages in place of the last trained values for evaluations.\n\n  The moving averages are computed using exponential decay.  You specify the\n  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n  variables are initialized with the same initial values as the trained\n  variables.  When you run the ops to maintain the moving averages, each\n  shadow variable is updated with the formula:\n\n    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n\n  This is mathematically equivalent to the classic formula below, but the use\n  of an `assign_sub` op (the `\"-=\"` in the formula) allows concurrent lockless\n  updates to the variables:\n\n    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n\n  Reasonable values for `decay` are close to 1.0, typically in the\n  multiple-nines range: 0.999, 0.9999, etc.\n\n  Example usage when creating a training model:\n\n  ```python\n  # Create variables.\n  var0 = tf.Variable(...)\n  var1 = tf.Variable(...)\n  # ... use the variables to build a training model...\n  ...\n  # Create an op that applies the optimizer.  This is what we usually\n  # would use as a training op.\n  opt_op = opt.minimize(my_loss, [var0, var1])\n\n  # Create an ExponentialMovingAverage object\n  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n\n  with tf.control_dependencies([opt_op]):\n      # Create the shadow variables, and add ops to maintain moving averages\n      # of var0 and var1. This also creates an op that will update the moving\n      # averages after each training step.  This is what we will use in place\n      # of the usual training op.\n      training_op = ema.apply([var0, var1])\n\n  ...train the model by running training_op...\n  ```\n\n  There are two ways to use the moving averages for evaluations:\n\n  *  Build a model that uses the shadow variables instead of the variables.\n     For this, use the `average()` method which returns the shadow variable\n     for a given variable.\n  *  Build a model normally but load the checkpoint files to evaluate by using\n     the shadow variable names.  For this use the `average_name()` method.  See\n     the `tf.train.Saver` for more\n     information on restoring saved variables.\n\n  Example of restoring the shadow variable values:\n\n  ```python\n  # Create a Saver that loads variables from their saved shadow values.\n  shadow_var0_name = ema.average_name(var0)\n  shadow_var1_name = ema.average_name(var1)\n  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n  saver.restore(...checkpoint filename...)\n  # var0 and var1 now hold the moving average values\n  ```\n  ",
        "klass": "tensorflow.python.training.moving_averages.ExponentialMovingAverage",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the gradient descent algorithm.\n  ",
        "klass": "tensorflow.python.training.gradient_descent.GradientDescentOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, recovery and hooks.\n\n  Example usage:\n\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with MonitoredSession(session_creator=ChiefSessionCreator(...),\n                        hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the monitored session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n  * calls `hook.after_create_session()`\n\n  Run: When `run()` is called, the monitored session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n  * if `AbortedError` or `UnavailableError` occurs, it recovers or\n    reinitializes the session before executing the run() call again\n\n\n  Exit: At the `close()`, the monitored session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the monitored_session is used as a context\n\n  How to set `tf.Session` arguments:\n\n  * In most cases you can set session arguments as follows:\n\n  ```python\n  MonitoredSession(\n    session_creator=ChiefSessionCreator(master=..., config=...))\n  ```\n\n  * In distributed setting for a non-chief worker, you can use following:\n\n  ```python\n  MonitoredSession(\n    session_creator=WorkerSessionCreator(master=..., config=...))\n  ```\n\n  See `MonitoredTrainingSession` for an example usage based on chief or worker.\n\n  Note: This is not a `tf.Session`. For example, it cannot do following:\n\n  * it cannot be set as default session.\n  * it cannot be sent to saver.save.\n  * it cannot be sent to tf.train.start_queue_runners.\n\n  Args:\n    session_creator: A factory object to create session. Typically a\n      `ChiefSessionCreator` which is the default one.\n    hooks: An iterable of `SessionRunHook' objects.\n\n  Returns:\n    A MonitoredSession object.\n  ",
        "klass": "tensorflow.python.training.monitored_session.MonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Saves and restores variables.\n\n  See [Variables](https://tensorflow.org/guide/variables)\n  for an overview of variables, saving and restoring.\n\n  The `Saver` class adds ops to save and restore variables to and from\n  *checkpoints*.  It also provides convenience methods to run these ops.\n\n  Checkpoints are binary files in a proprietary format which map variable names\n  to tensor values.  The best way to examine the contents of a checkpoint is to\n  load it using a `Saver`.\n\n  Savers can automatically number checkpoint filenames with a provided counter.\n  This lets you keep multiple checkpoints at different steps while training a\n  model.  For example you can number the checkpoint filenames with the training\n  step number.  To avoid filling up disks, savers manage checkpoint files\n  automatically. For example, they can keep only the N most recent files, or\n  one checkpoint for every N hours of training.\n\n  You number checkpoint filenames by passing a value to the optional\n  `global_step` argument to `save()`:\n\n  ```python\n  saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n  ...\n  saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n  ```\n\n  Additionally, optional arguments to the `Saver()` constructor let you control\n  the proliferation of checkpoint files on disk:\n\n  * `max_to_keep` indicates the maximum number of recent checkpoint files to\n    keep.  As new files are created, older files are deleted.   If None or 0,\n    no checkpoints are deleted from the filesystem but only the last one is\n    kept in the `checkpoint` file.  Defaults to 5 (that is, the 5 most recent\n    checkpoint files are kept.)\n\n  * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent\n    `max_to_keep` checkpoint files, you might want to keep one checkpoint file\n    for every N hours of training.  This can be useful if you want to later\n    analyze how a model progressed during a long training session.  For\n    example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep\n    one checkpoint file for every 2 hours of training.  The default value of\n    10,000 hours effectively disables the feature.\n\n  Note that you still have to call the `save()` method to save the model.\n  Passing these arguments to the constructor will not save variables\n  automatically for you.\n\n  A training program that saves regularly looks like:\n\n  ```python\n  ...\n  # Create a saver.\n  saver = tf.train.Saver(...variables...)\n  # Launch the graph and train, saving the model every 1,000 steps.\n  sess = tf.Session()\n  for step in xrange(1000000):\n      sess.run(..training_op..)\n      if step % 1000 == 0:\n          # Append the step number to the checkpoint name:\n          saver.save(sess, 'my-model', global_step=step)\n  ```\n\n  In addition to checkpoint files, savers keep a protocol buffer on disk with\n  the list of recent checkpoints. This is used to manage numbered checkpoint\n  files and by `latest_checkpoint()`, which makes it easy to discover the path\n  to the most recent checkpoint. That protocol buffer is stored in a file named\n  'checkpoint' next to the checkpoint files.\n\n  If you create several savers, you can specify a different filename for the\n  protocol buffer file in the call to `save()`.\n  ",
        "klass": "tensorflow.python.training.saver.Saver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, restoring, and hooks.\n\n  Please note that this utility is not recommended for distributed settings.\n  For distributed settings, please use `tf.train.MonitoredSession`. The\n  differences between `MonitoredSession` and `SingularMonitoredSession` are:\n\n  * `MonitoredSession` handles `AbortedError` and `UnavailableError` for\n    distributed settings, but `SingularMonitoredSession` does not.\n  * `MonitoredSession` can be created in `chief` or `worker` modes.\n    `SingularMonitoredSession` is always created as `chief`.\n  * You can access the raw `tf.Session` object used by\n    `SingularMonitoredSession`, whereas in MonitoredSession the raw session is\n    private. This can be used:\n      - To `run` without hooks.\n      - To save and restore.\n  * All other functionality is identical.\n\n  Example usage:\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with SingularMonitoredSession(hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the hooked session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n\n  Run: When `run()` is called, the hooked session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n\n  Exit: At the `close()`, the hooked session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the `SingularMonitoredSession` is used as a context.\n  ",
        "klass": "tensorflow.python.training.monitored_session.SingularMonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A training helper that checkpoints models and computes summaries.\n\n  This class is deprecated. Please use\n  `tf.train.MonitoredTrainingSession` instead.\n\n  The Supervisor is a small wrapper around a `Coordinator`, a `Saver`,\n  and a `SessionManager` that takes care of common needs of TensorFlow\n  training programs.\n\n  #### Use for a single program\n\n  ```python\n  with tf.Graph().as_default():\n    ...add operations to the graph...\n    # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.\n    sv = Supervisor(logdir='/tmp/mydir')\n    # Get a TensorFlow session managed by the supervisor.\n    with sv.managed_session(FLAGS.master) as sess:\n      # Use the session to train the graph.\n      while not sv.should_stop():\n        sess.run(<my_train_op>)\n  ```\n\n  Within the `with sv.managed_session()` block all variables in the graph have\n  been initialized.  In addition, a few services have been started to\n  checkpoint the model and add summaries to the event log.\n\n  If the program crashes and is restarted, the managed session automatically\n  reinitialize variables from the most recent checkpoint.\n\n  The supervisor is notified of any exception raised by one of the services.\n  After an exception is raised, `should_stop()` returns `True`.  In that case\n  the training loop should also stop.  This is why the training loop has to\n  check for `sv.should_stop()`.\n\n  Exceptions that indicate that the training inputs have been exhausted,\n  `tf.errors.OutOfRangeError`, also cause `sv.should_stop()` to return `True`\n  but are not re-raised from the `with` block: they indicate a normal\n  termination.\n\n  #### Use for multiple replicas\n\n  To train with replicas you deploy the same program in a `Cluster`.\n  One of the tasks must be identified as the *chief*: the task that handles\n  initialization, checkpoints, summaries, and recovery.  The other tasks\n  depend on the *chief* for these services.\n\n  The only change you have to do to the single program code is to indicate\n  if the program is running as the *chief*.\n\n  ```python\n  # Choose a task as the chief. This could be based on server_def.task_index,\n  # or job_def.name, or job_def.tasks. It's entirely up to the end user.\n  # But there can be only one *chief*.\n  is_chief = (server_def.task_index == 0)\n  server = tf.train.Server(server_def)\n\n  with tf.Graph().as_default():\n    ...add operations to the graph...\n    # Create a Supervisor that uses log directory on a shared file system.\n    # Indicate if you are the 'chief'\n    sv = Supervisor(logdir='/shared_directory/...', is_chief=is_chief)\n    # Get a Session in a TensorFlow server on the cluster.\n    with sv.managed_session(server.target) as sess:\n      # Use the session to train the graph.\n      while not sv.should_stop():\n        sess.run(<my_train_op>)\n  ```\n\n  In the *chief* task, the `Supervisor` works exactly as in the first example\n  above.  In the other tasks `sv.managed_session()` waits for the Model to have\n  been initialized before returning a session to the training code.  The\n  non-chief tasks depend on the chief task for initializing the model.\n\n  If one of the tasks crashes and restarts, `managed_session()`\n  checks if the Model is initialized.  If yes, it just creates a session and\n  returns it to the training code that proceeds normally.  If the model needs\n  to be initialized, the chief task takes care of reinitializing it; the other\n  tasks just wait for the model to have been initialized.\n\n  NOTE: This modified program still works fine as a single program.\n  The single program marks itself as the chief.\n\n  #### What `master` string to use\n\n  Whether you are running on your machine or in the cluster you can use the\n  following values for the --master flag:\n\n  * Specifying `''` requests an in-process session that does not use RPC.\n\n  * Specifying `'local'` requests a session that uses the RPC-based\n    \"Master interface\" to run TensorFlow programs. See\n    `tf.train.Server.create_local_server` for\n    details.\n\n  * Specifying `'grpc://hostname:port'` requests a session that uses\n    the RPC interface to a specific host, and also allows the in-process\n    master to access remote tensorflow workers. Often, it is\n    appropriate to pass `server.target` (for some `tf.train.Server`\n    named `server).\n\n  #### Advanced use\n\n  ##### Launching additional services\n\n  `managed_session()` launches the Checkpoint and Summary services (threads).\n  If you need more services to run you can simply launch them in the block\n  controlled by `managed_session()`.\n\n  Example: Start a thread to print losses.  We want this thread to run\n  every 60 seconds, so we launch it with `sv.loop()`.\n\n  ```python\n  ...\n  sv = Supervisor(logdir='/tmp/mydir')\n  with sv.managed_session(FLAGS.master) as sess:\n    sv.loop(60, print_loss, (sess, ))\n    while not sv.should_stop():\n      sess.run(my_train_op)\n  ```\n\n  ##### Launching fewer services\n\n  `managed_session()` launches the \"summary\" and \"checkpoint\" threads which use\n  either the optionally `summary_op` and `saver` passed to the constructor, or\n  default ones created automatically by the supervisor.  If you want to run\n  your own summary and checkpointing logic, disable these services by passing\n  `None` to the `summary_op` and `saver` parameters.\n\n  Example: Create summaries manually every 100 steps in the chief.\n\n  ```python\n  # Create a Supervisor with no automatic summaries.\n  sv = Supervisor(logdir='/tmp/mydir', is_chief=is_chief, summary_op=None)\n  # As summary_op was None, managed_session() does not start the\n  # summary thread.\n  with sv.managed_session(FLAGS.master) as sess:\n    for step in xrange(1000000):\n      if sv.should_stop():\n        break\n      if is_chief and step % 100 == 0:\n        # Create the summary every 100 chief steps.\n        sv.summary_computed(sess, sess.run(my_summary_op))\n      else:\n        # Train normally\n        sess.run(my_train_op)\n  ```\n\n  ##### Custom model initialization\n\n  `managed_session()` only supports initializing the model by running an\n  `init_op` or restoring from the latest checkpoint.  If you have special\n  initialization needs, see how to specify a `local_init_op` when creating the\n  supervisor.  You can also use the `SessionManager` directly to create a\n  session and check if it could be initialized automatically.\n  ",
        "klass": "tensorflow.python.training.supervisor.Supervisor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "range(stop) -> range object\nrange(start, stop[, step]) -> range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).",
        "klass": "range",
        "module": "range"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "Initializes a core version of DNNBoostedTreeCombinedEstimator.\n\n    Args:\n      dnn_hidden_units: List of hidden units per layer for DNN.\n      dnn_feature_columns: An iterable containing all the feature columns\n        used by the model's DNN.\n      tree_learner_config: A config for the tree learner.\n      num_trees: Number of trees to grow model to after training DNN.\n      tree_examples_per_layer: Number of examples to accumulate before\n        growing the tree a layer. This value has a big impact on model\n        quality and should be set equal to the number of examples in\n        training dataset if possible. It can also be a function that computes\n        the number of examples based on the depth of the layer that's\n        being built.\n      head: `Head` instance.\n      model_dir: Directory for model exports.\n      config: `RunConfig` of the estimator.\n      dnn_optimizer: string, `Optimizer` object, or callable that defines the\n        optimizer to use for training the DNN. If `None`, will use the Adagrad\n        optimizer with default learning rate.\n      dnn_activation_fn: Activation function applied to each layer of the DNN.\n        If `None`, will use `tf.nn.relu`.\n      dnn_dropout: When not `None`, the probability to drop out a given\n        unit in the DNN.\n      dnn_input_layer_partitioner: Partitioner for input layer of the DNN.\n        Defaults to `min_max_variable_partitioner` with `min_slice_size`\n        64 << 20.\n      dnn_input_layer_to_tree: Whether to provide the DNN's input layer\n      as a feature to the tree.\n      dnn_steps_to_train: Number of steps to train dnn for before switching\n        to gbdt.\n      predict_with_tree_only: Whether to use only the tree model output as the\n        final prediction.\n      tree_feature_columns: An iterable containing all the feature columns\n        used by the model's boosted trees. If dnn_input_layer_to_tree is\n        set to True, these features are in addition to dnn_feature_columns.\n      tree_center_bias: Whether a separate tree should be created for\n        first fitting the bias.\n      dnn_to_tree_distillation_param: A Tuple of (float, loss_fn), where the\n        float defines the weight of the distillation loss, and the loss_fn, for\n        computing distillation loss, takes dnn_logits, tree_logits and weight\n        tensor. If the entire tuple is None, no distillation will be applied. If\n        only the loss_fn is None, we will take the sigmoid/softmax cross entropy\n        loss be default. When distillation is applied, `predict_with_tree_only`\n        will be set to True.\n    ",
        "klass": "tensorflow.contrib.boosted_trees.estimator_batch.dnn_tree_combined_estimator.CoreDNNBoostedTreeCombinedEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A classifier that uses a combined DNN/GBDT model.",
        "klass": "tensorflow.contrib.boosted_trees.estimator_batch.dnn_tree_combined_estimator.DNNBoostedTreeCombinedClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "An estimator that uses a combined DNN/GBDT model.\n\n  Useful for training with user specified `Head`.\n  ",
        "klass": "tensorflow.contrib.boosted_trees.estimator_batch.dnn_tree_combined_estimator.DNNBoostedTreeCombinedEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.session_run_hook.SessionRunHook"
        ],
        "class_docstring": "Hook to save feature importance summaries.",
        "klass": "tensorflow.contrib.boosted_trees.estimator_batch.trainer_hooks.FeatureImportanceSummarySaver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.checkpointable.base.CheckpointableBase"
        ],
        "class_docstring": "Manages dependencies on other objects.\n\n  `Checkpointable` objects may have dependencies: other `Checkpointable` objects\n  which should be saved if the object declaring the dependency is saved. A\n  correctly saveable program has a dependency graph such that if changing a\n  global variable affects an object (e.g. changes the behavior of any of its\n  methods) then there is a chain of dependencies from the influenced object to\n  the variable.\n\n  Dependency edges have names, and are created implicitly when a\n  `Checkpointable` object is assigned to an attribute of another\n  `Checkpointable` object. For example:\n\n  ```\n  obj = Checkpointable()\n  obj.v = ResourceVariable(0.)\n  ```\n\n  The `Checkpointable` object `obj` now has a dependency named \"v\" on a\n  variable.\n\n  `Checkpointable` objects may specify `Tensor`s to be saved and restored\n  directly (e.g. a `Variable` indicating how to save itself) rather than through\n  dependencies on other objects. See\n  `Checkpointable._gather_saveables_for_checkpoint` for details.\n  ",
        "klass": "tensorflow.python.training.checkpointable.tracking.Checkpointable",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.cluster_resolver.cluster_resolver.ClusterResolver"
        ],
        "class_docstring": "Cluster Resolver for Google Cloud TPUs.\n\n  This is an implementation of cluster resolvers for the Google Cloud TPU\n  service. As Cloud TPUs are in alpha, you will need to specify a API definition\n  file for this to consume, in addition to a list of Cloud TPUs in your Google\n  Cloud Platform project.\n  ",
        "klass": "tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.cluster_resolver.cluster_resolver.ClusterResolver"
        ],
        "class_docstring": "Cluster Resolver for Google Cloud TPUs.\n\n  This is an implementation of cluster resolvers for the Google Cloud TPU\n  service. As Cloud TPUs are in alpha, you will need to specify a API definition\n  file for this to consume, in addition to a list of Cloud TPUs in your Google\n  Cloud Platform project.\n  ",
        "klass": "tensorflow.contrib.cluster_resolver.TPUClusterResolver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.experimental.ops.readers.CsvDatasetV1"
        ],
        "class_docstring": "A Dataset comprising lines from one or more CSV files.",
        "klass": "tensorflow.contrib.data.CsvDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.experimental.ops.readers.SqlDatasetV1"
        ],
        "class_docstring": "A `Dataset` consisting of the results from a SQL query.",
        "klass": "tensorflow.contrib.data.python.ops.readers.SqlDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "Distribution strategy that uses collective ops for all-reduce.\n\n  It is similar to the MirroredStrategy but it uses collective ops for\n  reduction.\n\n  When `cluster_spec` is given by the `configure` method, it turns into the\n  mulit-worker version that works on multiple workers with between-graph\n  replication.\n\n  Note: `configure` will be called by higher-level APIs if running in\n  distributed environment.\n  ",
        "klass": "tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "Mirrors vars to distribute across multiple devices and machines.\n\n  *** contrib version ***\n\n  This strategy uses one replica per device and sync replication for its\n  multi-GPU version.\n\n  When `cluster_spec` is given by the `configure` method., it turns into the\n  mulit-worker version that works on multiple workers with in-graph replication.\n  Note: `configure` will be called by higher-level APIs if running in\n  distributed environment.\n\n  There are several important concepts for distributed TensorFlow, e.g.\n  `client`, `job`, 'task', `cluster`, `in-graph replication` and\n  'synchronous training' and they have already been defined in the\n  [TensorFlow's documentation](https://www.tensorflow.org/deploy/distributed).\n  The distribution strategy inherits these concepts as well and in addition to\n  that we also clarify several more concepts:\n\n  * **In-graph replication**: the `client` creates a single `tf.Graph` that\n    specifies tasks for devices on all workers. The `client` then creates a\n    client session which will talk to the `master` service of a `worker`. Then\n    the `master` will partition the graph and distribute the work to all\n    participating workers.\n  * **Worker**: A `worker` is a TensorFlow `task` that usually maps to one\n    physical machine. We will have multiple `worker`s with different `task`\n    index. They all do similar things except for one worker checkpointing model\n    variables, writing summaries, etc. in addition to its ordinary work.\n\n  The multi-worker version of this class maps one replica to one device on a\n  worker. It mirrors all model variables on all replicas. For example, if you\n  have two `worker`s and each `worker` has 4 GPUs, it will create 8 copies of\n  the model variables on these 8 GPUs. Then like in MirroredStrategy, each\n  replica performs their computation with their own copy of variables unless in\n  cross-replica model where variable or tensor reduction happens.\n\n  Args:\n    devices: a list of device strings.\n    num_gpus: number of GPUs. For local training, either specify `devices` or\n      `num_gpus`. In distributed training, this must be specified as number of\n      GPUs on each worker.\n    num_gpus_per_worker: number of GPUs per worker. This is the same as\n      `num_gpus` and only one of `num_gpus` and `num_gpus_per_worker` can be\n      specified.\n    cross_device_ops: optional, a descedant of `CrossDeviceOps`. If this is not\n      set, the `configure` method will try to find the best one.\n    auto_shard_dataset: whether to auto-shard the dataset when there are\n      multiple workers.\n    cross_tower_ops: Deprecated alias for `cross_device_ops`.\n  ",
        "klass": "tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "TPU distribution strategy implementation.",
        "klass": "tensorflow.contrib.distribute.python.tpu_strategy.TPUStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "TPU distribution strategy implementation.",
        "klass": "tensorflow.contrib.distribute.TPUStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager when you are in `update()` or `update_non_slot()`.",
        "klass": "tensorflow.python.distribute.distribute_lib.UpdateContext",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Bernoulli distribution.\n\n  The Bernoulli distribution with `probs` parameter, i.e., the probability of a\n  `1` outcome (vs a `0` outcome).\n  ",
        "klass": "tensorflow.contrib.distributions.Bernoulli",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Beta distribution.\n\n  The Beta distribution is defined over the `(0, 1)` interval using parameters\n  `concentration1` (aka \"alpha\") and `concentration0` (aka \"beta\").\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; alpha, beta) = x**(alpha - 1) (1 - x)**(beta - 1) / Z\n  Z = Gamma(alpha) Gamma(beta) / Gamma(alpha + beta)\n  ```\n\n  where:\n\n  * `concentration1 = alpha`,\n  * `concentration0 = beta`,\n  * `Z` is the normalization constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The concentration parameters represent mean total counts of a `1` or a `0`,\n  i.e.,\n\n  ```none\n  concentration1 = alpha = mean * total_concentration\n  concentration0 = beta  = (1. - mean) * total_concentration\n  ```\n\n  where `mean` in `(0, 1)` and `total_concentration` is a positive real number\n  representing a mean `total_count = concentration1 + concentration0`.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  Warning: The samples can be zero due to finite precision.\n  This happens more often when some of the concentrations are very small.\n  Make sure to round the samples to `np.finfo(dtype).tiny` before computing the\n  density.\n\n  Samples of this distribution are reparameterized (pathwise differentiable).\n  The derivatives are computed using the approach described in the paper\n\n  [Michael Figurnov, Shakir Mohamed, Andriy Mnih.\n  Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Create a batch of three Beta distributions.\n  alpha = [1, 2, 3]\n  beta = [1, 2, 3]\n  dist = tfd.Beta(alpha, beta)\n\n  dist.sample([4, 5])  # Shape [4, 5, 3]\n\n  # `x` has three batch entries, each with two samples.\n  x = [[.1, .4, .5],\n       [.2, .3, .5]]\n  # Calculate the probability of each pair of samples under the corresponding\n  # distribution in `dist`.\n  dist.prob(x)         # Shape [2, 3]\n  ```\n\n  ```python\n  # Create batch_shape=[2, 3] via parameter broadcast:\n  alpha = [[1.], [2]]      # Shape [2, 1]\n  beta = [3., 4, 5]        # Shape [3]\n  dist = tfd.Beta(alpha, beta)\n\n  # alpha broadcast as: [[1., 1, 1,],\n  #                      [2, 2, 2]]\n  # beta broadcast as:  [[3., 4, 5],\n  #                      [3, 4, 5]]\n  # batch_Shape [2, 3]\n  dist.sample([4, 5])  # Shape [4, 5, 2, 3]\n\n  x = [.2, .3, .5]\n  # x will be broadcast as [[.2, .3, .5],\n  #                         [.2, .3, .5]],\n  # thus matching batch_shape [2, 3].\n  dist.prob(x)         # Shape [2, 3]\n  ```\n\n  Compute the gradients of samples w.r.t. the parameters:\n\n  ```python\n  alpha = tf.constant(1.0)\n  beta = tf.constant(2.0)\n  dist = tfd.Beta(alpha, beta)\n  samples = dist.sample(5)  # Shape [5]\n  loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n  # Unbiased stochastic gradients of the loss function\n  grads = tf.gradients(loss, [alpha, beta])\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Beta",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.beta.Beta"
        ],
        "class_docstring": "Beta with softplus transform of `concentration1` and `concentration0`.",
        "klass": "tensorflow.contrib.distributions.BetaWithSoftplusConcentration",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Binomial distribution.\n\n  This distribution is parameterized by `probs`, a (batch of) probabilities for\n  drawing a `1` and `total_count`, the number of trials per draw from the\n  Binomial.\n\n  #### Mathematical Details\n\n  The Binomial is a distribution over the number of `1`'s in `total_count`\n  independent trials, with each trial having the same probability of `1`, i.e.,\n  `probs`.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; n, p) = p**k (1 - p)**(n - k) / Z\n  Z = k! (n - k)! / n!\n  ```\n\n  where:\n  * `total_count = n`,\n  * `probs = p`,\n  * `Z` is the normalizing constant, and,\n  * `n!` is the factorial of `n`.\n\n  #### Examples\n\n  Create a single distribution, corresponding to 5 coin flips.\n\n  ```python\n  dist = Binomial(total_count=5., probs=.5)\n  ```\n\n  Create a single distribution (using logits), corresponding to 5 coin flips.\n\n  ```python\n  dist = Binomial(total_count=5., logits=0.)\n  ```\n\n  Creates 3 distributions with the third distribution most likely to have\n  successes.\n\n  ```python\n  p = [.2, .3, .8]\n  # n will be broadcast to [4., 4., 4.], to match p.\n  dist = Binomial(total_count=4., probs=p)\n  ```\n\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts same shape as p.\n  counts = [1., 2, 3]\n  dist.prob(counts)  # Shape [3]\n\n  # p will be broadcast to [[.2, .3, .8], [.2, .3, .8]] to match counts.\n  counts = [[1., 2, 1], [2, 2, 4]]\n  dist.prob(counts)  # Shape [2, 3]\n\n  # p will be broadcast to shape [5, 7, 3] to match counts.\n  counts = [[...]]  # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7, 3]\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.binomial.Binomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Binomial distribution.\n\n  This distribution is parameterized by `probs`, a (batch of) probabilities for\n  drawing a `1` and `total_count`, the number of trials per draw from the\n  Binomial.\n\n  #### Mathematical Details\n\n  The Binomial is a distribution over the number of `1`'s in `total_count`\n  independent trials, with each trial having the same probability of `1`, i.e.,\n  `probs`.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; n, p) = p**k (1 - p)**(n - k) / Z\n  Z = k! (n - k)! / n!\n  ```\n\n  where:\n  * `total_count = n`,\n  * `probs = p`,\n  * `Z` is the normalizing constant, and,\n  * `n!` is the factorial of `n`.\n\n  #### Examples\n\n  Create a single distribution, corresponding to 5 coin flips.\n\n  ```python\n  dist = Binomial(total_count=5., probs=.5)\n  ```\n\n  Create a single distribution (using logits), corresponding to 5 coin flips.\n\n  ```python\n  dist = Binomial(total_count=5., logits=0.)\n  ```\n\n  Creates 3 distributions with the third distribution most likely to have\n  successes.\n\n  ```python\n  p = [.2, .3, .8]\n  # n will be broadcast to [4., 4., 4.], to match p.\n  dist = Binomial(total_count=4., probs=p)\n  ```\n\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts same shape as p.\n  counts = [1., 2, 3]\n  dist.prob(counts)  # Shape [3]\n\n  # p will be broadcast to [[.2, .3, .8], [.2, .3, .8]] to match counts.\n  counts = [[1., 2, 1], [2, 2, 4]]\n  dist.prob(counts)  # Shape [2, 3]\n\n  # p will be broadcast to shape [5, 7, 3] to match counts.\n  counts = [[...]]  # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7, 3]\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.Binomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Categorical distribution.\n\n  The Categorical distribution is parameterized by either probabilities or\n  log-probabilities of a set of `K` classes. It is defined over the integers\n  `{0, 1, ..., K}`.\n\n  The Categorical distribution is closely related to the `OneHotCategorical` and\n  `Multinomial` distributions.  The Categorical distribution can be intuited as\n  generating samples according to `argmax{ OneHotCategorical(probs) }` itself\n  being identical to `argmax{ Multinomial(probs, total_count=1) }`.\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; pi) = prod_j pi_j**[k == j]\n  ```\n\n  #### Pitfalls\n\n  The number of classes, `K`, must not exceed:\n  - the largest integer representable by `self.dtype`, i.e.,\n    `2**(mantissa_bits+1)` (IEEE 754),\n  - the maximum `Tensor` index, i.e., `2**31-1`.\n\n  In other words,\n\n  ```python\n  K <= min(2**31-1, {\n    tf.float16: 2**11,\n    tf.float32: 2**24,\n    tf.float64: 2**53 }[param.dtype])\n  ```\n\n  Note: This condition is validated only when `self.validate_args = True`.\n\n  #### Examples\n\n  Creates a 3-class distribution with the 2nd class being most likely.\n\n  ```python\n  dist = Categorical(probs=[0.1, 0.5, 0.4])\n  n = 1e4\n  empirical_prob = tf.cast(\n      tf.histogram_fixed_width(\n        dist.sample(int(n)),\n        [0., 2],\n        nbins=3),\n      dtype=tf.float32) / n\n  # ==> array([ 0.1005,  0.5037,  0.3958], dtype=float32)\n  ```\n\n  Creates a 3-class distribution with the 2nd class being most likely.\n  Parameterized by [logits](https://en.wikipedia.org/wiki/Logit) rather than\n  probabilities.\n\n  ```python\n  dist = Categorical(logits=np.log([0.1, 0.5, 0.4])\n  n = 1e4\n  empirical_prob = tf.cast(\n      tf.histogram_fixed_width(\n        dist.sample(int(n)),\n        [0., 2],\n        nbins=3),\n      dtype=tf.float32) / n\n  # ==> array([0.1045,  0.5047, 0.3908], dtype=float32)\n  ```\n\n  Creates a 3-class distribution with the 3rd class being most likely.\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts is a scalar.\n  p = [0.1, 0.4, 0.5]\n  dist = Categorical(probs=p)\n  dist.prob(0)  # Shape []\n\n  # p will be broadcast to [[0.1, 0.4, 0.5], [0.1, 0.4, 0.5]] to match counts.\n  counts = [1, 0]\n  dist.prob(counts)  # Shape [2]\n\n  # p will be broadcast to shape [3, 5, 7, 3] to match counts.\n  counts = [[...]] # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7, 3]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Categorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.gamma.Gamma"
        ],
        "class_docstring": "Chi2 distribution.\n\n  The Chi2 distribution is defined over positive real numbers using a degrees of\n  freedom (\"df\") parameter.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; df, x > 0) = x**(0.5 df - 1) exp(-0.5 x) / Z\n  Z = 2**(0.5 df) Gamma(0.5 df)\n  ```\n\n  where:\n\n  * `df` denotes the degrees of freedom,\n  * `Z` is the normalization constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The Chi2 distribution is a special case of the Gamma distribution, i.e.,\n\n  ```python\n  Chi2(df) = Gamma(concentration=0.5 * df, rate=0.5)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.chi2.Chi2",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.gamma.Gamma"
        ],
        "class_docstring": "Chi2 distribution.\n\n  The Chi2 distribution is defined over positive real numbers using a degrees of\n  freedom (\"df\") parameter.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; df, x > 0) = x**(0.5 df - 1) exp(-0.5 x) / Z\n  Z = 2**(0.5 df) Gamma(0.5 df)\n  ```\n\n  where:\n\n  * `df` denotes the degrees of freedom,\n  * `Z` is the normalization constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The Chi2 distribution is a special case of the Gamma distribution, i.e.,\n\n  ```python\n  Chi2(df) = Gamma(concentration=0.5 * df, rate=0.5)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Chi2",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.deterministic._BaseDeterministic"
        ],
        "class_docstring": "Scalar `Deterministic` distribution on the real line.\n\n  The scalar `Deterministic` distribution is parameterized by a [batch] point\n  `loc` on the real line.  The distribution is supported at this point only,\n  and corresponds to a random variable that is constant, equal to `loc`.\n\n  See [Degenerate rv](https://en.wikipedia.org/wiki/Degenerate_distribution).\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) and cumulative distribution function (cdf)\n  are\n\n  ```none\n  pmf(x; loc) = 1, if x == loc, else 0\n  cdf(x; loc) = 1, if x >= loc, else 0\n  ```\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Initialize a single Deterministic supported at zero.\n  constant = tfd.Deterministic(0.)\n  constant.prob(0.)\n  ==> 1.\n  constant.prob(2.)\n  ==> 0.\n\n  # Initialize a [2, 2] batch of scalar constants.\n  loc = [[0., 1.], [2., 3.]]\n  x = [[0., 1.1], [1.99, 3.]]\n  constant = tfd.Deterministic(loc)\n  constant.prob(x)\n  ==> [[1., 0.], [0., 1.]]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.deterministic.Deterministic",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Dirichlet distribution.\n\n  The Dirichlet distribution is defined over the\n  [`(k-1)`-simplex](https://en.wikipedia.org/wiki/Simplex) using a positive,\n  length-`k` vector `concentration` (`k > 1`). The Dirichlet is identically the\n  Beta distribution when `k = 2`.\n\n  #### Mathematical Details\n\n  The Dirichlet is a distribution over the open `(k-1)`-simplex, i.e.,\n\n  ```none\n  S^{k-1} = { (x_0, ..., x_{k-1}) in R^k : sum_j x_j = 1 and all_j x_j > 0 }.\n  ```\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; alpha) = prod_j x_j**(alpha_j - 1) / Z\n  Z = prod_j Gamma(alpha_j) / Gamma(sum_j alpha_j)\n  ```\n\n  where:\n\n  * `x in S^{k-1}`, i.e., the `(k-1)`-simplex,\n  * `concentration = alpha = [alpha_0, ..., alpha_{k-1}]`, `alpha_j > 0`,\n  * `Z` is the normalization constant aka the [multivariate beta function](\n    https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function),\n    and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The `concentration` represents mean total counts of class occurrence, i.e.,\n\n  ```none\n  concentration = alpha = mean * total_concentration\n  ```\n\n  where `mean` in `S^{k-1}` and `total_concentration` is a positive real number\n  representing a mean total count.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  Warning: Some components of the samples can be zero due to finite precision.\n  This happens more often when some of the concentrations are very small.\n  Make sure to round the samples to `np.finfo(dtype).tiny` before computing the\n  density.\n\n  Samples of this distribution are reparameterized (pathwise differentiable).\n  The derivatives are computed using the approach described in the paper\n\n  [Michael Figurnov, Shakir Mohamed, Andriy Mnih.\n  Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Create a single trivariate Dirichlet, with the 3rd class being three times\n  # more frequent than the first. I.e., batch_shape=[], event_shape=[3].\n  alpha = [1., 2, 3]\n  dist = tfd.Dirichlet(alpha)\n\n  dist.sample([4, 5])  # shape: [4, 5, 3]\n\n  # x has one sample, one batch, three classes:\n  x = [.2, .3, .5]   # shape: [3]\n  dist.prob(x)       # shape: []\n\n  # x has two samples from one batch:\n  x = [[.1, .4, .5],\n       [.2, .3, .5]]\n  dist.prob(x)         # shape: [2]\n\n  # alpha will be broadcast to shape [5, 7, 3] to match x.\n  x = [[...]]   # shape: [5, 7, 3]\n  dist.prob(x)  # shape: [5, 7]\n  ```\n\n  ```python\n  # Create batch_shape=[2], event_shape=[3]:\n  alpha = [[1., 2, 3],\n           [4, 5, 6]]   # shape: [2, 3]\n  dist = tfd.Dirichlet(alpha)\n\n  dist.sample([4, 5])  # shape: [4, 5, 2, 3]\n\n  x = [.2, .3, .5]\n  # x will be broadcast as [[.2, .3, .5],\n  #                         [.2, .3, .5]],\n  # thus matching batch_shape [2, 3].\n  dist.prob(x)         # shape: [2]\n  ```\n\n  Compute the gradients of samples w.r.t. the parameters:\n\n  ```python\n  alpha = tf.constant([1.0, 2.0, 3.0])\n  dist = tfd.Dirichlet(alpha)\n  samples = dist.sample(5)  # Shape [5, 3]\n  loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n  # Unbiased stochastic gradients of the loss function\n  grads = tf.gradients(loss, alpha)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Dirichlet",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Dirichlet-Multinomial compound distribution.\n\n  The Dirichlet-Multinomial distribution is parameterized by a (batch of)\n  length-`K` `concentration` vectors (`K > 1`) and a `total_count` number of\n  trials, i.e., the number of trials per draw from the DirichletMultinomial. It\n  is defined over a (batch of) length-`K` vector `counts` such that\n  `tf.reduce_sum(counts, -1) = total_count`. The Dirichlet-Multinomial is\n  identically the Beta-Binomial distribution when `K = 2`.\n\n  #### Mathematical Details\n\n  The Dirichlet-Multinomial is a distribution over `K`-class counts, i.e., a\n  length-`K` vector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(n; alpha, N) = Beta(alpha + n) / (prod_j n_j!) / Z\n  Z = Beta(alpha) / N!\n  ```\n\n  where:\n\n  * `concentration = alpha = [alpha_0, ..., alpha_{K-1}]`, `alpha_j > 0`,\n  * `total_count = N`, `N` a positive integer,\n  * `N!` is `N` factorial, and,\n  * `Beta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j)` is the\n    [multivariate beta function](\n    https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function),\n    and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  Dirichlet-Multinomial is a [compound distribution](\n  https://en.wikipedia.org/wiki/Compound_probability_distribution), i.e., its\n  samples are generated as follows.\n\n    1. Choose class probabilities:\n       `probs = [p_0,...,p_{K-1}] ~ Dir(concentration)`\n    2. Draw integers:\n       `counts = [n_0,...,n_{K-1}] ~ Multinomial(total_count, probs)`\n\n  The last `concentration` dimension parametrizes a single Dirichlet-Multinomial\n  distribution. When calling distribution functions (e.g., `dist.prob(counts)`),\n  `concentration`, `total_count` and `counts` are broadcast to the same shape.\n  The last dimension of `counts` corresponds single Dirichlet-Multinomial\n  distributions.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  #### Pitfalls\n\n  The number of classes, `K`, must not exceed:\n  - the largest integer representable by `self.dtype`, i.e.,\n    `2**(mantissa_bits+1)` (IEE754),\n  - the maximum `Tensor` index, i.e., `2**31-1`.\n\n  In other words,\n\n  ```python\n  K <= min(2**31-1, {\n    tf.float16: 2**11,\n    tf.float32: 2**24,\n    tf.float64: 2**53 }[param.dtype])\n  ```\n\n  Note: This condition is validated only when `self.validate_args = True`.\n\n  #### Examples\n\n  ```python\n  alpha = [1., 2., 3.]\n  n = 2.\n  dist = DirichletMultinomial(n, alpha)\n  ```\n\n  Creates a 3-class distribution, with the 3rd class is most likely to be\n  drawn.\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts same shape as alpha.\n  counts = [0., 0., 2.]\n  dist.prob(counts)  # Shape []\n\n  # alpha will be broadcast to [[1., 2., 3.], [1., 2., 3.]] to match counts.\n  counts = [[1., 1., 0.], [1., 0., 1.]]\n  dist.prob(counts)  # Shape [2]\n\n  # alpha will be broadcast to shape [5, 7, 3] to match counts.\n  counts = [[...]]  # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7]\n  ```\n\n  Creates a 2-batch of 3-class distributions.\n\n  ```python\n  alpha = [[1., 2., 3.], [4., 5., 6.]]  # Shape [2, 3]\n  n = [3., 3.]\n  dist = DirichletMultinomial(n, alpha)\n\n  # counts will be broadcast to [[2., 1., 0.], [2., 1., 0.]] to match alpha.\n  counts = [2., 1., 0.]\n  dist.prob(counts)  # Shape [2]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.DirichletMultinomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "ExpRelaxedOneHotCategorical distribution with temperature and logits.\n\n  An ExpRelaxedOneHotCategorical distribution is a log-transformed\n  RelaxedOneHotCategorical distribution. The RelaxedOneHotCategorical is a\n  distribution over random probability vectors, vectors of positive real\n  values that sum to one, which continuously approximates a OneHotCategorical.\n  The degree of approximation is controlled by a temperature: as the temperature\n  goes to 0 the RelaxedOneHotCategorical becomes discrete with a distribution\n  described by the logits, as the temperature goes to infinity the\n  RelaxedOneHotCategorical becomes the constant distribution that is identically\n  the constant vector of (1/event_size, ..., 1/event_size).\n\n  Because computing log-probabilities of the RelaxedOneHotCategorical can\n  suffer from underflow issues, this class is one solution for loss\n  functions that depend on log-probabilities, such as the KL Divergence found\n  in the variational autoencoder loss. The KL divergence between two\n  distributions is invariant under invertible transformations, so evaluating\n  KL divergences of ExpRelaxedOneHotCategorical samples, which are always\n  followed by a `tf.exp` op, is equivalent to evaluating KL divergences of\n  RelaxedOneHotCategorical samples. See the appendix of Maddison et al., 2016\n  for more mathematical details, where this distribution is called the\n  ExpConcrete.\n\n  #### Examples\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution. If those samples\n  are followed by a `tf.exp` op, then they are distributed as a relaxed onehot\n  categorical.\n\n  ```python\n  temperature = 0.5\n  p = [0.1, 0.5, 0.4]\n  dist = ExpRelaxedOneHotCategorical(temperature, probs=p)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution.\n\n  ```python\n  temperature = 0.5\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very low, samples from\n  this distribution are almost discrete, with one component almost 0 and the\n  others very negative. The 2nd class is the most likely to be the largest\n  component in samples drawn from this distribution.\n\n  ```python\n  temperature = 1e-5\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very high, samples from\n  this distribution are usually close to the (-log(3), -log(3), -log(3)) vector.\n  The 2nd class is still the most likely to be the largest component\n  in samples drawn from this distribution.\n\n  ```python\n  temperature = 10\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:\n  A Continuous Relaxation of Discrete Random Variables. 2016.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical.ExpRelaxedOneHotCategorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "ExpRelaxedOneHotCategorical distribution with temperature and logits.\n\n  An ExpRelaxedOneHotCategorical distribution is a log-transformed\n  RelaxedOneHotCategorical distribution. The RelaxedOneHotCategorical is a\n  distribution over random probability vectors, vectors of positive real\n  values that sum to one, which continuously approximates a OneHotCategorical.\n  The degree of approximation is controlled by a temperature: as the temperature\n  goes to 0 the RelaxedOneHotCategorical becomes discrete with a distribution\n  described by the logits, as the temperature goes to infinity the\n  RelaxedOneHotCategorical becomes the constant distribution that is identically\n  the constant vector of (1/event_size, ..., 1/event_size).\n\n  Because computing log-probabilities of the RelaxedOneHotCategorical can\n  suffer from underflow issues, this class is one solution for loss\n  functions that depend on log-probabilities, such as the KL Divergence found\n  in the variational autoencoder loss. The KL divergence between two\n  distributions is invariant under invertible transformations, so evaluating\n  KL divergences of ExpRelaxedOneHotCategorical samples, which are always\n  followed by a `tf.exp` op, is equivalent to evaluating KL divergences of\n  RelaxedOneHotCategorical samples. See the appendix of Maddison et al., 2016\n  for more mathematical details, where this distribution is called the\n  ExpConcrete.\n\n  #### Examples\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution. If those samples\n  are followed by a `tf.exp` op, then they are distributed as a relaxed onehot\n  categorical.\n\n  ```python\n  temperature = 0.5\n  p = [0.1, 0.5, 0.4]\n  dist = ExpRelaxedOneHotCategorical(temperature, probs=p)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution.\n\n  ```python\n  temperature = 0.5\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very low, samples from\n  this distribution are almost discrete, with one component almost 0 and the\n  others very negative. The 2nd class is the most likely to be the largest\n  component in samples drawn from this distribution.\n\n  ```python\n  temperature = 1e-5\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, whose exp approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very high, samples from\n  this distribution are usually close to the (-log(3), -log(3), -log(3)) vector.\n  The 2nd class is still the most likely to be the largest component\n  in samples drawn from this distribution.\n\n  ```python\n  temperature = 10\n  logits = [-2, 2, 0]\n  dist = ExpRelaxedOneHotCategorical(temperature, logits=logits)\n  samples = dist.sample()\n  exp_samples = tf.exp(samples)\n  # exp_samples has the same distribution as samples from\n  # RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:\n  A Continuous Relaxation of Discrete Random Variables. 2016.\n  ",
        "klass": "tensorflow.contrib.distributions.ExpRelaxedOneHotCategorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.gamma.Gamma"
        ],
        "class_docstring": "Exponential distribution.\n\n  The Exponential distribution is parameterized by an event `rate` parameter.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; lambda, x > 0) = exp(-lambda x) / Z\n  Z = 1 / lambda\n  ```\n\n  where `rate = lambda` and `Z` is the normalizaing constant.\n\n  The Exponential distribution is a special case of the Gamma distribution,\n  i.e.,\n\n  ```python\n  Exponential(rate) = Gamma(concentration=1., rate)\n  ```\n\n  The Exponential distribution uses a `rate` parameter, or \"inverse scale\",\n  which can be intuited as,\n\n  ```none\n  X ~ Exponential(rate=1)\n  Y = X / rate\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Exponential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Gamma distribution.\n\n  The Gamma distribution is defined over positive real numbers using\n  parameters `concentration` (aka \"alpha\") and `rate` (aka \"beta\").\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; alpha, beta, x > 0) = x**(alpha - 1) exp(-x beta) / Z\n  Z = Gamma(alpha) beta**(-alpha)\n  ```\n\n  where:\n\n  * `concentration = alpha`, `alpha > 0`,\n  * `rate = beta`, `beta > 0`,\n  * `Z` is the normalizing constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function).\n\n  The cumulative density function (cdf) is,\n\n  ```none\n  cdf(x; alpha, beta, x > 0) = GammaInc(alpha, beta x) / Gamma(alpha)\n  ```\n\n  where `GammaInc` is the [lower incomplete Gamma function](\n  https://en.wikipedia.org/wiki/Incomplete_gamma_function).\n\n  The parameters can be intuited via their relationship to mean and stddev,\n\n  ```none\n  concentration = alpha = (mean / stddev)**2\n  rate = beta = mean / stddev**2 = concentration / mean\n  ```\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  Warning: The samples of this distribution are always non-negative. However,\n  the samples that are smaller than `np.finfo(dtype).tiny` are rounded\n  to this value, so it appears more often than it should.\n  This should only be noticeable when the `concentration` is very small, or the\n  `rate` is very large. See note in `tf.random_gamma` docstring.\n\n  Samples of this distribution are reparameterized (pathwise differentiable).\n  The derivatives are computed using the approach described in the paper\n\n  [Michael Figurnov, Shakir Mohamed, Andriy Mnih.\n  Implicit Reparameterization Gradients, 2018](https://arxiv.org/abs/1805.08498)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  dist = tfd.Gamma(concentration=3.0, rate=2.0)\n  dist2 = tfd.Gamma(concentration=[3.0, 4.0], rate=[2.0, 3.0])\n  ```\n\n  Compute the gradients of samples w.r.t. the parameters:\n\n  ```python\n  concentration = tf.constant(3.0)\n  rate = tf.constant(2.0)\n  dist = tfd.Gamma(concentration, rate)\n  samples = dist.sample(5)  # Shape [5]\n  loss = tf.reduce_mean(tf.square(samples))  # Arbitrary loss function\n  # Unbiased stochastic gradients of the loss function\n  grads = tf.gradients(loss, [concentration, rate])\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Gamma",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Geometric distribution.\n\n  The Geometric distribution is parameterized by p, the probability of a\n  positive event. It represents the probability that in k + 1 Bernoulli trials,\n  the first k trials failed, before seeing a success.\n\n  The pmf of this distribution is:\n\n  #### Mathematical Details\n\n  ```none\n  pmf(k; p) = (1 - p)**k * p\n  ```\n\n  where:\n\n  * `p` is the success probability, `0 < p <= 1`, and,\n  * `k` is a non-negative integer.\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.geometric.Geometric",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Independent distribution from batch of distributions.\n\n  This distribution is useful for regarding a collection of independent,\n  non-identical distributions as a single random variable. For example, the\n  `Independent` distribution composed of a collection of `Bernoulli`\n  distributions might define a distribution over an image (where each\n  `Bernoulli` is a distribution over each pixel).\n\n  More precisely, a collection of `B` (independent) `E`-variate random variables\n  (rv) `{X_1, ..., X_B}`, can be regarded as a `[B, E]`-variate random variable\n  `(X_1, ..., X_B)` with probability\n  `p(x_1, ..., x_B) = p_1(x_1) * ... * p_B(x_B)` where `p_b(X_b)` is the\n  probability of the `b`-th rv. More generally `B, E` can be arbitrary shapes.\n\n  Similarly, the `Independent` distribution specifies a distribution over `[B,\n  E]`-shaped events. It operates by reinterpreting the rightmost batch dims as\n  part of the event dimensions. The `reinterpreted_batch_ndims` parameter\n  controls the number of batch dims which are absorbed as event dims;\n  `reinterpreted_batch_ndims < len(batch_shape)`.  For example, the `log_prob`\n  function entails a `reduce_sum` over the rightmost `reinterpreted_batch_ndims`\n  after calling the base distribution's `log_prob`.  In other words, since the\n  batch dimension(s) index independent distributions, the resultant multivariate\n  will have independent components.\n\n  #### Mathematical Details\n\n  The probability function is,\n\n  ```none\n  prob(x; reinterpreted_batch_ndims) = tf.reduce_prod(\n      dist.prob(x),\n      axis=-1-range(reinterpreted_batch_ndims))\n  ```\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Make independent distribution from a 2-batch Normal.\n  ind = tfd.Independent(\n      distribution=tfd.Normal(loc=[-1., 1], scale=[0.1, 0.5]),\n      reinterpreted_batch_ndims=1)\n\n  # All batch dims have been \"absorbed\" into event dims.\n  ind.batch_shape  # ==> []\n  ind.event_shape  # ==> [2]\n\n  # Make independent distribution from a 2-batch bivariate Normal.\n  ind = tfd.Independent(\n      distribution=tfd.MultivariateNormalDiag(\n          loc=[[-1., 1], [1, -1]],\n          scale_identity_multiplier=[1., 0.5]),\n      reinterpreted_batch_ndims=1)\n\n  # All batch dims have been \"absorbed\" into event dims.\n  ind.batch_shape  # ==> []\n  ind.event_shape  # ==> [2, 2]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.independent.Independent",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "The Logistic distribution with location `loc` and `scale` parameters.\n\n  #### Mathematical details\n\n  The cumulative density function of this distribution is:\n\n  ```none\n  cdf(x; mu, sigma) = 1 / (1 + exp(-(x - mu) / sigma))\n  ```\n\n  where `loc = mu` and `scale = sigma`.\n\n  The Logistic distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ Logistic(loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  #### Examples\n\n  Examples of initialization of one or a batch of distributions.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Define a single scalar Logistic distribution.\n  dist = tfd.Logistic(loc=0., scale=3.)\n\n  # Evaluate the cdf at 1, returning a scalar.\n  dist.cdf(1.)\n\n  # Define a batch of two scalar valued Logistics.\n  # The first has mean 1 and scale 11, the second 2 and 22.\n  dist = tfd.Logistic(loc=[1, 2.], scale=[11, 22.])\n\n  # Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n  # returning a length two tensor.\n  dist.prob([0, 1.5])\n\n  # Get 3 samples, returning a 3 x 2 tensor.\n  dist.sample([3])\n\n  # Arguments are broadcast when possible.\n  # Define a batch of two scalar valued Logistics.\n  # Both have mean 1, but different scales.\n  dist = tfd.Logistic(loc=1., scale=[11, 22.])\n\n  # Evaluate the pdf of both distributions on the same point, 3.0,\n  # returning a length 2 tensor.\n  dist.prob(3.0)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.logistic.Logistic",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "The Logistic distribution with location `loc` and `scale` parameters.\n\n  #### Mathematical details\n\n  The cumulative density function of this distribution is:\n\n  ```none\n  cdf(x; mu, sigma) = 1 / (1 + exp(-(x - mu) / sigma))\n  ```\n\n  where `loc = mu` and `scale = sigma`.\n\n  The Logistic distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ Logistic(loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  #### Examples\n\n  Examples of initialization of one or a batch of distributions.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Define a single scalar Logistic distribution.\n  dist = tfd.Logistic(loc=0., scale=3.)\n\n  # Evaluate the cdf at 1, returning a scalar.\n  dist.cdf(1.)\n\n  # Define a batch of two scalar valued Logistics.\n  # The first has mean 1 and scale 11, the second 2 and 22.\n  dist = tfd.Logistic(loc=[1, 2.], scale=[11, 22.])\n\n  # Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n  # returning a length two tensor.\n  dist.prob([0, 1.5])\n\n  # Get 3 samples, returning a 3 x 2 tensor.\n  dist.sample([3])\n\n  # Arguments are broadcast when possible.\n  # Define a batch of two scalar valued Logistics.\n  # Both have mean 1, but different scales.\n  dist = tfd.Logistic(loc=1., scale=[11, 22.])\n\n  # Evaluate the pdf of both distributions on the same point, 3.0,\n  # returning a length 2 tensor.\n  dist.prob(3.0)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Logistic",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Mixture (same-family) distribution.\n\n  The `MixtureSameFamily` distribution implements a (batch of) mixture\n  distribution where all components are from different parameterizations of the\n  same distribution type. It is parameterized by a `Categorical` \"selecting\n  distribution\" (over `k` components) and a components distribution, i.e., a\n  `Distribution` with a rightmost batch shape (equal to `[k]`) which indexes\n  each (batch of) component.\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  ### Create a mixture of two scalar Gaussians:\n\n  gm = tfd.MixtureSameFamily(\n      mixture_distribution=tfd.Categorical(\n          probs=[0.3, 0.7]),\n      components_distribution=tfd.Normal(\n        loc=[-1., 1],       # One for each component.\n        scale=[0.1, 0.5]))  # And same here.\n\n  gm.mean()\n  # ==> 0.4\n\n  gm.variance()\n  # ==> 1.018\n\n  # Plot PDF.\n  x = np.linspace(-2., 3., int(1e4), dtype=np.float32)\n  import matplotlib.pyplot as plt\n  plt.plot(x, gm.prob(x).eval());\n\n  ### Create a mixture of two Bivariate Gaussians:\n\n  gm = tfd.MixtureSameFamily(\n      mixture_distribution=tfd.Categorical(\n          probs=[0.3, 0.7]),\n      components_distribution=tfd.MultivariateNormalDiag(\n          loc=[[-1., 1],  # component 1\n               [1, -1]],  # component 2\n          scale_identity_multiplier=[.3, .6]))\n\n  gm.mean()\n  # ==> array([ 0.4, -0.4], dtype=float32)\n\n  gm.covariance()\n  # ==> array([[ 1.119, -0.84],\n  #            [-0.84,  1.119]], dtype=float32)\n\n  # Plot PDF contours.\n  def meshgrid(x, y=x):\n    [gx, gy] = np.meshgrid(x, y, indexing='ij')\n    gx, gy = np.float32(gx), np.float32(gy)\n    grid = np.concatenate([gx.ravel()[None, :], gy.ravel()[None, :]], axis=0)\n    return grid.T.reshape(x.size, y.size, 2)\n  grid = meshgrid(np.linspace(-2, 2, 100, dtype=np.float32))\n  plt.contour(grid[..., 0], grid[..., 1], gm.prob(grid).eval());\n\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.mixture_same_family.MixtureSameFamily",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Multinomial distribution.\n\n  This Multinomial distribution is parameterized by `probs`, a (batch of)\n  length-`K` `prob` (probability) vectors (`K > 1`) such that\n  `tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\n  the number of trials per draw from the Multinomial. It is defined over a\n  (batch of) length-`K` vector `counts` such that\n  `tf.reduce_sum(counts, -1) = total_count`. The Multinomial is identically the\n  Binomial distribution when `K = 2`.\n\n  #### Mathematical Details\n\n  The Multinomial is a distribution over `K`-class counts, i.e., a length-`K`\n  vector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(n; pi, N) = prod_j (pi_j)**n_j / Z\n  Z = (prod_j n_j!) / N!\n  ```\n\n  where:\n  * `probs = pi = [pi_0, ..., pi_{K-1}]`, `pi_j > 0`, `sum_j pi_j = 1`,\n  * `total_count = N`, `N` a positive integer,\n  * `Z` is the normalization constant, and,\n  * `N!` denotes `N` factorial.\n\n  Distribution parameters are automatically broadcast in all functions; see\n  examples for details.\n\n  #### Pitfalls\n\n  The number of classes, `K`, must not exceed:\n  - the largest integer representable by `self.dtype`, i.e.,\n    `2**(mantissa_bits+1)` (IEE754),\n  - the maximum `Tensor` index, i.e., `2**31-1`.\n\n  In other words,\n\n  ```python\n  K <= min(2**31-1, {\n    tf.float16: 2**11,\n    tf.float32: 2**24,\n    tf.float64: 2**53 }[param.dtype])\n  ```\n\n  Note: This condition is validated only when `self.validate_args = True`.\n\n  #### Examples\n\n  Create a 3-class distribution, with the 3rd class is most likely to be drawn,\n  using logits.\n\n  ```python\n  logits = [-50., -43, 0]\n  dist = Multinomial(total_count=4., logits=logits)\n  ```\n\n  Create a 3-class distribution, with the 3rd class is most likely to be drawn.\n\n  ```python\n  p = [.2, .3, .5]\n  dist = Multinomial(total_count=4., probs=p)\n  ```\n\n  The distribution functions can be evaluated on counts.\n\n  ```python\n  # counts same shape as p.\n  counts = [1., 0, 3]\n  dist.prob(counts)  # Shape []\n\n  # p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\n  counts = [[1., 2, 1], [2, 2, 0]]\n  dist.prob(counts)  # Shape [2]\n\n  # p will be broadcast to shape [5, 7, 3] to match counts.\n  counts = [[...]]  # Shape [5, 7, 3]\n  dist.prob(counts)  # Shape [5, 7]\n  ```\n\n  Create a 2-batch of 3-class distributions.\n\n  ```python\n  p = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\n  dist = Multinomial(total_count=[4., 5], probs=p)\n\n  counts = [[2., 1, 1], [3, 1, 1]]\n  dist.prob(counts)  # Shape [2]\n\n  dist.sample(5) # Shape [5, 2, 3]\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.Multinomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.mvn_linear_operator.MultivariateNormalLinearOperator"
        ],
        "class_docstring": "The multivariate normal distribution on `R^k`.\n\n  The Multivariate Normal distribution is defined over `R^k` and parameterized\n  by a (batch of) length-`k` `loc` vector (aka \"mu\") and a (batch of) `k x k`\n  `scale` matrix; `covariance = scale @ scale.T` where `@` denotes\n  matrix-multiplication.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z,\n  y = inv(scale) @ (x - loc),\n  Z = (2 pi)**(0.5 k) |det(scale)|,\n  ```\n\n  where:\n\n  * `loc` is a vector in `R^k`,\n  * `scale` is a linear operator in `R^{k x k}`, `cov = scale @ scale.T`,\n  * `Z` denotes the normalization constant, and,\n  * `||y||**2` denotes the squared Euclidean norm of `y`.\n\n  A (non-batch) `scale` matrix is:\n\n  ```none\n  scale = diag(scale_diag + scale_identity_multiplier * ones(k))\n  ```\n\n  where:\n\n  * `scale_diag.shape = [k]`, and,\n  * `scale_identity_multiplier.shape = []`.\n\n  Additional leading dimensions (if any) will index batches.\n\n  If both `scale_diag` and `scale_identity_multiplier` are `None`, then\n  `scale` is the Identity matrix.\n\n  The MultivariateNormal distribution is a member of the [location-scale\n  family](https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.\n  Y = scale @ X + loc\n  ```\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Initialize a single 2-variate Gaussian.\n  mvn = tfd.MultivariateNormalDiag(\n      loc=[1., -1],\n      scale_diag=[1, 2.])\n\n  mvn.mean().eval()\n  # ==> [1., -1]\n\n  mvn.stddev().eval()\n  # ==> [1., 2]\n\n  # Evaluate this on an observation in `R^2`, returning a scalar.\n  mvn.prob([-1., 0]).eval()  # shape: []\n\n  # Initialize a 3-batch, 2-variate scaled-identity Gaussian.\n  mvn = tfd.MultivariateNormalDiag(\n      loc=[1., -1],\n      scale_identity_multiplier=[1, 2., 3])\n\n  mvn.mean().eval()  # shape: [3, 2]\n  # ==> [[1., -1]\n  #      [1, -1],\n  #      [1, -1]]\n\n  mvn.stddev().eval()  # shape: [3, 2]\n  # ==> [[1., 1],\n  #      [2, 2],\n  #      [3, 3]]\n\n  # Evaluate this on an observation in `R^2`, returning a length-3 vector.\n  mvn.prob([-1., 0]).eval()  # shape: [3]\n\n  # Initialize a 2-batch of 3-variate Gaussians.\n  mvn = tfd.MultivariateNormalDiag(\n      loc=[[1., 2, 3],\n           [11, 22, 33]]           # shape: [2, 3]\n      scale_diag=[[1., 2, 3],\n                  [0.5, 1, 1.5]])  # shape: [2, 3]\n\n  # Evaluate this on a two observations, each in `R^3`, returning a length-2\n  # vector.\n  x = [[-1., 0, 1],\n       [-11, 0, 11.]]   # shape: [2, 3].\n  mvn.prob(x).eval()    # shape: [2]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.MultivariateNormalDiag",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "NegativeBinomial distribution.\n\n  The NegativeBinomial distribution is related to the experiment of performing\n  Bernoulli trials in sequence. Given a Bernoulli trial with probability `p` of\n  success, the NegativeBinomial distribution represents the distribution over\n  the number of successes `s` that occur until we observe `f` failures.\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(s; f, p) = p**s (1 - p)**f / Z\n  Z = s! (f - 1)! / (s + f - 1)!\n  ```\n\n  where:\n  * `total_count = f`,\n  * `probs = p`,\n  * `Z` is the normalizaing constant, and,\n  * `n!` is the factorial of `n`.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.negative_binomial.NegativeBinomial",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "The Normal distribution with location `loc` and `scale` parameters.\n\n  #### Mathematical details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; mu, sigma) = exp(-0.5 (x - mu)**2 / sigma**2) / Z\n  Z = (2 pi sigma**2)**0.5\n  ```\n\n  where `loc = mu` is the mean, `scale = sigma` is the std. deviation, and, `Z`\n  is the normalization constant.\n\n  The Normal distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ Normal(loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  #### Examples\n\n  Examples of initialization of one or a batch of distributions.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Define a single scalar Normal distribution.\n  dist = tfd.Normal(loc=0., scale=3.)\n\n  # Evaluate the cdf at 1, returning a scalar.\n  dist.cdf(1.)\n\n  # Define a batch of two scalar valued Normals.\n  # The first has mean 1 and standard deviation 11, the second 2 and 22.\n  dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n\n  # Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n  # returning a length two tensor.\n  dist.prob([0, 1.5])\n\n  # Get 3 samples, returning a 3 x 2 tensor.\n  dist.sample([3])\n  ```\n\n  Arguments are broadcast when possible.\n\n  ```python\n  # Define a batch of two scalar valued Normals.\n  # Both have mean 1, but different standard deviations.\n  dist = tfd.Normal(loc=1., scale=[11, 22.])\n\n  # Evaluate the pdf of both distributions on the same point, 3.0,\n  # returning a length 2 tensor.\n  dist.prob(3.0)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Normal",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "OneHotCategorical distribution.\n\n  The categorical distribution is parameterized by the log-probabilities\n  of a set of classes. The difference between OneHotCategorical and Categorical\n  distributions is that OneHotCategorical is a discrete distribution over\n  one-hot bit vectors whereas Categorical is a discrete distribution over\n  positive integers. OneHotCategorical is equivalent to Categorical except\n  Categorical has event_dim=() while OneHotCategorical has event_dim=K, where\n  K is the number of classes.\n\n  This class provides methods to create indexed batches of OneHotCategorical\n  distributions. If the provided `logits` or `probs` is rank 2 or higher, for\n  every fixed set of leading dimensions, the last dimension represents one\n  single OneHotCategorical distribution. When calling distribution\n  functions (e.g. `dist.prob(x)`), `logits` and `x` are broadcast to the\n  same shape (if possible). In all cases, the last dimension of `logits,x`\n  represents single OneHotCategorical distributions.\n\n  #### Examples\n\n  Creates a 3-class distribution, with the 2nd class, the most likely to be\n  drawn from.\n\n  ```python\n  p = [0.1, 0.5, 0.4]\n  dist = OneHotCategorical(probs=p)\n  ```\n\n  Creates a 3-class distribution, with the 2nd class the most likely to be\n  drawn from, using logits.\n\n  ```python\n  logits = [-2, 2, 0]\n  dist = OneHotCategorical(logits=logits)\n  ```\n\n  Creates a 3-class distribution, with the 3rd class is most likely to be drawn.\n\n  ```python\n  # counts is a scalar.\n  p = [0.1, 0.4, 0.5]\n  dist = OneHotCategorical(probs=p)\n  dist.prob([0,1,0])  # Shape []\n\n  # p will be broadcast to [[0.1, 0.4, 0.5], [0.1, 0.4, 0.5]] to match.\n  samples = [[0,1,0], [1,0,0]]\n  dist.prob(samples)  # Shape [2]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.onehot_categorical.OneHotCategorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Poisson distribution.\n\n  The Poisson distribution is parameterized by an event `rate` parameter.\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; lambda, k >= 0) = (lambda^k / k!) / Z\n  Z = exp(lambda).\n  ```\n\n  where `rate = lambda` and `Z` is the normalizing constant.\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.poisson.Poisson",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Poisson distribution.\n\n  The Poisson distribution is parameterized by an event `rate` parameter.\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) is,\n\n  ```none\n  pmf(k; lambda, k >= 0) = (lambda^k / k!) / Z\n  Z = exp(lambda).\n  ```\n\n  where `rate = lambda` and `Z` is the normalizing constant.\n\n  ",
        "klass": "tensorflow.contrib.distributions.Poisson",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution"
        ],
        "class_docstring": "RelaxedBernoulli distribution with temperature and logits parameters.\n\n  The RelaxedBernoulli is a distribution over the unit interval (0,1), which\n  continuously approximates a Bernoulli. The degree of approximation is\n  controlled by a temperature: as the temperature goes to 0 the\n  RelaxedBernoulli becomes discrete with a distribution described by the\n  `logits` or `probs` parameters, as the temperature goes to infinity the\n  RelaxedBernoulli becomes the constant distribution that is identically 0.5.\n\n  The RelaxedBernoulli distribution is a reparameterized continuous\n  distribution that is the binary special case of the RelaxedOneHotCategorical\n  distribution (Maddison et al., 2016; Jang et al., 2016). For details on the\n  binary special case see the appendix of Maddison et al. (2016) where it is\n  referred to as BinConcrete. If you use this distribution, please cite both\n  papers.\n\n  Some care needs to be taken for loss functions that depend on the\n  log-probability of RelaxedBernoullis, because computing log-probabilities of\n  the RelaxedBernoulli can suffer from underflow issues. In many case loss\n  functions such as these are invariant under invertible transformations of\n  the random variables. The KL divergence, found in the variational autoencoder\n  loss, is an example. Because RelaxedBernoullis are sampled by a Logistic\n  random variable followed by a `tf.sigmoid` op, one solution is to treat\n  the Logistic as the random variable and `tf.sigmoid` as downstream. The\n  KL divergences of two Logistics, which are always followed by a `tf.sigmoid`\n  op, is equivalent to evaluating KL divergences of RelaxedBernoulli samples.\n  See Maddison et al., 2016 for more details where this distribution is called\n  the BinConcrete.\n\n  An alternative approach is to evaluate Bernoulli log probability or KL\n  directly on relaxed samples, as done in Jang et al., 2016. In this case,\n  guarantees on the loss are usually violated. For instance, using a Bernoulli\n  KL in a relaxed ELBO is no longer a lower bound on the log marginal\n  probability of the observation. Thus care and early stopping are important.\n\n  #### Examples\n\n  Creates three continuous distributions, which approximate 3 Bernoullis with\n  probabilities (0.1, 0.5, 0.4). Samples from these distributions will be in\n  the unit interval (0,1).\n\n  ```python\n  temperature = 0.5\n  p = [0.1, 0.5, 0.4]\n  dist = RelaxedBernoulli(temperature, probs=p)\n  ```\n\n  Creates three continuous distributions, which approximate 3 Bernoullis with\n  logits (-2, 2, 0). Samples from these distributions will be in\n  the unit interval (0,1).\n\n  ```python\n  temperature = 0.5\n  logits = [-2, 2, 0]\n  dist = RelaxedBernoulli(temperature, logits=logits)\n  ```\n\n  Creates three continuous distributions, whose sigmoid approximate 3 Bernoullis\n  with logits (-2, 2, 0).\n\n  ```python\n  temperature = 0.5\n  logits = [-2, 2, 0]\n  dist = Logistic(logits/temperature, 1./temperature)\n  samples = dist.sample()\n  sigmoid_samples = tf.sigmoid(samples)\n  # sigmoid_samples has the same distribution as samples from\n  # RelaxedBernoulli(temperature, logits=logits)\n  ```\n\n  Creates three continuous distributions, which approximate 3 Bernoullis with\n  logits (-2, 2, 0). Samples from these distributions will be in\n  the unit interval (0,1). Because the temperature is very low, samples from\n  these distributions are almost discrete, usually taking values very close to 0\n  or 1.\n\n  ```python\n  temperature = 1e-5\n  logits = [-2, 2, 0]\n  dist = RelaxedBernoulli(temperature, logits=logits)\n  ```\n\n  Creates three continuous distributions, which approximate 3 Bernoullis with\n  logits (-2, 2, 0). Samples from these distributions will be in\n  the unit interval (0,1). Because the temperature is very high, samples from\n  these distributions are usually close to the (0.5, 0.5, 0.5) vector.\n\n  ```python\n  temperature = 100\n  logits = [-2, 2, 0]\n  dist = RelaxedBernoulli(temperature, logits=logits)\n  ```\n\n  Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:\n  A Continuous Relaxation of Discrete Random Variables. 2016.\n\n  Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with\n  Gumbel-Softmax. 2016.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.relaxed_bernoulli.RelaxedBernoulli",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution"
        ],
        "class_docstring": "RelaxedOneHotCategorical distribution with temperature and logits.\n\n  The RelaxedOneHotCategorical is a distribution over random probability\n  vectors, vectors of positive real values that sum to one, which continuously\n  approximates a OneHotCategorical. The degree of approximation is controlled by\n  a temperature: as the temperature goes to 0 the RelaxedOneHotCategorical\n  becomes discrete with a distribution described by the `logits` or `probs`\n  parameters, as the temperature goes to infinity the RelaxedOneHotCategorical\n  becomes the constant distribution that is identically the constant vector of\n  (1/event_size, ..., 1/event_size).\n\n  The RelaxedOneHotCategorical distribution was concurrently introduced as the\n  Gumbel-Softmax (Jang et al., 2016) and Concrete (Maddison et al., 2016)\n  distributions for use as a reparameterized continuous approximation to the\n  `Categorical` one-hot distribution. If you use this distribution, please cite\n  both papers.\n\n  #### Examples\n\n  Creates a continuous distribution, which approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution.\n\n  ```python\n  temperature = 0.5\n  p = [0.1, 0.5, 0.4]\n  dist = RelaxedOneHotCategorical(temperature, probs=p)\n  ```\n\n  Creates a continuous distribution, which approximates a 3-class one-hot\n  categorical distribution. The 2nd class is the most likely to be the\n  largest component in samples drawn from this distribution.\n\n  ```python\n  temperature = 0.5\n  logits = [-2, 2, 0]\n  dist = RelaxedOneHotCategorical(temperature, logits=logits)\n  ```\n\n  Creates a continuous distribution, which approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very low, samples from\n  this distribution are almost discrete, with one component almost 1 and the\n  others nearly 0. The 2nd class is the most likely to be the largest component\n  in samples drawn from this distribution.\n\n  ```python\n  temperature = 1e-5\n  logits = [-2, 2, 0]\n  dist = RelaxedOneHotCategorical(temperature, logits=logits)\n  ```\n\n  Creates a continuous distribution, which approximates a 3-class one-hot\n  categorical distribution. Because the temperature is very high, samples from\n  this distribution are usually close to the (1/3, 1/3, 1/3) vector. The 2nd\n  class is still the most likely to be the largest component\n  in samples drawn from this distribution.\n\n  ```python\n  temperature = 10\n  logits = [-2, 2, 0]\n  dist = RelaxedOneHotCategorical(temperature, logits=logits)\n  ```\n\n  Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with\n  Gumbel-Softmax. 2016.\n\n  Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution:\n  A Continuous Relaxation of Discrete Random Variables. 2016.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.relaxed_onehot_categorical.RelaxedOneHotCategorical",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "A Transformed Distribution.\n\n  A `TransformedDistribution` models `p(y)` given a base distribution `p(x)`,\n  and a deterministic, invertible, differentiable transform, `Y = g(X)`. The\n  transform is typically an instance of the `Bijector` class and the base\n  distribution is typically an instance of the `Distribution` class.\n\n  A `Bijector` is expected to implement the following functions:\n  - `forward`,\n  - `inverse`,\n  - `inverse_log_det_jacobian`.\n  The semantics of these functions are outlined in the `Bijector` documentation.\n\n  We now describe how a `TransformedDistribution` alters the input/outputs of a\n  `Distribution` associated with a random variable (rv) `X`.\n\n  Write `cdf(Y=y)` for an absolutely continuous cumulative distribution function\n  of random variable `Y`; write the probability density function `pdf(Y=y) :=\n  d^k / (dy_1,...,dy_k) cdf(Y=y)` for its derivative wrt to `Y` evaluated at\n  `y`. Assume that `Y = g(X)` where `g` is a deterministic diffeomorphism,\n  i.e., a non-random, continuous, differentiable, and invertible function.\n  Write the inverse of `g` as `X = g^{-1}(Y)` and `(J o g)(x)` for the Jacobian\n  of `g` evaluated at `x`.\n\n  A `TransformedDistribution` implements the following operations:\n\n    * `sample`\n      Mathematically:   `Y = g(X)`\n      Programmatically: `bijector.forward(distribution.sample(...))`\n\n    * `log_prob`\n      Mathematically:   `(log o pdf)(Y=y) = (log o pdf o g^{-1})(y)\n                         + (log o abs o det o J o g^{-1})(y)`\n      Programmatically: `(distribution.log_prob(bijector.inverse(y))\n                         + bijector.inverse_log_det_jacobian(y))`\n\n    * `log_cdf`\n      Mathematically:   `(log o cdf)(Y=y) = (log o cdf o g^{-1})(y)`\n      Programmatically: `distribution.log_cdf(bijector.inverse(x))`\n\n    * and similarly for: `cdf`, `prob`, `log_survival_function`,\n     `survival_function`.\n\n  A simple example constructing a Log-Normal distribution from a Normal\n  distribution:\n\n  ```python\n  ds = tf.contrib.distributions\n  log_normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Exp(),\n    name=\"LogNormalTransformedDistribution\")\n  ```\n\n  A `LogNormal` made from callables:\n\n  ```python\n  ds = tf.contrib.distributions\n  log_normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Inline(\n      forward_fn=tf.exp,\n      inverse_fn=tf.log,\n      inverse_log_det_jacobian_fn=(\n        lambda y: -tf.reduce_sum(tf.log(y), axis=-1)),\n    name=\"LogNormalTransformedDistribution\")\n  ```\n\n  Another example constructing a Normal from a StandardNormal:\n\n  ```python\n  ds = tf.contrib.distributions\n  normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Affine(\n      shift=-1.,\n      scale_identity_multiplier=2.)\n    name=\"NormalTransformedDistribution\")\n  ```\n\n  A `TransformedDistribution`'s batch- and event-shape are implied by the base\n  distribution unless explicitly overridden by `batch_shape` or `event_shape`\n  arguments. Specifying an overriding `batch_shape` (`event_shape`) is\n  permitted only if the base distribution has scalar batch-shape (event-shape).\n  The bijector is applied to the distribution as if the distribution possessed\n  the overridden shape(s). The following example demonstrates how to construct a\n  multivariate Normal as a `TransformedDistribution`.\n\n  ```python\n  ds = tf.contrib.distributions\n  # We will create two MVNs with batch_shape = event_shape = 2.\n  mean = [[-1., 0],      # batch:0\n          [0., 1]]       # batch:1\n  chol_cov = [[[1., 0],\n               [0, 1]],  # batch:0\n              [[1, 0],\n               [2, 2]]]  # batch:1\n  mvn1 = ds.TransformedDistribution(\n      distribution=ds.Normal(loc=0., scale=1.),\n      bijector=ds.bijectors.Affine(shift=mean, scale_tril=chol_cov),\n      batch_shape=[2],  # Valid because base_distribution.batch_shape == [].\n      event_shape=[2])  # Valid because base_distribution.event_shape == [].\n  mvn2 = ds.MultivariateNormalTriL(loc=mean, scale_tril=chol_cov)\n  # mvn1.log_prob(x) == mvn2.log_prob(x)\n  ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "A Transformed Distribution.\n\n  A `TransformedDistribution` models `p(y)` given a base distribution `p(x)`,\n  and a deterministic, invertible, differentiable transform, `Y = g(X)`. The\n  transform is typically an instance of the `Bijector` class and the base\n  distribution is typically an instance of the `Distribution` class.\n\n  A `Bijector` is expected to implement the following functions:\n  - `forward`,\n  - `inverse`,\n  - `inverse_log_det_jacobian`.\n  The semantics of these functions are outlined in the `Bijector` documentation.\n\n  We now describe how a `TransformedDistribution` alters the input/outputs of a\n  `Distribution` associated with a random variable (rv) `X`.\n\n  Write `cdf(Y=y)` for an absolutely continuous cumulative distribution function\n  of random variable `Y`; write the probability density function `pdf(Y=y) :=\n  d^k / (dy_1,...,dy_k) cdf(Y=y)` for its derivative wrt to `Y` evaluated at\n  `y`. Assume that `Y = g(X)` where `g` is a deterministic diffeomorphism,\n  i.e., a non-random, continuous, differentiable, and invertible function.\n  Write the inverse of `g` as `X = g^{-1}(Y)` and `(J o g)(x)` for the Jacobian\n  of `g` evaluated at `x`.\n\n  A `TransformedDistribution` implements the following operations:\n\n    * `sample`\n      Mathematically:   `Y = g(X)`\n      Programmatically: `bijector.forward(distribution.sample(...))`\n\n    * `log_prob`\n      Mathematically:   `(log o pdf)(Y=y) = (log o pdf o g^{-1})(y)\n                         + (log o abs o det o J o g^{-1})(y)`\n      Programmatically: `(distribution.log_prob(bijector.inverse(y))\n                         + bijector.inverse_log_det_jacobian(y))`\n\n    * `log_cdf`\n      Mathematically:   `(log o cdf)(Y=y) = (log o cdf o g^{-1})(y)`\n      Programmatically: `distribution.log_cdf(bijector.inverse(x))`\n\n    * and similarly for: `cdf`, `prob`, `log_survival_function`,\n     `survival_function`.\n\n  A simple example constructing a Log-Normal distribution from a Normal\n  distribution:\n\n  ```python\n  ds = tf.contrib.distributions\n  log_normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Exp(),\n    name=\"LogNormalTransformedDistribution\")\n  ```\n\n  A `LogNormal` made from callables:\n\n  ```python\n  ds = tf.contrib.distributions\n  log_normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Inline(\n      forward_fn=tf.exp,\n      inverse_fn=tf.log,\n      inverse_log_det_jacobian_fn=(\n        lambda y: -tf.reduce_sum(tf.log(y), axis=-1)),\n    name=\"LogNormalTransformedDistribution\")\n  ```\n\n  Another example constructing a Normal from a StandardNormal:\n\n  ```python\n  ds = tf.contrib.distributions\n  normal = ds.TransformedDistribution(\n    distribution=ds.Normal(loc=0., scale=1.),\n    bijector=ds.bijectors.Affine(\n      shift=-1.,\n      scale_identity_multiplier=2.)\n    name=\"NormalTransformedDistribution\")\n  ```\n\n  A `TransformedDistribution`'s batch- and event-shape are implied by the base\n  distribution unless explicitly overridden by `batch_shape` or `event_shape`\n  arguments. Specifying an overriding `batch_shape` (`event_shape`) is\n  permitted only if the base distribution has scalar batch-shape (event-shape).\n  The bijector is applied to the distribution as if the distribution possessed\n  the overridden shape(s). The following example demonstrates how to construct a\n  multivariate Normal as a `TransformedDistribution`.\n\n  ```python\n  ds = tf.contrib.distributions\n  # We will create two MVNs with batch_shape = event_shape = 2.\n  mean = [[-1., 0],      # batch:0\n          [0., 1]]       # batch:1\n  chol_cov = [[[1., 0],\n               [0, 1]],  # batch:0\n              [[1, 0],\n               [2, 2]]]  # batch:1\n  mvn1 = ds.TransformedDistribution(\n      distribution=ds.Normal(loc=0., scale=1.),\n      bijector=ds.bijectors.Affine(shift=mean, scale_tril=chol_cov),\n      batch_shape=[2],  # Valid because base_distribution.batch_shape == [].\n      event_shape=[2])  # Valid because base_distribution.event_shape == [].\n  mvn2 = ds.MultivariateNormalTriL(loc=mean, scale_tril=chol_cov)\n  # mvn1.log_prob(x) == mvn2.log_prob(x)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.TransformedDistribution",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Uniform distribution with `low` and `high` parameters.\n\n  #### Mathematical Details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; a, b) = I[a <= x < b] / Z\n  Z = b - a\n  ```\n\n  where\n\n  - `low = a`,\n  - `high = b`,\n  - `Z` is the normalizing constant, and\n  - `I[predicate]` is the [indicator function](\n    https://en.wikipedia.org/wiki/Indicator_function) for `predicate`.\n\n  The parameters `low` and `high` must be shaped in a way that supports\n  broadcasting (e.g., `high - low` is a valid operation).\n\n  #### Examples\n\n  ```python\n  # Without broadcasting:\n  u1 = Uniform(low=3.0, high=4.0)  # a single uniform distribution [3, 4]\n  u2 = Uniform(low=[1.0, 2.0],\n               high=[3.0, 4.0])  # 2 distributions [1, 3], [2, 4]\n  u3 = Uniform(low=[[1.0, 2.0],\n                    [3.0, 4.0]],\n               high=[[1.5, 2.5],\n                     [3.5, 4.5]])  # 4 distributions\n  ```\n\n  ```python\n  # With broadcasting:\n  u1 = Uniform(low=3.0, high=[5.0, 6.0, 7.0])  # 3 distributions\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.Uniform",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.deterministic._BaseDeterministic"
        ],
        "class_docstring": "Vector `Deterministic` distribution on `R^k`.\n\n  The `VectorDeterministic` distribution is parameterized by a [batch] point\n  `loc in R^k`.  The distribution is supported at this point only,\n  and corresponds to a random variable that is constant, equal to `loc`.\n\n  See [Degenerate rv](https://en.wikipedia.org/wiki/Degenerate_distribution).\n\n  #### Mathematical Details\n\n  The probability mass function (pmf) is\n\n  ```none\n  pmf(x; loc)\n    = 1, if All[Abs(x - loc) <= atol + rtol * Abs(loc)],\n    = 0, otherwise.\n  ```\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Initialize a single VectorDeterministic supported at [0., 2.] in R^2.\n  constant = tfd.Deterministic([0., 2.])\n  constant.prob([0., 2.])\n  ==> 1.\n  constant.prob([0., 3.])\n  ==> 0.\n\n  # Initialize a [3] batch of constants on R^2.\n  loc = [[0., 1.], [2., 3.], [4., 5.]]\n  constant = tfd.VectorDeterministic(loc)\n  constant.prob([[0., 1.], [1.9, 3.], [3.99, 5.]])\n  ==> [1., 0., 0.]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.deterministic.VectorDeterministic",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Computes `Y = g(X) = Abs(X)`, element-wise.\n\n  This non-injective bijector allows for transformations of scalar distributions\n  with the absolute value function, which maps `(-inf, inf)` to `[0, inf)`.\n\n  * For `y in (0, inf)`, `AbsoluteValue.inverse(y)` returns the set inverse\n    `{x in (-inf, inf) : |x| = y}` as a tuple, `-y, y`.\n  * `AbsoluteValue.inverse(0)` returns `0, 0`, which is not the set inverse\n    (the set inverse is the singleton `{0}`), but \"works\" in conjunction with\n    `TransformedDistribution` to produce a left semi-continuous pdf.\n  * For `y < 0`, `AbsoluteValue.inverse(y)` happily returns the\n    wrong thing, `-y, y`.  This is done for efficiency.  If\n    `validate_args == True`, `y < 0` will raise an exception.\n\n\n  ```python\n  tfd = tf.contrib.distributions\n\n  abs = tfd.bijectors.AbsoluteValue()\n\n  abs.forward([-1., 0., 1.])\n  ==> [1., 0.,  1.]\n\n  abs.inverse(1.)\n  ==> [-1., 1.]\n\n  # The |dX/dY| is constant, == 1.  So Log|dX/dY| == 0.\n  abs.inverse_log_det_jacobian(1.)\n  ==> [0., 0.]\n\n  # Special case handling of 0.\n  abs.inverse(0.)\n  ==> [0., 0.]\n\n  abs.inverse_log_det_jacobian(0.)\n  ==> [0., 0.]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.absolute_value.AbsoluteValue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X; shift, scale) = scale @ X + shift`.\n\n  `shift` is a numeric `Tensor` and `scale` is a `LinearOperator`.\n\n  If `X` is a scalar then the forward transformation is: `scale * X + shift`\n  where `*` denotes the scalar product.\n\n  Note: we don't always simply transpose `X` (but write it this way for\n  brevity). Actually the input `X` undergoes the following transformation\n  before being premultiplied by `scale`:\n\n  1. If there are no sample dims, we call `X = tf.expand_dims(X, 0)`, i.e.,\n     `new_sample_shape = [1]`. Otherwise do nothing.\n  2. The sample shape is flattened to have one dimension, i.e.,\n     `new_sample_shape = [n]` where `n = tf.reduce_prod(old_sample_shape)`.\n  3. The sample dim is cyclically rotated left by 1, i.e.,\n     `new_shape = [B1,...,Bb, k, n]` where `n` is as above, `k` is the\n     event_shape, and `B1,...,Bb` are the batch shapes for each of `b` batch\n     dimensions.\n\n  (For more details see `shape.make_batch_of_event_sample_matrices`.)\n\n  The result of the above transformation is that `X` can be regarded as a batch\n  of matrices where each column is a draw from the distribution. After\n  premultiplying by `scale`, we take the inverse of this procedure. The input\n  `Y` also undergoes the same transformation before/after premultiplying by\n  `inv(scale)`.\n\n  Example Use:\n\n  ```python\n  linalg = tf.linalg\n\n  x = [1., 2, 3]\n\n  shift = [-1., 0., 1]\n  diag = [1., 2, 3]\n  scale = linalg.LinearOperatorDiag(diag)\n  affine = AffineLinearOperator(shift, scale)\n  # In this case, `forward` is equivalent to:\n  # y = scale @ x + shift\n  y = affine.forward(x)  # [0., 4, 10]\n\n  shift = [2., 3, 1]\n  tril = [[1., 0, 0],\n          [2, 1, 0],\n          [3, 2, 1]]\n  scale = linalg.LinearOperatorLowerTriangular(tril)\n  affine = AffineLinearOperator(shift, scale)\n  # In this case, `forward` is equivalent to:\n  # np.squeeze(np.matmul(tril, np.expand_dims(x, -1)), -1) + shift\n  y = affine.forward(x)  # [3., 7, 11]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.affine_linear_operator.AffineLinearOperator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X; shift, scale) = scale * X + shift`.\n\n  Examples:\n\n  ```python\n  # Y = X\n  b = AffineScalar()\n\n  # Y = X + shift\n  b = AffineScalar(shift=[1., 2, 3])\n\n  # Y = 2 * X + shift\n  b = AffineScalar(\n    shift=[1., 2, 3],\n    scale=2.)\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.affine_scalar.AffineScalar",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) s.t. X = g^-1(Y) = (Y - mean(Y)) / std(Y)`.\n\n  Applies Batch Normalization [(Ioffe and Szegedy, 2015)][1] to samples from a\n  data distribution. This can be used to stabilize training of normalizing\n  flows ([Papamakarios et al., 2016][3]; [Dinh et al., 2017][2])\n\n  When training Deep Neural Networks (DNNs), it is common practice to\n  normalize or whiten features by shifting them to have zero mean and\n  scaling them to have unit variance.\n\n  The `inverse()` method of the `BatchNormalization` bijector, which is used in\n  the log-likelihood computation of data samples, implements the normalization\n  procedure (shift-and-scale) using the mean and standard deviation of the\n  current minibatch.\n\n  Conversely, the `forward()` method of the bijector de-normalizes samples (e.g.\n  `X*std(Y) + mean(Y)` with the running-average mean and standard deviation\n  computed at training-time. De-normalization is useful for sampling.\n\n  ```python\n\n  dist = tfd.TransformedDistribution(\n      distribution=tfd.Normal()),\n      bijector=tfb.BatchNorm())\n\n  y = tfd.MultivariateNormalDiag(loc=1., scale=2.).sample(100)  # ~ N(1, 2)\n  x = dist.bijector.inverse(y)  # ~ N(0, 1)\n  y = dist.sample()  # ~ N(1, 2)\n  ```\n\n  During training time, `BatchNorm.inverse` and `BatchNorm.forward` are not\n  guaranteed to be inverses of each other because `inverse(y)` uses statistics\n  of the current minibatch, while `forward(x)` uses running-average statistics\n  accumulated from training. In other words,\n  `BatchNorm.inverse(BatchNorm.forward(...))` and\n  `BatchNorm.forward(BatchNorm.inverse(...))` will be identical when\n  `training=False` but may be different when `training=True`.\n\n  #### References\n\n  [1]: Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating\n       Deep Network Training by Reducing Internal Covariate Shift. In\n       _International Conference on Machine Learning_, 2015.\n       https://arxiv.org/abs/1502.03167\n\n  [2]: Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation\n       using Real NVP. In _International Conference on Learning\n       Representations_, 2017. https://arxiv.org/abs/1605.08803\n\n  [3]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.batch_normalization.BatchNormalization",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which applies a sequence of bijectors.\n\n  Example Use:\n\n  ```python\n  chain = Chain([Exp(), Softplus()], name=\"one_plus_exp\")\n  ```\n\n  Results in:\n\n  * Forward:\n\n   ```python\n   exp = Exp()\n   softplus = Softplus()\n   Chain([exp, softplus]).forward(x)\n   = exp.forward(softplus.forward(x))\n   = tf.exp(tf.log(1. + tf.exp(x)))\n   = 1. + tf.exp(x)\n   ```\n\n  * Inverse:\n\n   ```python\n   exp = Exp()\n   softplus = Softplus()\n   Chain([exp, softplus]).inverse(y)\n   = softplus.inverse(exp.inverse(y))\n   = tf.log(tf.exp(tf.log(y)) - 1.)\n   = tf.log(y - 1.)\n   ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.chain.Chain",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `g(X) = X @ X.T`; X is lower-triangular, positive-diagonal matrix.\n\n  Note: the upper-triangular part of X is ignored (whether or not its zero).\n\n  The surjectivity of g as a map from  the set of n x n positive-diagonal\n  lower-triangular matrices to the set of SPD matrices follows immediately from\n  executing the Cholesky factorization algorithm on an SPD matrix A to produce a\n  positive-diagonal lower-triangular matrix L such that `A = L @ L.T`.\n\n  To prove the injectivity of g, suppose that L_1 and L_2 are lower-triangular\n  with positive diagonals and satisfy `A = L_1 @ L_1.T = L_2 @ L_2.T`. Then\n    `inv(L_1) @ A @ inv(L_1).T = [inv(L_1) @ L_2] @ [inv(L_1) @ L_2].T = I`.\n  Setting `L_3 := inv(L_1) @ L_2`, that L_3 is a positive-diagonal\n  lower-triangular matrix follows from `inv(L_1)` being positive-diagonal\n  lower-triangular (which follows from the diagonal of a triangular matrix being\n  its spectrum), and that the product of two positive-diagonal lower-triangular\n  matrices is another positive-diagonal lower-triangular matrix.\n\n  A simple inductive argument (proceeding one column of L_3 at a time) shows\n  that, if `I = L_3 @ L_3.T`, with L_3 being lower-triangular with positive-\n  diagonal, then `L_3 = I`. Thus, `L_1 = L_2`, proving injectivity of g.\n\n  #### Examples\n\n  ```python\n  bijector.CholeskyOuterProduct().forward(x=[[1., 0], [2, 1]])\n  # Result: [[1., 2], [2, 5]], i.e., x @ x.T\n\n  bijector.CholeskyOuterProduct().inverse(y=[[1., 2], [2, 5]])\n  # Result: [[1., 0], [2, 1]], i.e., cholesky(y).\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.CholeskyOuterProduct",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.bijectors.power_transform.PowerTransform"
        ],
        "class_docstring": "Compute `Y = g(X) = exp(X)`.\n\n    Example Use:\n\n    ```python\n    # Create the Y=g(X)=exp(X) transform which works only on Tensors with 1\n    # batch ndim 2.\n    exp = Exp()\n    x = [[[1., 2],\n           [3, 4]],\n          [[5, 6],\n           [7, 8]]]\n    exp(x) == exp.forward(x)\n    log(x) == exp.inverse(x)\n    ```\n\n    Note: the exp(.) is applied element-wise but the Jacobian is a reduction\n    over the event space.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.exp.Exp",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.bijectors.power_transform.PowerTransform"
        ],
        "class_docstring": "Compute `Y = g(X) = exp(X)`.\n\n    Example Use:\n\n    ```python\n    # Create the Y=g(X)=exp(X) transform which works only on Tensors with 1\n    # batch ndim 2.\n    exp = Exp()\n    x = [[[1., 2],\n           [3, 4]],\n          [[5, 6],\n           [7, 8]]]\n    exp(x) == exp.forward(x)\n    log(x) == exp.inverse(x)\n    ```\n\n    Note: the exp(.) is applied element-wise but the Jacobian is a reduction\n    over the event space.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.Exp",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Transforms vectors to triangular.\n\n  Triangular matrix elements are filled in a clockwise spiral.\n\n  Given input with shape `batch_shape + [d]`, produces output with\n  shape `batch_shape + [n, n]`, where\n   `n = (-1 + sqrt(1 + 8 * d))/2`.\n  This follows by solving the quadratic equation\n   `d = 1 + 2 + ... + n = n * (n + 1)/2`.\n\n  #### Example\n\n  ```python\n  b = tfb.FillTriangular(upper=False)\n  b.forward([1, 2, 3, 4, 5, 6])\n  # ==> [[4, 0, 0],\n  #      [6, 5, 0],\n  #      [3, 2, 1]]\n\n  b = tfb.FillTriangular(upper=True)\n  b.forward([1, 2, 3, 4, 5, 6])\n  # ==> [[1, 2, 3],\n  #      [0, 5, 6],\n  #      [0, 0, 4]]\n\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.FillTriangular",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) = exp(-exp(-(X - loc) / scale))`.\n\n  This bijector maps inputs from `[-inf, inf]` to [0, 1]`. The inverse of the\n  bijector applied to a uniform random variable `X ~ U(0, 1) gives back a\n  random variable with the\n  [Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution):\n\n  ```none\n  Y ~ Gumbel(loc, scale)\n  pdf(y; loc, scale) = exp(\n    -( (y - loc) / scale + exp(- (y - loc) / scale) ) ) / scale\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.gumbel.Gumbel",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute Y = g(X) = X.\n\n    Example Use:\n\n    ```python\n    # Create the Y=g(X)=X transform which is intended for Tensors with 1 batch\n    # ndim and 1 event ndim (i.e., vector of vectors).\n    identity = Identity()\n    x = [[1., 2],\n         [3, 4]]\n    x == identity.forward(x) == identity.inverse(x)\n    ```\n\n  ",
        "klass": "tensorflow.python.ops.distributions.identity_bijector.Identity",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector constructed from custom callables.\n\n  Example Use:\n\n  ```python\n  exp = Inline(\n    forward_fn=tf.exp,\n    inverse_fn=tf.log,\n    inverse_log_det_jacobian_fn=(\n      lambda y: -tf.reduce_sum(tf.log(y), axis=-1)),\n    name=\"exp\")\n  ```\n\n  The above example is equivalent to the `Bijector` `Exp()`.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.inline.Inline",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which inverts another Bijector.\n\n  Example Use: [ExpGammaDistribution (see Background & Context)](\n  https://reference.wolfram.com/language/ref/ExpGammaDistribution.html)\n  models `Y=log(X)` where `X ~ Gamma`.\n\n  ```python\n  exp_gamma_distribution = TransformedDistribution(\n    distribution=Gamma(concentration=1., rate=2.),\n    bijector=bijector.Invert(bijector.Exp())\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.Invert",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) = (1 - (1 - X)**(1 / b))**(1 / a), X in [0, 1]`.\n\n  This bijector maps inputs from `[0, 1]` to [0, 1]`. The inverse of the\n  bijector applied to a uniform random variable `X ~ U(0, 1) gives back a\n  random variable with the [Kumaraswamy distribution](\n  https://en.wikipedia.org/wiki/Kumaraswamy_distribution):\n\n  ```none\n  Y ~ Kumaraswamy(a, b)\n  pdf(y; a, b, 0 <= y <= 1) = a * b * y ** (a - 1) * (1 - y**a) ** (b - 1)\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.kumaraswamy.Kumaraswamy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Affine MaskedAutoregressiveFlow bijector for vector-valued events.\n\n  The affine autoregressive flow [(Papamakarios et al., 2016)][3] provides a\n  relatively simple framework for user-specified (deep) architectures to learn\n  a distribution over vector-valued events. Regarding terminology,\n\n    \"Autoregressive models decompose the joint density as a product of\n    conditionals, and model each conditional in turn. Normalizing flows\n    transform a base density (e.g. a standard Gaussian) into the target density\n    by an invertible transformation with tractable Jacobian.\"\n    [(Papamakarios et al., 2016)][3]\n\n  In other words, the \"autoregressive property\" is equivalent to the\n  decomposition, `p(x) = prod{ p(x[i] | x[0:i]) : i=0, ..., d }`. The provided\n  `shift_and_log_scale_fn`, `masked_autoregressive_default_template`, achieves\n  this property by zeroing out weights in its `masked_dense` layers.\n\n  In the `tfp` framework, a \"normalizing flow\" is implemented as a\n  `tfp.bijectors.Bijector`. The `forward` \"autoregression\"\n  is implemented using a `tf.while_loop` and a deep neural network (DNN) with\n  masked weights such that the autoregressive property is automatically met in\n  the `inverse`.\n\n  A `TransformedDistribution` using `MaskedAutoregressiveFlow(...)` uses the\n  (expensive) forward-mode calculation to draw samples and the (cheap)\n  reverse-mode calculation to compute log-probabilities. Conversely, a\n  `TransformedDistribution` using `Invert(MaskedAutoregressiveFlow(...))` uses\n  the (expensive) forward-mode calculation to compute log-probabilities and the\n  (cheap) reverse-mode calculation to compute samples.  See \"Example Use\"\n  [below] for more details.\n\n  Given a `shift_and_log_scale_fn`, the forward and inverse transformations are\n  (a sequence of) affine transformations. A \"valid\" `shift_and_log_scale_fn`\n  must compute each `shift` (aka `loc` or \"mu\" in [Germain et al. (2015)][1])\n  and `log(scale)` (aka \"alpha\" in [Germain et al. (2015)][1]) such that each\n  are broadcastable with the arguments to `forward` and `inverse`, i.e., such\n  that the calculations in `forward`, `inverse` [below] are possible.\n\n  For convenience, `masked_autoregressive_default_template` is offered as a\n  possible `shift_and_log_scale_fn` function. It implements the MADE\n  architecture [(Germain et al., 2015)][1]. MADE is a feed-forward network that\n  computes a `shift` and `log(scale)` using `masked_dense` layers in a deep\n  neural network. Weights are masked to ensure the autoregressive property. It\n  is possible that this architecture is suboptimal for your task. To build\n  alternative networks, either change the arguments to\n  `masked_autoregressive_default_template`, use the `masked_dense` function to\n  roll-out your own, or use some other architecture, e.g., using `tf.layers`.\n\n  Warning: no attempt is made to validate that the `shift_and_log_scale_fn`\n  enforces the \"autoregressive property\".\n\n  Assuming `shift_and_log_scale_fn` has valid shape and autoregressive\n  semantics, the forward transformation is\n\n  ```python\n  def forward(x):\n    y = zeros_like(x)\n    event_size = x.shape[-1]\n    for _ in range(event_size):\n      shift, log_scale = shift_and_log_scale_fn(y)\n      y = x * math_ops.exp(log_scale) + shift\n    return y\n  ```\n\n  and the inverse transformation is\n\n  ```python\n  def inverse(y):\n    shift, log_scale = shift_and_log_scale_fn(y)\n    return (y - shift) / math_ops.exp(log_scale)\n  ```\n\n  Notice that the `inverse` does not need a for-loop. This is because in the\n  forward pass each calculation of `shift` and `log_scale` is based on the `y`\n  calculated so far (not `x`). In the `inverse`, the `y` is fully known, thus is\n  equivalent to the scaling used in `forward` after `event_size` passes, i.e.,\n  the \"last\" `y` used to compute `shift`, `log_scale`. (Roughly speaking, this\n  also proves the transform is bijective.)\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n  tfb = tfp.bijectors\n\n  dims = 5\n\n  # A common choice for a normalizing flow is to use a Gaussian for the base\n  # distribution. (However, any continuous distribution would work.) E.g.,\n  maf = tfd.TransformedDistribution(\n      distribution=tfd.Normal(loc=0., scale=1.),\n      bijector=tfb.MaskedAutoregressiveFlow(\n          shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\n              hidden_layers=[512, 512])),\n      event_shape=[dims])\n\n  x = maf.sample()  # Expensive; uses `tf.while_loop`, no Bijector caching.\n  maf.log_prob(x)   # Almost free; uses Bijector caching.\n  maf.log_prob(0.)  # Cheap; no `tf.while_loop` despite no Bijector caching.\n\n  # [Papamakarios et al. (2016)][3] also describe an Inverse Autoregressive\n  # Flow [(Kingma et al., 2016)][2]:\n  iaf = tfd.TransformedDistribution(\n      distribution=tfd.Normal(loc=0., scale=1.),\n      bijector=tfb.Invert(tfb.MaskedAutoregressiveFlow(\n          shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\n              hidden_layers=[512, 512]))),\n      event_shape=[dims])\n\n  x = iaf.sample()  # Cheap; no `tf.while_loop` despite no Bijector caching.\n  iaf.log_prob(x)   # Almost free; uses Bijector caching.\n  iaf.log_prob(0.)  # Expensive; uses `tf.while_loop`, no Bijector caching.\n\n  # In many (if not most) cases the default `shift_and_log_scale_fn` will be a\n  # poor choice. Here's an example of using a \"shift only\" version and with a\n  # different number/depth of hidden layers.\n  shift_only = True\n  maf_no_scale_hidden2 = tfd.TransformedDistribution(\n      distribution=tfd.Normal(loc=0., scale=1.),\n      bijector=tfb.MaskedAutoregressiveFlow(\n          tfb.masked_autoregressive_default_template(\n              hidden_layers=[32],\n              shift_only=shift_only),\n          is_constant_jacobian=shift_only),\n      event_shape=[dims])\n  ```\n\n  #### References\n\n  [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:\n       Masked Autoencoder for Distribution Estimation. In _International\n       Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509\n\n  [2]: Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya\n       Sutskever, and Max Welling. Improving Variational Inference with Inverse\n       Autoregressive Flow. In _Neural Information Processing Systems_, 2016.\n       https://arxiv.org/abs/1606.04934\n\n  [3]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.masked_autoregressive.MaskedAutoregressiveFlow",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Computes `g(L) = inv(L)`, where `L` is a lower-triangular matrix.\n\n  `L` must be nonsingular; equivalently, all diagonal entries of `L` must be\n  nonzero.\n\n  The input must have `rank >= 2`.  The input is treated as a batch of matrices\n  with batch shape `input.shape[:-2]`, where each matrix has dimensions\n  `input.shape[-2]` by `input.shape[-1]` (hence `input.shape[-2]` must equal\n  `input.shape[-1]`).\n\n  #### Examples\n\n  ```python\n  tfd.bijectors.MatrixInverseTriL().forward(x=[[1., 0], [2, 1]])\n  # Result: [[1., 0], [-2, 1]], i.e., inv(x)\n\n  tfd.bijectors.MatrixInverseTriL().inverse(y=[[1., 0], [-2, 1]])\n  # Result: [[1., 0], [2, 1]], i.e., inv(y).\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.MatrixInverseTriL",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which maps a tensor x_k that has increasing elements in the last\n  dimension to an unconstrained tensor y_k.\n\n  Both the domain and the codomain of the mapping is [-inf, inf], however,\n  the input of the forward mapping must be strictly increasing.\n  The inverse of the bijector applied to a normal random vector `y ~ N(0, 1)`\n  gives back a sorted random vector with the same distribution `x ~ N(0, 1)`\n  where `x = sort(y)`\n\n  On the last dimension of the tensor, Ordered bijector performs:\n  `y[0] = x[0]`\n  `y[1:] = math_ops.log(x[1:] - x[:-1])`\n\n  #### Example Use:\n\n  ```python\n  bijector.Ordered().forward([2, 3, 4])\n  # Result: [2., 0., 0.]\n\n  bijector.Ordered().inverse([0.06428002, -1.07774478, -0.71530371])\n  # Result: [0.06428002, 0.40464228, 0.8936858]\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.ordered.Ordered",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Permutes the rightmost dimension of a `Tensor`.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfb = tfp.bijectors\n\n  reverse = tfb.Permute(permutation=[2, 1, 0])\n\n  reverse.forward([-1., 0., 1.])\n  # ==> [1., 0., -1]\n\n  reverse.inverse([1., 0., -1])\n  # ==> [-1., 0., 1.]\n\n  reverse.forward_log_det_jacobian(any_value)\n  # ==> 0.\n\n  reverse.inverse_log_det_jacobian(any_value)\n  # ==> 0.\n  ```\n\n  Warning: `tf.estimator` may repeatedly build the graph thus\n  `Permute(np.random.permutation(event_size)).astype(\"int32\"))` is not a\n  reliable parameterization (nor would it be even if using `tf.constant`). A\n  safe alternative is to use `tf.get_variable` to achieve \"init once\" behavior,\n  i.e.,\n\n  ```python\n  def init_once(x, name):\n    return tf.get_variable(name, initializer=x, trainable=False)\n\n  Permute(permutation=init_once(\n      np.random.permutation(event_size).astype(\"int32\"),\n      name=\"permutation\"))\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.permute.Permute",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) = (1 + X * c)**(1 / c), X >= -1 / c`.\n\n  The [power transform](https://en.wikipedia.org/wiki/Power_transform) maps\n  inputs from `[0, inf]` to `[-1/c, inf]`; this is equivalent to the `inverse`\n  of this bijector.\n\n  This bijector is equivalent to the `Exp` bijector when `c=0`.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.power_transform.PowerTransform",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "RealNVP \"affine coupling layer\" for vector-valued events.\n\n  Real NVP models a normalizing flow on a `D`-dimensional distribution via a\n  single `D-d`-dimensional conditional distribution [(Dinh et al., 2017)][1]:\n\n  `y[d:D] = y[d:D] * math_ops.exp(log_scale_fn(y[d:D])) + shift_fn(y[d:D])`\n  `y[0:d] = x[0:d]`\n\n  The last `D-d` units are scaled and shifted based on the first `d` units only,\n  while the first `d` units are 'masked' and left unchanged. Real NVP's\n  `shift_and_log_scale_fn` computes vector-valued quantities. For\n  scale-and-shift transforms that do not depend on any masked units, i.e.\n  `d=0`, use the `tfb.Affine` bijector with learned parameters instead.\n\n  Masking is currently only supported for base distributions with\n  `event_ndims=1`. For more sophisticated masking schemes like checkerboard or\n  channel-wise masking [(Papamakarios et al., 2016)[4], use the `tfb.Permute`\n  bijector to re-order desired masked units into the first `d` units. For base\n  distributions with `event_ndims > 1`, use the `tfb.Reshape` bijector to\n  flatten the event shape.\n\n  Recall that the MAF bijector [(Papamakarios et al., 2016)][4] implements a\n  normalizing flow via an autoregressive transformation. MAF and IAF have\n  opposite computational tradeoffs - MAF can train all units in parallel but\n  must sample units sequentially, while IAF must train units sequentially but\n  can sample in parallel. In contrast, Real NVP can compute both forward and\n  inverse computations in parallel. However, the lack of an autoregressive\n  transformations makes it less expressive on a per-bijector basis.\n\n  A \"valid\" `shift_and_log_scale_fn` must compute each `shift` (aka `loc` or\n  \"mu\" in [Papamakarios et al. (2016)][4]) and `log(scale)` (aka \"alpha\" in\n  [Papamakarios et al. (2016)][4]) such that each are broadcastable with the\n  arguments to `forward` and `inverse`, i.e., such that the calculations in\n  `forward`, `inverse` [below] are possible. For convenience,\n  `real_nvp_default_nvp` is offered as a possible `shift_and_log_scale_fn`\n  function.\n\n  NICE [(Dinh et al., 2014)][2] is a special case of the Real NVP bijector\n  which discards the scale transformation, resulting in a constant-time\n  inverse-log-determinant-Jacobian. To use a NICE bijector instead of Real\n  NVP, `shift_and_log_scale_fn` should return `(shift, None)`, and\n  `is_constant_jacobian` should be set to `True` in the `RealNVP` constructor.\n  Calling `real_nvp_default_template` with `shift_only=True` returns one such\n  NICE-compatible `shift_and_log_scale_fn`.\n\n  Caching: the scalar input depth `D` of the base distribution is not known at\n  construction time. The first call to any of `forward(x)`, `inverse(x)`,\n  `inverse_log_det_jacobian(x)`, or `forward_log_det_jacobian(x)` memoizes\n  `D`, which is re-used in subsequent calls. This shape must be known prior to\n  graph execution (which is the case if using tf.layers).\n\n  #### Example Use\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n  tfb = tfp.bijectors\n\n  # A common choice for a normalizing flow is to use a Gaussian for the base\n  # distribution. (However, any continuous distribution would work.) E.g.,\n  num_dims = 3\n  num_samples = 1\n  nvp = tfd.TransformedDistribution(\n      distribution=tfd.MultivariateNormalDiag(loc=np.zeros(num_dims)),\n      bijector=tfb.RealNVP(\n          num_masked=2,\n          shift_and_log_scale_fn=tfb.real_nvp_default_template(\n              hidden_layers=[512, 512])))\n\n  x = nvp.sample(num_samples)\n  nvp.log_prob(x)\n  nvp.log_prob(np.zeros([num_samples, num_dims]))\n  ```\n\n  For more examples, see [Jang (2018)][3].\n\n  #### References\n\n  [1]: Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation\n       using Real NVP. In _International Conference on Learning\n       Representations_, 2017. https://arxiv.org/abs/1605.08803\n\n  [2]: Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear\n       Independent Components Estimation. _arXiv preprint arXiv:1410.8516_,\n       2014. https://arxiv.org/abs/1410.8516\n\n  [3]: Eric Jang. Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows.\n       _Technical Report_, 2018. http://blog.evjang.com/2018/01/nf2.html\n\n  [4]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked\n       Autoregressive Flow for Density Estimation. In _Neural Information\n       Processing Systems_, 2017. https://arxiv.org/abs/1705.07057\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.real_nvp.RealNVP",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Reshapes the `event_shape` of a `Tensor`.\n\n  The semantics generally follow that of `tf.reshape()`, with\n  a few differences:\n\n  * The user must provide both the input and output shape, so that\n    the transformation can be inverted. If an input shape is not\n    specified, the default assumes a vector-shaped input, i.e.,\n    event_shape_in = (-1,).\n  * The `Reshape` bijector automatically broadcasts over the leftmost\n    dimensions of its input (`sample_shape` and `batch_shape`); only\n    the rightmost `event_ndims_in` dimensions are reshaped. The\n    number of dimensions to reshape is inferred from the provided\n    `event_shape_in` (`event_ndims_in = len(event_shape_in)`).\n\n  Example usage:\n  ```python\n\n  import tensorflow_probability as tfp\n  tfb = tfp.bijectors\n\n  r = tfb.Reshape(event_shape_out=[1, -1])\n\n  r.forward([3., 4.])    # shape [2]\n  # ==> [[3., 4.]]       # shape [1, 2]\n\n  r.forward([[1., 2.], [3., 4.]])  # shape [2, 2]\n  # ==> [[[1., 2.]],\n  #      [[3., 4.]]]   # shape [2, 1, 2]\n\n  r.inverse([[3., 4.]])  # shape [1,2]\n  # ==> [3., 4.]         # shape [2]\n\n  r.forward_log_det_jacobian(any_value)\n  # ==> 0.\n\n  r.inverse_log_det_jacobian(any_value)\n  # ==> 0.\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.reshape.Reshape",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.distributions.python.ops.bijectors.chain.Chain"
        ],
        "class_docstring": "Transforms unconstrained vectors to TriL matrices with positive diagonal.\n\n  This is implemented as a simple `tfb.Chain` of `tfb.FillTriangular`\n  followed by `tfb.TransformDiagonal`, and provided mostly as a\n  convenience. The default setup is somewhat opinionated, using a\n  Softplus transformation followed by a small shift (`1e-5`) which\n  attempts to avoid numerical issues from zeros on the diagonal.\n\n  #### Examples\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n  tfb = tfp.bijectors\n\n  b = tfb.ScaleTriL(\n       diag_bijector=tfb.Exp(),\n       diag_shift=None)\n  b.forward(x=[0., 0., 0.])\n  # Result: [[1., 0.],\n  #          [0., 1.]]\n  b.inverse(y=[[1., 0],\n               [.5, 2]])\n  # Result: [log(2), .5, log(1)]\n\n  # Define a distribution over PSD matrices of shape `[3, 3]`,\n  # with `1 + 2 + 3 = 6` degrees of freedom.\n  dist = tfd.TransformedDistribution(\n          tfd.Normal(tf.zeros(6), tf.ones(6)),\n          tfb.Chain([tfb.CholeskyOuterProduct(), tfb.ScaleTriL()]))\n\n  # Using an identity transformation, ScaleTriL is equivalent to\n  # tfb.FillTriangular.\n  b = tfb.ScaleTriL(\n       diag_bijector=tfb.Identity(),\n       diag_shift=None)\n\n  # For greater control over initialization, one can manually encode\n  # pre- and post- shifts inside of `diag_bijector`.\n  b = tfb.ScaleTriL(\n       diag_bijector=tfb.Chain([\n         tfb.AffineScalar(shift=1e-3),\n         tfb.Softplus(),\n         tfb.AffineScalar(shift=0.5413)]),  # softplus_inverse(1.)\n                                            #  = log(expm1(1.)) = 0.5413\n       diag_shift=None)\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.ScaleTriL",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which computes `Y = g(X) = 1 / (1 + exp(-X))`.",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.sigmoid.Sigmoid",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) = Sinh( (Arcsinh(X) + skewness) * tailweight )`.\n\n  For `skewness in (-inf, inf)` and `tailweight in (0, inf)`, this\n  transformation is a\n  diffeomorphism of the real line `(-inf, inf)`.  The inverse transform is\n  `X = g^{-1}(Y) = Sinh( ArcSinh(Y) / tailweight - skewness )`.\n\n  The `SinhArcsinh` transformation of the Normal is described in\n  [Sinh-arcsinh distributions](https://www.jstor.org/stable/27798865)\n  This Bijector allows a similar transformation of any distribution supported on\n  `(-inf, inf)`.\n\n  #### Meaning of the parameters\n\n  * If `skewness = 0` and `tailweight = 1`, this transform is the identity.\n  * Positive (negative) `skewness` leads to positive (negative) skew.\n    * positive skew means, for unimodal `X` centered at zero, the mode of `Y` is\n      \"tilted\" to the right.\n    * positive skew means positive values of `Y` become more likely, and\n      negative values become less likely.\n  * Larger (smaller) `tailweight` leads to fatter (thinner) tails.\n    * Fatter tails mean larger values of `|Y|` become more likely.\n    * If `X` is a unit Normal, `tailweight < 1` leads to a distribution that is\n      \"flat\" around `Y = 0`, and a very steep drop-off in the tails.\n    * If `X` is a unit Normal, `tailweight > 1` leads to a distribution more\n      peaked at the mode with heavier tails.\n\n  To see the argument about the tails, note that for `|X| >> 1` and\n  `|X| >> (|skewness| * tailweight)**tailweight`, we have\n  `Y approx 0.5 X**tailweight e**(sign(X) skewness * tailweight)`.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.sinh_arcsinh.SinhArcsinh",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which computes `Y = g(X) = exp([X 0]) / sum(exp([X 0]))`.\n\n  To implement [softmax](https://en.wikipedia.org/wiki/Softmax_function) as a\n  bijection, the forward transformation appends a value to the input and the\n  inverse removes this coordinate. The appended coordinate represents a pivot,\n  e.g., `softmax(x) = exp(x-c) / sum(exp(x-c))` where `c` is the implicit last\n  coordinate.\n\n  Example Use:\n\n  ```python\n  bijector.SoftmaxCentered().forward(tf.log([2, 3, 4]))\n  # Result: [0.2, 0.3, 0.4, 0.1]\n  # Extra result: 0.1\n\n  bijector.SoftmaxCentered().inverse([0.2, 0.3, 0.4, 0.1])\n  # Result: tf.log([2, 3, 4])\n  # Extra coordinate removed.\n  ```\n\n  At first blush it may seem like the [Invariance of domain](\n  https://en.wikipedia.org/wiki/Invariance_of_domain) theorem implies this\n  implementation is not a bijection. However, the appended dimension\n  makes the (forward) image non-open and the theorem does not directly apply.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.softmax_centered.SoftmaxCentered",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which computes `Y = g(X) = Log[1 + exp(X)]`.\n\n  The softplus `Bijector` has the following two useful properties:\n\n  * The domain is the positive real numbers\n  * `softplus(x) approx x`, for large `x`, so it does not overflow as easily as\n    the `Exp` `Bijector`.\n\n  The optional nonzero `hinge_softness` parameter changes the transition at\n  zero.  With `hinge_softness = c`, the bijector is:\n\n    ```f_c(x) := c * g(x / c) = c * Log[1 + exp(x / c)].```\n\n  For large `x >> 1`, `c * Log[1 + exp(x / c)] approx c * Log[exp(x / c)] = x`,\n  so the behavior for large `x` is the same as the standard softplus.\n\n  As `c > 0` approaches 0 from the right, `f_c(x)` becomes less and less soft,\n  approaching `max(0, x)`.\n\n  * `c = 1` is the default.\n  * `c > 0` but small means `f(x) approx ReLu(x) = max(0, x)`.\n  * `c < 0` flips sign and reflects around the `y-axis`: `f_{-c}(x) = -f_c(-x)`.\n  * `c = 0` results in a non-bijective transformation and triggers an exception.\n\n    Example Use:\n\n    ```python\n    # Create the Y=g(X)=softplus(X) transform which works only on Tensors with 1\n    # batch ndim and 2 event ndims (i.e., vector of matrices).\n    softplus = Softplus()\n    x = [[[1., 2],\n          [3, 4]],\n         [[5, 6],\n          [7, 8]]]\n    log(1 + exp(x)) == softplus.forward(x)\n    log(exp(x) - 1) == softplus.inverse(x)\n    ```\n\n    Note: log(.) and exp(.) are applied element-wise but the Jacobian is a\n    reduction over the event space.\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.softplus.Softplus",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Bijector which computes `Y = g(X) = X / (1 + |X|)`.\n\n  The softsign `Bijector` has the following two useful properties:\n\n  * The domain is all real numbers\n  * `softsign(x) approx sgn(x)`, for large `|x|`.\n\n  #### Examples\n\n  ```python\n  # Create the Y = softsign(X) transform.\n  softsign = Softsign()\n  x = [[[1., 2],\n        [3, 4]],\n       [[5, 6],\n        [7, 8]]]\n  x / (1 + abs(x)) == softsign.forward(x)\n  x / (1 - abs(x)) == softsign.inverse(x)\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.softsign.Softsign",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `g(X) = X^2`; X is a positive real number.\n\n  g is a bijection between the non-negative real numbers (R_+) and the\n  non-negative real numbers.\n\n  #### Examples\n\n  ```python\n  bijector.Square().forward(x=[[1., 0], [2, 1]])\n  # Result: [[1., 0], [4, 1]], i.e., x^2\n\n  bijector.Square().inverse(y=[[1., 4], [9, 1]])\n  # Result: [[1., 2], [3, 1]], i.e., sqrt(y).\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.Square",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Applies a Bijector to the diagonal of a matrix.\n\n  #### Example\n\n  ```python\n  b = tfb.TransformDiagonal(diag_bijector=tfb.Exp())\n\n  b.forward([[1., 0.],\n             [0., 1.]])\n  # ==> [[2.718, 0.],\n         [0., 2.718]]\n  ```\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.TransformDiagonal",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.bijector_impl.Bijector"
        ],
        "class_docstring": "Compute `Y = g(X) = 1 - exp((-X / scale) ** concentration), X >= 0`.\n\n  This bijector maps inputs from `[0, inf]` to [0, 1]`. The inverse of the\n  bijector applied to a uniform random variable `X ~ U(0, 1) gives back a\n  random variable with the\n  [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution):\n\n  ```none\n  Y ~ Weibull(scale, concentration)\n  pdf(y; scale, concentration, y >= 0) = (scale / concentration) * (\n    scale / concentration) ** (concentration - 1) * exp(\n      -(y / scale) ** concentration)\n  ```\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.bijectors.weibull.Weibull",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution"
        ],
        "class_docstring": "A vector version of Student's t-distribution on `R^k`.\n\n  #### Mathematical details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; df, mu, Sigma) = (1 + ||y||**2 / df)**(-0.5 (df + 1)) / Z\n  where,\n  y = inv(Sigma) (x - mu)\n  Z = abs(det(Sigma)) ( sqrt(df pi) Gamma(0.5 df) / Gamma(0.5 (df + 1)) )**k\n  ```\n\n  where:\n  * `loc = mu`; a vector in `R^k`,\n  * `scale = Sigma`; a lower-triangular matrix in `R^{k x k}`,\n  * `Z` denotes the normalization constant, and,\n  * `Gamma` is the [gamma function](\n    https://en.wikipedia.org/wiki/Gamma_function), and,\n  * `||y||**2` denotes the [squared Euclidean norm](\n  https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of `y`.\n\n  The VectorStudentT distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ StudentT(df, loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  Notice that the `scale` matrix has semantics closer to std. deviation than\n  covariance (but it is not std. deviation).\n\n  This distribution is an Affine transformation of iid\n  [Student's t-distributions](\n  https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n  and should not be confused with the [Multivariate Student's t-distribution](\n  https://en.wikipedia.org/wiki/Multivariate_t-distribution). The\n  traditional Multivariate Student's t-distribution is type of\n  [elliptical distribution](\n  https://en.wikipedia.org/wiki/Elliptical_distribution); it has PDF:\n\n  ```none\n  pdf(x; df, mu, Sigma) = (1 + ||y||**2 / df)**(-0.5 (df + k)) / Z\n  where,\n  y = inv(Sigma) (x - mu)\n  Z = abs(det(Sigma)) sqrt(df pi)**k Gamma(0.5 df) / Gamma(0.5 (df + k))\n  ```\n\n  Notice that the Multivariate Student's t-distribution uses `k` where the\n  Vector Student's t-distribution has a `1`. Conversely the Vector version has a\n  broader application of the power-`k` in the normalization constant.\n\n  #### Examples\n\n  A single instance of a \"Vector Student's t-distribution\" is defined by a mean\n  vector of length `k` and a scale matrix of shape `k x k`.\n\n  Extra leading dimensions, if provided, allow for batches.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Initialize a single 3-variate vector Student's t-distribution.\n  mu = [1., 2, 3]\n  chol = [[1., 0, 0.],\n          [1, 3, 0],\n          [1, 2, 3]]\n  vt = tfd.VectorStudentT(df=2, loc=mu, scale_tril=chol)\n\n  # Evaluate this on an observation in R^3, returning a scalar.\n  vt.prob([-1., 0, 1])\n\n  # Initialize a batch of two 3-variate vector Student's t-distributions.\n  mu = [[1., 2, 3],\n        [11, 22, 33]]\n  chol = ...  # shape 2 x 3 x 3, lower triangular, positive diagonal.\n  vt = tfd.VectorStudentT(loc=mu, scale_tril=chol)\n\n  # Evaluate this on a two observations, each in R^3, returning a length two\n  # tensor.\n  x = [[-1, 0, 1],\n       [-11, 0, 11]]\n  vt.prob(x)\n  ```\n\n  For more examples of how to construct the `scale` matrix, see the\n  `tf.contrib.distributions.bijectors.Affine` docstring.\n\n  ",
        "klass": "tensorflow.contrib.distributions.python.ops.vector_student_t._VectorStudentT",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Single residual block contained in a _RevBlock. Each `_Residual` object has\n  two _ResidualInner objects, corresponding to the `F` and `G` functions in the\n  paper.\n  ",
        "klass": "tensorflow.contrib.eager.python.examples.revnet.blocks._Residual",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "RevNet that depends on all the blocks.",
        "klass": "tensorflow.contrib.eager.python.examples.revnet.revnet.RevNet",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.eager.python.metrics_impl.Mean"
        ],
        "class_docstring": "Calculates how often `predictions` matches `labels`.\n  Attributes:\n    name: name of the accuracy object\n    dtype: data type of the tensor\n  ",
        "klass": "tensorflow.contrib.eager.python.metrics.Accuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.eager.python.metrics_impl.Mean"
        ],
        "class_docstring": "Calculates how often `predictions` matches `labels`.\n\n  This class is compatible with `tf.keras.losses.binary_crossentropy`,\n  `tf.losses.sigmoid_cross_entropy`,\n  `tf.nn.sigmoid_cross_entropy_with_logits`.\n  If there is more than one label, this will become multi-label classification.\n\n  Attributes:\n    name: name of the accuracy object.\n    threshold: Used for rounding off the predictions.\n               If the predictions are,\n                1. probabilities then set the threshold to 0.5.\n                2. logits then set the threshold to 0.\n              You can set the threshold appropriately,\n              to trade off with precision and recall.\n    dtype: data type of tensor.\n  ",
        "klass": "tensorflow.contrib.eager.python.metrics.BinaryAccuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.eager.python.metrics_impl.Mean"
        ],
        "class_docstring": "Calculates how often `predictions` matches `labels`.\n\n  This class is compatible with `tf.keras.losses.categorical_crossentropy`,\n  `tf.nn.softmax_cross_entropy_with_logits_v2`,\n  `tf.losses.softmax_cross_entropy`.\n\n  Attributes:\n    name: name of the accuracy object.\n    dtype: data type of tensor.\n  ",
        "klass": "tensorflow.contrib.eager.python.metrics.CategoricalAccuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.eager.python.metrics_impl.Metric"
        ],
        "class_docstring": "Computes the (weighted) mean of the given values.",
        "klass": "tensorflow.contrib.eager.python.metrics.Mean",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.eager.python.metrics_impl.Mean"
        ],
        "class_docstring": "Calculates how often `predictions` matches `labels`.\n\n  This class is compatible with\n  `tf.keras.losses.sparse_categorical_crossentropy`,\n  `tf.nn.sparse_softmax_cross_entropy_with_logits`,\n  `tf.losses.sparse_softmax_cross_entropy`.\n\n  Attributes:\n    name: name of the accuracy object\n    dtype: data type of tensor.\n  ",
        "klass": "tensorflow.contrib.eager.python.metrics.SparseAccuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.variables.RefVariable"
        ],
        "class_docstring": "Variable based on resource handles.\n\n  See the [Variables How To](https://tensorflow.org/guide/variables)\n  for a high level overview.\n\n  A `ResourceVariable` allows you to maintain state across subsequent calls to\n  session.run.\n\n  The `ResourceVariable` constructor requires an initial value for the variable,\n  which can be a `Tensor` of any type and shape. The initial value defines the\n  type and shape of the variable. After construction, the type and shape of\n  the variable are fixed. The value can be changed using one of the assign\n  methods.\n\n  Just like any `Tensor`, variables created with\n  `tf.Variable(use_resource=True)` can be used as inputs for other Ops in the\n  graph. Additionally, all the operators overloaded for the `Tensor` class are\n  carried over to variables, so you can also add nodes to the graph by just\n  doing arithmetic on variables.\n\n  Unlike ref-based variable, a ResourceVariable has well-defined semantics. Each\n  usage of a ResourceVariable in a TensorFlow graph adds a read_value operation\n  to the graph. The Tensors returned by a read_value operation are guaranteed to\n  see all modifications to the value of the variable which happen in any\n  operation on which the read_value depends on (either directly, indirectly, or\n  via a control dependency) and guaranteed to not see any modification to the\n  value of the variable from operations that depend on the read_value operation.\n  Updates from operations that have no dependency relationship to the read_value\n  operation might or might not be visible to read_value.\n\n  For example, if there is more than one assignment to a ResourceVariable in\n  a single session.run call there is a well-defined value for each operation\n  which uses the variable's value if the assignments and the read are connected\n  by edges in the graph. Consider the following example, in which two writes\n  can cause tf.Variable and tf.ResourceVariable to behave differently:\n\n  ```python\n  a = tf.Variable(1.0, use_resource=True)\n  a.initializer.run()\n\n  assign = a.assign(2.0)\n  with tf.control_dependencies([assign]):\n    b = a.read_value()\n  with tf.control_dependencies([b]):\n    other_assign = a.assign(3.0)\n  with tf.control_dependencies([other_assign]):\n    # Will print 2.0 because the value was read before other_assign ran. If\n    # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.\n    tf.Print(b, [b]).eval()\n  ```\n  ",
        "klass": "tensorflow.python.ops.resource_variable_ops.ResourceVariable",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "defaultdict(default_factory[, ...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.\n",
        "klass": "collections.defaultdict",
        "module": "collections"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "An estimator for GMM clustering.",
        "klass": "tensorflow.contrib.factorization.python.ops.gmm.GMM",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "An estimator for GMM clustering.",
        "klass": "tensorflow.contrib.factorization.GMM",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tensorflow Gaussian mixture model clustering class.",
        "klass": "tensorflow.contrib.factorization.python.ops.gmm_ops.GmmAlgorithm",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tensorflow Gaussian mixture model clustering class.",
        "klass": "tensorflow.contrib.factorization.GmmAlgorithm",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An Estimator for K-Means clustering.\n\n  Example:\n  ```\n  import numpy as np\n  import tensorflow as tf\n\n  num_points = 100\n  dimensions = 2\n  points = np.random.uniform(0, 1000, [num_points, dimensions])\n\n  def input_fn():\n    return tf.train.limit_epochs(\n        tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\n\n  num_clusters = 5\n  kmeans = tf.contrib.factorization.KMeansClustering(\n      num_clusters=num_clusters, use_mini_batch=False)\n\n  # train\n  num_iterations = 10\n  previous_centers = None\n  for _ in xrange(num_iterations):\n    kmeans.train(input_fn)\n    cluster_centers = kmeans.cluster_centers()\n    if previous_centers is not None:\n      print 'delta:', cluster_centers - previous_centers\n    previous_centers = cluster_centers\n    print 'score:', kmeans.score(input_fn)\n  print 'cluster centers:', cluster_centers\n\n  # map the input points to their clusters\n  cluster_indices = list(kmeans.predict_cluster_index(input_fn))\n  for i, point in enumerate(points):\n    cluster_index = cluster_indices[i]\n    center = cluster_centers[cluster_index]\n    print 'point:', point, 'is in cluster', cluster_index, 'centered at', center\n  ```\n\n  The `SavedModel` saved by the `export_savedmodel` method does not include the\n  cluster centers. However, the cluster centers may be retrieved by the\n  latest checkpoint saved during training. Specifically,\n  ```\n  kmeans.cluster_centers()\n  ```\n  is equivalent to\n  ```\n  tf.train.load_variable(\n      kmeans.model_dir, KMeansClustering.CLUSTER_CENTERS_VAR_NAME)\n  ```\n  ",
        "klass": "tensorflow.contrib.factorization.KMeansClustering",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.framework.tensor_spec.TensorSpec"
        ],
        "class_docstring": "A `TensorSpec` that specifies minimum and maximum values.\n\n  Example usage:\n  ```python\n  spec = tensor_spec.BoundedTensorSpec((1, 2, 3), tf.float32, 0, (5, 5, 5))\n  tf_minimum = tf.convert_to_tensor(spec.minimum, dtype=spec.dtype)\n  tf_maximum = tf.convert_to_tensor(spec.maximum, dtype=spec.dtype)\n  ```\n\n  Bounds are meant to be inclusive. This is especially important for\n  integer types. The following spec will be satisfied by tensors\n  with values in the set {0, 1, 2}:\n  ```python\n  spec = tensor_spec.BoundedTensorSpec((3, 5), tf.int32, 0, 2)\n  ```\n  ",
        "klass": "tensorflow.python.framework.tensor_spec.BoundedTensorSpec",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for Generative Adversarial Networks (GANs).\n\n  This Estimator is backed by TFGAN. The network functions follow the TFGAN API\n  except for one exception: if either `generator_fn` or `discriminator_fn` have\n  an argument called `mode`, then the tf.Estimator mode is passed in for that\n  argument. This helps with operations like batch normalization, which have\n  different train and evaluation behavior.\n\n  Example:\n\n  ```python\n      import tensorflow as tf\n      tfgan = tf.contrib.gan\n\n      # See TFGAN's `train.py` for a description of the generator and\n      # discriminator API.\n      def generator_fn(generator_inputs):\n        ...\n        return generated_data\n\n      def discriminator_fn(data, conditioning):\n        ...\n        return logits\n\n      # Create GAN estimator.\n      stargan_estimator = tfgan.estimator.StarGANEstimator(\n          model_dir,\n          generator_fn=generator_fn,\n          discriminator_fn=discriminator_fn,\n          loss_fn=loss_fn,\n          generator_optimizer=tf.train.AdamOptimizer(0.1, 0.5),\n          discriminator_optimizer=tf.train.AdamOptimizer(0.1, 0.5))\n\n      # Train estimator.\n      stargan_estimator.train(train_input_fn, steps)\n\n      # Evaluate resulting estimator.\n      stargan_estimator.evaluate(eval_input_fn)\n\n      # Generate samples from generator.\n      stargan_estimator = np.array([\n          x for x in stargan_estimator.predict(predict_input_fn)])\n  ```\n  ",
        "klass": "tensorflow.contrib.gan.python.estimator.python.stargan_estimator_impl.StarGANEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "_io._TextIOBase"
        ],
        "class_docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.",
        "klass": "_io.StringIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "tensorflow.python.data.ops.dataset_ops.DatasetSource"
        ],
        "class_docstring": "A Sequence File Dataset that reads the sequence file.",
        "klass": "tensorflow.contrib.hadoop.python.ops.hadoop_dataset_ops.SequenceFileDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.ops.dataset_ops.DatasetSource"
        ],
        "class_docstring": "A Kafka Dataset that consumes the message.\n  ",
        "klass": "tensorflow.contrib.kafka.python.ops.kafka_dataset_ops.KafkaDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.kernel_methods.python.kernel_estimators._KernelEstimator"
        ],
        "class_docstring": "Linear classifier using kernel methods as feature preprocessing.\n\n  It trains a linear model after possibly mapping initial input features into\n  a mapped space using explicit kernel mappings. Due to the kernel mappings,\n  training a linear classifier in the mapped (output) space can detect\n  non-linearities in the input space.\n\n  The user can provide a list of kernel mappers to be applied to all or a subset\n  of existing feature_columns. This way, the user can effectively provide 2\n  types of feature columns:\n\n  * those passed as elements of feature_columns in the classifier's constructor\n  * those appearing as a key of the kernel_mappers dict.\n\n  If a column appears in feature_columns only, no mapping is applied to it. If\n  it appears as a key in kernel_mappers, the corresponding kernel mappers are\n  applied to it. Note that it is possible that a column appears in both places.\n  Currently kernel_mappers are supported for _RealValuedColumns only.\n\n  Example usage:\n  ```\n  real_column_a = real_valued_column(name='real_column_a',...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n  kernel_mappers = {real_column_a : [RandomFourierFeatureMapper(...)]}\n  optimizer = ...\n\n  # real_column_a is used as a feature in both its initial and its transformed\n  # (mapped) form. sparse_column_b is not affected by kernel mappers.\n  kernel_classifier = KernelLinearClassifier(\n      feature_columns=[real_column_a, sparse_column_b],\n      model_dir=...,\n      optimizer=optimizer,\n      kernel_mappers=kernel_mappers)\n\n  # real_column_a is used as a feature in its transformed (mapped) form only.\n  # sparse_column_b is not affected by kernel mappers.\n  kernel_classifier = KernelLinearClassifier(\n      feature_columns=[sparse_column_b],\n      model_dir=...,\n      optimizer=optimizer,\n      kernel_mappers=kernel_mappers)\n\n  # Input builders\n  def train_input_fn: # returns x, y\n    ...\n  def eval_input_fn: # returns x, y\n    ...\n\n  kernel_classifier.fit(input_fn=train_input_fn)\n  kernel_classifier.evaluate(input_fn=eval_input_fn)\n  kernel_classifier.predict(...)\n  ```\n\n  Input of `fit` and `evaluate` should have following features, otherwise there\n  will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.kernel_methods.python.kernel_estimators.KernelLinearClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.ops.dataset_ops.DatasetSource"
        ],
        "class_docstring": "A Kinesis Dataset that consumes the message.\n\n  Kinesis is a managed service provided by AWS for data streaming.\n  This dataset reads messages from Kinesis with each message presented\n  as a `tf.string`.\n\n  For example, we can construct and use the KinesisDataset as follows:\n  ```python\n  tf.enable_eager_execution()\n\n  dataset = tf.contrib.kinesis.KinesisDataset(\n      \"kinesis_stream_name\", read_indefinitely=False)\n  for element in dataset:\n    print(element)\n  ```\n\n  Since Kinesis is a data streaming service, data may not be available\n  at the time it is being read. The argument `read_indefinitely` is\n  used to control the behavior in this situation. If `read_indefinitely`\n  is `True`, then `KinesisDataset` will keep retrying to retrieve data\n  from the stream. If `read_indefinitely` is `False`, an `OutOfRangeError`\n  is returned immediately instead.\n  ",
        "klass": "tensorflow.contrib.kinesis.python.ops.kinesis_dataset_ops.KinesisDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Utility class for mapping to and from another shape.\n\n  For example, say you have a function `crop_center` which expects a\n  LabeledTensor with axes named ['batch', 'row', 'column', 'depth'], and\n  you have a LabeledTensor `masked_image_lt` with axes ['batch', 'row',\n  'column', 'channel', 'mask'].\n\n  To call `crop_center` with `masked_image_lt` you'd normally have to write:\n\n  >>> reshape_lt = lt.reshape(masked_image_lt, ['channel', 'mask'], ['depth'])\n  >>> crop_lt = crop_center(reshape_lt)\n  >>> result_lt = lt.reshape(crop_lt, ['depth'],\n  ...   [masked_image_lt.axes['channel'], masked_image_lt.axes['mask']])\n\n  ReshapeCoder takes care of this renaming logic for you, allowing you to\n  instead write:\n\n  >>> rc = ReshapeCoder(['channel', 'mask'], ['depth'])\n  >>> result_lt = rc.decode(crop_center(rc.encode(masked_image_lt)))\n\n  Here, `decode` restores the original axes 'channel' and 'mask', so\n  `crop_center` must not have modified the size of the 'depth' axis.\n  ",
        "klass": "tensorflow.contrib.labeled_tensor.python.ops.sugar.ReshapeCoder",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Dictionary that remembers insertion order",
        "klass": "collections.OrderedDict",
        "module": "collections"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A classifier for TensorFlow DNN models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n\n  def input_fn_predict: # returns x, None\n    pass\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n     `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.DNNClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A classifier for TensorFlow Linear and DNN joined training models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Note: New users must set `fix_global_step_increment_bug=True` when creating an\n  estimator.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNLinearCombinedClassifier(\n      # common settings\n      n_classes=n_classes,\n      weight_column_name=weight_column_name,\n      # wide settings\n      linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],\n      linear_optimizer=tf.train.FtrlOptimizer(...),\n      # deep settings\n      dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.train.AdagradOptimizer(...))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_predict: # returns x, None.\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = DNNLinearCombinedClassifier(\n      n_classes=n_classes,\n      linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],\n      dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      dnn_hidden_units=[1000, 500, 100],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n        `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n      - if `column` is a `SparseColumn`, a feature with `key=column.name`\n        whose `value` is a `SparseTensor`.\n      - if `column` is a `WeightedSparseColumn`, two features: the first with\n        `key` the id column name, the second with `key` the weight column name.\n        Both features' `value` must be a `SparseTensor`.\n      - if `column` is a `RealValuedColumn, a feature with `key=column.name`\n        whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.dnn_linear_combined.DNNLinearCombinedClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A classifier for TensorFlow Linear and DNN joined training models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Note: New users must set `fix_global_step_increment_bug=True` when creating an\n  estimator.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNLinearCombinedClassifier(\n      # common settings\n      n_classes=n_classes,\n      weight_column_name=weight_column_name,\n      # wide settings\n      linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],\n      linear_optimizer=tf.train.FtrlOptimizer(...),\n      # deep settings\n      dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.train.AdagradOptimizer(...))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_predict: # returns x, None.\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = DNNLinearCombinedClassifier(\n      n_classes=n_classes,\n      linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],\n      dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      dnn_hidden_units=[1000, 500, 100],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n        `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n      - if `column` is a `SparseColumn`, a feature with `key=column.name`\n        whose `value` is a `SparseTensor`.\n      - if `column` is a `WeightedSparseColumn`, two features: the first with\n        `key` the id column name, the second with `key` the weight column name.\n        Both features' `value` must be a `SparseTensor`.\n      - if `column` is a `RealValuedColumn, a feature with `key=column.name`\n        whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.DNNLinearCombinedClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A regressor for TensorFlow DNN models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNRegressor(\n      feature_columns=[sparse_feature_a, sparse_feature_b],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNRegressor(\n      feature_columns=[sparse_feature_a, sparse_feature_b],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n    pass\n  estimator.predict_scores(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.dnn.DNNRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A regressor for TensorFlow DNN models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNRegressor(\n      feature_columns=[sparse_feature_a, sparse_feature_b],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNRegressor(\n      feature_columns=[sparse_feature_a, sparse_feature_b],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n    pass\n  estimator.predict_scores(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.DNNRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Dynamically unrolled RNN (deprecated).\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n  ",
        "klass": "tensorflow.contrib.learn.DynamicRnnEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.BaseEstimator"
        ],
        "class_docstring": "Estimator class is the basic TensorFlow model trainer/evaluator.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.BaseEstimator"
        ],
        "class_docstring": "Estimator class is the basic TensorFlow model trainer/evaluator.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n  ",
        "klass": "tensorflow.contrib.learn.Estimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Experiment is a class containing all information needed to train a model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  After an experiment is created (by passing an Estimator and inputs for\n  training and evaluation), an Experiment instance knows how to invoke training\n  and eval loops in a sensible fashion for distributed training.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.experiment.Experiment",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "An Estimator for K-Means clustering.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.kmeans.KMeansClustering",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Linear classifier model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Train a linear model to classify instances into one of multiple possible\n  classes. When number of possible classes is 2, this is binary classification.\n\n  Example:\n\n  ```python\n  sparse_column_a = sparse_column_with_hash_bucket(...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearClassifier(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearClassifier(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using the SDCAOptimizer.\n  estimator = LinearClassifier(\n     feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n     optimizer=tf.contrib.linear_optimizer.SDCAOptimizer(\n       example_id_column='example_id',\n       num_loss_partitions=...,\n       symmetric_l2_regularization=2.0\n     ))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_predict: # returns x, None.\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = LinearClassifier(\n      n_classes=n_classes,\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.LinearClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Linear regressor model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Train a linear regression model to predict label value given observation of\n  feature values.\n\n  Example:\n\n  ```python\n  sparse_column_a = sparse_column_with_hash_bucket(...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  estimator = LinearRegressor(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b])\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  estimator.predict(x=x)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a KeyError:\n\n  * if `weight_column_name` is not `None`:\n    key=weight_column_name, value=a `Tensor`\n  * for column in `feature_columns`:\n    - if isinstance(column, `SparseColumn`):\n        key=column.name, value=a `SparseTensor`\n    - if isinstance(column, `WeightedSparseColumn`):\n        {key=id column name, value=a `SparseTensor`,\n         key=weight column name, value=a `SparseTensor`}\n    - if isinstance(column, `RealValuedColumn`):\n        key=column.name, value=a `Tensor`\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.linear.LinearRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Linear regressor model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Train a linear regression model to predict label value given observation of\n  feature values.\n\n  Example:\n\n  ```python\n  sparse_column_a = sparse_column_with_hash_bucket(...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  estimator = LinearRegressor(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b])\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  estimator.predict(x=x)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a KeyError:\n\n  * if `weight_column_name` is not `None`:\n    key=weight_column_name, value=a `Tensor`\n  * for column in `feature_columns`:\n    - if isinstance(column, `SparseColumn`):\n        key=column.name, value=a `SparseTensor`\n    - if isinstance(column, `WeightedSparseColumn`):\n        {key=id column name, value=a `SparseTensor`,\n         key=weight column name, value=a `SparseTensor`}\n    - if isinstance(column, `RealValuedColumn`):\n        key=column.name, value=a `Tensor`\n  ",
        "klass": "tensorflow.contrib.learn.LinearRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Support Vector Machine (SVM) model for binary classification.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Currently, only linear SVMs are supported. For the underlying optimization\n  problem, the `SDCAOptimizer` is used. For performance and convergence tuning,\n  the num_loss_partitions parameter passed to `SDCAOptimizer` (see `__init__()`\n  method), should be set to (#concurrent train ops per worker) x (#workers). If\n  num_loss_partitions is larger or equal to this value, convergence is\n  guaranteed but becomes slower as num_loss_partitions increases. If it is set\n  to a smaller value, the optimizer is more aggressive in reducing the global\n  loss but convergence is not guaranteed. The recommended value in an\n  `Estimator` (where there is one process per worker) is the number of workers\n  running the train steps. It defaults to 1 (single machine).\n\n  Example:\n\n  ```python\n  real_feature_column = real_valued_column(...)\n  sparse_feature_column = sparse_column_with_hash_bucket(...)\n\n  estimator = SVM(\n      example_id_column='example_id',\n      feature_columns=[real_feature_column, sparse_feature_column],\n      l2_regularization=10.0)\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  estimator.predict(x=x)\n  ```\n\n  Input of `fit` and `evaluate` should have following features, otherwise there\n  will be a `KeyError`:\n    a feature with `key=example_id_column` whose value is a `Tensor` of dtype\n    string.\n    if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n    for each `column` in `feature_columns`:\n      - if `column` is a `SparseColumn`, a feature with `key=column.name`\n        whose `value` is a `SparseTensor`.\n      - if `column` is a `RealValuedColumn, a feature with `key=column.name`\n        whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.estimators.svm.SVM",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Support Vector Machine (SVM) model for binary classification.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Currently, only linear SVMs are supported. For the underlying optimization\n  problem, the `SDCAOptimizer` is used. For performance and convergence tuning,\n  the num_loss_partitions parameter passed to `SDCAOptimizer` (see `__init__()`\n  method), should be set to (#concurrent train ops per worker) x (#workers). If\n  num_loss_partitions is larger or equal to this value, convergence is\n  guaranteed but becomes slower as num_loss_partitions increases. If it is set\n  to a smaller value, the optimizer is more aggressive in reducing the global\n  loss but convergence is not guaranteed. The recommended value in an\n  `Estimator` (where there is one process per worker) is the number of workers\n  running the train steps. It defaults to 1 (single machine).\n\n  Example:\n\n  ```python\n  real_feature_column = real_valued_column(...)\n  sparse_feature_column = sparse_column_with_hash_bucket(...)\n\n  estimator = SVM(\n      example_id_column='example_id',\n      feature_columns=[real_feature_column, sparse_feature_column],\n      l2_regularization=10.0)\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  estimator.predict(x=x)\n  ```\n\n  Input of `fit` and `evaluate` should have following features, otherwise there\n  will be a `KeyError`:\n    a feature with `key=example_id_column` whose value is a `Tensor` of dtype\n    string.\n    if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n    for each `column` in `feature_columns`:\n      - if `column` is a `SparseColumn`, a feature with `key=column.name`\n        whose `value` is a `SparseTensor`.\n      - if `column` is a `RealValuedColumn, a feature with `key=column.name`\n        whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.SVM",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "A classifier for TensorFlow DNN models.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Example:\n\n  ```python\n  sparse_feature_a = sparse_column_with_hash_bucket(...)\n  sparse_feature_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,\n                                          ...)\n  sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,\n                                          ...)\n\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256])\n\n  # Or estimator using the ProximalAdagradOptimizer optimizer with\n  # regularization.\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      optimizer=tf.train.ProximalAdagradOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n\n  def input_fn_predict: # returns x, None\n    pass\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = DNNClassifier(\n      feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\n      hidden_units=[1024, 512, 256],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n     `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.DNNClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Linear classifier model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Train a linear model to classify instances into one of multiple possible\n  classes. When number of possible classes is 2, this is binary classification.\n\n  Example:\n\n  ```python\n  sparse_column_a = sparse_column_with_hash_bucket(...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearClassifier(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearClassifier(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using the SDCAOptimizer.\n  estimator = LinearClassifier(\n     feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n     optimizer=tf.contrib.linear_optimizer.SDCAOptimizer(\n       example_id_column='example_id',\n       num_loss_partitions=...,\n       symmetric_l2_regularization=2.0\n     ))\n\n  # Input builders\n  def input_fn_train: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_eval: # returns x, y (where y represents label's class index).\n    ...\n  def input_fn_predict: # returns x, None.\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  # predict_classes returns class indices.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  If the user specifies `label_keys` in constructor, labels must be strings from\n  the `label_keys` vocabulary. Example:\n\n  ```python\n  label_keys = ['label0', 'label1', 'label2']\n  estimator = LinearClassifier(\n      n_classes=n_classes,\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b],\n      label_keys=label_keys)\n\n  def input_fn_train: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.fit(input_fn=input_fn_train)\n\n  def input_fn_eval: # returns x, y (where y is one of label_keys).\n    pass\n  estimator.evaluate(input_fn=input_fn_eval)\n  def input_fn_predict: # returns x, None\n  # predict_classes returns one of label_keys.\n  estimator.predict_classes(input_fn=input_fn_predict)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column_name` is not `None`, a feature with\n    `key=weight_column_name` whose value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.LinearClassifier",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "Linear regressor model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Train a linear regression model to predict label value given observation of\n  feature values.\n\n  Example:\n\n  ```python\n  sparse_column_a = sparse_column_with_hash_bucket(...)\n  sparse_column_b = sparse_column_with_hash_bucket(...)\n\n  sparse_feature_a_x_sparse_feature_b = crossed_column(...)\n\n  estimator = LinearRegressor(\n      feature_columns=[sparse_column_a, sparse_feature_a_x_sparse_feature_b])\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n  estimator.predict(x=x)\n  ```\n\n  Input of `fit` and `evaluate` should have following features,\n    otherwise there will be a KeyError:\n\n  * if `weight_column_name` is not `None`:\n    key=weight_column_name, value=a `Tensor`\n  * for column in `feature_columns`:\n    - if isinstance(column, `SparseColumn`):\n        key=column.name, value=a `SparseTensor`\n    - if isinstance(column, `WeightedSparseColumn`):\n        {key=id column name, value=a `SparseTensor`,\n         key=weight column name, value=a `SparseTensor`}\n    - if isinstance(column, `RealValuedColumn`):\n        key=column.name, value=a `Tensor`\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.LinearRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a function object.\n\n  code\n    a code object\n  globals\n    the globals dictionary\n  name\n    a string that overrides the name from the code object\n  argdefs\n    a tuple that specifies the default argument values\n  closure\n    a tuple that supplies the bindings for free variables",
        "klass": "function",
        "module": "function"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, recovery and hooks.\n\n  Example usage:\n\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with MonitoredSession(session_creator=ChiefSessionCreator(...),\n                        hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the monitored session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n  * calls `hook.after_create_session()`\n\n  Run: When `run()` is called, the monitored session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n  * if `AbortedError` or `UnavailableError` occurs, it recovers or\n    reinitializes the session before executing the run() call again\n\n\n  Exit: At the `close()`, the monitored session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the monitored_session is used as a context\n\n  How to set `tf.Session` arguments:\n\n  * In most cases you can set session arguments as follows:\n\n  ```python\n  MonitoredSession(\n    session_creator=ChiefSessionCreator(master=..., config=...))\n  ```\n\n  * In distributed setting for a non-chief worker, you can use following:\n\n  ```python\n  MonitoredSession(\n    session_creator=WorkerSessionCreator(master=..., config=...))\n  ```\n\n  See `MonitoredTrainingSession` for an example usage based on chief or worker.\n\n  Note: This is not a `tf.Session`. For example, it cannot do following:\n\n  * it cannot be set as default session.\n  * it cannot be sent to saver.save.\n  * it cannot be sent to tf.train.start_queue_runners.\n\n  Args:\n    session_creator: A factory object to create session. Typically a\n      `ChiefSessionCreator` which is the default one.\n    hooks: An iterable of `SessionRunHook' objects.\n\n  Returns:\n    A MonitoredSession object.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.monitored_session.MonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Categorical variables vocabulary class.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Accumulates and provides mapping from classes to indexes.\n  Can be easily used for words.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Categorical variables vocabulary class.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Accumulates and provides mapping from classes to indexes.\n  Can be easily used for words.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.preprocessing.CategoricalVocabulary",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Maps documents to sequences of word ids.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n  ",
        "klass": "tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "zip(*iterables) --> zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.",
        "klass": "zip",
        "module": "zip"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A sharded version of MutableDenseHashTable.\n\n  It is designed to be interface compatible with LookupInterface and\n  MutableDenseHashTable, with the exception of the export method, which is\n  replaced by an export_sharded method.\n\n  The _ShardedMutableDenseHashTable keeps `num_shards` MutableDenseHashTable\n  internally. The shard is computed via the modulo operation on the key.\n  ",
        "klass": "tensorflow.contrib.linear_optimizer.python.ops.sharded_mutable_dense_hashtable.ShardedMutableDenseHashTable",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.lookup_ops.InitializableLookupTableBase"
        ],
        "class_docstring": "A generic hash table implementation.\n\n  Example usage:\n\n  ```python\n  table = tf.HashTable(\n      tf.KeyValueTensorInitializer(keys, values), -1)\n  out = table.lookup(input_tensor)\n  table.init.run()\n  print(out.eval())\n  ```\n  ",
        "klass": "tensorflow.contrib.lookup.HashTable",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the AddSign update.\n\n  See [Bello et al., ICML2017],\n  [Neural Optimizer Search with RL](https://arxiv.org/abs/1709.07417).\n  ",
        "klass": "tensorflow.contrib.opt.python.training.addsign.AddSignOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Wrapper optimizer that checks and drops stale gradient.\n\n  This optimizer records the global step for each worker before computing\n  gradients and compares it with the global step at the time of applying the\n  gradients. If the difference is larger than a threshold, it will drop all\n  the computed gradients.\n  ",
        "klass": "tensorflow.contrib.opt.python.training.drop_stale_gradient_optimizer.DropStaleGradientOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Wrapper optimizer that implements the Elastic Average SGD algorithm.\n  This is an async optimizer. During the training, Each worker will update\n  the local variables and maintains its own local_step, which starts from 0\n  and is incremented by 1 after each update of local variables. Whenever\n  the communication period divides the local step, the worker requests\n  the current global center variables and then computed the elastic difference\n  between global center variables and local variables. The elastic difference\n  then be used to update both local variables and global variables.\n  ",
        "klass": "tensorflow.contrib.opt.python.training.elastic_average_optimizer.ElasticAverageOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.optimizer_v2.optimizer_v2.OptimizerV2"
        ],
        "class_docstring": "Optimizer that implements the GGT algorithm.\n\n  GGT has an advantage over sgd and adam on large models with poor conditioning,\n  for example language models and CNNs,\n  see [[ABCHSZZ 2018]](https://arxiv.org/pdf/1806.02958.pdf).\n  ",
        "klass": "tensorflow.contrib.opt.python.training.ggt.GGTOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.adam.AdamOptimizer"
        ],
        "class_docstring": "Variant of the Adam optimizer that handles sparse updates more efficiently.\n\n  The original Adam algorithm maintains two moving-average accumulators for\n  each trainable variable; the accumulators are updated at every step.\n  This class provides lazier handling of gradient updates for sparse variables.\n  It only updates moving-average accumulators for sparse variable indices that\n  appear in the current batch, rather than updating the accumulators for all\n  indices. Compared with the original Adam optimizer, it can provide large\n  improvements in model training throughput for some applications. However, it\n  provides slightly different semantics than the original Adam algorithm, and\n  may lead to different empirical results.\n  ",
        "klass": "tensorflow.contrib.opt.python.training.lazy_adam_optimizer.LazyAdamOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Wrapper optimizer that implements the Model Average algorithm.\n\n  This is a sync optimizer. During the training, each worker will update\n  the local variables and maintains its own local_step, which starts from 0\n  and is incremented by 1 after each update of local variables. Whenever the\n  interval_steps divides the local step, the local variables from all the\n  workers will be averaged and assigned to global center variables. Then the\n  local variables will be assigned by global center variables.\n  ",
        "klass": "tensorflow.contrib.opt.python.training.model_average_optimizer.ModelAverageOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that computes a moving average of the variables.\n\n  Empirically it has been found that using the moving average of the trained\n  parameters of a deep network is better than using its trained parameters\n  directly. This optimizer allows you to compute this moving average and swap\n  the variables at save time so that any code outside of the training loop will\n  use by default the averaged values instead of the original ones.\n\n  Example of usage:\n\n  ```python\n\n  // Encapsulate your favorite optimizer (here the momentum one)\n  // inside the MovingAverageOptimizer.\n  opt = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)\n  opt = tf.contrib.opt.MovingAverageOptimizer(opt)\n  // Then create your model and all its variables.\n  model = build_model()\n  // Add the training op that optimizes using opt.\n  // This needs to be called before swapping_saver().\n  opt.minimize(cost, var_list)\n  // Then create your saver like this:\n  saver = opt.swapping_saver()\n  // Pass it to your training loop.\n      slim.learning.train(\n          model,\n          ...\n          saver=saver)\n  ```\n\n  Note that for evaluation, the normal saver should be used instead of\n  swapping_saver().\n  ",
        "klass": "tensorflow.contrib.opt.python.training.moving_average_optimizer.MovingAverageOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that computes a moving average of the variables.\n\n  Empirically it has been found that using the moving average of the trained\n  parameters of a deep network is better than using its trained parameters\n  directly. This optimizer allows you to compute this moving average and swap\n  the variables at save time so that any code outside of the training loop will\n  use by default the averaged values instead of the original ones.\n\n  Example of usage:\n\n  ```python\n\n  // Encapsulate your favorite optimizer (here the momentum one)\n  // inside the MovingAverageOptimizer.\n  opt = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)\n  opt = tf.contrib.opt.MovingAverageOptimizer(opt)\n  // Then create your model and all its variables.\n  model = build_model()\n  // Add the training op that optimizes using opt.\n  // This needs to be called before swapping_saver().\n  opt.minimize(cost, var_list)\n  // Then create your saver like this:\n  saver = opt.swapping_saver()\n  // Pass it to your training loop.\n      slim.learning.train(\n          model,\n          ...\n          saver=saver)\n  ```\n\n  Note that for evaluation, the normal saver should be used instead of\n  swapping_saver().\n  ",
        "klass": "tensorflow.contrib.opt.MovingAverageOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Optimizer wrapper making all-zero gradients harmless.\n\n  This might be useful when a multi-task loss is used,\n  and some components of the loss might be\n  not present (e.g. masked out) in some training batches.\n  Technically their gradient would be zero,\n  which would normally affect the optimizer state\n  (e.g. push running average to zero).\n  However this is not the desired behaviour,\n  since the missing loss component\n  should be treated as unknown rather than zero.\n\n  This wrapper filters out all-zero gradient tensors,\n  therefore preserving the optimizer state.\n\n  If gradient clipping by global norm is used,\n  the provided function clip_gradients_by_global_norm\n  should be used (and specified explicitly by the user).\n  Otherwise the global norm would be underestimated\n  because of all-zero tensors that should be ignored.\n\n  The gradient calculation and application\n  are delegated to an underlying optimizer.\n  The gradient application is altered only for all-zero tensors.\n\n  Example:\n  ```python\n  momentum_optimizer = tf.train.MomentumOptimizer(\n    learning_rate, momentum=0.9)\n  multitask_momentum_optimizer = tf.contrib.opt.MultitaskOptimizerWrapper(\n    momentum_optimizer)\n  gradvars = multitask_momentum_optimizer.compute_gradients(\n    loss)\n  gradvars_clipped, _ = tf.contrib.opt.clip_gradients_by_global_norm(\n    gradvars, 15.0)\n  train_op = multitask_momentum_optimizer.apply_gradients(\n    gradvars_clipped, global_step=batch)\n  ```\n  ",
        "klass": "tensorflow.contrib.opt.python.training.multitask_optimizer_wrapper.MultitaskOptimizerWrapper",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the PowerSign update.\n\n  See [Bello et al., ICML2017],\n  [Neural Optimizer Search with RL](https://arxiv.org/abs/1709.07417).\n  ",
        "klass": "tensorflow.contrib.opt.python.training.powersign.PowerSignOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.optimizer_v2.optimizer_v2.OptimizerV2"
        ],
        "class_docstring": "Optimizer that implements the Adadelta algorithm.\n\n  See [M. D. Zeiler](http://arxiv.org/abs/1212.5701)\n  ([pdf](http://arxiv.org/pdf/1212.5701v1.pdf))\n  ",
        "klass": "tensorflow.contrib.optimizer_v2.adadelta.AdadeltaOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.RNNCell"
        ],
        "class_docstring": "Basic attention cell wrapper.\n\n  Implementation based on https://arxiv.org/abs/1601.06733.\n  ",
        "klass": "tensorflow.contrib.rnn.AttentionCellWrapper",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.RNNCell"
        ],
        "class_docstring": "Operator that ensures an RNNCell runs on a particular device.",
        "klass": "tensorflow.contrib.rnn.DeviceWrapper",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.LayerRNNCell"
        ],
        "class_docstring": "Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n\n  Note that this cell is not optimized for performance. Please use\n  `tf.contrib.cudnn_rnn.CudnnGRU` for better performance on GPU, or\n  `tf.contrib.rnn.GRUBlockCellV2` for better performance on CPU.\n\n  Args:\n    num_units: int, The number of units in the GRU cell.\n    activation: Nonlinearity to use.  Default: `tanh`.\n    reuse: (optional) Python boolean describing whether to reuse variables\n     in an existing scope.  If not `True`, and the existing scope already has\n     the given variables, an error is raised.\n    kernel_initializer: (optional) The initializer to use for the weight and\n    projection matrices.\n    bias_initializer: (optional) The initializer to use for the bias.\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such\n      cases.\n    dtype: Default dtype of the layer (default of `None` means use the type\n      of the first input). Required when `build` is called before `call`.\n    **kwargs: Dict, keyword named properties for common layer attributes, like\n      `trainable` etc when constructing the cell from configs of get_config().\n  ",
        "klass": "tensorflow.contrib.rnn.GRUCell",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.RNNCell"
        ],
        "class_docstring": "Wraps another `RNNCell` with attention.\n  ",
        "klass": "tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.BaseStagingArea"
        ],
        "class_docstring": "Class for staging inputs. No ordering guarantees.\n\n  A `StagingArea` is a TensorFlow data structure that stores tensors across\n  multiple steps, and exposes operations that can put and get tensors.\n\n  Each `StagingArea` element is a tuple of one or more tensors, where each\n  tuple component has a static dtype, and may have a static shape.\n\n  The capacity of a `StagingArea` may be bounded or unbounded.\n  It supports multiple concurrent producers and consumers; and\n  provides exactly-once delivery.\n\n  Each element of a `StagingArea` is a fixed-length tuple of tensors whose\n  dtypes are described by `dtypes`, and whose shapes are optionally described\n  by the `shapes` argument.\n\n  If the `shapes` argument is specified, each component of a staging area\n  element must have the respective fixed shape. If it is\n  unspecified, different elements may have different shapes,\n\n  It can be configured with a capacity in which case\n  put(values) will block until space becomes available.\n\n  Similarly, it can be configured with a memory limit which\n  will block put(values) until space is available.\n  This is mostly useful for limiting the number of tensors on\n  devices such as GPUs.\n\n  All get() and peek() commands block if the requested data\n  is not present in the Staging Area.\n\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.StagingArea",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.BaseStagingArea"
        ],
        "class_docstring": "Class for staging inputs. No ordering guarantees.\n\n  A `StagingArea` is a TensorFlow data structure that stores tensors across\n  multiple steps, and exposes operations that can put and get tensors.\n\n  Each `StagingArea` element is a tuple of one or more tensors, where each\n  tuple component has a static dtype, and may have a static shape.\n\n  The capacity of a `StagingArea` may be bounded or unbounded.\n  It supports multiple concurrent producers and consumers; and\n  provides exactly-once delivery.\n\n  Each element of a `StagingArea` is a fixed-length tuple of tensors whose\n  dtypes are described by `dtypes`, and whose shapes are optionally described\n  by the `shapes` argument.\n\n  If the `shapes` argument is specified, each component of a staging area\n  element must have the respective fixed shape. If it is\n  unspecified, different elements may have different shapes,\n\n  It can be configured with a capacity in which case\n  put(values) will block until space becomes available.\n\n  Similarly, it can be configured with a memory limit which\n  will block put(values) until space is available.\n  This is mostly useful for limiting the number of tensors on\n  devices such as GPUs.\n\n  All get() and peek() commands block if the requested data\n  is not present in the Staging Area.\n\n  ",
        "klass": "tensorflow.contrib.staging.StagingArea",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator"
        ],
        "class_docstring": "An estimator that can train and evaluate a random forest.\n\n  Example:\n\n  ```python\n  params = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n      num_classes=2, num_features=40, num_trees=10, max_nodes=1000)\n\n  # Estimator using the default graph builder.\n  estimator = TensorForestEstimator(params, model_dir=model_dir)\n\n  # Or estimator using TrainingLossForest as the graph builder.\n  estimator = TensorForestEstimator(\n      params, graph_builder_class=tensor_forest.TrainingLossForest,\n      model_dir=model_dir)\n\n  # Input builders\n  def input_fn_train: # returns x, y\n    ...\n  def input_fn_eval: # returns x, y\n    ...\n  estimator.fit(input_fn=input_fn_train)\n  estimator.evaluate(input_fn=input_fn_eval)\n\n  # Predict returns an iterable of dicts.\n  results = list(estimator.predict(x=x))\n  prob0 = results[0][eval_metrics.INFERENCE_PROB_NAME]\n  prediction0 = results[0][eval_metrics.INFERENCE_PRED_NAME]\n  ```\n  ",
        "klass": "tensorflow.contrib.tensor_forest.client.random_forest.TensorForestEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.tensor_forest.hybrid.python.hybrid_model.HybridModel"
        ],
        "class_docstring": "A model that places a decision tree embedding before a neural net.",
        "klass": "tensorflow.contrib.tensor_forest.hybrid.python.models.decisions_to_data_then_nn.DecisionsToDataThenNN",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.tensor_forest.hybrid.python.hybrid_model.HybridModel"
        ],
        "class_docstring": "A model that combines a decision forest embedding with a neural net.",
        "klass": "tensorflow.contrib.tensor_forest.hybrid.python.models.forest_to_data_then_nn.ForestToDataThenNN",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.tensor_forest.hybrid.python.hybrid_model.HybridModel"
        ],
        "class_docstring": "A model that places a soft decision tree embedding before a neural net.",
        "klass": "tensorflow.contrib.tensor_forest.hybrid.python.models.k_feature_decisions_to_data_then_nn.KFeatureDecisionsToDataThenNN",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Builds TF graphs for random forest training and inference.",
        "klass": "tensorflow.contrib.tensor_forest.python.tensor_forest.RandomForestGraphs",
        "module": "tensorflow"
    },
    {
        "base_classes": [],
        "class_docstring": "The base class of the class hierarchy.\n\nWhen called, it accepts no arguments and returns a new featureless\ninstance that has no instance attributes and cannot be given any.\n",
        "klass": "object",
        "module": "object"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Property attribute.\n\n  fget\n    function to be used for getting an attribute value\n  fset\n    function to be used for setting an attribute value\n  fdel\n    function to be used for del'ing an attribute\n  doc\n    docstring\n\nTypical use is to define a managed attribute x:\n\nclass C(object):\n    def getx(self): return self._x\n    def setx(self, value): self._x = value\n    def delx(self): del self._x\n    x = property(getx, setx, delx, \"I'm the 'x' property.\")\n\nDecorators make defining new properties or modifying existing ones easy:\n\nclass C(object):\n    @property\n    def x(self):\n        \"I am the 'x' property.\"\n        return self._x\n    @x.setter\n    def x(self, value):\n        self._x = value\n    @x.deleter\n    def x(self):\n        del self._x",
        "klass": "property",
        "module": "property"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A Context that captures RunMetadata and performs profiling.\n\n  ```python\n    # Trace steps 100~200, profile at [150, 200] and dump profile at 200.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir',\n                                          trace_steps=range(100, 200, 3),\n                                          dump_steps=[200]) as pctx:\n      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n      pctx.add_auto_profiling('op', opts, [150, 200])\n      train_loop().\n\n    # Tracing only.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir') as pctx:\n      # Run train/eval loop for at least few hundred steps. Profiles will be\n      # dumped to train_dir. Use web UI or command line to do profiling.\n      train_loop().\n\n    # When session object is available, do explicit trace, profile and dump.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir',\n                                          trace_steps=[],\n                                          dump_steps=[]) as pctx:\n      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n      pctx.trace_next_step()\n      _ = session.run(train_op)\n      pctx.profiler.profile_operations(options=opts)\n  ```\n\n  Args:\n    profile_dir: Directory to store profiles.\n    trace_steps: A list of session run steps to trace. If None, use\n        pre-defined steps.\n    dump_steps: A list of steps to dump the profile to `profile_dir`. If None,\n        use pre-defined steps.\n    enabled: If false, everything is disabled with minimal overhead. It allows\n        user to only enable profiling when needed.\n    debug: If true, also dumps the raw trace RunMetadata text file to\n        profile_dir. And print debugging message. Useful for bug report.\n  ",
        "klass": "tensorflow.python.profiler.profile_context.ProfileContext",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A Context that captures RunMetadata and performs profiling.\n\n  ```python\n    # Trace steps 100~200, profile at [150, 200] and dump profile at 200.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir',\n                                          trace_steps=range(100, 200, 3),\n                                          dump_steps=[200]) as pctx:\n      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n      pctx.add_auto_profiling('op', opts, [150, 200])\n      train_loop().\n\n    # Tracing only.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir') as pctx:\n      # Run train/eval loop for at least few hundred steps. Profiles will be\n      # dumped to train_dir. Use web UI or command line to do profiling.\n      train_loop().\n\n    # When session object is available, do explicit trace, profile and dump.\n    with tf.contrib.tfprof.ProfileContext('/tmp/train_dir',\n                                          trace_steps=[],\n                                          dump_steps=[]) as pctx:\n      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()\n      pctx.trace_next_step()\n      _ = session.run(train_op)\n      pctx.profiler.profile_operations(options=opts)\n  ```\n\n  Args:\n    profile_dir: Directory to store profiles.\n    trace_steps: A list of session run steps to trace. If None, use\n        pre-defined steps.\n    dump_steps: A list of steps to dump the profile to `profile_dir`. If None,\n        use pre-defined steps.\n    enabled: If false, everything is disabled with minimal overhead. It allows\n        user to only enable profiling when needed.\n    debug: If true, also dumps the raw trace RunMetadata text file to\n        profile_dir. And print debugging message. Useful for bug report.\n  ",
        "klass": "tensorflow.contrib.tfprof.ProfileContext",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.estimators.TimeSeriesRegressor"
        ],
        "class_docstring": "An Estimator for an (optionally non-linear) autoregressive model.\n\n  ARRegressor is a window-based model, inputting fixed windows of length\n  `input_window_size` and outputting fixed windows of length\n  `output_window_size`. These two parameters must add up to the window_size\n  passed to the `Chunker` used to create an `input_fn` for training or\n  evaluation. `RandomWindowInputFn` is suggested for both training and\n  evaluation, although it may be seeded for deterministic evaluation.\n  ",
        "klass": "tensorflow.contrib.timeseries.ARRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.estimators.StateSpaceRegressor"
        ],
        "class_docstring": "An Estimator for structural time series models.\n\n  \"Structural\" refers to the fact that this model explicitly accounts for\n  structure in the data, such as periodicity and trends.\n\n  `StructuralEnsembleRegressor` is a state space model. It contains components\n  for modeling level, local linear trends, periodicity, and mean-reverting\n  transients via a moving average component. Multivariate series are fit with\n  full covariance matrices for observation and latent state transition noise,\n  each feature of the multivariate series having its own latent components.\n\n  Note that unlike `ARRegressor`, `StructuralEnsembleRegressor` is sequential,\n  and so accepts variable window sizes with the same model.\n\n  For training, `RandomWindowInputFn` is recommended as an `input_fn`. Model\n  state is managed through `ChainingStateManager`: since state space models are\n  inherently sequential, we save state from previous iterations to get\n  approximate/eventual consistency while achieving good performance through\n  batched computation.\n\n  For evaluation, either pass a significant chunk of the series in a single\n  window (e.g. set `window_size` to the whole series with\n  `WholeDatasetInputFn`), or use enough random evaluation iterations to cover\n  several passes through the whole dataset. Either method will ensure that stale\n  saved state has been flushed.\n  ",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.estimators.StructuralEnsembleRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.estimators.StateSpaceRegressor"
        ],
        "class_docstring": "An Estimator for structural time series models.\n\n  \"Structural\" refers to the fact that this model explicitly accounts for\n  structure in the data, such as periodicity and trends.\n\n  `StructuralEnsembleRegressor` is a state space model. It contains components\n  for modeling level, local linear trends, periodicity, and mean-reverting\n  transients via a moving average component. Multivariate series are fit with\n  full covariance matrices for observation and latent state transition noise,\n  each feature of the multivariate series having its own latent components.\n\n  Note that unlike `ARRegressor`, `StructuralEnsembleRegressor` is sequential,\n  and so accepts variable window sizes with the same model.\n\n  For training, `RandomWindowInputFn` is recommended as an `input_fn`. Model\n  state is managed through `ChainingStateManager`: since state space models are\n  inherently sequential, we save state from previous iterations to get\n  approximate/eventual consistency while achieving good performance through\n  batched computation.\n\n  For evaluation, either pass a significant chunk of the series in a single\n  window (e.g. set `window_size` to the whole series with\n  `WholeDatasetInputFn`), or use enough random evaluation iterations to cover\n  several passes through the whole dataset. Either method will ensure that stale\n  saved state has been flushed.\n  ",
        "klass": "tensorflow.contrib.timeseries.StructuralEnsembleRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An Estimator to fit and evaluate a time series model.",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.estimators.TimeSeriesRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.estimators.TimeSeriesRegressor"
        ],
        "class_docstring": "An Estimator for an LSTM autoregressive model.\n\n  LSTMAutoRegressor is a window-based model, inputting fixed windows of length\n  `input_window_size` and outputting fixed windows of length\n  `output_window_size`. These two parameters must add up to the window_size\n  of data returned by the `input_fn`.\n\n  Each periodicity in the `periodicities` arg is divided by the `num_timesteps`\n  into timesteps that are represented as time features added to the model.\n\n  A good heuristic for picking an appropriate periodicity for a given data set\n  would be the length of cycles in the data. For example, energy usage in a\n  home is typically cyclic each day. If the time feature in a home energy\n  usage dataset is in the unit of hours, then 24 would be an appropriate\n  periodicity. Similarly, a good heuristic for `num_timesteps` is how often the\n  data is expected to change within the cycle. For the aforementioned home\n  energy usage dataset and periodicity of 24, then 48 would be a reasonable\n  value if usage is expected to change every half hour.\n\n  Each feature's value for a given example with time t is the difference\n  between t and the start of the timestep it falls under. If it doesn't fall\n  under a feature's associated timestep, then that feature's value is zero.\n\n  For example: if `periodicities` = (9, 12) and `num_timesteps` = 3, then 6\n  features would be added to the model, 3 for periodicity 9 and 3 for\n  periodicity 12.\n\n  For an example data point where t = 17:\n  - It's in the 3rd timestep for periodicity 9 (2nd period is 9-18 and 3rd\n    timestep is 15-18)\n  - It's in the 2nd timestep for periodicity 12 (2nd period is 12-24 and\n    2nd timestep is between 16-20).\n\n  Therefore the 6 added features for this row with t = 17 would be:\n\n  # Feature name (periodicity#_timestep#), feature value\n  P9_T1, 0 # not in first timestep\n  P9_T2, 0 # not in second timestep\n  P9_T3, 2 # 17 - 15 since 15 is the start of the 3rd timestep\n  P12_T1, 0 # not in first timestep\n  P12_T2, 1 # 17 - 16 since 16 is the start of the 2nd timestep\n  P12_T3, 0 # not in third timestep\n\n  Example Code:\n\n  ```python\n  extra_feature_columns = (\n      feature_column.numeric_column(\"exogenous_variable\"),\n  )\n\n  estimator = LSTMAutoRegressor(\n      periodicities=10,\n      input_window_size=10,\n      output_window_size=5,\n      model_dir=\"/path/to/model/dir\",\n      num_features=1,\n      extra_feature_columns=extra_feature_columns,\n      num_timesteps=50,\n      num_units=10,\n      optimizer=tf.train.ProximalAdagradOptimizer(...))\n\n  # Input builders\n  def input_fn_train():\n    return {\n      \"times\": tf.range(15)[None, :],\n      \"values\": tf.random_normal(shape=[1, 15, 1])\n    }\n  estimator.train(input_fn=input_fn_train, steps=100)\n\n  def input_fn_eval():\n    pass\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n\n  def input_fn_predict():\n    pass\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n  ",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.estimators.LSTMAutoRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.estimators.TimeSeriesRegressor"
        ],
        "class_docstring": "An Estimator for general state space models.",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.estimators.StateSpaceRegressor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A minimal wrapper for models which do not need state management.",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.state_management.PassthroughStateManager",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.state_space_models.state_space_model.StateSpaceEnsemble"
        ],
        "class_docstring": "Implements ensembles of independent state space models.\n\n  Useful for fitting multiple independent state space models together while\n  keeping their specifications decoupled. The \"ensemble\" is simply a state space\n  model with the observation models of its members concatenated, and the\n  transition matrices and noise transforms stacked in block-diagonal\n  matrices. This means that the dimensionality of the ensemble's state is the\n  sum of those of its components, which can lead to slow and memory-intensive\n  training and inference as the posterior (shape [state dimension x state\n  dimension]) gets large.\n\n  Each individual model j's state at time t is defined by:\n\n  state[t, j] = StateTransition[j] * state[t-1, j]\n      + NoiseTransform[j] * StateNoise[t, j]\n  StateNoise[t, j] ~ Gaussian(0, StateNoiseCovariance[j])\n\n  and the ensemble observation model is:\n\n  observation[t] = Sum { ObservationModel[j] * state[t, j] }\n      + ObservationNoise[t]\n  ObservationNoise[t] ~ Gaussian(0, ObservationNoiseCovariance)\n  ",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.state_space_models.state_space_model.StateSpaceIndependentEnsemble",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.contrib.timeseries.python.timeseries.state_space_models.state_space_model.StateSpaceModel"
        ],
        "class_docstring": "A VARMA model implementation as a special case of the state space model.",
        "klass": "tensorflow.contrib.timeseries.python.timeseries.state_space_models.varma.VARMA",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "An optimizer that averages gradients across TPU shards.",
        "klass": "tensorflow.contrib.tpu.CrossShardOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A helper object to build a device infeed queue.\n\n  The InfeedQueue builds the host-side and device-side Ops to enqueue and\n  dequeue elements, respectively, and ensures that their types and\n  shapes match.\n  ",
        "klass": "tensorflow.contrib.tpu.python.tpu.tpu_feed.InfeedQueue",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "Estimator with TPU support.\n\n  TPUEstimator also supports training on CPU and GPU. You don't need to define\n  a separate `tf.estimator.Estimator`.\n\n  TPUEstimator handles many of the details of running on TPU devices, such as\n  replicating inputs and models for each core, and returning to host\n  periodically to run hooks.\n\n  TPUEstimator transforms a global batch size in params to a per-shard batch\n  size when calling the `input_fn` and `model_fn`. Users should specify\n  global batch size in constructor, and then get the batch size for each shard\n  in `input_fn` and `model_fn` by `params['batch_size']`.\n\n  - For training, `model_fn` gets per-core batch size; `input_fn` may get\n    per-core or per-host batch size depending on `per_host_input_for_training`\n    in `TPUConfig` (See docstring for TPUConfig for details).\n\n  - For evaluation and prediction, `model_fn` gets per-core batch size and\n    `input_fn` get per-host batch size.\n\n  Evaluation\n  ==========\n\n  `model_fn` should return `TPUEstimatorSpec`, which expects the `eval_metrics`\n  for TPU evaluation. However, if eval_on_tpu is False, `model_fn` must return\n  `EstimatorSpec` and the evaluation will execute on CPU or GPU; in this case\n  the following discussion on TPU evaluation does not apply.\n\n  `TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`, where\n  `tensors` could be a list of any nested structure of `Tensor`s (See\n  `TPUEstimatorSpec` for details).  `metric_fn` takes the `tensors` and returns\n  a dict from metric string name to the result of calling a metric function,\n  namely a `(metric_tensor, update_op)` tuple.\n\n  One can set `use_tpu` to `False` for testing. All training, evaluation, and\n  predict will be executed on CPU. `input_fn` and `model_fn` will receive\n  `train_batch_size` or `eval_batch_size` unmodified as `params['batch_size']`.\n\n  Current limitations:\n  --------------------\n\n  1. TPU evaluation only works on a single host (one TPU worker) except\n     BROADCAST mode.\n\n  2. `input_fn` for evaluation should **NOT** raise an end-of-input exception\n     (`OutOfRangeError` or `StopIteration`). And all evaluation steps and all\n     batches should have the same size.\n\n  Example (MNIST):\n  ----------------\n\n  ```\n  # The metric Fn which runs on CPU.\n  def metric_fn(labels, logits):\n    predictions = tf.argmax(logits, 1)\n    return {\n      'accuracy': tf.metrics.precision(\n          labels=labels, predictions=predictions),\n    }\n\n  # Your model Fn which runs on TPU (eval_metrics is list in this example)\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, [labels, logits]))\n\n  # or specify the eval_metrics tensors as dict.\n  def model_fn(features, labels, mode, config, params):\n    ...\n    final_layer_output = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, {\n              'labels': labels,\n              'logits': final_layer_output,\n          }))\n  ```\n\n  Prediction\n  ==========\n\n  Prediction on TPU is an experimental feature to support large batch inference.\n  It is not designed for latency-critical system. In addition, due to some\n  usability issues, for prediction with small dataset, CPU `.predict`, i.e.,\n  creating a new `TPUEstimator` instance with `use_tpu=False`, might be more\n  convenient.\n\n  Note: In contrast to TPU training/evaluation, the `input_fn` for prediction\n  *should* raise an end-of-input exception (`OutOfRangeError` or\n  `StopIteration`), which serves as the stopping signal to `TPUEstimator`. To be\n  precise, the ops created by `input_fn` produce one batch of the data.\n  The `predict()` API processes one batch at a time. When reaching the end of\n  the data source, an end-of-input exception should be raised by one of these\n  operations. The user usually does not need to do this manually. As long as the\n  dataset is not repeated forever, the `tf.data` API will raise an end-of-input\n  exception automatically after the last batch has been produced.\n\n  Note: Estimator.predict returns a Python generator. Please consume all the\n  data from the generator so that TPUEstimator can shutdown the TPU system\n  properly for user.\n\n  Current limitations:\n  --------------------\n  1. TPU prediction only works on a single host (one TPU worker).\n\n  2. `input_fn` must return a `Dataset` instance rather than `features`. In\n  fact, .train() and .evaluate() also support Dataset as return value.\n\n  Example (MNIST):\n  ----------------\n  ```\n  height = 32\n  width = 32\n  total_examples = 100\n\n  def predict_input_fn(params):\n    batch_size = params['batch_size']\n\n    images = tf.random_uniform(\n        [total_examples, height, width, 3], minval=-1, maxval=1)\n\n    dataset = tf.data.Dataset.from_tensor_slices(images)\n    dataset = dataset.map(lambda images: {'image': images})\n\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n  def model_fn(features, labels, params, mode):\n     # Generate predictions, called 'output', from features['image']\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions={\n              'predictions': output,\n              'is_padding': features['is_padding']\n          })\n\n  tpu_est = TPUEstimator(\n      model_fn=model_fn,\n      ...,\n      predict_batch_size=16)\n\n  # Fully consume the generator so that TPUEstimator can shutdown the TPU\n  # system.\n  for item in tpu_est.predict(input_fn=input_fn):\n    # Filter out item if the `is_padding` is 1.\n    # Process the 'predictions'\n  ```\n\n  Exporting\n  =========\n\n  `export_savedmodel` exports 2 metagraphs, one with `tag_constants.SERVING`,\n  and another with `tag_constants.SERVING` and `tag_constants.TPU`.\n  At serving time, these tags are used to select metagraph to load.\n\n  Before running the graph on TPU, TPU system needs to be initialized. If\n  TensorFlow Serving model-server is used, this is done automatically. If\n  not, please call `session.run(tpu.initialize_system())`.\n\n  `tpu.outside_compilation` can be used to wrap TPU incompatible ops in\n  `model_fn`.\n\n  Example:\n  ----------------\n\n  ```\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n    export_outputs = {\n      'logits': export_output_lib.PredictOutput(\n        {'logits': logits})\n    }\n\n    def host_call(logits):\n      class_ids = math_ops.argmax(logits)\n      classes = string_ops.as_string(class_ids)\n      export_outputs['classes'] =\n        export_output_lib.ClassificationOutput(classes=classes)\n\n    tpu.outside_compilation(host_call, logits)\n\n    ...\n  ```\n\n  ",
        "klass": "tensorflow.contrib.tpu.python.tpu.tpu_estimator.TPUEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "Estimator with TPU support.\n\n  TPUEstimator also supports training on CPU and GPU. You don't need to define\n  a separate `tf.estimator.Estimator`.\n\n  TPUEstimator handles many of the details of running on TPU devices, such as\n  replicating inputs and models for each core, and returning to host\n  periodically to run hooks.\n\n  TPUEstimator transforms a global batch size in params to a per-shard batch\n  size when calling the `input_fn` and `model_fn`. Users should specify\n  global batch size in constructor, and then get the batch size for each shard\n  in `input_fn` and `model_fn` by `params['batch_size']`.\n\n  - For training, `model_fn` gets per-core batch size; `input_fn` may get\n    per-core or per-host batch size depending on `per_host_input_for_training`\n    in `TPUConfig` (See docstring for TPUConfig for details).\n\n  - For evaluation and prediction, `model_fn` gets per-core batch size and\n    `input_fn` get per-host batch size.\n\n  Evaluation\n  ==========\n\n  `model_fn` should return `TPUEstimatorSpec`, which expects the `eval_metrics`\n  for TPU evaluation. However, if eval_on_tpu is False, `model_fn` must return\n  `EstimatorSpec` and the evaluation will execute on CPU or GPU; in this case\n  the following discussion on TPU evaluation does not apply.\n\n  `TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`, where\n  `tensors` could be a list of any nested structure of `Tensor`s (See\n  `TPUEstimatorSpec` for details).  `metric_fn` takes the `tensors` and returns\n  a dict from metric string name to the result of calling a metric function,\n  namely a `(metric_tensor, update_op)` tuple.\n\n  One can set `use_tpu` to `False` for testing. All training, evaluation, and\n  predict will be executed on CPU. `input_fn` and `model_fn` will receive\n  `train_batch_size` or `eval_batch_size` unmodified as `params['batch_size']`.\n\n  Current limitations:\n  --------------------\n\n  1. TPU evaluation only works on a single host (one TPU worker) except\n     BROADCAST mode.\n\n  2. `input_fn` for evaluation should **NOT** raise an end-of-input exception\n     (`OutOfRangeError` or `StopIteration`). And all evaluation steps and all\n     batches should have the same size.\n\n  Example (MNIST):\n  ----------------\n\n  ```\n  # The metric Fn which runs on CPU.\n  def metric_fn(labels, logits):\n    predictions = tf.argmax(logits, 1)\n    return {\n      'accuracy': tf.metrics.precision(\n          labels=labels, predictions=predictions),\n    }\n\n  # Your model Fn which runs on TPU (eval_metrics is list in this example)\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, [labels, logits]))\n\n  # or specify the eval_metrics tensors as dict.\n  def model_fn(features, labels, mode, config, params):\n    ...\n    final_layer_output = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, {\n              'labels': labels,\n              'logits': final_layer_output,\n          }))\n  ```\n\n  Prediction\n  ==========\n\n  Prediction on TPU is an experimental feature to support large batch inference.\n  It is not designed for latency-critical system. In addition, due to some\n  usability issues, for prediction with small dataset, CPU `.predict`, i.e.,\n  creating a new `TPUEstimator` instance with `use_tpu=False`, might be more\n  convenient.\n\n  Note: In contrast to TPU training/evaluation, the `input_fn` for prediction\n  *should* raise an end-of-input exception (`OutOfRangeError` or\n  `StopIteration`), which serves as the stopping signal to `TPUEstimator`. To be\n  precise, the ops created by `input_fn` produce one batch of the data.\n  The `predict()` API processes one batch at a time. When reaching the end of\n  the data source, an end-of-input exception should be raised by one of these\n  operations. The user usually does not need to do this manually. As long as the\n  dataset is not repeated forever, the `tf.data` API will raise an end-of-input\n  exception automatically after the last batch has been produced.\n\n  Note: Estimator.predict returns a Python generator. Please consume all the\n  data from the generator so that TPUEstimator can shutdown the TPU system\n  properly for user.\n\n  Current limitations:\n  --------------------\n  1. TPU prediction only works on a single host (one TPU worker).\n\n  2. `input_fn` must return a `Dataset` instance rather than `features`. In\n  fact, .train() and .evaluate() also support Dataset as return value.\n\n  Example (MNIST):\n  ----------------\n  ```\n  height = 32\n  width = 32\n  total_examples = 100\n\n  def predict_input_fn(params):\n    batch_size = params['batch_size']\n\n    images = tf.random_uniform(\n        [total_examples, height, width, 3], minval=-1, maxval=1)\n\n    dataset = tf.data.Dataset.from_tensor_slices(images)\n    dataset = dataset.map(lambda images: {'image': images})\n\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n  def model_fn(features, labels, params, mode):\n     # Generate predictions, called 'output', from features['image']\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions={\n              'predictions': output,\n              'is_padding': features['is_padding']\n          })\n\n  tpu_est = TPUEstimator(\n      model_fn=model_fn,\n      ...,\n      predict_batch_size=16)\n\n  # Fully consume the generator so that TPUEstimator can shutdown the TPU\n  # system.\n  for item in tpu_est.predict(input_fn=input_fn):\n    # Filter out item if the `is_padding` is 1.\n    # Process the 'predictions'\n  ```\n\n  Exporting\n  =========\n\n  `export_savedmodel` exports 2 metagraphs, one with `tag_constants.SERVING`,\n  and another with `tag_constants.SERVING` and `tag_constants.TPU`.\n  At serving time, these tags are used to select metagraph to load.\n\n  Before running the graph on TPU, TPU system needs to be initialized. If\n  TensorFlow Serving model-server is used, this is done automatically. If\n  not, please call `session.run(tpu.initialize_system())`.\n\n  `tpu.outside_compilation` can be used to wrap TPU incompatible ops in\n  `model_fn`.\n\n  Example:\n  ----------------\n\n  ```\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n    export_outputs = {\n      'logits': export_output_lib.PredictOutput(\n        {'logits': logits})\n    }\n\n    def host_call(logits):\n      class_ids = math_ops.argmax(logits)\n      classes = string_ops.as_string(class_ids)\n      export_outputs['classes'] =\n        export_output_lib.ClassificationOutput(classes=classes)\n\n    tpu.outside_compilation(host_call, logits)\n\n    ...\n  ```\n\n  ",
        "klass": "tensorflow.contrib.tpu.TPUEstimator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An object use to hold the sharding policy for a Tensor.\n  ",
        "klass": "tensorflow.contrib.tpu.python.tpu.tpu_sharding.ShardingPolicy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes 'hyper-param=value' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description='Train my model.')\n  parser.add_argument('--hparams', type=str,\n                      help='Comma separated list of \"name=value\" pairs.')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=['relu', 'tanh'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then 'hparams' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> ['relu', 'tanh']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json('{\"learning_rate\": 0.3, \"activations\": \"relu\"}')\n  ```\n  ",
        "klass": "tensorflow.contrib.training.python.training.hparam.HParams",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes 'hyper-param=value' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description='Train my model.')\n  parser.add_argument('--hparams', type=str,\n                      help='Comma separated list of \"name=value\" pairs.')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=['relu', 'tanh'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then 'hparams' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> ['relu', 'tanh']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json('{\"learning_rate\": 0.3, \"activations\": \"relu\"}')\n  ```\n  ",
        "klass": "tensorflow.contrib.training.HParams",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.data_flow_ops.BaseStagingArea"
        ],
        "class_docstring": "A `MapStagingArea` is a TensorFlow data structure that stores tensors\n  across multiple steps, and exposes operations that can put and get tensors.\n\n  Each `MapStagingArea` element is a (key, value) pair.\n  Only int64 keys are supported, other types should be\n  hashed to produce a key.\n  Values are a tuple of one or more tensors.\n  Each tuple component has a static dtype,\n  and may have a static shape.\n\n  The capacity of a `MapStagingArea` may be bounded or unbounded.\n  It supports multiple concurrent producers and consumers; and\n  provides exactly-once delivery.\n\n  Each value tuple of a `MapStagingArea` is a fixed-length tuple of tensors\n  whose\n  dtypes are described by `dtypes`, and whose shapes are optionally described\n  by the `shapes` argument.\n\n  If the `shapes` argument is specified, each component of a staging area\n  element must have the respective fixed shape. If it is\n  unspecified, different elements may have different shapes,\n\n  It behaves like an associative container with support for:\n\n   - put(key, values)\n   - peek(key)         like dict.get(key)\n   - get(key)          like dict.pop(key)\n   - get(key=None)     like dict.popitem()\n   - size()\n   - clear()\n\n  If ordered a tree structure ordered by key will be used and\n  get(key=None) will remove (key, value) pairs in increasing key order.\n  Otherwise a hashtable\n\n  It can be configured with a capacity in which case\n  put(key, values) will block until space becomes available.\n\n  Similarly, it can be configured with a memory limit which\n  will block put(key, values) until space is available.\n  This is mostly useful for limiting the number of tensors on\n  devices such as GPUs.\n\n  All get() and peek() commands block if the requested\n  (key, value) pair is not present in the staging area.\n\n  Partial puts are supported and will be placed in an incomplete\n  map until such time as all values associated with the key have\n  been inserted. Once completed, this (key, value) pair will be\n  inserted into the map. Data in the incomplete map\n  counts towards the memory limit, but not towards capacity limit.\n\n  Partial gets from the map are also supported.\n  This removes the partially requested tensors from the entry,\n  but the entry is only removed from the map once all tensors\n  associated with it are removed.\n  ",
        "klass": "tensorflow.python.ops.data_flow_ops.MapStagingArea",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.checkpointable.base.CheckpointableBase"
        ],
        "class_docstring": "See the [Variables Guide](https://tensorflow.org/guide/variables).\n\n  A variable maintains state in the graph across calls to `run()`. You add a\n  variable to the graph by constructing an instance of the class `Variable`.\n\n  The `Variable()` constructor requires an initial value for the variable,\n  which can be a `Tensor` of any type and shape. The initial value defines the\n  type and shape of the variable. After construction, the type and shape of\n  the variable are fixed. The value can be changed using one of the assign\n  methods.\n\n  If you want to change the shape of a variable later you have to use an\n  `assign` Op with `validate_shape=False`.\n\n  Just like any `Tensor`, variables created with `Variable()` can be used as\n  inputs for other Ops in the graph. Additionally, all the operators\n  overloaded for the `Tensor` class are carried over to variables, so you can\n  also add nodes to the graph by just doing arithmetic on variables.\n\n  ```python\n  import tensorflow as tf\n\n  # Create a variable.\n  w = tf.Variable(<initial-value>, name=<optional-name>)\n\n  # Use the variable in the graph like any Tensor.\n  y = tf.matmul(w, ...another variable or tensor...)\n\n  # The overloaded operators are available too.\n  z = tf.sigmoid(w + y)\n\n  # Assign a new value to the variable with `assign()` or a related method.\n  w.assign(w + 1.0)\n  w.assign_add(1.0)\n  ```\n\n  When you launch the graph, variables have to be explicitly initialized before\n  you can run Ops that use their value. You can initialize a variable by\n  running its *initializer op*, restoring the variable from a save file, or\n  simply running an `assign` Op that assigns a value to the variable. In fact,\n  the variable *initializer op* is just an `assign` Op that assigns the\n  variable's initial value to the variable itself.\n\n  ```python\n  # Launch the graph in a session.\n  with tf.Session() as sess:\n      # Run the variable initializer.\n      sess.run(w.initializer)\n      # ...you now can run ops that use the value of 'w'...\n  ```\n\n  The most common initialization pattern is to use the convenience function\n  `global_variables_initializer()` to add an Op to the graph that initializes\n  all the variables. You then run that Op after launching the graph.\n\n  ```python\n  # Add an Op to initialize global variables.\n  init_op = tf.global_variables_initializer()\n\n  # Launch the graph in a session.\n  with tf.Session() as sess:\n      # Run the Op that initializes global variables.\n      sess.run(init_op)\n      # ...you can now run any Op that uses variable values...\n  ```\n\n  If you need to create a variable with an initial value dependent on another\n  variable, use the other variable's `initialized_value()`. This ensures that\n  variables are initialized in the right order.\n\n  All variables are automatically collected in the graph where they are\n  created. By default, the constructor adds the new variable to the graph\n  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n  `global_variables()` returns the contents of that collection.\n\n  When building a machine learning model it is often convenient to distinguish\n  between variables holding the trainable model parameters and other variables\n  such as a `global step` variable used to count training steps. To make this\n  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n  `True`, the new variable is also added to the graph collection\n  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n  `trainable_variables()` returns the contents of this collection. The\n  various `Optimizer` classes use this collection as the default list of\n  variables to optimize.\n\n  WARNING: tf.Variable objects by default have a non-intuitive memory model. A\n  Variable is represented internally as a mutable Tensor which can\n  non-deterministically alias other Tensors in a graph. The set of operations\n  which consume a Variable and can lead to aliasing is undetermined and can\n  change across TensorFlow versions. Avoid writing code which relies on the\n  value of a Variable either changing or not changing as other operations\n  happen. For example, using Variable objects or simple functions thereof as\n  predicates in a `tf.cond` is dangerous and error-prone:\n\n  ```\n  v = tf.Variable(True)\n  tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.\n  ```\n\n  Here replacing adding `use_resource=True` when constructing the variable will\n  fix any nondeterminism issues:\n  ```\n  v = tf.Variable(True, use_resource=True)\n  tf.cond(v, lambda: v.assign(False), my_false_fn)\n  ```\n\n  To use the replacement for variables which does\n  not have these issues:\n\n  * Add `use_resource=True` when constructing `tf.Variable`;\n  * Call `tf.get_variable_scope().set_use_resource(True)` inside a\n    `tf.variable_scope` before the `tf.get_variable()` call.\n  ",
        "klass": "tensorflow.python.ops.variables.Variable",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tensor list wrapper API-compatible with Python built-in list.",
        "klass": "tensorflow.python.autograph.utils.tensor_list.TensorList",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.experimental.ops.indexed_dataset_ops.IndexedDataset"
        ],
        "class_docstring": "IdentityIndexedDataset is a trivial indexed dataset used for testing.\n  ",
        "klass": "tensorflow.python.data.experimental.ops.indexed_dataset_ops.IdentityIndexedDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.ops.dataset_ops.DatasetV1"
        ],
        "class_docstring": "A substitute for `Dataset.interleave()` on a fixed list of datasets.",
        "klass": "tensorflow.python.data.experimental.ops.interleave_ops._DirectedInterleaveDataset",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.data.ops.optional_ops.Optional"
        ],
        "class_docstring": "Concrete implementation of `tf.data.experimental.Optional`.\n\n  NOTE(mrry): This implementation is kept private, to avoid defining\n  `Optional.__init__()` in the public API.\n  ",
        "klass": "tensorflow.python.data.ops.optional_ops._OptionalImpl",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.debug.wrappers.framework.NonInteractiveDebugWrapperSession"
        ],
        "class_docstring": "Debug Session wrapper that dumps debug data to filesystem.",
        "klass": "tensorflow.python.debug.wrappers.dumping_wrapper.DumpingDebugWrapperSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.cluster_resolver.cluster_resolver.ClusterResolver"
        ],
        "class_docstring": "Cluster Resolver for Google Cloud TPUs.\n\n  This is an implementation of cluster resolvers for the Google Cloud TPU\n  service. As Cloud TPUs are in alpha, you will need to specify a API definition\n  file for this to consume, in addition to a list of Cloud TPUs in your Google\n  Cloud Platform project.\n  ",
        "klass": "tensorflow.python.distribute.cluster_resolver.TPUClusterResolver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow Linear and DNN joined models for regression.\n\n  Note: This estimator is also known as wide-n-deep.\n\n  Example:\n\n  ```python\n  numeric_feature = numeric_column(...)\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = DNNLinearCombinedRegressor(\n      # wide settings\n      linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n      linear_optimizer=tf.train.FtrlOptimizer(...),\n      # deep settings\n      dnn_feature_columns=[\n          categorical_feature_a_emb, categorical_feature_b_emb,\n          numeric_feature],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.train.ProximalAdagradOptimizer(...),\n      # warm-start settings\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # To apply L1 and L2 regularization, you can set dnn_optimizer to:\n  tf.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001,\n      l2_regularization_strength=0.001)\n  # To apply learning rate decay, you can set dnn_optimizer to a callable:\n  lambda: tf.AdamOptimizer(\n      learning_rate=tf.exponential_decay(\n          learning_rate=0.1,\n          global_step=tf.get_global_step(),\n          decay_steps=10000,\n          decay_rate=0.96)\n  # It is the same for linear_optimizer.\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedRegressor",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "Linear classifier model.\n\n  Train a linear model to classify instances into one of multiple possible\n  classes. When number of possible classes is 2, this is binary classification.\n\n  Example:\n\n  ```python\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=lambda: tf.train.FtrlOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = LinearClassifier(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n    otherwise there will be a `KeyError`:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using softmax cross entropy.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.python.estimator.canned.linear.LinearClassifier",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.Estimator"
        ],
        "class_docstring": "An estimator for TensorFlow Linear regression problems.\n\n  Train a linear regression model to predict label value given observation of\n  feature values.\n\n  Example:\n\n  ```python\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n\n  # Estimator using the default optimizer.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b])\n\n  # Or estimator using the FTRL optimizer with regularization.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=tf.train.FtrlOptimizer(\n        learning_rate=0.1,\n        l1_regularization_strength=0.001\n      ))\n\n  # Or estimator using an optimizer with a learning rate decay.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      optimizer=lambda: tf.train.FtrlOptimizer(\n          learning_rate=tf.exponential_decay(\n              learning_rate=0.1,\n              global_step=tf.get_global_step(),\n              decay_steps=10000,\n              decay_rate=0.96))\n\n  # Or estimator with warm-starting from a previous checkpoint.\n  estimator = LinearRegressor(\n      feature_columns=[categorical_column_a,\n                       categorical_feature_a_x_categorical_feature_b],\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train)\n  metrics = estimator.evaluate(input_fn=input_fn_eval)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n    otherwise there will be a KeyError:\n\n  * if `weight_column` is not `None`, a feature with `key=weight_column` whose\n    value is a `Tensor`.\n  * for each `column` in `feature_columns`:\n    - if `column` is a `SparseColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedSparseColumn`, two features: the first with\n      `key` the id column name, the second with `key` the weight column name.\n      Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `RealValuedColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.python.estimator.canned.linear.LinearRegressor",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow_estimator.python.estimator.estimator.EstimatorV2"
        ],
        "class_docstring": "Estimator class to train and evaluate TensorFlow models.\n\n  The `Estimator` object wraps a model which is specified by a `model_fn`,\n  which, given inputs and a number of other parameters, returns the ops\n  necessary to perform training, evaluation, or predictions.\n\n  All outputs (checkpoints, event files, etc.) are written to `model_dir`, or a\n  subdirectory thereof. If `model_dir` is not set, a temporary directory is\n  used.\n\n  The `config` argument can be passed `tf.estimator.RunConfig` object containing\n  information about the execution environment. It is passed on to the\n  `model_fn`, if the `model_fn` has a parameter named \"config\" (and input\n  functions in the same manner). If the `config` parameter is not passed, it is\n  instantiated by the `Estimator`. Not passing config means that defaults useful\n  for local execution are used. `Estimator` makes config available to the model\n  (for instance, to allow specialization based on the number of workers\n  available), and also uses some of its fields to control internals, especially\n  regarding checkpointing.\n\n  The `params` argument contains hyperparameters. It is passed to the\n  `model_fn`, if the `model_fn` has a parameter named \"params\", and to the input\n  functions in the same manner. `Estimator` only passes params along, it does\n  not inspect it. The structure of `params` is therefore entirely up to the\n  developer.\n\n  None of `Estimator`'s methods can be overridden in subclasses (its\n  constructor enforces this). Subclasses should use `model_fn` to configure\n  the base class, and may add methods implementing specialized functionality.\n\n  @compatibility(eager)\n  Calling methods of `Estimator` will work while eager execution is enabled.\n  However, the `model_fn` and `input_fn` is not executed eagerly, `Estimator`\n  will switch to graph mode before calling all user-provided functions (incl.\n  hooks), so their code has to be compatible with graph mode execution. Note\n  that `input_fn` code using `tf.data` generally works in both graph and eager\n  modes.\n  @end_compatibility\n  ",
        "klass": "tensorflow.python.estimator.estimator.Estimator",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from numpy arrays.",
        "klass": "tensorflow_estimator.python.estimator.inputs.queues.feeding_functions._ArrayFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from numpy arrays.",
        "klass": "tensorflow.python.estimator.inputs.queues.feeding_functions._ArrayFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from `OrderedDict`s of numpy arrays.",
        "klass": "tensorflow_estimator.python.estimator.inputs.queues.feeding_functions._OrderedDictNumpyFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from `OrderedDict`s of numpy arrays.",
        "klass": "tensorflow.python.estimator.inputs.queues.feeding_functions._OrderedDictNumpyFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from pandas `DataFrames`.",
        "klass": "tensorflow_estimator.python.estimator.inputs.queues.feeding_functions._PandasFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates feed dictionaries from pandas `DataFrames`.",
        "klass": "tensorflow.python.estimator.inputs.queues.feeding_functions._PandasFeedFn",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class specifies the configurations for an `Estimator` run.",
        "klass": "tensorflow.python.estimator.run_config.RunConfig",
        "module": "tensorflow_estimator"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.base_layer.Layer"
        ],
        "class_docstring": "A layer that produces a dense `Tensor` based on given `feature_columns`.\n\n  Generally a single example in training data is described with FeatureColumns.\n  At the first layer of the model, this column oriented data should be converted\n  to a single `Tensor`.\n\n  This layer can be called multiple times with different features.\n\n  Example:\n\n  ```python\n  price = numeric_column('price')\n  keywords_embedded = embedding_column(\n      categorical_column_with_hash_bucket(\"keywords\", 10K), dimensions=16)\n  columns = [price, keywords_embedded, ...]\n  feature_layer = DenseFeatures(columns)\n\n  features = tf.parse_example(..., features=make_parse_example_spec(columns))\n  dense_tensor = feature_layer(features)\n  for units in [128, 64, 32]:\n    dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\n  prediction = tf.layers.dense(dense_tensor, 1).\n  ```\n  ",
        "klass": "tensorflow.python.feature_column.feature_column_v2.DenseFeatures",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.base_layer.Layer"
        ],
        "class_docstring": "Layer that contains logic for `LinearModel`.",
        "klass": "tensorflow.python.feature_column.feature_column_v2._LinearModelLayer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager to automatically add control dependencies.\n\n  Code under this context manager will act as if a sensible set of control\n  dependencies were present. More specifically:\n    1. All stateful ops in the scope will execute\n    2. Stateful ops which modify the same resource will execute in program order\n\n  Note: creating variables in an automatic control dependencies context is not\n  supported (the value of the variables will never change as they will keep\n  getting reinitialized).\n\n  NOT THREAD SAFE\n  ",
        "klass": "tensorflow.python.framework.auto_control_deps.AutomaticControlDependencies",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.framework.errors_impl.OpError"
        ],
        "class_docstring": "Raised when an operation receives an invalid argument.\n\n  This may occur, for example, if an operation is receives an input\n  tensor that has an invalid value or shape. For example, the\n  `tf.matmul` op will raise this\n  error if it receives an input that is not a matrix, and the\n  `tf.reshape` op will raise\n  this error if the new shape does not match the number of elements in the input\n  tensor.\n\n  @@__init__\n  ",
        "klass": "tensorflow.python.framework.errors.InvalidArgumentError",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager to check for C API status.",
        "klass": "tensorflow.python.framework.errors.raise_exception_on_not_ok_status",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.framework.ops.Graph"
        ],
        "class_docstring": "Graph representing a function body.\n\n  Attributes:\n    name: The name of the function.\n    inputs: Placeholder tensors representing the inputs to this function. The\n      tensors are in this FuncGraph. This represents \"regular\" inputs as well as\n      captured inputs (i.e. the values of self.captures), with the regular\n      inputs coming first.\n    outputs: Tensors that will be returned by this function. The tensors are in\n      this FuncGraph.\n    structured_outputs: A possibly-nested python object which will be returned\n      by this function. The Tensors in this structure are the same as those of\n      self.outputs. Note that this structure might contain Python `None`s.\n    variables: Variables that should be watched during function execution.\n    outer_graph: The graph this function is defined in. May be another FuncGraph\n      or the global default Graph.\n    captures: Maps external tensor -> internal tensor (i.e. input placeholder).\n      The entries are in the order they were captured.\n    seed: The graph-level random seed.\n  ",
        "klass": "tensorflow.python.framework.func_graph.FuncGraph",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.network.Network"
        ],
        "class_docstring": "`Model` groups layers into an object with training and inference features.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n  ",
        "klass": "tensorflow.python.keras.Model",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Linear stack of layers.\n\n  Arguments:\n      layers: list of layers to add to the model.\n\n  Example:\n\n  ```python\n  # Optionally, the first layer can receive an `input_shape` argument:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  # Afterwards, we do automatic shape inference:\n  model.add(Dense(32))\n\n  # This is identical to the following:\n  model = Sequential()\n  model.add(Dense(32, input_dim=500))\n\n  # And to the following:\n  model = Sequential()\n  model.add(Dense(32, batch_input_shape=(None, 500)))\n\n  # Note that you can also omit the `input_shape` argument:\n  # In that case the model gets built the first time you call `fit` (or other\n  # training and evaluation methods).\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.compile(optimizer=optimizer, loss=loss)\n  # This builds the model for the first time:\n  model.fit(x, y, batch_size=32, epochs=10)\n\n  # Note that when using this delayed-build pattern (no input shape specified),\n  # the model doesn't have any weights until the first call\n  # to a training/evaluation method (since it isn't yet built):\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.weights  # returns []\n\n  # Whereas if you specify the input shape, the model gets built continuously\n  # as you are adding layers:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  model.add(Dense(32))\n  model.weights  # returns list of length 4\n\n  When using the delayed-build pattern (no input shape specified), you can\n  choose to manually build your model by calling `build(batch_input_shape)`:\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.build((None, 500))\n  model.weights  # returns list of length 4\n  ```\n  ",
        "klass": "tensorflow.python.keras.Sequential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for use when defining a Python op.\n\n  This context manager validates that the given `values` are from the\n  same graph, makes that graph the default graph, and pushes a\n  name scope in that graph (see\n  `tf.Graph.name_scope`\n  for more details on that).\n\n  For example, to define a new Python op called `my_op`:\n\n  ```python\n  def my_op(a, b, c, name=None):\n    with tf.name_scope(name, \"MyOp\", [a, b, c]) as scope:\n      a = tf.convert_to_tensor(a, name=\"a\")\n      b = tf.convert_to_tensor(b, name=\"b\")\n      c = tf.convert_to_tensor(c, name=\"c\")\n      # Define some computation that uses `a`, `b`, and `c`.\n      return foo_op(..., name=scope)\n  ```\n  ",
        "klass": "tensorflow.python.keras.backend.name_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.merge._Merge"
        ],
        "class_docstring": "Layer that adds a list of inputs.\n\n  It takes as input a list of tensors,\n  all of the same shape, and returns\n  a single tensor (also of the same shape).\n\n  Examples:\n\n  ```python\n      import keras\n\n      input1 = keras.layers.Input(shape=(16,))\n      x1 = keras.layers.Dense(8, activation='relu')(input1)\n      input2 = keras.layers.Input(shape=(32,))\n      x2 = keras.layers.Dense(8, activation='relu')(input2)\n      added = keras.layers.Add()([x1, x2])  # equivalent to added =\n      keras.layers.add([x1, x2])\n\n      out = keras.layers.Dense(4)(added)\n      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n  ```\n  ",
        "klass": "tensorflow.python.keras.layers.Add",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.merge._Merge"
        ],
        "class_docstring": "Layer that concatenates a list of inputs.\n\n  It takes as input a list of tensors,\n  all of the same shape except for the concatenation axis,\n  and returns a single tensor, the concatenation of all inputs.\n\n  Arguments:\n      axis: Axis along which to concatenate.\n      **kwargs: standard layer keyword arguments.\n  ",
        "klass": "tensorflow.python.keras.layers.Concatenate",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.convolutional_recurrent.ConvRNN2D"
        ],
        "class_docstring": "Convolutional LSTM.\n\n  It is similar to an LSTM layer, but the input transformations\n  and recurrent transformations are both convolutional.\n\n  Arguments:\n    filters: Integer, the dimensionality of the output space\n        (i.e. the number of output filters in the convolution).\n    kernel_size: An integer or tuple/list of n integers, specifying the\n        dimensions of the convolution window.\n    strides: An integer or tuple/list of n integers,\n        specifying the strides of the convolution.\n        Specifying any stride value != 1 is incompatible with specifying\n        any `dilation_rate` value != 1.\n    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n    data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, time, ..., channels)`\n        while `channels_first` corresponds to\n        inputs with shape `(batch, time, channels, ...)`.\n        It defaults to the `image_data_format` value found in your\n        Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be \"channels_last\".\n    dilation_rate: An integer or tuple/list of n integers, specifying\n        the dilation rate to use for dilated convolution.\n        Currently, specifying any `dilation_rate` value != 1 is\n        incompatible with specifying any `strides` value != 1.\n    activation: Activation function to use.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step.\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs.\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix,\n        used for the linear transformation of the recurrent state.\n    bias_initializer: Initializer for the bias vector.\n    unit_forget_bias: Boolean.\n        If True, add 1 to the bias of the forget gate at initialization.\n        Use in combination with `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.]\n        (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix.\n    recurrent_regularizer: Regularizer function applied to\n        the `recurrent_kernel` weights matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to.\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix.\n    recurrent_constraint: Constraint function applied to\n        the `recurrent_kernel` weights matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence.\n    go_backwards: Boolean (default False).\n        If True, process the input sequence backwards.\n    stateful: Boolean (default False). If True, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the inputs.\n    recurrent_dropout: Float between 0 and 1.\n        Fraction of the units to drop for\n        the linear transformation of the recurrent state.\n\n  Input shape:\n    - if data_format='channels_first'\n        5D tensor with shape:\n        `(samples, time, channels, rows, cols)`\n    - if data_format='channels_last'\n        5D tensor with shape:\n        `(samples, time, rows, cols, channels)`\n\n  Output shape:\n    - if `return_sequences`\n         - if data_format='channels_first'\n            5D tensor with shape:\n            `(samples, time, filters, output_row, output_col)`\n         - if data_format='channels_last'\n            5D tensor with shape:\n            `(samples, time, output_row, output_col, filters)`\n    - else\n        - if data_format ='channels_first'\n            4D tensor with shape:\n            `(samples, filters, output_row, output_col)`\n        - if data_format='channels_last'\n            4D tensor with shape:\n            `(samples, output_row, output_col, filters)`\n        where o_row and o_col depend on the shape of the filter and\n        the padding\n\n  Raises:\n    ValueError: in case of invalid constructor arguments.\n\n  References:\n    - [Convolutional LSTM Network: A Machine Learning Approach for\n    Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n    The current implementation does not include the feedback loop on the\n    cells output.\n\n  ",
        "klass": "tensorflow.python.keras.layers.ConvLSTM2D",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.merge._Merge"
        ],
        "class_docstring": "Layer that computes a dot product between samples in two tensors.\n\n  E.g. if applied to a list of two tensors `a` and `b` of shape\n  `(batch_size, n)`, the output will be a tensor of shape `(batch_size, 1)`\n  where each entry `i` will be the dot product between\n  `a[i]` and `b[i]`.\n\n  Arguments:\n      axes: Integer or tuple of integers,\n          axis or axes along which to take the dot product.\n      normalize: Whether to L2-normalize samples along the\n          dot product axis before taking the dot product.\n          If set to True, then the output of the dot product\n          is the cosine proximity between the two samples.\n      **kwargs: Standard layer keyword arguments.\n  ",
        "klass": "tensorflow.python.keras.layers.Dot",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.base_layer.Layer"
        ],
        "class_docstring": "Turns positive integers (indexes) into dense vectors of fixed size.\n\n  eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\n  This layer can only be used as the first layer in a model.\n\n  Example:\n\n  ```python\n    model = Sequential()\n    model.add(Embedding(1000, 64, input_length=10))\n    # the model will take as input an integer matrix of size (batch,\n    # input_length).\n    # the largest integer (i.e. word index) in the input should be no larger\n    # than 999 (vocabulary size).\n    # now model.output_shape == (None, 10, 64), where None is the batch\n    # dimension.\n\n    input_array = np.random.randint(1000, size=(32, 10))\n\n    model.compile('rmsprop', 'mse')\n    output_array = model.predict(input_array)\n    assert output_array.shape == (32, 10, 64)\n  ```\n\n  Arguments:\n    input_dim: int > 0. Size of the vocabulary,\n        i.e. maximum integer index + 1.\n    output_dim: int >= 0. Dimension of the dense embedding.\n    embeddings_initializer: Initializer for the `embeddings` matrix.\n    embeddings_regularizer: Regularizer function applied to\n        the `embeddings` matrix.\n    embeddings_constraint: Constraint function applied to\n        the `embeddings` matrix.\n    mask_zero: Whether or not the input value 0 is a special \"padding\"\n        value that should be masked out.\n        This is useful when using recurrent layers\n        which may take variable length input.\n        If this is `True` then all subsequent layers\n        in the model need to support masking or an exception will be raised.\n        If mask_zero is set to True, as a consequence, index 0 cannot be\n        used in the vocabulary (input_dim should equal size of\n        vocabulary + 1).\n    input_length: Length of input sequences, when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n\n  Input shape:\n      2D tensor with shape: `(batch_size, input_length)`.\n\n  Output shape:\n      3D tensor with shape: `(batch_size, input_length, output_dim)`.\n\n  ",
        "klass": "tensorflow.python.keras.layers.Embedding",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.recurrent.RNN"
        ],
        "class_docstring": "Gated Recurrent Unit - Cho et al. 2014.\n\n  There are two variants. The default one is based on 1406.1078v3 and\n  has reset gate applied to hidden state before matrix multiplication. The\n  other one is based on original 1406.1078v1 and has the order reversed.\n\n  The second variant is compatible with CuDNNGRU (GPU-only) and allows\n  inference on CPU. Thus it has separate biases for `kernel` and\n  `recurrent_kernel`. Use `'reset_after'=True` and\n  `recurrent_activation='sigmoid'`.\n\n  Arguments:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use.\n          Default: hyperbolic tangent (`tanh`).\n          If you pass `None`, no activation is applied\n          (ie. \"linear\" activation: `a(x) = x`).\n      recurrent_activation: Activation function to use\n          for the recurrent step.\n          Default: hard sigmoid (`hard_sigmoid`).\n          If you pass `None`, no activation is applied\n          (ie. \"linear\" activation: `a(x) = x`).\n      use_bias: Boolean, whether the layer uses a bias vector.\n      kernel_initializer: Initializer for the `kernel` weights matrix,\n          used for the linear transformation of the inputs.\n      recurrent_initializer: Initializer for the `recurrent_kernel`\n          weights matrix,\n          used for the linear transformation of the recurrent state.\n      bias_initializer: Initializer for the bias vector.\n      kernel_regularizer: Regularizer function applied to\n          the `kernel` weights matrix.\n      recurrent_regularizer: Regularizer function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      activity_regularizer: Regularizer function applied to\n          the output of the layer (its \"activation\")..\n      kernel_constraint: Constraint function applied to\n          the `kernel` weights matrix.\n      recurrent_constraint: Constraint function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n      dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the inputs.\n      recurrent_dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the recurrent state.\n      implementation: Implementation mode, either 1 or 2.\n          Mode 1 will structure its operations as a larger number of\n          smaller dot products and additions, whereas mode 2 will\n          batch them into fewer, larger operations. These modes will\n          have different performance profiles on different hardware and\n          for different applications.\n      return_sequences: Boolean. Whether to return the last output\n          in the output sequence, or the full sequence.\n      return_state: Boolean. Whether to return the last state\n          in addition to the output.\n      go_backwards: Boolean (default False).\n          If True, process the input sequence backwards and return the\n          reversed sequence.\n      stateful: Boolean (default False). If True, the last state\n          for each sample at index i in a batch will be used as initial\n          state for the sample of index i in the following batch.\n      unroll: Boolean (default False).\n          If True, the network will be unrolled,\n          else a symbolic loop will be used.\n          Unrolling can speed-up a RNN,\n          although it tends to be more memory-intensive.\n          Unrolling is only suitable for short sequences.\n      reset_after: GRU convention (whether to apply reset gate after or\n          before matrix multiplication). False = \"before\" (default),\n          True = \"after\" (CuDNN compatible).\n\n  ",
        "klass": "tensorflow.python.keras.layers.GRU",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.recurrent.RNN"
        ],
        "class_docstring": "Long Short-Term Memory layer - Hochreiter 1997.\n\n   Note that this cell is not optimized for performance on GPU. Please use\n  `tf.keras.layers.CuDNNLSTM` for better performance on GPU.\n\n  Arguments:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use.\n          Default: hyperbolic tangent (`tanh`).\n          If you pass `None`, no activation is applied\n          (ie. \"linear\" activation: `a(x) = x`).\n      recurrent_activation: Activation function to use\n          for the recurrent step.\n          Default: hard sigmoid (`hard_sigmoid`).\n          If you pass `None`, no activation is applied\n          (ie. \"linear\" activation: `a(x) = x`).\n      use_bias: Boolean, whether the layer uses a bias vector.\n      kernel_initializer: Initializer for the `kernel` weights matrix,\n          used for the linear transformation of the inputs..\n      recurrent_initializer: Initializer for the `recurrent_kernel`\n          weights matrix,\n          used for the linear transformation of the recurrent state..\n      bias_initializer: Initializer for the bias vector.\n      unit_forget_bias: Boolean.\n          If True, add 1 to the bias of the forget gate at initialization.\n          Setting it to true will also force `bias_initializer=\"zeros\"`.\n          This is recommended in [Jozefowicz et\n            al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n      kernel_regularizer: Regularizer function applied to\n          the `kernel` weights matrix.\n      recurrent_regularizer: Regularizer function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      activity_regularizer: Regularizer function applied to\n          the output of the layer (its \"activation\")..\n      kernel_constraint: Constraint function applied to\n          the `kernel` weights matrix.\n      recurrent_constraint: Constraint function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n      dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the inputs.\n      recurrent_dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the recurrent state.\n      implementation: Implementation mode, either 1 or 2.\n          Mode 1 will structure its operations as a larger number of\n          smaller dot products and additions, whereas mode 2 will\n          batch them into fewer, larger operations. These modes will\n          have different performance profiles on different hardware and\n          for different applications.\n      return_sequences: Boolean. Whether to return the last output.\n          in the output sequence, or the full sequence.\n      return_state: Boolean. Whether to return the last state\n          in addition to the output.\n      go_backwards: Boolean (default False).\n          If True, process the input sequence backwards and return the\n          reversed sequence.\n      stateful: Boolean (default False). If True, the last state\n          for each sample at index i in a batch will be used as initial\n          state for the sample of index i in the following batch.\n      unroll: Boolean (default False).\n          If True, the network will be unrolled,\n          else a symbolic loop will be used.\n          Unrolling can speed-up a RNN,\n          although it tends to be more memory-intensive.\n          Unrolling is only suitable for short sequences.\n\n  ",
        "klass": "tensorflow.python.keras.layers.LSTM",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.base_layer.Layer"
        ],
        "class_docstring": "Wraps arbitrary expression as a `Layer` object.\n\n  Examples:\n\n  ```python\n      # add a x -> x^2 layer\n      model.add(Lambda(lambda x: x ** 2))\n  ```\n  ```python\n      # add a layer that returns the concatenation\n      # of the positive part of the input and\n      # the opposite of the negative part\n\n      def antirectifier(x):\n          x -= K.mean(x, axis=1, keepdims=True)\n          x = K.l2_normalize(x, axis=1)\n          pos = K.relu(x)\n          neg = K.relu(-x)\n          return K.concatenate([pos, neg], axis=1)\n\n      model.add(Lambda(antirectifier))\n  ```\n\n  Arguments:\n      function: The function to be evaluated.\n          Takes input tensor as first argument.\n      output_shape: Expected output shape from function.\n            This argument can be inferred if not explicitly provided.\n            Can be a tuple or function.\n            If a tuple, it only specifies the first dimension onward;\n                 sample dimension is assumed either the same as the input:\n                 `output_shape = (input_shape[0], ) + output_shape`\n                 or, the input is `None` and\n                 the sample dimension is also `None`:\n                 `output_shape = (None, ) + output_shape`\n            If a function, it specifies the entire shape as a function of the\n            input shape: `output_shape = f(input_shape)`\n      arguments: optional dictionary of keyword arguments to be passed\n            to the function.\n\n  Input shape:\n      Arbitrary. Use the keyword argument input_shape\n      (tuple of integers, does not include the samples axis)\n      when using this layer as the first layer in a model.\n\n  Output shape:\n      Specified by `output_shape` argument\n  ",
        "klass": "tensorflow.python.keras.layers.Lambda",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.recurrent.RNN"
        ],
        "class_docstring": "Fully-connected RNN where the output is to be fed back to input.\n\n  Arguments:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use.\n          Default: hyperbolic tangent (`tanh`).\n          If you pass None, no activation is applied\n          (ie. \"linear\" activation: `a(x) = x`).\n      use_bias: Boolean, whether the layer uses a bias vector.\n      kernel_initializer: Initializer for the `kernel` weights matrix,\n          used for the linear transformation of the inputs.\n      recurrent_initializer: Initializer for the `recurrent_kernel`\n          weights matrix,\n          used for the linear transformation of the recurrent state.\n      bias_initializer: Initializer for the bias vector.\n      kernel_regularizer: Regularizer function applied to\n          the `kernel` weights matrix.\n      recurrent_regularizer: Regularizer function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_regularizer: Regularizer function applied to the bias vector.\n      activity_regularizer: Regularizer function applied to\n          the output of the layer (its \"activation\")..\n      kernel_constraint: Constraint function applied to\n          the `kernel` weights matrix.\n      recurrent_constraint: Constraint function applied to\n          the `recurrent_kernel` weights matrix.\n      bias_constraint: Constraint function applied to the bias vector.\n      dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the inputs.\n      recurrent_dropout: Float between 0 and 1.\n          Fraction of the units to drop for\n          the linear transformation of the recurrent state.\n      return_sequences: Boolean. Whether to return the last output\n          in the output sequence, or the full sequence.\n      return_state: Boolean. Whether to return the last state\n          in addition to the output.\n      go_backwards: Boolean (default False).\n          If True, process the input sequence backwards and return the\n          reversed sequence.\n      stateful: Boolean (default False). If True, the last state\n          for each sample at index i in a batch will be used as initial\n          state for the sample of index i in the following batch.\n      unroll: Boolean (default False).\n          If True, the network will be unrolled,\n          else a symbolic loop will be used.\n          Unrolling can speed-up a RNN,\n          although it tends to be more memory-intensive.\n          Unrolling is only suitable for short sequences.\n  ",
        "klass": "tensorflow.python.keras.layers.SimpleRNN",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.merge._Merge"
        ],
        "class_docstring": "Layer that subtracts two inputs.\n\n  It takes as input a list of tensors of size 2,\n  both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\n  also of the same shape.\n\n  Examples:\n\n  ```python\n      import keras\n\n      input1 = keras.layers.Input(shape=(16,))\n      x1 = keras.layers.Dense(8, activation='relu')(input1)\n      input2 = keras.layers.Input(shape=(32,))\n      x2 = keras.layers.Dense(8, activation='relu')(input2)\n      # Equivalent to subtracted = keras.layers.subtract([x1, x2])\n      subtracted = keras.layers.Subtract()([x1, x2])\n\n      out = keras.layers.Dense(4)(subtracted)\n      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n  ```\n  ",
        "klass": "tensorflow.python.keras.layers.Subtract",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.network.Network"
        ],
        "class_docstring": "`Model` groups layers into an object with training and inference features.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n  ",
        "klass": "tensorflow.python.keras.models.Model",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Linear stack of layers.\n\n  Arguments:\n      layers: list of layers to add to the model.\n\n  Example:\n\n  ```python\n  # Optionally, the first layer can receive an `input_shape` argument:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  # Afterwards, we do automatic shape inference:\n  model.add(Dense(32))\n\n  # This is identical to the following:\n  model = Sequential()\n  model.add(Dense(32, input_dim=500))\n\n  # And to the following:\n  model = Sequential()\n  model.add(Dense(32, batch_input_shape=(None, 500)))\n\n  # Note that you can also omit the `input_shape` argument:\n  # In that case the model gets built the first time you call `fit` (or other\n  # training and evaluation methods).\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.compile(optimizer=optimizer, loss=loss)\n  # This builds the model for the first time:\n  model.fit(x, y, batch_size=32, epochs=10)\n\n  # Note that when using this delayed-build pattern (no input shape specified),\n  # the model doesn't have any weights until the first call\n  # to a training/evaluation method (since it isn't yet built):\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.weights  # returns []\n\n  # Whereas if you specify the input shape, the model gets built continuously\n  # as you are adding layers:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  model.add(Dense(32))\n  model.weights  # returns list of length 4\n\n  When using the delayed-build pattern (no input shape specified), you can\n  choose to manually build your model by calling `build(batch_input_shape)`:\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.build((None, 500))\n  model.weights  # returns list of length 4\n  ```\n  ",
        "klass": "tensorflow.python.keras.models.Sequential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2"
        ],
        "class_docstring": "Optimizer that implements the Adam algorithm.\n\n  Adam optimization is a stochastic gradient descent method that is based on\n  adaptive estimation of first-order and second-order moments. According to the\n  reference, the method is 'computationally efficient, has little memory\n  requirement, invariant to diagonal rescaling of gradients, and is well suited\n  for problems that are large in terms of data/parameters'.\n\n  Note, amsgrad is currently not supported and the argument can only be False.\n\n  # References\n      See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n        ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n      For AMSGrad see [Reddi et al., 2-18]\n        (https://openreview.net/pdf?id=ryQu7f-RZ)\n  ",
        "klass": "tensorflow.python.keras.optimizer_v2.adam.Adam",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.optimizers.Optimizer",
            "tensorflow.python.training.checkpointable.base.CheckpointableBase"
        ],
        "class_docstring": "Wrapper class for native TensorFlow optimizers.\n  ",
        "klass": "tensorflow.python.keras.optimizers.TFOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "contextlib.AbstractContextManager"
        ],
        "class_docstring": "Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    ",
        "klass": "contextlib.closing",
        "module": "contextlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "FileIO class that exposes methods to read / write to / from files.\n\n  The constructor takes the following arguments:\n  name: name of the file\n  mode: one of 'r', 'w', 'a', 'r+', 'w+', 'a+'. Append 'b' for bytes mode.\n\n  Can be used as an iterator to iterate over lines in the file.\n\n  The default buffer size used for the BufferedInputStream used for reading\n  the file line by line is 1024 * 512 bytes.\n  ",
        "klass": "tensorflow.python.lib.io.file_io.FileIO",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to write records to a TFRecords file.\n\n  This class implements `__enter__` and `__exit__`, and can be used\n  in `with` blocks like a normal file.\n  ",
        "klass": "tensorflow.python.lib.io.python_io.TFRecordWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.checkpointable.tracking.TrackableResource"
        ],
        "class_docstring": "Creates TreeEnsemble resource.",
        "klass": "tensorflow.python.ops.boosted_trees_ops.TreeEnsemble",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.linalg.linear_operator.LinearOperator"
        ],
        "class_docstring": "`LinearOperator` acting like a [batch] square diagonal matrix.\n\n  This operator acts like a [batch] diagonal matrix `A` with shape\n  `[B1,...,Bb, N, N]` for some `b >= 0`.  The first `b` indices index a\n  batch member.  For every batch index `(i1,...,ib)`, `A[i1,...,ib, : :]` is\n  an `N x N` matrix.  This matrix `A` is not materialized, but for\n  purposes of broadcasting this shape will be relevant.\n\n  `LinearOperatorDiag` is initialized with a (batch) vector.\n\n  ```python\n  # Create a 2 x 2 diagonal linear operator.\n  diag = [1., -1.]\n  operator = LinearOperatorDiag(diag)\n\n  operator.to_dense()\n  ==> [[1.,  0.]\n       [0., -1.]]\n\n  operator.shape\n  ==> [2, 2]\n\n  operator.log_abs_determinant()\n  ==> scalar Tensor\n\n  x = ... Shape [2, 4] Tensor\n  operator.matmul(x)\n  ==> Shape [2, 4] Tensor\n\n  # Create a [2, 3] batch of 4 x 4 linear operators.\n  diag = tf.random_normal(shape=[2, 3, 4])\n  operator = LinearOperatorDiag(diag)\n\n  # Create a shape [2, 1, 4, 2] vector.  Note that this shape is compatible\n  # since the batch dimensions, [2, 1], are broadcast to\n  # operator.batch_shape = [2, 3].\n  y = tf.random_normal(shape=[2, 1, 4, 2])\n  x = operator.solve(y)\n  ==> operator.matmul(x) = y\n  ```\n\n  #### Shape compatibility\n\n  This operator acts on [batch] matrix with compatible shape.\n  `x` is a batch matrix with compatible shape for `matmul` and `solve` if\n\n  ```\n  operator.shape = [B1,...,Bb] + [N, N],  with b >= 0\n  x.shape =   [C1,...,Cc] + [N, R],\n  and [C1,...,Cc] broadcasts with [B1,...,Bb] to [D1,...,Dd]\n  ```\n\n  #### Performance\n\n  Suppose `operator` is a `LinearOperatorDiag` of shape `[N, N]`,\n  and `x.shape = [N, R]`.  Then\n\n  * `operator.matmul(x)` involves `N * R` multiplications.\n  * `operator.solve(x)` involves `N` divisions and `N * R` multiplications.\n  * `operator.determinant()` involves a size `N` `reduce_prod`.\n\n  If instead `operator` and `x` have shape `[B1,...,Bb, N, N]` and\n  `[B1,...,Bb, N, R]`, every operation increases in complexity by `B1*...*Bb`.\n\n  #### Matrix property hints\n\n  This `LinearOperator` is initialized with boolean flags of the form `is_X`,\n  for `X = non_singular, self_adjoint, positive_definite, square`.\n  These have the following meaning:\n\n  * If `is_X == True`, callers should expect the operator to have the\n    property `X`.  This is a promise that should be fulfilled, but is *not* a\n    runtime assert.  For example, finite floating point precision may result\n    in these promises being violated.\n  * If `is_X == False`, callers should expect the operator to not have `X`.\n  * If `is_X == None` (the default), callers should have no expectation either\n    way.\n  ",
        "klass": "tensorflow.python.ops.linalg.linalg.LinearOperatorDiag",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.linalg.linear_operator.LinearOperator"
        ],
        "class_docstring": "`LinearOperator` acting like a [batch] square lower triangular matrix.\n\n  This operator acts like a [batch] lower triangular matrix `A` with shape\n  `[B1,...,Bb, N, N]` for some `b >= 0`.  The first `b` indices index a\n  batch member.  For every batch index `(i1,...,ib)`, `A[i1,...,ib, : :]` is\n  an `N x N` matrix.\n\n  `LinearOperatorLowerTriangular` is initialized with a `Tensor` having\n  dimensions `[B1,...,Bb, N, N]`. The upper triangle of the last two\n  dimensions is ignored.\n\n  ```python\n  # Create a 2 x 2 lower-triangular linear operator.\n  tril = [[1., 2.], [3., 4.]]\n  operator = LinearOperatorLowerTriangular(tril)\n\n  # The upper triangle is ignored.\n  operator.to_dense()\n  ==> [[1., 0.]\n       [3., 4.]]\n\n  operator.shape\n  ==> [2, 2]\n\n  operator.log_abs_determinant()\n  ==> scalar Tensor\n\n  x = ... Shape [2, 4] Tensor\n  operator.matmul(x)\n  ==> Shape [2, 4] Tensor\n\n  # Create a [2, 3] batch of 4 x 4 linear operators.\n  tril = tf.random_normal(shape=[2, 3, 4, 4])\n  operator = LinearOperatorLowerTriangular(tril)\n  ```\n\n  #### Shape compatibility\n\n  This operator acts on [batch] matrix with compatible shape.\n  `x` is a batch matrix with compatible shape for `matmul` and `solve` if\n\n  ```\n  operator.shape = [B1,...,Bb] + [N, N],  with b >= 0\n  x.shape =        [B1,...,Bb] + [N, R],  with R >= 0.\n  ```\n\n  #### Performance\n\n  Suppose `operator` is a `LinearOperatorLowerTriangular` of shape `[N, N]`,\n  and `x.shape = [N, R]`.  Then\n\n  * `operator.matmul(x)` involves `N^2 * R` multiplications.\n  * `operator.solve(x)` involves `N * R` size `N` back-substitutions.\n  * `operator.determinant()` involves a size `N` `reduce_prod`.\n\n  If instead `operator` and `x` have shape `[B1,...,Bb, N, N]` and\n  `[B1,...,Bb, N, R]`, every operation increases in complexity by `B1*...*Bb`.\n\n  #### Matrix property hints\n\n  This `LinearOperator` is initialized with boolean flags of the form `is_X`,\n  for `X = non_singular, self_adjoint, positive_definite, square`.\n  These have the following meaning:\n\n  * If `is_X == True`, callers should expect the operator to have the\n    property `X`.  This is a promise that should be fulfilled, but is *not* a\n    runtime assert.  For example, finite floating point precision may result\n    in these promises being violated.\n  * If `is_X == False`, callers should expect the operator to not have `X`.\n  * If `is_X == None` (the default), callers should have no expectation either\n    way.\n  ",
        "klass": "tensorflow.python.ops.linalg.linalg.LinearOperatorLowerTriangular",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.linalg.linear_operator_identity.BaseLinearOperatorIdentity"
        ],
        "class_docstring": "`LinearOperator` acting like a scaled [batch] identity matrix `A = c I`.\n\n  This operator acts like a scaled [batch] identity matrix `A` with shape\n  `[B1,...,Bb, N, N]` for some `b >= 0`.  The first `b` indices index a\n  batch member.  For every batch index `(i1,...,ib)`, `A[i1,...,ib, : :]` is\n  a scaled version of the `N x N` identity matrix.\n\n  `LinearOperatorIdentity` is initialized with `num_rows`, and a `multiplier`\n  (a `Tensor`) of shape `[B1,...,Bb]`.  `N` is set to `num_rows`, and the\n  `multiplier` determines the scale for each batch member.\n\n  ```python\n  # Create a 2 x 2 scaled identity matrix.\n  operator = LinearOperatorIdentity(num_rows=2, multiplier=3.)\n\n  operator.to_dense()\n  ==> [[3., 0.]\n       [0., 3.]]\n\n  operator.shape\n  ==> [2, 2]\n\n  operator.log_abs_determinant()\n  ==> 2 * Log[3]\n\n  x = ... Shape [2, 4] Tensor\n  operator.matmul(x)\n  ==> 3 * x\n\n  y = tf.random_normal(shape=[3, 2, 4])\n  # Note that y.shape is compatible with operator.shape because operator.shape\n  # is broadcast to [3, 2, 2].\n  x = operator.solve(y)\n  ==> 3 * x\n\n  # Create a 2-batch of 2x2 identity matrices\n  operator = LinearOperatorIdentity(num_rows=2, multiplier=5.)\n  operator.to_dense()\n  ==> [[[5., 0.]\n        [0., 5.]],\n       [[5., 0.]\n        [0., 5.]]]\n\n  x = ... Shape [2, 2, 3]\n  operator.matmul(x)\n  ==> 5 * x\n\n  # Here the operator and x have different batch_shape, and are broadcast.\n  x = ... Shape [1, 2, 3]\n  operator.matmul(x)\n  ==> 5 * x\n  ```\n\n  ### Shape compatibility\n\n  This operator acts on [batch] matrix with compatible shape.\n  `x` is a batch matrix with compatible shape for `matmul` and `solve` if\n\n  ```\n  operator.shape = [B1,...,Bb] + [N, N],  with b >= 0\n  x.shape =   [C1,...,Cc] + [N, R],\n  and [C1,...,Cc] broadcasts with [B1,...,Bb] to [D1,...,Dd]\n  ```\n\n  ### Performance\n\n  * `operator.matmul(x)` is `O(D1*...*Dd*N*R)`\n  * `operator.solve(x)` is `O(D1*...*Dd*N*R)`\n  * `operator.determinant()` is `O(D1*...*Dd)`\n\n  #### Matrix property hints\n\n  This `LinearOperator` is initialized with boolean flags of the form `is_X`,\n  for `X = non_singular, self_adjoint, positive_definite, square`.\n  These have the following meaning\n  * If `is_X == True`, callers should expect the operator to have the\n    property `X`.  This is a promise that should be fulfilled, but is *not* a\n    runtime assert.  For example, finite floating point precision may result\n    in these promises being violated.\n  * If `is_X == False`, callers should expect the operator to not have `X`.\n  * If `is_X == None` (the default), callers should have no expectation either\n    way.\n  ",
        "klass": "tensorflow.python.ops.linalg.linalg.LinearOperatorScaledIdentity",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.linalg.linear_operator.LinearOperator"
        ],
        "class_docstring": "`LinearOperator` acting like a [batch] zero matrix.\n\n  This operator acts like a [batch] zero matrix `A` with shape\n  `[B1,...,Bb, N, M]` for some `b >= 0`.  The first `b` indices index a\n  batch member.  For every batch index `(i1,...,ib)`, `A[i1,...,ib, : :]` is\n  an `N x M` matrix.  This matrix `A` is not materialized, but for\n  purposes of broadcasting this shape will be relevant.\n\n  `LinearOperatorZeros` is initialized with `num_rows`, and optionally\n  `num_columns, `batch_shape`, and `dtype` arguments.  If `num_columns` is\n  `None`, then this operator will be initialized as a square matrix. If\n  `batch_shape` is `None`, this operator efficiently passes through all\n  arguments.  If `batch_shape` is provided, broadcasting may occur, which will\n  require making copies.\n\n  ```python\n  # Create a 2 x 2 zero matrix.\n  operator = LinearOperatorZero(num_rows=2, dtype=tf.float32)\n\n  operator.to_dense()\n  ==> [[0., 0.]\n       [0., 0.]]\n\n  operator.shape\n  ==> [2, 2]\n\n  operator.determinant()\n  ==> 0.\n\n  x = ... Shape [2, 4] Tensor\n  operator.matmul(x)\n  ==> Shape [2, 4] Tensor, same as x.\n\n  # Create a 2-batch of 2x2 zero matrices\n  operator = LinearOperatorZeros(num_rows=2, batch_shape=[2])\n  operator.to_dense()\n  ==> [[[0., 0.]\n        [0., 0.]],\n       [[0., 0.]\n        [0., 0.]]]\n\n  # Here, even though the operator has a batch shape, the input is the same as\n  # the output, so x can be passed through without a copy.  The operator is able\n  # to detect that no broadcast is necessary because both x and the operator\n  # have statically defined shape.\n  x = ... Shape [2, 2, 3]\n  operator.matmul(x)\n  ==> Shape [2, 2, 3] Tensor, same as tf.zeros_like(x)\n\n  # Here the operator and x have different batch_shape, and are broadcast.\n  # This requires a copy, since the output is different size than the input.\n  x = ... Shape [1, 2, 3]\n  operator.matmul(x)\n  ==> Shape [2, 2, 3] Tensor, equal to tf.zeros_like([x, x])\n  ```\n\n  ### Shape compatibility\n\n  This operator acts on [batch] matrix with compatible shape.\n  `x` is a batch matrix with compatible shape for `matmul` and `solve` if\n\n  ```\n  operator.shape = [B1,...,Bb] + [N, M],  with b >= 0\n  x.shape =   [C1,...,Cc] + [M, R],\n  and [C1,...,Cc] broadcasts with [B1,...,Bb] to [D1,...,Dd]\n  ```\n\n  #### Matrix property hints\n\n  This `LinearOperator` is initialized with boolean flags of the form `is_X`,\n  for `X = non_singular, self_adjoint, positive_definite, square`.\n  These have the following meaning:\n\n  * If `is_X == True`, callers should expect the operator to have the\n    property `X`.  This is a promise that should be fulfilled, but is *not* a\n    runtime assert.  For example, finite floating point precision may result\n    in these promises being violated.\n  * If `is_X == False`, callers should expect the operator to not have `X`.\n  * If `is_X == None` (the default), callers should have no expectation either\n    way.\n  ",
        "klass": "tensorflow.python.ops.linalg.linalg.LinearOperatorZeros",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.LayerRNNCell"
        ],
        "class_docstring": "Long short-term memory unit (LSTM) recurrent network cell.\n\n  The default non-peephole implementation is based on:\n\n    https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf\n\n  Felix Gers, Jurgen Schmidhuber, and Fred Cummins.\n  \"Learning to forget: Continual prediction with LSTM.\" IET, 850-855, 1999.\n\n  The peephole implementation is based on:\n\n    https://research.google.com/pubs/archive/43905.pdf\n\n  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n  \"Long short-term memory recurrent neural network architectures for\n   large scale acoustic modeling.\" INTERSPEECH, 2014.\n\n  The class uses optional peep-hole connections, optional cell clipping, and\n  an optional projection layer.\n\n  Note that this cell is not optimized for performance. Please use\n  `tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n  `tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\n  better performance on CPU.\n  ",
        "klass": "tensorflow.python.ops.rnn_cell.LSTMCell",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context for the variable_scope, see `variable_scope` for docs.",
        "klass": "tensorflow.python.ops.variable_scope._pure_variable_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This class is responsible for loading tests according to various criteria\n    and returning them wrapped in a TestSuite\n    ",
        "klass": "unittest.loader.TestLoader",
        "module": "unittest"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A test runner class that displays results in textual form.\n\n    It prints out the names of tests as they are run, errors as they\n    occur, and a summary of the results at the end of the test run.\n    ",
        "klass": "unittest.runner.TextTestRunner",
        "module": "unittest"
    },
    {
        "base_classes": [
            "tensorflow.python.saved_model.builder_impl._SavedModelBuilder"
        ],
        "class_docstring": "Builds the `SavedModel` protocol buffer and saves variables and assets.\n\n  The `SavedModelBuilder` class provides functionality to build a `SavedModel`\n  protocol buffer. Specifically, this allows multiple meta graphs to be saved as\n  part of a single language-neutral `SavedModel`, while sharing variables and\n  assets.\n\n  To build a SavedModel, the first meta graph must be saved with variables.\n  Subsequent meta graphs will simply be saved with their graph definitions. If\n  assets need to be saved and written or copied to disk, they can be provided\n  when the meta graph def is added. If multiple meta graph defs are associated\n  an asset of the same name, only the first version is retained.\n\n  Each meta graph added to the SavedModel must be annotated with tags. The tags\n  provide a means to identify the specific meta graph to load and restore, along\n  with the shared set of variables and assets.\n\n  Typical usage for the `SavedModelBuilder`:\n  ```python\n  ...\n  builder = tf.saved_model.Builder(export_dir)\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph_and_variables(sess,\n                                    [\"foo-tag\"],\n                                    signature_def_map=foo_signatures,\n                                    assets_collection=foo_assets)\n  ...\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph([\"bar-tag\", \"baz-tag\"])\n  ...\n\n  builder.save()\n  ```\n\n  Note: This function will only be available through the v1 compatibility\n  library as tf.compat.v1.saved_model.builder.SavedModelBuilder or\n  tf.compat.v1.saved_model.Builder. Tensorflow 2.0 will introduce a new\n  object-based method of creating SavedModels.\n  ",
        "klass": "tensorflow.python.saved_model.builder.SavedModelBuilder",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager when you are in `update()` or `update_non_slot()`.",
        "klass": "tensorflow.python.training.distribute.UpdateContext",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A minimal utility class for saving and restoring checkpoints.\n\n  Note that this is a low-level utility which stores Tensors in the keys\n  specified by `SaveableObject`s. Higher-level utilities for object-based\n  checkpointing are built on top of it.\n  ",
        "klass": "tensorflow.python.training.saving.functional_saver.Saver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Maintains moving averages of variables by employing an exponential decay.\n\n  When training a model, it is often beneficial to maintain moving averages of\n  the trained parameters.  Evaluations that use averaged parameters sometimes\n  produce significantly better results than the final trained values.\n\n  The `apply()` method adds shadow copies of trained variables and add ops that\n  maintain a moving average of the trained variables in their shadow copies.\n  It is used when building the training model.  The ops that maintain moving\n  averages are typically run after each training step.\n  The `average()` and `average_name()` methods give access to the shadow\n  variables and their names.  They are useful when building an evaluation\n  model, or when restoring a model from a checkpoint file.  They help use the\n  moving averages in place of the last trained values for evaluations.\n\n  The moving averages are computed using exponential decay.  You specify the\n  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n  variables are initialized with the same initial values as the trained\n  variables.  When you run the ops to maintain the moving averages, each\n  shadow variable is updated with the formula:\n\n    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n\n  This is mathematically equivalent to the classic formula below, but the use\n  of an `assign_sub` op (the `\"-=\"` in the formula) allows concurrent lockless\n  updates to the variables:\n\n    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n\n  Reasonable values for `decay` are close to 1.0, typically in the\n  multiple-nines range: 0.999, 0.9999, etc.\n\n  Example usage when creating a training model:\n\n  ```python\n  # Create variables.\n  var0 = tf.Variable(...)\n  var1 = tf.Variable(...)\n  # ... use the variables to build a training model...\n  ...\n  # Create an op that applies the optimizer.  This is what we usually\n  # would use as a training op.\n  opt_op = opt.minimize(my_loss, [var0, var1])\n\n  # Create an ExponentialMovingAverage object\n  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n\n  with tf.control_dependencies([opt_op]):\n      # Create the shadow variables, and add ops to maintain moving averages\n      # of var0 and var1. This also creates an op that will update the moving\n      # averages after each training step.  This is what we will use in place\n      # of the usual training op.\n      training_op = ema.apply([var0, var1])\n\n  ...train the model by running training_op...\n  ```\n\n  There are two ways to use the moving averages for evaluations:\n\n  *  Build a model that uses the shadow variables instead of the variables.\n     For this, use the `average()` method which returns the shadow variable\n     for a given variable.\n  *  Build a model normally but load the checkpoint files to evaluate by using\n     the shadow variable names.  For this use the `average_name()` method.  See\n     the `tf.train.Saver` for more\n     information on restoring saved variables.\n\n  Example of restoring the shadow variable values:\n\n  ```python\n  # Create a Saver that loads variables from their saved shadow values.\n  shadow_var0_name = ema.average_name(var0)\n  shadow_var1_name = ema.average_name(var1)\n  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n  saver.restore(...checkpoint filename...)\n  # var0 and var1 now hold the moving average values\n  ```\n  ",
        "klass": "tensorflow.python.training.training.ExponentialMovingAverage",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the gradient descent algorithm.\n  ",
        "klass": "tensorflow.python.training.training.GradientDescentOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, recovery and hooks.\n\n  Example usage:\n\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with MonitoredSession(session_creator=ChiefSessionCreator(...),\n                        hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the monitored session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n  * calls `hook.after_create_session()`\n\n  Run: When `run()` is called, the monitored session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n  * if `AbortedError` or `UnavailableError` occurs, it recovers or\n    reinitializes the session before executing the run() call again\n\n\n  Exit: At the `close()`, the monitored session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the monitored_session is used as a context\n\n  How to set `tf.Session` arguments:\n\n  * In most cases you can set session arguments as follows:\n\n  ```python\n  MonitoredSession(\n    session_creator=ChiefSessionCreator(master=..., config=...))\n  ```\n\n  * In distributed setting for a non-chief worker, you can use following:\n\n  ```python\n  MonitoredSession(\n    session_creator=WorkerSessionCreator(master=..., config=...))\n  ```\n\n  See `MonitoredTrainingSession` for an example usage based on chief or worker.\n\n  Note: This is not a `tf.Session`. For example, it cannot do following:\n\n  * it cannot be set as default session.\n  * it cannot be sent to saver.save.\n  * it cannot be sent to tf.train.start_queue_runners.\n\n  Args:\n    session_creator: A factory object to create session. Typically a\n      `ChiefSessionCreator` which is the default one.\n    hooks: An iterable of `SessionRunHook' objects.\n\n  Returns:\n    A MonitoredSession object.\n  ",
        "klass": "tensorflow.python.training.training.MonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  This class is deprecated. For synchrononous training, please use [Distribution\n  Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch. Please note that\n     some workers can consume multiple minibatches, while some may not consume\n     even one. This is because each worker fetches minibatches as long as\n     a token exists. If one worker is stuck for some reason and does not\n     consume a token, another worker can use it.\n\n  For the replicas:\n\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  ",
        "klass": "tensorflow.python.training.training.SyncReplicasOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Record operations for automatic differentiation.\n\n  Operations are recorded if they are executed within this context manager and\n  at least one of their inputs is being \"watched\".\n\n  Trainable variables (created by `tf.Variable` or `tf.get_variable`, where\n  `trainable=True` is default in both cases) are automatically watched. Tensors\n  can be manually watched by invoking the `watch` method on this context\n  manager.\n\n  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n  be computed as:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    y = x * x\n  dy_dx = g.gradient(y, x) # Will compute to 6.0\n  ```\n\n  GradientTapes can be nested to compute higher-order derivatives. For example,\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape() as g:\n    g.watch(x)\n    with tf.GradientTape() as gg:\n      gg.watch(x)\n      y = x * x\n    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n  ```\n\n  By default, the resources held by a GradientTape are released as soon as\n  GradientTape.gradient() method is called. To compute multiple gradients over\n  the same computation, create a persistent gradient tape. This allows multiple\n  calls to the gradient() method as resources are released when the tape object\n  is garbage collected. For example:\n\n  ```python\n  x = tf.constant(3.0)\n  with tf.GradientTape(persistent=True) as g:\n    g.watch(x)\n    y = x * x\n    z = y * y\n  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n  dy_dx = g.gradient(y, x)  # 6.0\n  del g  # Drop the reference to the tape\n  ```\n\n  By default GradientTape will automatically watch any trainable variables that\n  are accessed inside the context. If you want fine grained control over which\n  variables are watched you can disable automatic tracking by passing\n  `watch_accessed_variables=False` to the tape constructor:\n\n  ```python\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(variable_a)\n    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n    z = variable_b ** 3  # No gradients will be avaialble since `variable_b` is\n                         # not being watched.\n  ```\n\n  Note that when using models you should ensure that your variables exist when\n  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n  first iteration not have any gradients:\n\n  ```python\n  a = tf.keras.layers.Dense(32)\n  b = tf.keras.layers.Dense(32)\n\n  with tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                             # `a.variables` will return an empty list and the\n                             # tape will not be watching anything.\n    result = b(a(inputs))\n    tape.gradient(result, a.variables)  # The result of this computation will be\n                                        # a list of `None`s since a's variables\n                                        # are not being watched.\n  ```\n\n  Note that only tensors with real or complex dtypes are differentiable.\n  ",
        "klass": "tensorflow.compat.v1.GradientTape",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A TensorFlow computation, represented as a dataflow graph.\n\n  A `Graph` contains a set of\n  `tf.Operation` objects,\n  which represent units of computation; and\n  `tf.Tensor` objects, which represent\n  the units of data that flow between operations.\n\n  A default `Graph` is always registered, and accessible by calling\n  `tf.get_default_graph`.\n  To add an operation to the default graph, simply call one of the functions\n  that defines a new `Operation`:\n\n  ```python\n  c = tf.constant(4.0)\n  assert c.graph is tf.get_default_graph()\n  ```\n\n  Another typical usage involves the\n  `tf.Graph.as_default`\n  context manager, which overrides the current default graph for the\n  lifetime of the context:\n\n  ```python\n  g = tf.Graph()\n  with g.as_default():\n    # Define operations and tensors in `g`.\n    c = tf.constant(30.0)\n    assert c.graph is g\n  ```\n\n  Important note: This class *is not* thread-safe for graph construction. All\n  operations should be created from a single thread, or external\n  synchronization must be provided. Unless otherwise specified, all methods\n  are not thread-safe.\n\n  A `Graph` instance supports an arbitrary number of \"collections\"\n  that are identified by name. For convenience when building a large\n  graph, collections can store groups of related objects: for\n  example, the `tf.Variable` uses a collection (named\n  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n  all variables that are created during the construction of a graph. The caller\n  may define additional collections by specifying a new name.\n  ",
        "klass": "tensorflow.compat.v1.Graph",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.client.session.BaseSession"
        ],
        "class_docstring": "A class for running TensorFlow operations.\n\n  A `Session` object encapsulates the environment in which `Operation`\n  objects are executed, and `Tensor` objects are evaluated. For\n  example:\n\n  ```python\n  # Build a graph.\n  a = tf.constant(5.0)\n  b = tf.constant(6.0)\n  c = a * b\n\n  # Launch the graph in a session.\n  sess = tf.Session()\n\n  # Evaluate the tensor `c`.\n  print(sess.run(c))\n  ```\n\n  A session may own resources, such as\n  `tf.Variable`, `tf.QueueBase`,\n  and `tf.ReaderBase`. It is important to release\n  these resources when they are no longer required. To do this, either\n  invoke the `tf.Session.close` method on the session, or use\n  the session as a context manager. The following two examples are\n  equivalent:\n\n  ```python\n  # Using the `close()` method.\n  sess = tf.Session()\n  sess.run(...)\n  sess.close()\n\n  # Using the context manager.\n  with tf.Session() as sess:\n    sess.run(...)\n  ```\n\n  The\n  [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto)\n  protocol buffer exposes various configuration options for a\n  session. For example, to create a session that uses soft constraints\n  for device placement, and log the resulting placement decisions,\n  create a session as follows:\n\n  ```python\n  # Launch the graph in a session that allows soft device placement and\n  # logs the placement decisions.\n  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                          log_device_placement=True))\n  ```\n  ",
        "klass": "tensorflow.compat.v1.Session",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for use when defining a Python op.\n\n  This context manager validates that the given `values` are from the\n  same graph, makes that graph the default graph, and pushes a\n  name scope in that graph (see\n  `tf.Graph.name_scope`\n  for more details on that).\n\n  For example, to define a new Python op called `my_op`:\n\n  ```python\n  def my_op(a, b, c, name=None):\n    with tf.name_scope(name, \"MyOp\", [a, b, c]) as scope:\n      a = tf.convert_to_tensor(a, name=\"a\")\n      b = tf.convert_to_tensor(b, name=\"b\")\n      c = tf.convert_to_tensor(c, name=\"c\")\n      # Define some computation that uses `a`, `b`, and `c`.\n      return foo_op(..., name=scope)\n  ```\n  ",
        "klass": "tensorflow.compat.v1.name_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for defining ops that creates variables (layers).\n\n  This context manager validates that the (optional) `values` are from the same\n  graph, ensures that graph is the default graph, and pushes a name scope and a\n  variable scope.\n\n  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,\n  then `default_name` is used.  In that case, if the same name has been\n  previously used in the same scope, it will be made unique by appending `_N`\n  to it.\n\n  Variable scope allows you to create new variables and to share already created\n  ones while providing checks to not create or share by accident. For details,\n  see the [Variable Scope How To](https://tensorflow.org/guide/variables), here\n  we present only a few basic examples.\n\n  Simple example of how to create a new variable:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      with tf.variable_scope(\"bar\"):\n          v = tf.get_variable(\"v\", [1])\n          assert v.name == \"foo/bar/v:0\"\n  ```\n\n  Simple example of how to reenter a premade variable scope safely:\n\n  ```python\n  with tf.variable_scope(\"foo\") as vs:\n    pass\n\n  # Re-enter the variable scope.\n  with tf.variable_scope(vs,\n                         auxiliary_name_scope=False) as vs1:\n    # Restore the original name_scope.\n    with tf.name_scope(vs1.original_name_scope):\n        v = tf.get_variable(\"v\", [1])\n        assert v.name == \"foo/v:0\"\n        c = tf.constant([1], name=\"c\")\n        assert c.name == \"foo/c:0\"\n  ```\n\n  Basic example of sharing a variable AUTO_REUSE:\n\n  ```python\n  def foo():\n    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n      v = tf.get_variable(\"v\", [1])\n    return v\n\n  v1 = foo()  # Creates v.\n  v2 = foo()  # Gets the same, existing v.\n  assert v1 == v2\n  ```\n\n  Basic example of sharing a variable with reuse=True:\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n  with tf.variable_scope(\"foo\", reuse=True):\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  Sharing a variable by capturing a scope and setting reuse:\n\n  ```python\n  with tf.variable_scope(\"foo\") as scope:\n      v = tf.get_variable(\"v\", [1])\n      scope.reuse_variables()\n      v1 = tf.get_variable(\"v\", [1])\n  assert v1 == v\n  ```\n\n  To prevent accidental sharing of variables, we raise an exception when getting\n  an existing variable in a non-reusing scope.\n\n  ```python\n  with tf.variable_scope(\"foo\"):\n      v = tf.get_variable(\"v\", [1])\n      v1 = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v already exists ...\").\n  ```\n\n  Similarly, we raise an exception when trying to get a variable that does not\n  exist in reuse mode.\n\n  ```python\n  with tf.variable_scope(\"foo\", reuse=True):\n      v = tf.get_variable(\"v\", [1])\n      #  Raises ValueError(\"... v does not exists ...\").\n  ```\n\n  Note that the `reuse` flag is inherited: if we open a reusing scope, then all\n  its sub-scopes become reusing as well.\n\n  A note about name scoping: Setting `reuse` does not impact the naming of other\n  ops such as mult. See related discussion on\n  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n\n  Note that up to and including version 1.0, it was allowed (though explicitly\n  discouraged) to pass False to the reuse argument, yielding undocumented\n  behaviour slightly different from None. Starting at 1.1.0 passing None and\n  False as reuse has exactly the same effect.\n\n  A note about using variable scopes in multi-threaded environment: Variable\n  scopes are thread local, so one thread will not see another thread's current\n  scope. Also, when using `default_name`, unique scopes names are also generated\n  only on a per thread basis. If the same name was used within a different\n  thread, that doesn't prevent a new thread from creating the same scope.\n  However, the underlying variable store is shared across threads (within the\n  same graph). As such, if another thread tries to create a new variable with\n  the same name as a variable created by a previous thread, it will fail unless\n  reuse is True.\n\n  Further, each thread starts with an empty variable scope. So if you wish to\n  preserve name prefixes from a scope from the main thread, you should capture\n  the main thread's scope and re-enter it in each thread. For e.g.\n\n  ```\n  main_thread_scope = variable_scope.get_variable_scope()\n\n  # Thread's target function:\n  def thread_target_fn(captured_scope):\n    with variable_scope.variable_scope(captured_scope):\n      # .... regular code for this thread\n\n\n  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))\n  ```\n  ",
        "klass": "tensorflow.compat.v1.variable_scope",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "The Normal distribution with location `loc` and `scale` parameters.\n\n  #### Mathematical details\n\n  The probability density function (pdf) is,\n\n  ```none\n  pdf(x; mu, sigma) = exp(-0.5 (x - mu)**2 / sigma**2) / Z\n  Z = (2 pi sigma**2)**0.5\n  ```\n\n  where `loc = mu` is the mean, `scale = sigma` is the std. deviation, and, `Z`\n  is the normalization constant.\n\n  The Normal distribution is a member of the [location-scale family](\n  https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n  constructed as,\n\n  ```none\n  X ~ Normal(loc=0, scale=1)\n  Y = loc + scale * X\n  ```\n\n  #### Examples\n\n  Examples of initialization of one or a batch of distributions.\n\n  ```python\n  import tensorflow_probability as tfp\n  tfd = tfp.distributions\n\n  # Define a single scalar Normal distribution.\n  dist = tfd.Normal(loc=0., scale=3.)\n\n  # Evaluate the cdf at 1, returning a scalar.\n  dist.cdf(1.)\n\n  # Define a batch of two scalar valued Normals.\n  # The first has mean 1 and standard deviation 11, the second 2 and 22.\n  dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n\n  # Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n  # returning a length two tensor.\n  dist.prob([0, 1.5])\n\n  # Get 3 samples, returning a 3 x 2 tensor.\n  dist.sample([3])\n  ```\n\n  Arguments are broadcast when possible.\n\n  ```python\n  # Define a batch of two scalar valued Normals.\n  # Both have mean 1, but different standard deviations.\n  dist = tfd.Normal(loc=1., scale=[11, 22.])\n\n  # Evaluate the pdf of both distributions on the same point, 3.0,\n  # returning a length 2 tensor.\n  dist.prob(3.0)\n  ```\n\n  ",
        "klass": "tensorflow.compat.v1.distributions.Normal",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.Metric"
        ],
        "class_docstring": "Computes the (weighted) mean of the given values.\n\n  For example, if values is [1, 3, 5, 7] then the mean is 4.\n  If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n\n  This metric creates two variables, `total` and `count` that are used to\n  compute the average of `values`. This average is ultimately returned as `mean`\n  which is an idempotent operation that simply divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.Mean()\n  m.update_state([1, 3, 5, 7])\n  print('Final result: ', m.result().numpy())  # Final result: 4.0\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))\n  model.compile('sgd', loss='mse')\n  ```\n  ",
        "klass": "tensorflow.compat.v1.keras.metrics.Mean",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.MeanMetricWrapper"
        ],
        "class_docstring": "Calculates how often predictions matches integer labels.\n\n  For example, if `y_true` is [[2], [1]] and `y_pred` is\n  [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n  If the weights were specified as [0.7, 0.3] then the categorical accuracy\n  would be .3.\n\n  This metric creates two local variables, `total` and `count` that are used to\n  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n  ultimately returned as `sparse categorical accuracy`: an idempotent operation\n  that simply divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.SparseCategoricalAccuracy()\n  m.update_state([[2], [1]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n  print('Final result: ', m.result().numpy())  # Final result: 0.5\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.compile(\n      'sgd',\n      loss='mse',\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n  ```\n  ",
        "klass": "tensorflow.compat.v1.keras.metrics.SparseCategoricalAccuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.saved_model.builder_impl._SavedModelBuilder"
        ],
        "class_docstring": "Builds the `SavedModel` protocol buffer and saves variables and assets.\n\n  The `SavedModelBuilder` class provides functionality to build a `SavedModel`\n  protocol buffer. Specifically, this allows multiple meta graphs to be saved as\n  part of a single language-neutral `SavedModel`, while sharing variables and\n  assets.\n\n  To build a SavedModel, the first meta graph must be saved with variables.\n  Subsequent meta graphs will simply be saved with their graph definitions. If\n  assets need to be saved and written or copied to disk, they can be provided\n  when the meta graph def is added. If multiple meta graph defs are associated\n  an asset of the same name, only the first version is retained.\n\n  Each meta graph added to the SavedModel must be annotated with tags. The tags\n  provide a means to identify the specific meta graph to load and restore, along\n  with the shared set of variables and assets.\n\n  Typical usage for the `SavedModelBuilder`:\n  ```python\n  ...\n  builder = tf.saved_model.Builder(export_dir)\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph_and_variables(sess,\n                                    [\"foo-tag\"],\n                                    signature_def_map=foo_signatures,\n                                    assets_collection=foo_assets)\n  ...\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph([\"bar-tag\", \"baz-tag\"])\n  ...\n\n  builder.save()\n  ```\n\n  Note: This function will only be available through the v1 compatibility\n  library as tf.compat.v1.saved_model.builder.SavedModelBuilder or\n  tf.compat.v1.saved_model.Builder. Tensorflow 2.0 will introduce a new\n  object-based method of creating SavedModels.\n  ",
        "klass": "tensorflow.compat.v1.saved_model.builder.SavedModelBuilder",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.summary.writer.writer.SummaryToEventTransformer"
        ],
        "class_docstring": "Writes `Summary` protocol buffers to event files.\n\n  The `FileWriter` class provides a mechanism to create an event file in a\n  given directory and add summaries and events to it. The class updates the\n  file contents asynchronously. This allows a training program to call methods\n  to add data to the file directly from the training loop, without slowing down\n  training.\n\n  When constructed with a `tf.Session` parameter, a `FileWriter` instead forms\n  a compatibility layer over new graph-based summaries (`tf.contrib.summary`)\n  to facilitate the use of new summary writing with pre-existing code that\n  expects a `FileWriter` instance.\n  ",
        "klass": "tensorflow.compat.v1.summary.FileWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adam algorithm.\n\n  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n  ",
        "klass": "tensorflow.compat.v1.train.AdamOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the gradient descent algorithm.\n  ",
        "klass": "tensorflow.compat.v1.train.GradientDescentOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, recovery and hooks.\n\n  Example usage:\n\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with MonitoredSession(session_creator=ChiefSessionCreator(...),\n                        hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the monitored session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n  * calls `hook.after_create_session()`\n\n  Run: When `run()` is called, the monitored session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n  * if `AbortedError` or `UnavailableError` occurs, it recovers or\n    reinitializes the session before executing the run() call again\n\n\n  Exit: At the `close()`, the monitored session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the monitored_session is used as a context\n\n  How to set `tf.Session` arguments:\n\n  * In most cases you can set session arguments as follows:\n\n  ```python\n  MonitoredSession(\n    session_creator=ChiefSessionCreator(master=..., config=...))\n  ```\n\n  * In distributed setting for a non-chief worker, you can use following:\n\n  ```python\n  MonitoredSession(\n    session_creator=WorkerSessionCreator(master=..., config=...))\n  ```\n\n  See `MonitoredTrainingSession` for an example usage based on chief or worker.\n\n  Note: This is not a `tf.Session`. For example, it cannot do following:\n\n  * it cannot be set as default session.\n  * it cannot be sent to saver.save.\n  * it cannot be sent to tf.train.start_queue_runners.\n\n  Args:\n    session_creator: A factory object to create session. Typically a\n      `ChiefSessionCreator` which is the default one.\n    hooks: An iterable of `SessionRunHook' objects.\n\n  Returns:\n    A MonitoredSession object.\n  ",
        "klass": "tensorflow.compat.v1.train.MonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Saves and restores variables.\n\n  See [Variables](https://tensorflow.org/guide/variables)\n  for an overview of variables, saving and restoring.\n\n  The `Saver` class adds ops to save and restore variables to and from\n  *checkpoints*.  It also provides convenience methods to run these ops.\n\n  Checkpoints are binary files in a proprietary format which map variable names\n  to tensor values.  The best way to examine the contents of a checkpoint is to\n  load it using a `Saver`.\n\n  Savers can automatically number checkpoint filenames with a provided counter.\n  This lets you keep multiple checkpoints at different steps while training a\n  model.  For example you can number the checkpoint filenames with the training\n  step number.  To avoid filling up disks, savers manage checkpoint files\n  automatically. For example, they can keep only the N most recent files, or\n  one checkpoint for every N hours of training.\n\n  You number checkpoint filenames by passing a value to the optional\n  `global_step` argument to `save()`:\n\n  ```python\n  saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n  ...\n  saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n  ```\n\n  Additionally, optional arguments to the `Saver()` constructor let you control\n  the proliferation of checkpoint files on disk:\n\n  * `max_to_keep` indicates the maximum number of recent checkpoint files to\n    keep.  As new files are created, older files are deleted.   If None or 0,\n    no checkpoints are deleted from the filesystem but only the last one is\n    kept in the `checkpoint` file.  Defaults to 5 (that is, the 5 most recent\n    checkpoint files are kept.)\n\n  * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent\n    `max_to_keep` checkpoint files, you might want to keep one checkpoint file\n    for every N hours of training.  This can be useful if you want to later\n    analyze how a model progressed during a long training session.  For\n    example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep\n    one checkpoint file for every 2 hours of training.  The default value of\n    10,000 hours effectively disables the feature.\n\n  Note that you still have to call the `save()` method to save the model.\n  Passing these arguments to the constructor will not save variables\n  automatically for you.\n\n  A training program that saves regularly looks like:\n\n  ```python\n  ...\n  # Create a saver.\n  saver = tf.train.Saver(...variables...)\n  # Launch the graph and train, saving the model every 1,000 steps.\n  sess = tf.Session()\n  for step in xrange(1000000):\n      sess.run(..training_op..)\n      if step % 1000 == 0:\n          # Append the step number to the checkpoint name:\n          saver.save(sess, 'my-model', global_step=step)\n  ```\n\n  In addition to checkpoint files, savers keep a protocol buffer on disk with\n  the list of recent checkpoints. This is used to manage numbered checkpoint\n  files and by `latest_checkpoint()`, which makes it easy to discover the path\n  to the most recent checkpoint. That protocol buffer is stored in a file named\n  'checkpoint' next to the checkpoint files.\n\n  If you create several savers, you can specify a different filename for the\n  protocol buffer file in the call to `save()`.\n  ",
        "klass": "tensorflow.compat.v1.train.Saver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, restoring, and hooks.\n\n  Please note that this utility is not recommended for distributed settings.\n  For distributed settings, please use `tf.train.MonitoredSession`. The\n  differences between `MonitoredSession` and `SingularMonitoredSession` are:\n\n  * `MonitoredSession` handles `AbortedError` and `UnavailableError` for\n    distributed settings, but `SingularMonitoredSession` does not.\n  * `MonitoredSession` can be created in `chief` or `worker` modes.\n    `SingularMonitoredSession` is always created as `chief`.\n  * You can access the raw `tf.Session` object used by\n    `SingularMonitoredSession`, whereas in MonitoredSession the raw session is\n    private. This can be used:\n      - To `run` without hooks.\n      - To save and restore.\n  * All other functionality is identical.\n\n  Example usage:\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with SingularMonitoredSession(hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the hooked session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n\n  Run: When `run()` is called, the hooked session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n\n  Exit: At the `close()`, the hooked session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the `SingularMonitoredSession` is used as a context.\n  ",
        "klass": "tensorflow.compat.v1.train.SingularMonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.distribute.distribute_lib.DistributionStrategy"
        ],
        "class_docstring": "Mirrors vars to distribute across multiple devices and machines.\n\n  This strategy uses one replica per device and sync replication for its\n  multi-GPU version.\n\n  The multi-worker version will be added in the fture.\n\n  Args:\n    devices: a list of device strings.\n    cross_device_ops: optional, a descedant of `CrossDeviceOps`. If this is not\n      set, nccl will be use by default.\n  ",
        "klass": "tensorflow.distribute.MirroredStrategy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.distributions.distribution.Distribution"
        ],
        "class_docstring": "Bernoulli distribution.\n\n  The Bernoulli distribution with `probs` parameter, i.e., the probability of a\n  `1` outcome (vs a `0` outcome).\n  ",
        "klass": "tensorflow.distributions.Bernoulli",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager to check for C API status.",
        "klass": "tensorflow.errors.raise_exception_on_not_ok_status",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.lib.io.file_io.FileIO"
        ],
        "class_docstring": "File I/O wrappers without thread locking.\n\n  Note, that this  is somewhat like builtin Python  file I/O, but\n  there are  semantic differences to  make it more  efficient for\n  some backing filesystems.  For example, a write  mode file will\n  not  be opened  until the  first  write call  (to minimize  RPC\n  invocations in network filesystems).\n  ",
        "klass": "tensorflow.gfile.FastGFile",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.lib.io.file_io.FileIO"
        ],
        "class_docstring": "File I/O wrappers without thread locking.\n\n  Note, that this  is somewhat like builtin Python  file I/O, but\n  there are  semantic differences to  make it more  efficient for\n  some backing filesystems.  For example, a write  mode file will\n  not  be opened  until the  first  write call  (to minimize  RPC\n  invocations in network filesystems).\n  ",
        "klass": "tensorflow.gfile.GFile",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to write records to a TFRecords file.\n\n  This class implements `__enter__` and `__exit__`, and can be used\n  in `with` blocks like a normal file.\n  ",
        "klass": "tensorflow.io.TFRecordWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.lib.io.file_io.FileIO"
        ],
        "class_docstring": "File I/O wrappers without thread locking.\n\n  Note, that this  is somewhat like builtin Python  file I/O, but\n  there are  semantic differences to  make it more  efficient for\n  some backing filesystems.  For example, a write  mode file will\n  not  be opened  until the  first  write call  (to minimize  RPC\n  invocations in network filesystems).\n  ",
        "klass": "tensorflow.io.gfile.GFile",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.network.Network"
        ],
        "class_docstring": "`Model` groups layers into an object with training and inference features.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n  ",
        "klass": "tensorflow.keras.Model",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Linear stack of layers.\n\n  Arguments:\n      layers: list of layers to add to the model.\n\n  Example:\n\n  ```python\n  # Optionally, the first layer can receive an `input_shape` argument:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  # Afterwards, we do automatic shape inference:\n  model.add(Dense(32))\n\n  # This is identical to the following:\n  model = Sequential()\n  model.add(Dense(32, input_dim=500))\n\n  # And to the following:\n  model = Sequential()\n  model.add(Dense(32, batch_input_shape=(None, 500)))\n\n  # Note that you can also omit the `input_shape` argument:\n  # In that case the model gets built the first time you call `fit` (or other\n  # training and evaluation methods).\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.compile(optimizer=optimizer, loss=loss)\n  # This builds the model for the first time:\n  model.fit(x, y, batch_size=32, epochs=10)\n\n  # Note that when using this delayed-build pattern (no input shape specified),\n  # the model doesn't have any weights until the first call\n  # to a training/evaluation method (since it isn't yet built):\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.weights  # returns []\n\n  # Whereas if you specify the input shape, the model gets built continuously\n  # as you are adding layers:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  model.add(Dense(32))\n  model.weights  # returns list of length 4\n\n  When using the delayed-build pattern (no input shape specified), you can\n  choose to manually build your model by calling `build(batch_input_shape)`:\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.build((None, 500))\n  model.weights  # returns list of length 4\n  ```\n  ",
        "klass": "tensorflow.keras.Sequential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.MeanMetricWrapper"
        ],
        "class_docstring": "Calculates how often predictions matches labels.\n\n  For example, if `y_true` is [1, 2, 3, 4] and `y_pred` is [0, 2, 3, 4]\n  then the accuracy is 3/4 or .75.  If the weights were specified as\n  [1, 1, 0, 0] then the accuracy would be 1/2 or .5.\n\n  This metric creates two local variables, `total` and `count` that are used to\n  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n  ultimately returned as `binary accuracy`: an idempotent operation that simply\n  divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.Accuracy()\n  m.update_state([1, 2, 3, 4], [0, 2, 3, 4])\n  print('Final result: ', m.result().numpy())  # Final result: 0.75\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.Accuracy()])\n  ```\n  ",
        "klass": "tensorflow.keras.metrics.Accuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.Metric"
        ],
        "class_docstring": "Computes the (weighted) mean of the given values.\n\n  For example, if values is [1, 3, 5, 7] then the mean is 4.\n  If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n\n  This metric creates two variables, `total` and `count` that are used to\n  compute the average of `values`. This average is ultimately returned as `mean`\n  which is an idempotent operation that simply divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.Mean()\n  m.update_state([1, 3, 5, 7])\n  print('Final result: ', m.result().numpy())  # Final result: 4.0\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))\n  model.compile('sgd', loss='mse')\n  ```\n  ",
        "klass": "tensorflow.keras.metrics.Mean",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.metrics.MeanMetricWrapper"
        ],
        "class_docstring": "Calculates how often predictions matches integer labels.\n\n  For example, if `y_true` is [[2], [1]] and `y_pred` is\n  [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n  If the weights were specified as [0.7, 0.3] then the categorical accuracy\n  would be .3.\n\n  This metric creates two local variables, `total` and `count` that are used to\n  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n  ultimately returned as `sparse categorical accuracy`: an idempotent operation\n  that simply divides `total` by `count`.\n\n  If `sample_weight` is `None`, weights default to 1.\n  Use `sample_weight` of 0 to mask values.\n\n  Usage:\n\n  ```python\n  m = tf.keras.metrics.SparseCategoricalAccuracy()\n  m.update_state([[2], [1]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n  print('Final result: ', m.result().numpy())  # Final result: 0.5\n  ```\n\n  Usage with tf.keras API:\n\n  ```python\n  model = keras.models.Model(inputs, outputs)\n  model.compile(\n      'sgd',\n      loss='mse',\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n  ```\n  ",
        "klass": "tensorflow.keras.metrics.SparseCategoricalAccuracy",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.network.Network"
        ],
        "class_docstring": "`Model` groups layers into an object with training and inference features.\n\n  There are two ways to instantiate a `Model`:\n\n  1 - With the \"functional API\", where you start from `Input`,\n  you chain layer calls to specify the model's forward pass,\n  and finally you create your model from inputs and outputs:\n\n  ```python\n  import tensorflow as tf\n\n  inputs = tf.keras.Input(shape=(3,))\n  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n  ```\n\n  2 - By subclassing the `Model` class: in that case, you should define your\n  layers in `__init__` and you should implement the model's forward pass\n  in `call`.\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n\n    def call(self, inputs):\n      x = self.dense1(inputs)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n\n  If you subclass `Model`, you can optionally have\n  a `training` argument (boolean) in `call`, which you can use to specify\n  a different behavior in training and inference:\n\n  ```python\n  import tensorflow as tf\n\n  class MyModel(tf.keras.Model):\n\n    def __init__(self):\n      super(MyModel, self).__init__()\n      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n      self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n      x = self.dense1(inputs)\n      if training:\n        x = self.dropout(x, training=training)\n      return self.dense2(x)\n\n  model = MyModel()\n  ```\n  ",
        "klass": "tensorflow.keras.models.Model",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.engine.training.Model"
        ],
        "class_docstring": "Linear stack of layers.\n\n  Arguments:\n      layers: list of layers to add to the model.\n\n  Example:\n\n  ```python\n  # Optionally, the first layer can receive an `input_shape` argument:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  # Afterwards, we do automatic shape inference:\n  model.add(Dense(32))\n\n  # This is identical to the following:\n  model = Sequential()\n  model.add(Dense(32, input_dim=500))\n\n  # And to the following:\n  model = Sequential()\n  model.add(Dense(32, batch_input_shape=(None, 500)))\n\n  # Note that you can also omit the `input_shape` argument:\n  # In that case the model gets built the first time you call `fit` (or other\n  # training and evaluation methods).\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.compile(optimizer=optimizer, loss=loss)\n  # This builds the model for the first time:\n  model.fit(x, y, batch_size=32, epochs=10)\n\n  # Note that when using this delayed-build pattern (no input shape specified),\n  # the model doesn't have any weights until the first call\n  # to a training/evaluation method (since it isn't yet built):\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.weights  # returns []\n\n  # Whereas if you specify the input shape, the model gets built continuously\n  # as you are adding layers:\n  model = Sequential()\n  model.add(Dense(32, input_shape=(500,)))\n  model.add(Dense(32))\n  model.weights  # returns list of length 4\n\n  When using the delayed-build pattern (no input shape specified), you can\n  choose to manually build your model by calling `build(batch_input_shape)`:\n  model = Sequential()\n  model.add(Dense(32))\n  model.add(Dense(32))\n  model.build((None, 500))\n  model.weights  # returns list of length 4\n  ```\n  ",
        "klass": "tensorflow.keras.models.Sequential",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "keras_preprocessing.image.image_data_generator.ImageDataGenerator"
        ],
        "class_docstring": "Generate batches of tensor image data with real-time data augmentation.\n\n   The data will be looped over (in batches).\n\n  Arguments:\n      featurewise_center: Boolean.\n          Set input mean to 0 over the dataset, feature-wise.\n      samplewise_center: Boolean. Set each sample mean to 0.\n      featurewise_std_normalization: Boolean.\n          Divide inputs by std of the dataset, feature-wise.\n      samplewise_std_normalization: Boolean. Divide each input by its std.\n      zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n      zca_whitening: Boolean. Apply ZCA whitening.\n      rotation_range: Int. Degree range for random rotations.\n      width_shift_range: Float, 1-D array-like or int\n          - float: fraction of total width, if < 1, or pixels if >= 1.\n          - 1-D array-like: random elements from the array.\n          - int: integer number of pixels from interval\n              `(-width_shift_range, +width_shift_range)`\n          - With `width_shift_range=2` possible values\n              are integers `[-1, 0, +1]`,\n              same as with `width_shift_range=[-1, 0, +1]`,\n              while with `width_shift_range=1.0` possible values are floats\n              in the interval [-1.0, +1.0).\n      height_shift_range: Float, 1-D array-like or int\n          - float: fraction of total height, if < 1, or pixels if >= 1.\n          - 1-D array-like: random elements from the array.\n          - int: integer number of pixels from interval\n              `(-height_shift_range, +height_shift_range)`\n          - With `height_shift_range=2` possible values\n              are integers `[-1, 0, +1]`,\n              same as with `height_shift_range=[-1, 0, +1]`,\n              while with `height_shift_range=1.0` possible values are floats\n              in the interval [-1.0, +1.0).\n      brightness_range: Tuple or list of two floats. Range for picking\n          a brightness shift value from.\n      shear_range: Float. Shear Intensity\n          (Shear angle in counter-clockwise direction in degrees)\n      zoom_range: Float or [lower, upper]. Range for random zoom.\n          If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n      channel_shift_range: Float. Range for random channel shifts.\n      fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n          Default is 'nearest'.\n          Points outside the boundaries of the input are filled\n          according to the given mode:\n          - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n          - 'nearest':  aaaaaaaa|abcd|dddddddd\n          - 'reflect':  abcddcba|abcd|dcbaabcd\n          - 'wrap':  abcdabcd|abcd|abcdabcd\n      cval: Float or Int.\n          Value used for points outside the boundaries\n          when `fill_mode = \"constant\"`.\n      horizontal_flip: Boolean. Randomly flip inputs horizontally.\n      vertical_flip: Boolean. Randomly flip inputs vertically.\n      rescale: rescaling factor. Defaults to None.\n          If None or 0, no rescaling is applied,\n          otherwise we multiply the data by the value provided\n          (after applying all other transformations).\n      preprocessing_function: function that will be implied on each input.\n          The function will run after the image is resized and augmented.\n          The function should take one argument:\n          one image (Numpy tensor with rank 3),\n          and should output a Numpy tensor with the same shape.\n      data_format: Image data format,\n          either \"channels_first\" or \"channels_last\".\n          \"channels_last\" mode means that the images should have shape\n          `(samples, height, width, channels)`,\n          \"channels_first\" mode means that the images should have shape\n          `(samples, channels, height, width)`.\n          It defaults to the `image_data_format` value found in your\n          Keras config file at `~/.keras/keras.json`.\n          If you never set it, then it will be \"channels_last\".\n      validation_split: Float. Fraction of images reserved for validation\n          (strictly between 0 and 1).\n      dtype: Dtype to use for the generated arrays.\n\n  Examples:\n\n  Example of using `.flow(x, y)`:\n\n  ```python\n  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n  y_train = np_utils.to_categorical(y_train, num_classes)\n  y_test = np_utils.to_categorical(y_test, num_classes)\n  datagen = ImageDataGenerator(\n      featurewise_center=True,\n      featurewise_std_normalization=True,\n      rotation_range=20,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      horizontal_flip=True)\n  # compute quantities required for featurewise normalization\n  # (std, mean, and principal components if ZCA whitening is applied)\n  datagen.fit(x_train)\n  # fits the model on batches with real-time data augmentation:\n  model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n                      steps_per_epoch=len(x_train) / 32, epochs=epochs)\n  # here's a more \"manual\" example\n  for e in range(epochs):\n      print('Epoch', e)\n      batches = 0\n      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n          model.fit(x_batch, y_batch)\n          batches += 1\n          if batches >= len(x_train) / 32:\n              # we need to break the loop by hand because\n              # the generator loops indefinitely\n              break\n  ```\n\n  Example of using `.flow_from_directory(directory)`:\n\n  ```python\n  train_datagen = ImageDataGenerator(\n          rescale=1./255,\n          shear_range=0.2,\n          zoom_range=0.2,\n          horizontal_flip=True)\n  test_datagen = ImageDataGenerator(rescale=1./255)\n  train_generator = train_datagen.flow_from_directory(\n          'data/train',\n          target_size=(150, 150),\n          batch_size=32,\n          class_mode='binary')\n  validation_generator = test_datagen.flow_from_directory(\n          'data/validation',\n          target_size=(150, 150),\n          batch_size=32,\n          class_mode='binary')\n  model.fit_generator(\n          train_generator,\n          steps_per_epoch=2000,\n          epochs=50,\n          validation_data=validation_generator,\n          validation_steps=800)\n  ```\n\n  Example of transforming images and masks together.\n\n  ```python\n  # we create two instances with the same arguments\n  data_gen_args = dict(featurewise_center=True,\n                       featurewise_std_normalization=True,\n                       rotation_range=90,\n                       width_shift_range=0.1,\n                       height_shift_range=0.1,\n                       zoom_range=0.2)\n  image_datagen = ImageDataGenerator(**data_gen_args)\n  mask_datagen = ImageDataGenerator(**data_gen_args)\n  # Provide the same seed and keyword arguments to the fit and flow methods\n  seed = 1\n  image_datagen.fit(images, augment=True, seed=seed)\n  mask_datagen.fit(masks, augment=True, seed=seed)\n  image_generator = image_datagen.flow_from_directory(\n      'data/images',\n      class_mode=None,\n      seed=seed)\n  mask_generator = mask_datagen.flow_from_directory(\n      'data/masks',\n      class_mode=None,\n      seed=seed)\n  # combine generators into one which yields image and masks\n  train_generator = zip(image_generator, mask_generator)\n  model.fit_generator(\n      train_generator,\n      steps_per_epoch=2000,\n      epochs=50)\n  ```\n  ",
        "klass": "tensorflow.keras.preprocessing.image.ImageDataGenerator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.keras.layers.core.Flatten",
            "tensorflow.python.layers.base.Layer"
        ],
        "class_docstring": "Flattens an input tensor while preserving the batch axis (axis 0).\n\n  Arguments:\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs.\n      `channels_last` corresponds to inputs with shape\n      `(batch, ..., channels)` while `channels_first` corresponds to\n      inputs with shape `(batch, channels, ...)`.\n\n  Examples:\n\n  ```\n    x = tf.placeholder(shape=(None, 4, 4), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, 16)`\n\n    x = tf.placeholder(shape=(None, 3, None), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, None)`\n  ```\n  ",
        "klass": "tensorflow.layers.Flatten",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.ops.rnn_cell_impl.LayerRNNCell"
        ],
        "class_docstring": "The most basic RNN cell.\n\n  Note that this cell is not optimized for performance. Please use\n  `tf.contrib.cudnn_rnn.CudnnRNNTanh` for better performance on GPU.\n\n  Args:\n    num_units: int, The number of units in the RNN cell.\n    activation: Nonlinearity to use.  Default: `tanh`. It could also be string\n      that is within Keras activation function names.\n    reuse: (optional) Python boolean describing whether to reuse variables\n     in an existing scope.  If not `True`, and the existing scope already has\n     the given variables, an error is raised.\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such\n      cases.\n    dtype: Default dtype of the layer (default of `None` means use the type\n      of the first input). Required when `build` is called before `call`.\n    **kwargs: Dict, keyword named properties for common layer attributes, like\n      `trainable` etc when constructing the cell from configs of get_config().\n  ",
        "klass": "tensorflow.nn.rnn_cell.BasicRNNCell",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to write records to a TFRecords file.\n\n  This class implements `__enter__` and `__exit__`, and can be used\n  in `with` blocks like a normal file.\n  ",
        "klass": "tensorflow.python_io.TFRecordWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.saved_model.builder_impl._SavedModelBuilder"
        ],
        "class_docstring": "Builds the `SavedModel` protocol buffer and saves variables and assets.\n\n  The `SavedModelBuilder` class provides functionality to build a `SavedModel`\n  protocol buffer. Specifically, this allows multiple meta graphs to be saved as\n  part of a single language-neutral `SavedModel`, while sharing variables and\n  assets.\n\n  To build a SavedModel, the first meta graph must be saved with variables.\n  Subsequent meta graphs will simply be saved with their graph definitions. If\n  assets need to be saved and written or copied to disk, they can be provided\n  when the meta graph def is added. If multiple meta graph defs are associated\n  an asset of the same name, only the first version is retained.\n\n  Each meta graph added to the SavedModel must be annotated with tags. The tags\n  provide a means to identify the specific meta graph to load and restore, along\n  with the shared set of variables and assets.\n\n  Typical usage for the `SavedModelBuilder`:\n  ```python\n  ...\n  builder = tf.saved_model.Builder(export_dir)\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph_and_variables(sess,\n                                    [\"foo-tag\"],\n                                    signature_def_map=foo_signatures,\n                                    assets_collection=foo_assets)\n  ...\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    ...\n    builder.add_meta_graph([\"bar-tag\", \"baz-tag\"])\n  ...\n\n  builder.save()\n  ```\n\n  Note: This function will only be available through the v1 compatibility\n  library as tf.compat.v1.saved_model.builder.SavedModelBuilder or\n  tf.compat.v1.saved_model.Builder. Tensorflow 2.0 will introduce a new\n  object-based method of creating SavedModels.\n  ",
        "klass": "tensorflow.saved_model.builder.SavedModelBuilder",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.summary.writer.writer.SummaryToEventTransformer"
        ],
        "class_docstring": "Writes `Summary` protocol buffers to event files.\n\n  The `FileWriter` class provides a mechanism to create an event file in a\n  given directory and add summaries and events to it. The class updates the\n  file contents asynchronously. This allows a training program to call methods\n  to add data to the file directly from the training loop, without slowing down\n  training.\n\n  When constructed with a `tf.Session` parameter, a `FileWriter` instead forms\n  a compatibility layer over new graph-based summaries (`tf.contrib.summary`)\n  to facilitate the use of new summary writing with pre-existing code that\n  expects a `FileWriter` instance.\n  ",
        "klass": "tensorflow.summary.FileWriter",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adadelta algorithm.\n\n  See [M. D. Zeiler](http://arxiv.org/abs/1212.5701)\n  ([pdf](http://arxiv.org/pdf/1212.5701v1.pdf))\n  ",
        "klass": "tensorflow.train.AdadeltaOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adagrad algorithm.\n\n  See this [paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n  or this\n  [intro](https://ppasupat.github.io/a9online/uploads/proximal_notes.pdf).\n  ",
        "klass": "tensorflow.train.AdagradOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Adam algorithm.\n\n  See [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n  ",
        "klass": "tensorflow.train.AdamOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.checkpointable.tracking.Checkpointable"
        ],
        "class_docstring": "Groups checkpointable objects, saving and restoring them.\n\n  `Checkpoint`'s constructor accepts keyword arguments whose values are types\n  that contain checkpointable state, such as `tf.train.Optimizer`\n  implementations, `tf.Variable`, `tf.keras.Layer` implementations, or\n  `tf.keras.Model` implementations. It saves these values with a checkpoint, and\n  maintains a `save_counter` for numbering checkpoints.\n\n  Example usage when graph building:\n\n  ```python\n  import tensorflow as tf\n  import os\n\n  checkpoint_directory = \"/tmp/training_checkpoints\"\n  checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n  status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n  train_op = optimizer.minimize( ... )\n  status.assert_consumed()  # Optional sanity checks.\n  with tf.Session() as session:\n    # Use the Session to restore variables, or initialize them if\n    # tf.train.latest_checkpoint returned None.\n    status.initialize_or_restore(session)\n    for _ in range(num_training_steps):\n      session.run(train_op)\n    checkpoint.save(file_prefix=checkpoint_prefix)\n  ```\n\n  Example usage with eager execution enabled:\n\n  ```python\n  import tensorflow as tf\n  import os\n\n  tf.enable_eager_execution()\n\n  checkpoint_directory = \"/tmp/training_checkpoints\"\n  checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n  status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n  for _ in range(num_training_steps):\n    optimizer.minimize( ... )  # Variables will be restored on creation.\n  status.assert_consumed()  # Optional sanity checks.\n  checkpoint.save(file_prefix=checkpoint_prefix)\n  ```\n\n  `Checkpoint.save` and `Checkpoint.restore` write and read object-based\n  checkpoints, in contrast to `tf.train.Saver` which writes and reads\n  `variable.name` based checkpoints. Object-based checkpointing saves a graph of\n  dependencies between Python objects (`Layer`s, `Optimizer`s, `Variable`s,\n  etc.) with named edges, and this graph is used to match variables when\n  restoring a checkpoint. It can be more robust to changes in the Python\n  program, and helps to support restore-on-create for variables when executing\n  eagerly. Prefer `tf.train.Checkpoint` over `tf.train.Saver` for new code.\n\n  `Checkpoint` objects have dependencies on the objects passed as keyword\n  arguments to their constructors, and each dependency is given a name that is\n  identical to the name of the keyword argument for which it was created.\n  TensorFlow classes like `Layer`s and `Optimizer`s will automatically add\n  dependencies on their variables (e.g. \"kernel\" and \"bias\" for\n  `tf.keras.layers.Dense`). Inheriting from `tf.keras.Model` makes managing\n  dependencies easy in user-defined classes, since `Model` hooks into attribute\n  assignment. For example:\n\n  ```python\n  class Regress(tf.keras.Model):\n\n    def __init__(self):\n      super(Regress, self).__init__()\n      self.input_transform = tf.keras.layers.Dense(10)\n      # ...\n\n    def call(self, inputs):\n      x = self.input_transform(inputs)\n      # ...\n  ```\n\n  This `Model` has a dependency named \"input_transform\" on its `Dense` layer,\n  which in turn depends on its variables. As a result, saving an instance of\n  `Regress` using `tf.train.Checkpoint` will also save all the variables created\n  by the `Dense` layer.\n\n  Attributes:\n    save_counter: Incremented when `save()` is called. Used to number\n      checkpoints.\n  ",
        "klass": "tensorflow.train.Checkpoint",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a cluster as a set of \"tasks\", organized into \"jobs\".\n\n  A `tf.train.ClusterSpec` represents the set of processes that\n  participate in a distributed TensorFlow computation. Every\n  `tf.train.Server` is constructed in a particular cluster.\n\n  To create a cluster with two jobs and five tasks, you specify the\n  mapping from job names to lists of network addresses (typically\n  hostname-port pairs).\n\n  ```python\n  cluster = tf.train.ClusterSpec({\"worker\": [\"worker0.example.com:2222\",\n                                             \"worker1.example.com:2222\",\n                                             \"worker2.example.com:2222\"],\n                                  \"ps\": [\"ps0.example.com:2222\",\n                                         \"ps1.example.com:2222\"]})\n  ```\n\n  Each job may also be specified as a sparse mapping from task indices\n  to network addresses. This enables a server to be configured without\n  needing to know the identity of (for example) all other worker\n  tasks:\n\n  ```python\n  cluster = tf.train.ClusterSpec({\"worker\": {1: \"worker1.example.com:2222\"},\n                                  \"ps\": [\"ps0.example.com:2222\",\n                                         \"ps1.example.com:2222\"]})\n  ```\n  ",
        "klass": "tensorflow.train.ClusterSpec",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A coordinator for threads.\n\n  This class implements a simple mechanism to coordinate the termination of a\n  set of threads.\n\n  #### Usage:\n\n  ```python\n  # Create a coordinator.\n  coord = Coordinator()\n  # Start a number of threads, passing the coordinator to each of them.\n  ...start thread 1...(coord, ...)\n  ...start thread N...(coord, ...)\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  ```\n\n  Any of the threads can call `coord.request_stop()` to ask for all the threads\n  to stop.  To cooperate with the requests, each thread must check for\n  `coord.should_stop()` on a regular basis.  `coord.should_stop()` returns\n  `True` as soon as `coord.request_stop()` has been called.\n\n  A typical thread running with a coordinator will do something like:\n\n  ```python\n  while not coord.should_stop():\n    ...do some work...\n  ```\n\n  #### Exception handling:\n\n  A thread can report an exception to the coordinator as part of the\n  `request_stop()` call.  The exception will be re-raised from the\n  `coord.join()` call.\n\n  Thread code:\n\n  ```python\n  try:\n    while not coord.should_stop():\n      ...do some work...\n  except Exception as e:\n    coord.request_stop(e)\n  ```\n\n  Main code:\n\n  ```python\n  try:\n    ...\n    coord = Coordinator()\n    # Start a number of threads, passing the coordinator to each of them.\n    ...start thread 1...(coord, ...)\n    ...start thread N...(coord, ...)\n    # Wait for all the threads to terminate.\n    coord.join(threads)\n  except Exception as e:\n    ...exception that was passed to coord.request_stop()\n  ```\n\n  To simplify the thread implementation, the Coordinator provides a\n  context handler `stop_on_exception()` that automatically requests a stop if\n  an exception is raised.  Using the context handler the thread code above\n  can be written as:\n\n  ```python\n  with coord.stop_on_exception():\n    while not coord.should_stop():\n      ...do some work...\n  ```\n\n  #### Grace period for stopping:\n\n  After a thread has called `coord.request_stop()` the other threads have a\n  fixed time to stop, this is called the 'stop grace period' and defaults to 2\n  minutes.  If any of the threads is still alive after the grace period expires\n  `coord.join()` raises a RuntimeError reporting the laggards.\n\n  ```python\n  try:\n    ...\n    coord = Coordinator()\n    # Start a number of threads, passing the coordinator to each of them.\n    ...start thread 1...(coord, ...)\n    ...start thread N...(coord, ...)\n    # Wait for all the threads to terminate, give them 10s grace period\n    coord.join(threads, stop_grace_period_secs=10)\n  except RuntimeError:\n    ...one of the threads took more than 10s to stop after request_stop()\n    ...was called.\n  except Exception:\n    ...exception that was passed to coord.request_stop()\n  ```\n  ",
        "klass": "tensorflow.train.Coordinator",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Maintains moving averages of variables by employing an exponential decay.\n\n  When training a model, it is often beneficial to maintain moving averages of\n  the trained parameters.  Evaluations that use averaged parameters sometimes\n  produce significantly better results than the final trained values.\n\n  The `apply()` method adds shadow copies of trained variables and add ops that\n  maintain a moving average of the trained variables in their shadow copies.\n  It is used when building the training model.  The ops that maintain moving\n  averages are typically run after each training step.\n  The `average()` and `average_name()` methods give access to the shadow\n  variables and their names.  They are useful when building an evaluation\n  model, or when restoring a model from a checkpoint file.  They help use the\n  moving averages in place of the last trained values for evaluations.\n\n  The moving averages are computed using exponential decay.  You specify the\n  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n  variables are initialized with the same initial values as the trained\n  variables.  When you run the ops to maintain the moving averages, each\n  shadow variable is updated with the formula:\n\n    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n\n  This is mathematically equivalent to the classic formula below, but the use\n  of an `assign_sub` op (the `\"-=\"` in the formula) allows concurrent lockless\n  updates to the variables:\n\n    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n\n  Reasonable values for `decay` are close to 1.0, typically in the\n  multiple-nines range: 0.999, 0.9999, etc.\n\n  Example usage when creating a training model:\n\n  ```python\n  # Create variables.\n  var0 = tf.Variable(...)\n  var1 = tf.Variable(...)\n  # ... use the variables to build a training model...\n  ...\n  # Create an op that applies the optimizer.  This is what we usually\n  # would use as a training op.\n  opt_op = opt.minimize(my_loss, [var0, var1])\n\n  # Create an ExponentialMovingAverage object\n  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n\n  with tf.control_dependencies([opt_op]):\n      # Create the shadow variables, and add ops to maintain moving averages\n      # of var0 and var1. This also creates an op that will update the moving\n      # averages after each training step.  This is what we will use in place\n      # of the usual training op.\n      training_op = ema.apply([var0, var1])\n\n  ...train the model by running training_op...\n  ```\n\n  There are two ways to use the moving averages for evaluations:\n\n  *  Build a model that uses the shadow variables instead of the variables.\n     For this, use the `average()` method which returns the shadow variable\n     for a given variable.\n  *  Build a model normally but load the checkpoint files to evaluate by using\n     the shadow variable names.  For this use the `average_name()` method.  See\n     the `tf.train.Saver` for more\n     information on restoring saved variables.\n\n  Example of restoring the shadow variable values:\n\n  ```python\n  # Create a Saver that loads variables from their saved shadow values.\n  shadow_var0_name = ema.average_name(var0)\n  shadow_var1_name = ema.average_name(var1)\n  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n  saver.restore(...checkpoint filename...)\n  # var0 and var1 now hold the moving average values\n  ```\n  ",
        "klass": "tensorflow.train.ExponentialMovingAverage",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the gradient descent algorithm.\n  ",
        "klass": "tensorflow.train.GradientDescentOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the Momentum algorithm.\n\n  Computes (if `use_nesterov = False`):\n\n  ```\n  accumulation = momentum * accumulation + gradient\n  variable -= learning_rate * accumulation\n  ```\n\n  Note that in the dense version of this algorithm, `accumulation` is updated\n  and applied regardless of a gradient's value, whereas the sparse version (when\n  the gradient is an `IndexedSlices`, typically because of `tf.gather` or an\n  embedding) only updates variable slices and corresponding `accumulation` terms\n  when that part of the variable was used in the forward pass.\n  ",
        "klass": "tensorflow.train.MomentumOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, recovery and hooks.\n\n  Example usage:\n\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with MonitoredSession(session_creator=ChiefSessionCreator(...),\n                        hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the monitored session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n  * calls `hook.after_create_session()`\n\n  Run: When `run()` is called, the monitored session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n  * if `AbortedError` or `UnavailableError` occurs, it recovers or\n    reinitializes the session before executing the run() call again\n\n\n  Exit: At the `close()`, the monitored session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the monitored_session is used as a context\n\n  How to set `tf.Session` arguments:\n\n  * In most cases you can set session arguments as follows:\n\n  ```python\n  MonitoredSession(\n    session_creator=ChiefSessionCreator(master=..., config=...))\n  ```\n\n  * In distributed setting for a non-chief worker, you can use following:\n\n  ```python\n  MonitoredSession(\n    session_creator=WorkerSessionCreator(master=..., config=...))\n  ```\n\n  See `MonitoredTrainingSession` for an example usage based on chief or worker.\n\n  Note: This is not a `tf.Session`. For example, it cannot do following:\n\n  * it cannot be set as default session.\n  * it cannot be sent to saver.save.\n  * it cannot be sent to tf.train.start_queue_runners.\n\n  Args:\n    session_creator: A factory object to create session. Typically a\n      `ChiefSessionCreator` which is the default one.\n    hooks: An iterable of `SessionRunHook' objects.\n\n  Returns:\n    A MonitoredSession object.\n  ",
        "klass": "tensorflow.train.MonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the proximal gradient descent algorithm.\n\n  See this [paper](http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting.pdf).\n  ",
        "klass": "tensorflow.train.ProximalGradientDescentOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Optimizer that implements the RMSProp algorithm.\n\n  See the\n  [paper](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n  ",
        "klass": "tensorflow.train.RMSPropOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Saves and restores variables.\n\n  See [Variables](https://tensorflow.org/guide/variables)\n  for an overview of variables, saving and restoring.\n\n  The `Saver` class adds ops to save and restore variables to and from\n  *checkpoints*.  It also provides convenience methods to run these ops.\n\n  Checkpoints are binary files in a proprietary format which map variable names\n  to tensor values.  The best way to examine the contents of a checkpoint is to\n  load it using a `Saver`.\n\n  Savers can automatically number checkpoint filenames with a provided counter.\n  This lets you keep multiple checkpoints at different steps while training a\n  model.  For example you can number the checkpoint filenames with the training\n  step number.  To avoid filling up disks, savers manage checkpoint files\n  automatically. For example, they can keep only the N most recent files, or\n  one checkpoint for every N hours of training.\n\n  You number checkpoint filenames by passing a value to the optional\n  `global_step` argument to `save()`:\n\n  ```python\n  saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n  ...\n  saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n  ```\n\n  Additionally, optional arguments to the `Saver()` constructor let you control\n  the proliferation of checkpoint files on disk:\n\n  * `max_to_keep` indicates the maximum number of recent checkpoint files to\n    keep.  As new files are created, older files are deleted.   If None or 0,\n    no checkpoints are deleted from the filesystem but only the last one is\n    kept in the `checkpoint` file.  Defaults to 5 (that is, the 5 most recent\n    checkpoint files are kept.)\n\n  * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent\n    `max_to_keep` checkpoint files, you might want to keep one checkpoint file\n    for every N hours of training.  This can be useful if you want to later\n    analyze how a model progressed during a long training session.  For\n    example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep\n    one checkpoint file for every 2 hours of training.  The default value of\n    10,000 hours effectively disables the feature.\n\n  Note that you still have to call the `save()` method to save the model.\n  Passing these arguments to the constructor will not save variables\n  automatically for you.\n\n  A training program that saves regularly looks like:\n\n  ```python\n  ...\n  # Create a saver.\n  saver = tf.train.Saver(...variables...)\n  # Launch the graph and train, saving the model every 1,000 steps.\n  sess = tf.Session()\n  for step in xrange(1000000):\n      sess.run(..training_op..)\n      if step % 1000 == 0:\n          # Append the step number to the checkpoint name:\n          saver.save(sess, 'my-model', global_step=step)\n  ```\n\n  In addition to checkpoint files, savers keep a protocol buffer on disk with\n  the list of recent checkpoints. This is used to manage numbered checkpoint\n  files and by `latest_checkpoint()`, which makes it easy to discover the path\n  to the most recent checkpoint. That protocol buffer is stored in a file named\n  'checkpoint' next to the checkpoint files.\n\n  If you create several savers, you can specify a different filename for the\n  protocol buffer file in the call to `save()`.\n  ",
        "klass": "tensorflow.train.Saver",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.monitored_session._MonitoredSession"
        ],
        "class_docstring": "Session-like object that handles initialization, restoring, and hooks.\n\n  Please note that this utility is not recommended for distributed settings.\n  For distributed settings, please use `tf.train.MonitoredSession`. The\n  differences between `MonitoredSession` and `SingularMonitoredSession` are:\n\n  * `MonitoredSession` handles `AbortedError` and `UnavailableError` for\n    distributed settings, but `SingularMonitoredSession` does not.\n  * `MonitoredSession` can be created in `chief` or `worker` modes.\n    `SingularMonitoredSession` is always created as `chief`.\n  * You can access the raw `tf.Session` object used by\n    `SingularMonitoredSession`, whereas in MonitoredSession the raw session is\n    private. This can be used:\n      - To `run` without hooks.\n      - To save and restore.\n  * All other functionality is identical.\n\n  Example usage:\n  ```python\n  saver_hook = CheckpointSaverHook(...)\n  summary_hook = SummarySaverHook(...)\n  with SingularMonitoredSession(hooks=[saver_hook, summary_hook]) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n  ```\n\n  Initialization: At creation time the hooked session does following things\n  in given order:\n\n  * calls `hook.begin()` for each given hook\n  * finalizes the graph via `scaffold.finalize()`\n  * create session\n  * initializes the model via initialization ops provided by `Scaffold`\n  * restores variables if a checkpoint exists\n  * launches queue runners\n\n  Run: When `run()` is called, the hooked session does following things:\n\n  * calls `hook.before_run()`\n  * calls TensorFlow `session.run()` with merged fetches and feed_dict\n  * calls `hook.after_run()`\n  * returns result of `session.run()` asked by user\n\n  Exit: At the `close()`, the hooked session does following things in order:\n\n  * calls `hook.end()`\n  * closes the queue runners and the session\n  * suppresses `OutOfRange` error which indicates that all inputs have been\n    processed if the `SingularMonitoredSession` is used as a context.\n  ",
        "klass": "tensorflow.train.SingularMonitoredSession",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A training helper that checkpoints models and computes summaries.\n\n  This class is deprecated. Please use\n  `tf.train.MonitoredTrainingSession` instead.\n\n  The Supervisor is a small wrapper around a `Coordinator`, a `Saver`,\n  and a `SessionManager` that takes care of common needs of TensorFlow\n  training programs.\n\n  #### Use for a single program\n\n  ```python\n  with tf.Graph().as_default():\n    ...add operations to the graph...\n    # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.\n    sv = Supervisor(logdir='/tmp/mydir')\n    # Get a TensorFlow session managed by the supervisor.\n    with sv.managed_session(FLAGS.master) as sess:\n      # Use the session to train the graph.\n      while not sv.should_stop():\n        sess.run(<my_train_op>)\n  ```\n\n  Within the `with sv.managed_session()` block all variables in the graph have\n  been initialized.  In addition, a few services have been started to\n  checkpoint the model and add summaries to the event log.\n\n  If the program crashes and is restarted, the managed session automatically\n  reinitialize variables from the most recent checkpoint.\n\n  The supervisor is notified of any exception raised by one of the services.\n  After an exception is raised, `should_stop()` returns `True`.  In that case\n  the training loop should also stop.  This is why the training loop has to\n  check for `sv.should_stop()`.\n\n  Exceptions that indicate that the training inputs have been exhausted,\n  `tf.errors.OutOfRangeError`, also cause `sv.should_stop()` to return `True`\n  but are not re-raised from the `with` block: they indicate a normal\n  termination.\n\n  #### Use for multiple replicas\n\n  To train with replicas you deploy the same program in a `Cluster`.\n  One of the tasks must be identified as the *chief*: the task that handles\n  initialization, checkpoints, summaries, and recovery.  The other tasks\n  depend on the *chief* for these services.\n\n  The only change you have to do to the single program code is to indicate\n  if the program is running as the *chief*.\n\n  ```python\n  # Choose a task as the chief. This could be based on server_def.task_index,\n  # or job_def.name, or job_def.tasks. It's entirely up to the end user.\n  # But there can be only one *chief*.\n  is_chief = (server_def.task_index == 0)\n  server = tf.train.Server(server_def)\n\n  with tf.Graph().as_default():\n    ...add operations to the graph...\n    # Create a Supervisor that uses log directory on a shared file system.\n    # Indicate if you are the 'chief'\n    sv = Supervisor(logdir='/shared_directory/...', is_chief=is_chief)\n    # Get a Session in a TensorFlow server on the cluster.\n    with sv.managed_session(server.target) as sess:\n      # Use the session to train the graph.\n      while not sv.should_stop():\n        sess.run(<my_train_op>)\n  ```\n\n  In the *chief* task, the `Supervisor` works exactly as in the first example\n  above.  In the other tasks `sv.managed_session()` waits for the Model to have\n  been initialized before returning a session to the training code.  The\n  non-chief tasks depend on the chief task for initializing the model.\n\n  If one of the tasks crashes and restarts, `managed_session()`\n  checks if the Model is initialized.  If yes, it just creates a session and\n  returns it to the training code that proceeds normally.  If the model needs\n  to be initialized, the chief task takes care of reinitializing it; the other\n  tasks just wait for the model to have been initialized.\n\n  NOTE: This modified program still works fine as a single program.\n  The single program marks itself as the chief.\n\n  #### What `master` string to use\n\n  Whether you are running on your machine or in the cluster you can use the\n  following values for the --master flag:\n\n  * Specifying `''` requests an in-process session that does not use RPC.\n\n  * Specifying `'local'` requests a session that uses the RPC-based\n    \"Master interface\" to run TensorFlow programs. See\n    `tf.train.Server.create_local_server` for\n    details.\n\n  * Specifying `'grpc://hostname:port'` requests a session that uses\n    the RPC interface to a specific host, and also allows the in-process\n    master to access remote tensorflow workers. Often, it is\n    appropriate to pass `server.target` (for some `tf.train.Server`\n    named `server).\n\n  #### Advanced use\n\n  ##### Launching additional services\n\n  `managed_session()` launches the Checkpoint and Summary services (threads).\n  If you need more services to run you can simply launch them in the block\n  controlled by `managed_session()`.\n\n  Example: Start a thread to print losses.  We want this thread to run\n  every 60 seconds, so we launch it with `sv.loop()`.\n\n  ```python\n  ...\n  sv = Supervisor(logdir='/tmp/mydir')\n  with sv.managed_session(FLAGS.master) as sess:\n    sv.loop(60, print_loss, (sess, ))\n    while not sv.should_stop():\n      sess.run(my_train_op)\n  ```\n\n  ##### Launching fewer services\n\n  `managed_session()` launches the \"summary\" and \"checkpoint\" threads which use\n  either the optionally `summary_op` and `saver` passed to the constructor, or\n  default ones created automatically by the supervisor.  If you want to run\n  your own summary and checkpointing logic, disable these services by passing\n  `None` to the `summary_op` and `saver` parameters.\n\n  Example: Create summaries manually every 100 steps in the chief.\n\n  ```python\n  # Create a Supervisor with no automatic summaries.\n  sv = Supervisor(logdir='/tmp/mydir', is_chief=is_chief, summary_op=None)\n  # As summary_op was None, managed_session() does not start the\n  # summary thread.\n  with sv.managed_session(FLAGS.master) as sess:\n    for step in xrange(1000000):\n      if sv.should_stop():\n        break\n      if is_chief and step % 100 == 0:\n        # Create the summary every 100 chief steps.\n        sv.summary_computed(sess, sess.run(my_summary_op))\n      else:\n        # Train normally\n        sess.run(my_train_op)\n  ```\n\n  ##### Custom model initialization\n\n  `managed_session()` only supports initializing the model by running an\n  `init_op` or restoring from the latest checkpoint.  If you have special\n  initialization needs, see how to specify a `local_init_op` when creating the\n  supervisor.  You can also use the `SessionManager` directly to create a\n  session and check if it could be initialized automatically.\n  ",
        "klass": "tensorflow.train.Supervisor",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "tensorflow.python.training.optimizer.Optimizer"
        ],
        "class_docstring": "Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  This class is deprecated. For synchrononous training, please use [Distribution\n  Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch. Please note that\n     some workers can consume multiple minibatches, while some may not consume\n     even one. This is because each worker fetches minibatches as long as\n     a token exists. If one worker is stuck for some reason and does not\n     consume a token, another worker can use it.\n\n  For the replicas:\n\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  ",
        "klass": "tensorflow.train.SyncReplicasOptimizer",
        "module": "tensorflow"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "\n    Represents a filesystem path.\n\n    For documentation on individual methods, consult their\n    counterparts in :mod:`os.path`.\n\n    Some methods are additionally included from :mod:`shutil`.\n    The functions are linked directly into the class namespace\n    such that they will be bound to the Path instance. For example,\n    ``Path(src).copy(target)`` is equivalent to\n    ``shutil.copy(src, target)``. Therefore, when referencing\n    the docs for these methods, assume `src` references `self`,\n    the Path instance.\n    ",
        "klass": "path.Path",
        "module": "path"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    ",
        "klass": "queue.Queue",
        "module": "queue"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "chain(*iterables) --> chain object\n\nReturn a chain object whose .__next__() method returns elements from the\nfirst iterable until it is exhausted, then elements from the next\niterable, until all of the iterables are exhausted.",
        "klass": "itertools.chain",
        "module": "itertools"
    },
    {
        "base_classes": [
            "pyparsing._PositionToken"
        ],
        "class_docstring": "Matches if current position is at the beginning of a line within\n    the parse string\n\n    Example::\n\n        test = '''\\\n        AAA this line\n        AAA and this line\n          AAA but not this one\n        B AAA and definitely not this one\n        '''\n\n        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):\n            print(t)\n\n    prints::\n\n        ['AAA', ' this line']\n        ['AAA', ' and this line']\n\n    ",
        "klass": "pyparsing.LineStart",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "Token to exactly match a specified string.\n\n    Example::\n\n        Literal('blah').parseString('blah')  # -> ['blah']\n        Literal('blah').parseString('blahfooblah')  # -> ['blah']\n        Literal('blah').parseString('bla')  # -> Exception: Expected \"blah\"\n\n    For case-insensitive matching, use :class:`CaselessLiteral`.\n\n    For keyword matching (force word break before and after the matched string),\n    use :class:`Keyword` or :class:`CaselessKeyword`.\n    ",
        "klass": "pyparsing.Literal",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.ParseElementEnhance"
        ],
        "class_docstring": "Token for skipping over all undefined text until the matched\n    expression is found.\n\n    Parameters:\n     - expr - target expression marking the end of the data to be skipped\n     - include - (default= ``False``) if True, the target expression is also parsed\n          (the skipped text and target expression are returned as a 2-element list).\n     - ignore - (default= ``None``) used to define grammars (typically quoted strings and\n          comments) that might contain false matches to the target expression\n     - failOn - (default= ``None``) define expressions that are not allowed to be\n          included in the skipped test; if found before the target expression is found,\n          the SkipTo is not a match\n\n    Example::\n\n        report = '''\n            Outstanding Issues Report - 1 Jan 2000\n\n               # | Severity | Description                               |  Days Open\n            -----+----------+-------------------------------------------+-----------\n             101 | Critical | Intermittent system crash                 |          6\n              94 | Cosmetic | Spelling error on Login ('log|n')         |         14\n              79 | Minor    | System slow when running too many reports |         47\n            '''\n        integer = Word(nums)\n        SEP = Suppress('|')\n        # use SkipTo to simply match everything up until the next SEP\n        # - ignore quoted strings, so that a '|' character inside a quoted string does not match\n        # - parse action will call token.strip() for each matched token, i.e., the description body\n        string_data = SkipTo(SEP, ignore=quotedString)\n        string_data.setParseAction(tokenMap(str.strip))\n        ticket_expr = (integer(\"issue_num\") + SEP\n                      + string_data(\"sev\") + SEP\n                      + string_data(\"desc\") + SEP\n                      + integer(\"days_open\"))\n\n        for tkt in ticket_expr.searchString(report):\n            print tkt.dump()\n\n    prints::\n\n        ['101', 'Critical', 'Intermittent system crash', '6']\n        - days_open: 6\n        - desc: Intermittent system crash\n        - issue_num: 101\n        - sev: Critical\n        ['94', 'Cosmetic', \"Spelling error on Login ('log|n')\", '14']\n        - days_open: 14\n        - desc: Spelling error on Login ('log|n')\n        - issue_num: 94\n        - sev: Cosmetic\n        ['79', 'Minor', 'System slow when running too many reports', '47']\n        - days_open: 47\n        - desc: System slow when running too many reports\n        - issue_num: 79\n        - sev: Minor\n    ",
        "klass": "pyparsing.SkipTo",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "\n    Token for matching words composed of allowed character sets.\n    Defined with string containing all allowed initial characters,\n    an optional string containing allowed body characters (if omitted,\n    defaults to the initial character set), and an optional minimum,\n    maximum, and/or exact length.  The default value for C{min} is 1 (a\n    minimum value < 1 is not valid); the default values for C{max} and C{exact}\n    are 0, meaning no maximum or exact length restriction. An optional\n    C{excludeChars} parameter can list characters that might be found in \n    the input C{bodyChars} string; useful to define a word of all printables\n    except for one or two characters, for instance.\n    \n    L{srange} is useful for defining custom character set strings for defining \n    C{Word} expressions, using range notation from regular expression character sets.\n    \n    A common mistake is to use C{Word} to match a specific literal string, as in \n    C{Word(\"Address\")}. Remember that C{Word} uses the string argument to define\n    I{sets} of matchable characters. This expression would match \"Add\", \"AAA\",\n    \"dAred\", or any other word made up of the characters 'A', 'd', 'r', 'e', and 's'.\n    To match an exact literal string, use L{Literal} or L{Keyword}.\n\n    pyparsing includes helper strings for building Words:\n     - L{alphas}\n     - L{nums}\n     - L{alphanums}\n     - L{hexnums}\n     - L{alphas8bit} (alphabetic characters in ASCII range 128-255 - accented, tilded, umlauted, etc.)\n     - L{punc8bit} (non-alphabetic characters in ASCII range 128-255 - currency, symbols, superscripts, diacriticals, etc.)\n     - L{printables} (any non-whitespace character)\n\n    Example::\n        # a word composed of digits\n        integer = Word(nums) # equivalent to Word(\"0123456789\") or Word(srange(\"0-9\"))\n        \n        # a word with a leading capital, and zero or more lowercase\n        capital_word = Word(alphas.upper(), alphas.lower())\n\n        # hostnames are alphanumeric, with leading alpha, and '-'\n        hostname = Word(alphas, alphanums+'-')\n        \n        # roman numeral (not a strict parser, accepts invalid mix of characters)\n        roman = Word(\"IVXLCDM\")\n        \n        # any string of non-whitespace characters, except for ','\n        csv_value = Word(printables, excludeChars=\",\")\n    ",
        "klass": "pyparsing.Word",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "_markupbase.ParserBase"
        ],
        "class_docstring": "Find tags and other markup and call handler functions.\n\n    Usage:\n        p = HTMLParser()\n        p.feed(data)\n        ...\n        p.close()\n\n    Start tags are handled by calling self.handle_starttag() or\n    self.handle_startendtag(); end tags by self.handle_endtag().  The\n    data between tags is passed from the parser to the derived class\n    by calling self.handle_data() with the data as argument (the data\n    may be split up in arbitrary chunks).  If convert_charrefs is\n    True the character references are converted automatically to the\n    corresponding Unicode character (and self.handle_data() is no\n    longer split in chunks), otherwise they are passed by calling\n    self.handle_entityref() or self.handle_charref() with the string\n    containing respectively the named or numeric reference as the\n    argument.\n    ",
        "klass": "html.parser.HTMLParser",
        "module": "html"
    },
    {
        "base_classes": [
            "pyparsing.ParseElementEnhance"
        ],
        "class_docstring": "Optional matching of the given expression.\n\n    Parameters:\n     - expr - expression that must match zero or more times\n     - default (optional) - value to be returned if the optional expression is not found.\n\n    Example::\n\n        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier\n        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))\n        zip.runTests('''\n            # traditional ZIP code\n            12345\n\n            # ZIP+4 form\n            12101-0001\n\n            # invalid ZIP\n            98765-\n            ''')\n\n    prints::\n\n        # traditional ZIP code\n        12345\n        ['12345']\n\n        # ZIP+4 form\n        12101-0001\n        ['12101-0001']\n\n        # invalid ZIP\n        98765-\n             ^\n        FAIL: Expected end of text (at char 5), (line:1, col:6)\n    ",
        "klass": "pyparsing.Optional",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "Token for matching strings that match a given regular\n    expression. Defined with string specifying the regular expression in\n    a form recognized by the stdlib Python  `re module <https://docs.python.org/3/library/re.html>`_.\n    If the given regex contains named groups (defined using ``(?P<name>...)``),\n    these will be preserved as named parse results.\n\n    If instead of the Python stdlib re module you wish to use a different RE module\n    (such as the `regex` module), you can replace it by either building your\n    Regex object with a compiled RE that was compiled using regex:\n\n    Example::\n\n        realnum = Regex(r\"[+-]?\\d+\\.\\d*\")\n        date = Regex(r'(?P<year>\\d{4})-(?P<month>\\d\\d?)-(?P<day>\\d\\d?)')\n        # ref: https://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression\n        roman = Regex(r\"M{0,4}(CM|CD|D?{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\")\n\n        # use regex module instead of stdlib re module to construct a Regex using\n        # a compiled regular expression\n        import regex\n        parser = pp.Regex(regex.compile(r'[0-9]'))\n\n    ",
        "klass": "pyparsing.Regex",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.TokenConverter"
        ],
        "class_docstring": "Converter for ignoring the results of a parsed expression.\n\n    Example::\n\n        source = \"a, b, c,d\"\n        wd = Word(alphas)\n        wd_list1 = wd + ZeroOrMore(',' + wd)\n        print(wd_list1.parseString(source))\n\n        # often, delimiters that are useful during parsing are just in the\n        # way afterward - use Suppress to keep them out of the parsed output\n        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)\n        print(wd_list2.parseString(source))\n\n    prints::\n\n        ['a', ',', 'b', ',', 'c', ',', 'd']\n        ['a', 'b', 'c', 'd']\n\n    (See also :class:`delimitedList`.)\n    ",
        "klass": "pyparsing.Suppress",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing._MultipleMatch"
        ],
        "class_docstring": "Repetition of one or more of the given expression.\n\n    Parameters:\n     - expr - expression that must match one or more times\n     - stopOn - (default= ``None``) - expression for a terminating sentinel\n          (only required if the sentinel would ordinarily match the repetition\n          expression)\n\n    Example::\n\n        data_word = Word(alphas)\n        label = data_word + FollowedBy(':')\n        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))\n\n        text = \"shape: SQUARE posn: upper left color: BLACK\"\n        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]\n\n        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data\n        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]\n\n        # could also be written as\n        (attr_expr * (1,)).parseString(text).pprint()\n    ",
        "klass": "pyparsing.OneOrMore",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing._MultipleMatch"
        ],
        "class_docstring": "Optional repetition of zero or more of the given expression.\n\n    Parameters:\n     - expr - expression that must match zero or more times\n     - stopOn - (default= ``None``) - expression for a terminating sentinel\n          (only required if the sentinel would ordinarily match the repetition\n          expression)\n\n    Example: similar to :class:`OneOrMore`\n    ",
        "klass": "pyparsing.ZeroOrMore",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A small DSL for generating XML. For example,\n        with XMLBuilder(\"ROOT\") as ctx:\n            ctx.P(\"Some Text\")\n            with ctx.SECT(level=4):\n                ctx.P(\"More\")\n\n        ctx.xml_str:\n        <ROOT>\n            <P>Some Text</P>\n            <SECT level=\"4\">\n                <P>More</P>\n            </SECT>\n        </ROOT>",
        "klass": "regparser.test_utils.xml_builder.XMLBuilder",
        "module": "regparser"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n        An addon that handles reading from file on startup.\n    ",
        "klass": "mitmproxy.addons.readfile.ReadFile",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "mitmproxy.addons.readfile.ReadFile"
        ],
        "class_docstring": "Support the special case of \"-\" for reading from stdin",
        "klass": "mitmproxy.addons.readfile.ReadFileStdin",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n        This addon handles authentication to systems upstream from us for the\n        upstream proxy and reverse proxy mode. There are 3 cases:\n\n        - Upstream proxy CONNECT requests should have authentication added, and\n          subsequent already connected requests should not.\n        - Upstream proxy regular requests\n        - Reverse proxy regular requests (CONNECT is invalid in this mode)\n    ",
        "klass": "mitmproxy.addons.upstream_auth.UpstreamAuth",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "mitmproxy.controller.Reply"
        ],
        "class_docstring": "\n    A reply object that is not connected to anything. In contrast to regular\n    Reply objects, DummyReply objects are reset to \"start\" at the end of an\n    handler so that they can be used multiple times. Useful when we need an\n    object to seem like it has a channel, and during testing.\n    ",
        "klass": "mitmproxy.controller.DummyReply",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Messages sent through a channel are decorated with a \"reply\" attribute. This\n    object is used to respond to the message through the return channel.\n    ",
        "klass": "mitmproxy.controller.Reply",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "mitmproxy.stateobject.StateObject"
        ],
        "class_docstring": "\n        An Error.\n\n        This is distinct from an protocol error response (say, a HTTP code 500),\n        which is represented by a normal HTTPResponse object. This class is\n        responsible for indicating errors that fall outside of normal protocol\n        communications, like interrupted connections, timeouts, protocol errors.\n\n        Exposes the following attributes:\n\n            msg: Message describing the error\n            timestamp: Seconds since the epoch\n    ",
        "klass": "mitmproxy.flow.Error",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This class is wrapping up connection to SQLITE DB.\n    ",
        "klass": "mitmproxy.io.db.DBHandler",
        "module": "mitmproxy"
    },
    {
        "base_classes": [
            "argparse._AttributeHolder",
            "argparse._ActionsContainer"
        ],
        "class_docstring": "Object for parsing command line strings into Python objects.\n\n    Keyword Arguments:\n        - prog -- The name of the program (default: sys.argv[0])\n        - usage -- A usage message (default: auto-generated from arguments)\n        - description -- A description of what the program does\n        - epilog -- Text following the argument descriptions\n        - parents -- Parsers whose arguments should be copied into this one\n        - formatter_class -- HelpFormatter class for printing help messages\n        - prefix_chars -- Characters that prefix optional arguments\n        - fromfile_prefix_chars -- Characters that prefix files containing\n            additional arguments\n        - argument_default -- The default value for all arguments\n        - conflict_handler -- String indicating how to handle conflicts\n        - add_help -- Add a -h/-help option\n        - allow_abbrev -- Allow long options to be abbreviated unambiguously\n    ",
        "klass": "argparse.ArgumentParser",
        "module": "argparse"
    },
    {
        "base_classes": [
            "argparse._AttributeHolder"
        ],
        "class_docstring": "Simple object for storing attributes.\n\n    Implements equality by attribute names and values, and provides a simple\n    string representation.\n    ",
        "klass": "argparse.Namespace",
        "module": "argparse"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a new circuit.\n\n    A circuit is a list of instructions bound to some registers.\n\n    Args:\n        regs: list(:class:`Register`) or list(``int``) The registers to be\n            included in the circuit.\n\n                * If a list of :class:`Register` objects, represents the :class:`QuantumRegister`\n                  and/or :class:`ClassicalRegister` objects to include in the circuit.\n\n                For example:\n\n                * ``QuantumCircuit(QuantumRegister(4))``\n                * ``QuantumCircuit(QuantumRegister(4), ClassicalRegister(3))``\n                * ``QuantumCircuit(QuantumRegister(4, 'qr0'), QuantumRegister(2, 'qr1'))``\n\n                * If a list of ``int``, the amount of qubits and/or classical\n                bits to include in the circuit. It can either be a single\n                int for just the number of quantum bits, or 2 ints for the number of\n                quantum bits and classical bits, respectively.\n\n\n                For example:\n\n                * ``QuantumCircuit(4) # A QuantumCircuit with 4 qubits``\n                * ``QuantumCircuit(4, 3) # A QuantumCircuit with 4 qubits and 3 classical bits``\n\n\n        name (str): the name of the quantum circuit. If not set, an\n            automatically generated string will be assigned.\n\n    Raises:\n        CircuitError: if the circuit name, if given, is not valid.\n\n    Examples:\n\n        Construct a simple Bell state circuit.\n\n        .. jupyter-execute::\n\n            from qiskit import QuantumCircuit\n\n            qc = QuantumCircuit(2, 2)\n            qc.h(0)\n            qc.cx(0, 1)\n            qc.measure([0, 1], [0, 1])\n            qc.draw()\n\n        Construct a 5 qubit GHZ circuit.\n\n        .. jupyter-execute::\n\n            from qiskit import QuantumCircuit\n\n            qc = QuantumCircuit(5)\n            qc.h(0)\n            qc.cx(0, range(1, 5))\n            qc.measure_all()\n\n        Construct a 4 qubit Berstein-Vazirani circuit using registers.\n\n        .. jupyter-execute::\n\n            from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit\n\n            qr = QuantumRegister(3, 'q')\n            anc = QuantumRegister(1, 'ancilla')\n            cr = ClassicalRegister(3, 'c')\n            qc = QuantumCircuit(qr, anc, cr)\n\n            qc.x(anc[0])\n            qc.h(anc[0])\n            qc.h(qr[0:3])\n            qc.cx(qr[0:3], anc[0])\n            qc.h(qr[0:3])\n            qc.barrier(qr)\n            qc.measure(qr, cr)\n\n            qc.draw()\n    ",
        "klass": "qiskit.QuantumCircuit",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "zip_longest(iter1 [,iter2 [...]], [fillvalue=None]) --> zip_longest object\n\nReturn a zip_longest object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the longest iterable in the argument sequence\nis exhausted and then it raises StopIteration.  When the shorter iterables\nare exhausted, the fillvalue is substituted in their place.  The fillvalue\ndefaults to None or can be specified by a keyword argument.\n",
        "klass": "itertools.zip_longest",
        "module": "itertools"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    ",
        "klass": "collections.Counter",
        "module": "collections"
    },
    {
        "base_classes": [
            "unittest.mock.MagicMixin",
            "unittest.mock.Mock"
        ],
        "class_docstring": "\n    MagicMock is a subclass of Mock with default implementations\n    of most of the magic methods. You can use MagicMock without having to\n    configure the magic methods yourself.\n\n    If you use the `spec` or `spec_set` arguments then *only* magic\n    methods that exist in the spec will be created.\n\n    Attributes and the return value of a `MagicMock` will also be `MagicMocks`.\n    ",
        "klass": "unittest.mock.MagicMock",
        "module": "unittest"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "product(*iterables, repeat=1) --> product object\n\nCartesian product of input iterables.  Equivalent to nested for-loops.\n\nFor example, product(A, B) returns the same as:  ((x,y) for x in A for y in B).\nThe leftmost iterators are in the outermost for-loop, so the output tuples\ncycle in a manner similar to an odometer (with the rightmost element changing\non every iteration).\n\nTo compute the product of an iterable with itself, specify the number\nof repetitions with the optional repeat keyword argument. For example,\nproduct(A, repeat=4) means the same as product(A, A, A, A).\n\nproduct('ab', range(3)) --> ('a',0) ('a',1) ('a',2) ('b',0) ('b',1) ('b',2)\nproduct((0,1), (0,1), (0,1)) --> (0,0,0) (0,0,1) (0,1,0) (0,1,1) (1,0,0) ...",
        "klass": "itertools.product",
        "module": "itertools"
    },
    {
        "base_classes": [
            "qiskit.transpiler.basepasses.TransformationPass"
        ],
        "class_docstring": "Expand a gate in a circuit using its decomposition rules.",
        "klass": "qiskit.transpiler.passes.Decompose",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "qiskit.transpiler.basepasses.TransformationPass"
        ],
        "class_docstring": "Map input circuit onto a backend topology via insertion of SWAPs.\n\n    Implementation of Sven Jandura's swap mapper submission for the 2018 Qiskit\n    Developer Challenge, adapted to integrate into the transpiler architecture.\n\n    The role of the swapper pass is to modify the starting circuit to be compatible\n    with the target device's topology (the set of two-qubit gates available on the\n    hardware.) To do this, the pass will insert SWAP gates to relocate the virtual\n    qubits for each upcoming gate onto a set of coupled physical qubits. However, as\n    SWAP gates are particularly lossy, the goal is to accomplish this remapping while\n    introducing the fewest possible additional SWAPs.\n\n    This algorithm searches through the available combinations of SWAP gates by means\n    of a narrowed best first/beam search, described as follows:\n\n    - Start with a layout of virtual qubits onto physical qubits.\n    - Find any gates in the input circuit which can be performed with the current\n      layout and mark them as mapped.\n    - For all possible SWAP gates, calculate the layout that would result from their\n      application and rank them according to the distance of the resulting layout\n      over upcoming gates (see _calc_layout_distance.)\n    - For the four (SEARCH_WIDTH) highest-ranking SWAPs, repeat the above process on\n      the layout that would be generated if they were applied.\n    - Repeat this process down to a depth of four (SEARCH_DEPTH) SWAPs away from the\n      initial layout, for a total of 256 (SEARCH_WIDTH^SEARCH_DEPTH) prospective\n      layouts.\n    - Choose the layout which maximizes the number of two-qubit which could be\n      performed. Add its mapped gates, including the SWAPs generated, to the\n      output circuit.\n    - Repeat the above until all gates from the initial circuit are mapped.\n\n    For more details on the algorithm, see Sven's blog post:\n    https://medium.com/qiskit/improving-a-quantum-compiler-48410d7a7084\n    ",
        "klass": "qiskit.transpiler.passes.LookaheadSwap",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "qiskit.transpiler.basepasses.TransformationPass"
        ],
        "class_docstring": "Optimize chains of single-qubit u1, u2, u3 gates by combining them into a single gate.",
        "klass": "qiskit.transpiler.passes.Optimize1qGates",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "qiskit.transpiler.basepasses.TransformationPass"
        ],
        "class_docstring": "Recursively expands 3q+ gates until the circuit only contains 2q or 1q gates.",
        "klass": "qiskit.transpiler.passes.Unroll3qOrMore",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "qiskit.transpiler.basepasses.TransformationPass"
        ],
        "class_docstring": "Unroll a circuit to a given basis.\n\n    Unroll (expand) non-basis, non-opaque instructions recursively\n    to a desired basis, using decomposition rules defined for each instruction.\n    ",
        "klass": "qiskit.transpiler.passes.Unroller",
        "module": "qiskit"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "make an iterator that returns consecutive keys and groups from the iterable\n\n  iterable\n    Elements to divide into groups according to the key function.\n  key\n    A function for computing the group category for each element.\n    If the key function is not specified or is None, the element itself\n    is used for grouping.",
        "klass": "itertools.groupby",
        "module": "itertools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "classmethod(function) -> method\n\nConvert a function to be a class method.\n\nA class method receives the class as implicit first argument,\njust like an instance method receives the instance.\nTo declare a class method, use this idiom:\n\n  class C:\n      @classmethod\n      def f(cls, arg1, arg2, ...):\n          ...\n\nIt can be called either on the class (e.g. C.f()) or on an instance\n(e.g. C().f()).  The instance is ignored except for its class.\nIf a class method is called for a derived class, the derived class\nobject is passed as the implied first argument.\n\nClass methods are different than C++ or Java static methods.\nIf you want those, see the staticmethod builtin.",
        "klass": "method",
        "module": "method"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A simple attribute-based namespace.\n\nSimpleNamespace(**kwargs)",
        "klass": "types.SimpleNamespace",
        "module": "types"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "date(year, month, day) --> date object",
        "klass": "datetime.date",
        "module": "datetime"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A string class for supporting $-substitutions.",
        "klass": "string.Template",
        "module": "string"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "ndarray(shape, dtype=float, buffer=None, offset=0,\n            strides=None, order=None)\n\n    An array object represents a multidimensional, homogeneous array\n    of fixed-size items.  An associated data-type object describes the\n    format of each element in the array (its byte-order, how many bytes it\n    occupies in memory, whether it is an integer, a floating point number,\n    or something else, etc.)\n\n    Arrays should be constructed using `array`, `zeros` or `empty` (refer\n    to the See Also section below).  The parameters given here refer to\n    a low-level method (`ndarray(...)`) for instantiating an array.\n\n    For more information, refer to the `numpy` module and examine the\n    methods and attributes of an array.\n\n    Parameters\n    ----------\n    (for the __new__ method; see Notes below)\n\n    shape : tuple of ints\n        Shape of created array.\n    dtype : data-type, optional\n        Any object that can be interpreted as a numpy data type.\n    buffer : object exposing buffer interface, optional\n        Used to fill the array with data.\n    offset : int, optional\n        Offset of array data in buffer.\n    strides : tuple of ints, optional\n        Strides of data in memory.\n    order : {'C', 'F'}, optional\n        Row-major (C-style) or column-major (Fortran-style) order.\n\n    Attributes\n    ----------\n    T : ndarray\n        Transpose of the array.\n    data : buffer\n        The array's elements, in memory.\n    dtype : dtype object\n        Describes the format of the elements in the array.\n    flags : dict\n        Dictionary containing information related to memory use, e.g.,\n        'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.\n    flat : numpy.flatiter object\n        Flattened version of the array as an iterator.  The iterator\n        allows assignments, e.g., ``x.flat = 3`` (See `ndarray.flat` for\n        assignment examples; TODO).\n    imag : ndarray\n        Imaginary part of the array.\n    real : ndarray\n        Real part of the array.\n    size : int\n        Number of elements in the array.\n    itemsize : int\n        The memory use of each array element in bytes.\n    nbytes : int\n        The total number of bytes required to store the array data,\n        i.e., ``itemsize * size``.\n    ndim : int\n        The array's number of dimensions.\n    shape : tuple of ints\n        Shape of the array.\n    strides : tuple of ints\n        The step-size required to move from one element to the next in\n        memory. For example, a contiguous ``(3, 4)`` array of type\n        ``int16`` in C-order has strides ``(8, 2)``.  This implies that\n        to move from element to element in memory requires jumps of 2 bytes.\n        To move from row-to-row, one needs to jump 8 bytes at a time\n        (``2 * 4``).\n    ctypes : ctypes object\n        Class containing properties of the array needed for interaction\n        with ctypes.\n    base : ndarray\n        If the array is a view into another array, that array is its `base`\n        (unless that array is also a view).  The `base` array is where the\n        array data is actually stored.\n\n    See Also\n    --------\n    array : Construct an array.\n    zeros : Create an array, each element of which is zero.\n    empty : Create an array, but leave its allocated memory unchanged (i.e.,\n            it contains \"garbage\").\n    dtype : Create a data-type.\n\n    Notes\n    -----\n    There are two modes of creating an array using ``__new__``:\n\n    1. If `buffer` is None, then only `shape`, `dtype`, and `order`\n       are used.\n    2. If `buffer` is an object exposing the buffer interface, then\n       all keywords are interpreted.\n\n    No ``__init__`` method is needed because the array is fully initialized\n    after the ``__new__`` method.\n\n    Examples\n    --------\n    These examples illustrate the low-level `ndarray` constructor.  Refer\n    to the `See Also` section above for easier ways of constructing an\n    ndarray.\n\n    First mode, `buffer` is None:\n\n    >>> np.ndarray(shape=(2,2), dtype=float, order='F')\n    array([[0.0e+000, 0.0e+000], # random\n           [     nan, 2.5e-323]])\n\n    Second mode:\n\n    >>> np.ndarray((2,), buffer=np.array([1,2,3]),\n    ...            offset=np.int_().itemsize,\n    ...            dtype=int) # offset = 1*itemsize, i.e. skip first element\n    array([2, 3])",
        "klass": "numpy.ndarray",
        "module": "numpy"
    },
    {
        "base_classes": [
            "pdb.Pdb"
        ],
        "class_docstring": "Modified Pdb class, does not load readline.\n\n    for a standalone version that uses prompt_toolkit, see\n    `IPython.terminal.debugger.TerminalPdb` and\n    `IPython.terminal.debugger.set_trace()`\n    ",
        "klass": "IPython.core.debugger.Pdb",
        "module": "IPython"
    },
    {
        "base_classes": [
            "pathlib.PurePath"
        ],
        "class_docstring": "PurePath subclass that can make system calls.\n\n    Path represents a filesystem path but unlike PurePath, also offers\n    methods to do system calls on path objects. Depending on your system,\n    instantiating a Path will return either a PosixPath or a WindowsPath\n    object. You can also instantiate a PosixPath or WindowsPath directly,\n    but cannot instantiate a WindowsPath on a POSIX system or vice versa.\n    ",
        "klass": "pathlib.Path",
        "module": "pathlib"
    },
    {
        "base_classes": [
            "IPython.core.formatters.BaseFormatter"
        ],
        "class_docstring": "The default pretty-printer.\n\n    This uses :mod:`IPython.lib.pretty` to compute the format data of\n    the object. If the object cannot be pretty printed, :func:`repr` is used.\n    See the documentation of :mod:`IPython.lib.pretty` for details on\n    how to write pretty printers.  Here is a simple example::\n\n        def dtype_pprinter(obj, p, cycle):\n            if cycle:\n                return p.text('dtype(...)')\n            if hasattr(obj, 'fields'):\n                if obj.fields is None:\n                    p.text(repr(obj))\n                else:\n                    p.begin_group(7, 'dtype([')\n                    for i, field in enumerate(obj.descr):\n                        if i > 0:\n                            p.text(',')\n                            p.breakable()\n                        p.pretty(field)\n                    p.end_group(7, '])')\n    ",
        "klass": "IPython.core.formatters.PlainTextFormatter",
        "module": "IPython"
    },
    {
        "base_classes": [
            "IPython.utils.text.FullEvalFormatter"
        ],
        "class_docstring": "Formatter allowing Itpl style $foo replacement, for names and attribute\n    access only. Standard {foo} replacement also works, and allows full\n    evaluation of its arguments.\n\n    Examples\n    --------\n    ::\n\n        In [1]: f = DollarFormatter()\n        In [2]: f.format('{n//4}', n=8)\n        Out[2]: '2'\n\n        In [3]: f.format('23 * 76 is $result', result=23*76)\n        Out[3]: '23 * 76 is 1748'\n\n        In [4]: f.format('$a or {b}', a=1, b=2)\n        Out[4]: '1 or 2'\n    ",
        "klass": "IPython.utils.text.DollarFormatter",
        "module": "IPython"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": " The main 'connection' object for PickleShare database ",
        "klass": "pickleshare.PickleShareDB",
        "module": "pickleshare"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create and return a temporary directory.  This has the same\n    behavior as mkdtemp but can be used as a context manager.  For\n    example:\n\n        with TemporaryDirectory() as tmpdir:\n            ...\n\n    Upon exiting the context, the directory and everything contained\n    in it are removed.\n    ",
        "klass": "tempfile.TemporaryDirectory",
        "module": "tempfile"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context for prepending a directory to sys.path for a second.",
        "klass": "IPython.utils.syspathcontext.prepended_to_syspath",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "context manager for capturing stdout/err",
        "klass": "IPython.utils.capture.capture_output",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Execute a child program in a new process.\n\n    For a complete description of the arguments see the Python documentation.\n\n    Arguments:\n      args: A string, or a sequence of program arguments.\n\n      bufsize: supplied as the buffering argument to the open() function when\n          creating the stdin/stdout/stderr pipe file objects\n\n      executable: A replacement program to execute.\n\n      stdin, stdout and stderr: These specify the executed programs' standard\n          input, standard output and standard error file handles, respectively.\n\n      preexec_fn: (POSIX only) An object to be called in the child process\n          just before the child is executed.\n\n      close_fds: Controls closing or inheriting of file descriptors.\n\n      shell: If true, the command will be executed through the shell.\n\n      cwd: Sets the current directory before the child is executed.\n\n      env: Defines the environment variables for the new process.\n\n      text: If true, decode stdin, stdout and stderr using the given encoding\n          (if set) or the system default otherwise.\n\n      universal_newlines: Alias of text, provided for backwards compatibility.\n\n      startupinfo and creationflags (Windows only)\n\n      restore_signals (POSIX only)\n\n      start_new_session (POSIX only)\n\n      pass_fds (POSIX only)\n\n      encoding and errors: Text mode encoding and error handling to use for\n          file objects stdin, stdout and stderr.\n\n    Attributes:\n        stdin, stdout, stderr, pid, returncode\n    ",
        "klass": "subprocess.Popen",
        "module": "subprocess"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to manage a pool of backgrounded threaded jobs.\n\n    Below, we assume that 'jobs' is a BackgroundJobManager instance.\n    \n    Usage summary (see the method docstrings for details):\n\n      jobs.new(...) -> start a new job\n      \n      jobs() or jobs.status() -> print status summary of all jobs\n\n      jobs[N] -> returns job number N.\n\n      foo = jobs[N].result -> assign to variable foo the result of job N\n\n      jobs[N].traceback() -> print the traceback of dead job N\n\n      jobs.remove(N) -> remove (finished) job N\n\n      jobs.flush() -> remove all finished jobs\n      \n    As a convenience feature, BackgroundJobManager instances provide the\n    utility result and traceback methods which retrieve the corresponding\n    information from the jobs list:\n\n      jobs.result(N) <--> jobs[N].result\n      jobs.traceback(N) <--> jobs[N].traceback()\n\n    While this appears minor, it allows you to use tab completion\n    interactively on the job manager instance.\n    ",
        "klass": "IPython.lib.backgroundjobs.BackgroundJobManager",
        "module": "IPython"
    },
    {
        "base_classes": [
            "subprocess.SubprocessError"
        ],
        "class_docstring": "Raised when run() is called with check=True and the process\n    returns a non-zero exit status.\n\n    Attributes:\n      cmd, returncode, stdout, stderr, output\n    ",
        "klass": "subprocess.CalledProcessError",
        "module": "subprocess"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for embedding a local file link in an IPython session, based on path\n\n    e.g. to embed a link that was generated in the IPython notebook as my/data.txt\n\n    you would do::\n\n        local_file = FileLink(\"my/data.txt\")\n        display(local_file)\n\n    or in the HTML notebook, just::\n\n        FileLink(\"my/data.txt\")\n    ",
        "klass": "IPython.lib.display.FileLink",
        "module": "IPython"
    },
    {
        "base_classes": [
            "IPython.lib.display.FileLink"
        ],
        "class_docstring": "Class for embedding local file links in an IPython session, based on path\n\n    e.g. to embed links to files that were generated in the IPython notebook\n    under ``my/data``, you would do::\n\n        local_files = FileLinks(\"my/data\")\n        display(local_files)\n\n    or in the HTML notebook, just::\n\n        FileLinks(\"my/data\")\n    ",
        "klass": "IPython.lib.display.FileLinks",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a module object.\n\nThe name must be a string; the optional doc argument can have any type.",
        "klass": "module",
        "module": "module"
    },
    {
        "base_classes": [
            "distutils.version.Version"
        ],
        "class_docstring": "Version numbering for anarchists and software realists.\n    Implements the standard interface for version number classes as\n    described above.  A version number consists of a series of numbers,\n    separated by either periods or strings of letters.  When comparing\n    version numbers, the numeric components will be compared\n    numerically, and the alphabetic components lexically.  The following\n    are all valid version numbers, in no particular order:\n\n        1.5.1\n        1.5.2b2\n        161\n        3.10a\n        8.02\n        3.4j\n        1996.07.12\n        3.2.pl0\n        3.1.1.6\n        2g6\n        11g\n        0.960923\n        2.2beta29\n        1.13++\n        5.5.kw\n        2.0b1pl0\n\n    In fact, there is no such thing as an invalid version number under\n    this scheme; the rules for comparison are simple and predictable,\n    but may not always give the results you want (for some definition\n    of \"want\").\n    ",
        "klass": "distutils.version.LooseVersion",
        "module": "distutils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "deque([iterable[, maxlen]]) --> deque object\n\nA list-like sequence optimized for data accesses near its endpoints.",
        "klass": "collections.deque",
        "module": "collections"
    },
    {
        "base_classes": [
            "prompt_toolkit.key_binding.key_bindings.KeyBindingsBase"
        ],
        "class_docstring": "\n    A container for a set of key bindings.\n\n    Example usage::\n\n        kb = KeyBindings()\n\n        @kb.add('c-t')\n        def _(event):\n            print('Control-T pressed')\n\n        @kb.add('c-a', 'c-b')\n        def _(event):\n            print('Control-A pressed, followed by Control-B')\n\n        @kb.add('c-x', filter=is_searching)\n        def _(event):\n            print('Control-X pressed')  # Works only if we are searching.\n\n    ",
        "klass": "prompt_toolkit.key_binding.key_bindings.KeyBindings",
        "module": "prompt_toolkit"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print(test.name, '->', runner.run(test))\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    ",
        "klass": "doctest.DocTestRunner",
        "module": "doctest"
    },
    {
        "base_classes": [
            "IPython.testing.tools.AssertPrints"
        ],
        "class_docstring": "Context manager for checking that certain output *isn't* produced.\n\n    Counterpart of AssertPrints",
        "klass": "IPython.testing.tools.AssertNotPrints",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager for testing that code prints certain text.\n\n    Examples\n    --------\n    >>> with AssertPrints(\"abc\", suppress=False):\n    ...     print(\"abcd\")\n    ...     print(\"def\")\n    ...\n    abcd\n    def\n    ",
        "klass": "IPython.testing.tools.AssertPrints",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "context manager for capturing stdout/err",
        "klass": "IPython.utils.io.capture_output",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create and return a temporary directory.  This has the same\n    behavior as mkdtemp but can be used as a context manager.  For\n    example:\n\n        with TemporaryDirectory() as tmpdir:\n            ...\n\n    Upon exiting the context, the directory and everything contained\n    in it are removed.\n    ",
        "klass": "IPython.utils.tempdir.TemporaryDirectory",
        "module": "tempfile"
    },
    {
        "base_classes": [
            "tempfile.TemporaryDirectory"
        ],
        "class_docstring": "\n    Creates a temporary directory and sets the cwd to that directory.\n    Automatically reverts to previous cwd upon cleanup.\n    Usage example:\n\n        with TemporaryWorkingDirectory() as tmpdir:\n            ...\n    ",
        "klass": "IPython.utils.tempdir.TemporaryWorkingDirectory",
        "module": "IPython"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Abstract base class for time zone info objects.",
        "klass": "datetime.tzinfo",
        "module": "datetime"
    },
    {
        "base_classes": [
            "logging.StreamHandler"
        ],
        "class_docstring": "\n    A handler class which writes formatted logging records to disk files.\n    ",
        "klass": "logging.FileHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    the style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    ",
        "klass": "logging.Formatter",
        "module": "logging"
    },
    {
        "base_classes": [
            "logging.Filterer"
        ],
        "class_docstring": "\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    ",
        "klass": "logging.Handler",
        "module": "logging"
    },
    {
        "base_classes": [
            "logging.Filterer"
        ],
        "class_docstring": "\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    ",
        "klass": "logging.Logger",
        "module": "logging"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    ",
        "klass": "logging.LoggerAdapter",
        "module": "logging"
    },
    {
        "base_classes": [
            "logging.Handler"
        ],
        "class_docstring": "\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    ",
        "klass": "logging.NullHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "logging.Handler"
        ],
        "class_docstring": "\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    ",
        "klass": "logging.StreamHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "attrgetter(attr, ...) --> attrgetter object\n\nReturn a callable object that fetches the given attribute(s) from its operand.\nAfter f = attrgetter('name'), the call f(r) returns r.name.\nAfter g = attrgetter('name', 'date'), the call g(r) returns (r.name, r.date).\nAfter h = attrgetter('name.first', 'name.last'), the call h(r) returns\n(r.name.first, r.name.last).",
        "klass": "operator.attrgetter",
        "module": "operator"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    ",
        "klass": "threading.Thread",
        "module": "threading"
    },
    {
        "base_classes": [
            "BaseType"
        ],
        "class_docstring": "\n    Interface Type Definition\n\n    When a field can return one of a heterogeneous set of types, a Interface type\n    is used to describe what types are possible, what fields are in common across\n    all types, as well as a function to determine which type is actually used\n    when the field is resolved.\n\n    .. code:: python\n\n        from graphene import Interface, String\n\n        class HasAddress(Interface):\n            class Meta:\n                description = \"Address fields\"\n\n            address1 = String()\n            address2 = String()\n\n    If a field returns an Interface Type, the ambiguous type of the object can be determined using\n    ``resolve_type`` on Interface and an ObjectType with ``Meta.possible_types`` or ``is_type_of``.\n\n    Meta:\n        name (str): Name of the GraphQL type (must be unique in schema). Defaults to class\n            name.\n        description (str): Description of the GraphQL type in the schema. Defaults to class\n            docstring.\n        fields (Dict[str, graphene.Field]): Dictionary of field name to Field. Not recommended to\n            use (prefer class attributes).\n    ",
        "klass": "Interface",
        "module": "Interface"
    },
    {
        "base_classes": [
            "AbstractNode"
        ],
        "class_docstring": "An object with an ID",
        "klass": "Node",
        "module": "Node"
    },
    {
        "base_classes": [
            "Scalar"
        ],
        "class_docstring": "\n    The `Time` scalar type represents a Time value as\n    specified by\n    [iso8601](https://en.wikipedia.org/wiki/ISO_8601).\n    ",
        "klass": "Time",
        "module": "Time"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "type(object_or_name, bases, dict)\ntype(object) -> the object's type\ntype(name, bases, dict) -> a new type",
        "klass": "type",
        "module": "type"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "uri [,options] -> a logical connection to an XML-RPC server\n\n    uri is the connection point on the server, given as\n    scheme://host/target.\n\n    The standard implementation always supports the \"http\" scheme.  If\n    SSL socket support is available (Python 2.0), it also supports\n    \"https\".\n\n    If the target part and the slash preceding it are both omitted,\n    \"/RPC2\" is assumed.\n\n    The following options can be given as keyword arguments:\n\n        transport: a transport factory\n        encoding: the request encoding (default is UTF-8)\n\n    All 8-bit strings passed to the server proxy are assumed to use\n    the given encoding.\n    ",
        "klass": "xmlrpc.client.ServerProxy",
        "module": "xmlrpc"
    },
    {
        "base_classes": [
            "http.cookies.BaseCookie"
        ],
        "class_docstring": "\n    SimpleCookie supports strings as cookie values.  When setting\n    the value using the dictionary assignment notation, SimpleCookie\n    calls the builtin str() to convert the value to a string.  Values\n    received from HTTP are kept as strings.\n    ",
        "klass": "http.cookies.SimpleCookie",
        "module": "http"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "array(typecode [, initializer]) -> array\n\nReturn a new array whose items are restricted by typecode, and\ninitialized from the optional initializer value, which must be a list,\nstring or iterable over elements of the appropriate type.\n\nArrays represent basic values and behave very much like lists, except\nthe type of objects stored in them is constrained. The type is specified\nat object creation time by using a type code, which is a single character.\nThe following type codes are defined:\n\n    Type code   C Type             Minimum size in bytes\n    'b'         signed integer     1\n    'B'         unsigned integer   1\n    'u'         Unicode character  2 (see note)\n    'h'         signed integer     2\n    'H'         unsigned integer   2\n    'i'         signed integer     2\n    'I'         unsigned integer   2\n    'l'         signed integer     4\n    'L'         unsigned integer   4\n    'q'         signed integer     8 (see note)\n    'Q'         unsigned integer   8 (see note)\n    'f'         floating point     4\n    'd'         floating point     8\n\nNOTE: The 'u' typecode corresponds to Python's unicode character. On\nnarrow builds this is 2-bytes on wide builds this is 4-bytes.\n\nNOTE: The 'q' and 'Q' type codes are only available if the platform\nC compiler used to build Python supports 'long long', or, on Windows,\n'__int64'.\n\nMethods:\n\nappend() -- append a new item to the end of the array\nbuffer_info() -- return information giving the current memory info\nbyteswap() -- byteswap all the items of the array\ncount() -- return number of occurrences of an object\nextend() -- extend array by appending multiple elements from an iterable\nfromfile() -- read items from a file object\nfromlist() -- append items from the list\nfrombytes() -- append items from the string\nindex() -- return index of first occurrence of an object\ninsert() -- insert a new item into the array at a provided position\npop() -- remove and return item (default last)\nremove() -- remove first occurrence of an object\nreverse() -- reverse the order of the items in the array\ntofile() -- write all items to a file object\ntolist() -- return the array converted to an ordinary list\ntobytes() -- return the array converted to a string\n\nAttributes:\n\ntypecode -- the typecode character used to create the array\nitemsize -- the length in bytes of one array item\n",
        "klass": "array.array",
        "module": "array"
    },
    {
        "base_classes": [
            "random.Random"
        ],
        "class_docstring": "Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    ",
        "klass": "random.SystemRandom",
        "module": "random"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "islice(iterable, stop) --> islice object\nislice(iterable, start, stop[, step]) --> islice object\n\nReturn an iterator whose next() method returns selected values from an\niterable.  If start is specified, will skip all preceding elements;\notherwise, start defaults to zero.  Step defaults to one.  If\nspecified as another value, step determines how many values are\nskipped between successive calls.  Works like a slice() on a list\nbut returns an iterator.",
        "klass": "itertools.islice",
        "module": "itertools"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object using pyfsapfs.file_entry",
        "klass": "dfvfs.file_io.apfs_file_io.APFSFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object of a compressed stream.",
        "klass": "dfvfs.file_io.compressed_stream_io.CompressedStream",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object of a encoded stream.",
        "klass": "dfvfs.file_io.encoded_stream_io.EncodedStream",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "Base class for file object-based file-like object.",
        "klass": "dfvfs.file_io.file_object_io.FileObjectIO",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object using pyfsntfs.",
        "klass": "dfvfs.file_io.ntfs_file_io.NTFSFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object using pyvshadow.",
        "klass": "dfvfs.file_io.vshadow_file_io.VShadowFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object using zipfile.",
        "klass": "dfvfs.file_io.zip_file_io.ZipFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Text file interface for file-like objects.",
        "klass": "dfvfs.helpers.text_file.TextFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "SQLite database file using a file-like object.",
        "klass": "dfvfs.lib.sqlite_database.SQLiteDatabaseFile",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.volume.volume_system.VolumeSystem"
        ],
        "class_docstring": "Volume system that uses pyfsapfs.",
        "klass": "dfvfs.volume.apfs_volume_system.APFSVolumeSystem",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.volume.volume_system.VolumeSystem"
        ],
        "class_docstring": "Volume system that uses pyvslvm.",
        "klass": "dfvfs.volume.lvm_volume_system.LVMVolumeSystem",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "dfvfs.volume.volume_system.VolumeSystem"
        ],
        "class_docstring": "Volume system that uses pyvshadow.",
        "klass": "dfvfs.volume.vshadow_volume_system.VShadowVolumeSystem",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    * + specReference :URI\n    * + language :CharacterString\n    * + id :CharacterString\n    * + title :CharacterString\n    * + abstract :CharacterString [0..1]\n    * + updateDate :CharacterString [0..1]\n    * + author :CharacterString [0..*]\n    * + publisher :CharacterString [0..1]\n    * + creator :Creator [0..1]\n    * +----+ creatorApplication :CreatorApplication [0..1]\n    * +----+ creatorDisplay :CreatorDisplay [0..1]\n    * + rights :CharacterString [0..1]\n    * + areaOfInterest :GM_Envelope [0..1]\n    * + timeIntervalOfInterest :TM_GeometricPrimitive [0..1]\n    * + keyword :CharacterString [0..*]\n    * + extension :Any [0..*]\n    ",
        "klass": "owslib.owscontext.core.OwcContext",
        "module": "owslib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Profiler(timer=None, timeunit=None, subcalls=True, builtins=True)\n\n    Builds a profiler object using the specified timer function.\n    The default timer is a fast built-in one based on real time.\n    For custom timer functions returning integers, timeunit can\n    be a float specifying a scale (i.e. how long each integer unit\n    is, in seconds).\n",
        "klass": "_lsprof.Profiler",
        "module": "_lsprof"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "itemgetter(item, ...) --> itemgetter object\n\nReturn a callable object that fetches the given item(s) from its operand.\nAfter f = itemgetter(2), the call f(r) returns r[2].\nAfter g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])",
        "klass": "operator.itemgetter",
        "module": "operator"
    },
    {
        "base_classes": [
            "plotnine.geoms.geom_path.geom_path"
        ],
        "class_docstring": "\nConnected points\n\n\n.. rubric:: Usage\n\n::\n\n    geom_line(mapping=None, data=None, stat='identity', position='identity',\n              na_rm=False, inherit_aes=True, show_legend=None, lineend='butt',\n              linejoin='round', arrow=None, **kwargs)\n\nOnly the ``mapping`` and ``data`` can be positional, the rest must\nbe keyword arguments. ``**kwargs`` can be aesthetics (or parameters)\nused by the ``stat``.\n\n\nParameters\n----------\nmapping : aes, optional\n    Aesthetic mappings created with :meth:`~plotnine.aes`. If specified and :py:`inherit.aes=True`, it is combined with the default mapping for the plot. You must supply mapping if there is no plot mapping.\n    \n    ========= =========\n    Aesthetic Default value\n    ========= =========\n    **x**     \n    **y**     \n    alpha     :py:`1`\n    color     :py:`'black'`\n    group     \n    linetype  :py:`'solid'`\n    size      :py:`0.5`\n    ========= =========\n\n    The **bold** aesthetics are required.\n\ndata : dataframe, optional\n    The data to be displayed in this layer. If :py:`None`, the data from from the :py:`ggplot()` call is used. If specified, it overrides the data from the :py:`ggplot()` call.\nstat : str or stat, optional (default: identity)\n    The statistical transformation to use on the data for this layer. If it is a string, it must be the registered and known to Plotnine.\nposition : str or position, optional (default: identity)\n    Position adjustment. If it is a string, it must be registered and known to Plotnine.\nna_rm : bool, optional (default: False)\n    If :py:`False`, removes missing values with a warning. If :py:`True` silently removes missing values.\ninherit_aes : bool, optional (default: True)\n    If :py:`False`, overrides the default aesthetics.\nshow_legend : bool or dict, optional (default: None)\n    Whether this layer should be included in the legends. :py:`None` the default, includes any aesthetics that are mapped. If a :class:`bool`, :py:`False` never includes and :py:`True` always includes. A :class:`dict` can be used to *exclude* specific aesthetis of the layer from showing in the legend. e.g :py:`show_legend={'color': False}`, any other aesthetic are included by default.\n\n\nSee Also\n--------\nplotnine.geoms.geom_path : For documentation of other parameters.\n",
        "klass": "plotnine.geom_line",
        "module": "plotnine"
    },
    {
        "base_classes": [
            "plotnine.labels.labs"
        ],
        "class_docstring": "\n    Create y-axis label\n\n    Parameters\n    ----------\n    ylab : str\n        y-axis label\n    ",
        "klass": "plotnine.ylab",
        "module": "plotnine"
    },
    {
        "base_classes": [
            "contextlib.AbstractContextManager"
        ],
        "class_docstring": "Context manager to suppress specified exceptions\n\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    ",
        "klass": "contextlib.suppress",
        "module": "contextlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Return elements from the iterable until it is exhausted. Then repeat the sequence indefinitely.",
        "klass": "itertools.cycle",
        "module": "itertools"
    },
    {
        "base_classes": [
            "matplotlib.patches.Patch"
        ],
        "class_docstring": "\n    A rectangle with lower left at *xy* = (*x*, *y*) with\n    specified *width*, *height* and rotation *angle*.\n    ",
        "klass": "matplotlib.patches.Rectangle",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.collections.Collection"
        ],
        "class_docstring": "\n    All parameters must be sequences or scalars; if scalars, they will\n    be converted to sequences.  The property of the ith line\n    segment is::\n\n       prop[i % len(props)]\n\n    i.e., the properties cycle if the ``len`` of props is less than the\n    number of segments.\n    ",
        "klass": "matplotlib.collections.LineCollection",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.collections.Collection"
        ],
        "class_docstring": "\n    A generic collection of patches.\n\n    This makes it easier to assign a color map to a heterogeneous\n    collection of patches.\n\n    This also may improve plotting speed, since PatchCollection will\n    draw faster than a large number of patches.\n    ",
        "klass": "matplotlib.collections.PatchCollection",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.artist.Artist"
        ],
        "class_docstring": "Handle storing and drawing of text in window or data coordinates.",
        "klass": "matplotlib.text.Text",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.colors.Colormap"
        ],
        "class_docstring": "\n    Colormap object generated from a list of colors.\n\n    This may be most useful when indexing directly into a colormap,\n    but it can also be used to generate special colormaps for ordinary\n    mapping.\n\n    Parameters\n    ----------\n    colors : list, array\n        List of Matplotlib color specifications, or an equivalent Nx3 or Nx4\n        floating point array (*N* rgb or rgba values).\n    name : str, optional\n        String to identify the colormap.\n    N : int, optional\n        Number of entries in the map. The default is *None*, in which case\n        there is one colormap entry for each element in the list of colors.\n        If::\n\n            N < len(colors)\n\n        the list will be truncated at *N*. If::\n\n            N > len(colors)\n\n        the list will be extended by repetition.\n    ",
        "klass": "matplotlib.colors.ListedColormap",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Univariate Kernel Density Estimator.\n\n    Parameters\n    ----------\n    endog : array-like\n        The variable for which the density estimate is desired.\n\n    Notes\n    -----\n    If cdf, sf, cumhazard, or entropy are computed, they are computed based on\n    the definition of the kernel rather than the FFT approximation, even if\n    the density is fit with FFT = True.\n\n    `KDEUnivariate` is much faster than `KDEMultivariate`, due to its FFT-based\n    implementation.  It should be preferred for univariate, continuous data.\n    `KDEMultivariate` also supports mixed data.\n\n    See Also\n    --------\n    KDEMultivariate\n    kdensity, kdensityfft\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n\n    >>> nobs = 300\n    >>> np.random.seed(1234)  # Seed random generator\n    >>> dens = sm.nonparametric.KDEUnivariate(np.random.normal(size=nobs))\n    >>> dens.fit()\n    >>> plt.plot(dens.cdf)\n    >>> plt.show()\n\n    ",
        "klass": "statsmodels.nonparametric.kde.KDEUnivariate",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a kernel-density estimate using Gaussian kernels.\n\n    Kernel density estimation is a way to estimate the probability density\n    function (PDF) of a random variable in a non-parametric way.\n    `gaussian_kde` works for both uni-variate and multi-variate data.   It\n    includes automatic bandwidth determination.  The estimation works best for\n    a unimodal distribution; bimodal or multi-modal distributions tend to be\n    oversmoothed.\n\n    Parameters\n    ----------\n    dataset : array_like\n        Datapoints to estimate from. In case of univariate data this is a 1-D\n        array, otherwise a 2-D array with shape (# of dims, # of data).\n    bw_method : str, scalar or callable, optional\n        The method used to calculate the estimator bandwidth.  This can be\n        'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n        this will be used directly as `kde.factor`.  If a callable, it should\n        take a `gaussian_kde` instance as only parameter and return a scalar.\n        If None (default), 'scott' is used.  See Notes for more details.\n    weights : array_like, optional\n        weights of datapoints. This must be the same shape as dataset.\n        If None (default), the samples are assumed to be equally weighted\n\n    Attributes\n    ----------\n    dataset : ndarray\n        The dataset with which `gaussian_kde` was initialized.\n    d : int\n        Number of dimensions.\n    n : int\n        Number of datapoints.\n    neff : int\n        Effective number of datapoints.\n\n        .. versionadded:: 1.2.0\n    factor : float\n        The bandwidth factor, obtained from `kde.covariance_factor`, with which\n        the covariance matrix is multiplied.\n    covariance : ndarray\n        The covariance matrix of `dataset`, scaled by the calculated bandwidth\n        (`kde.factor`).\n    inv_cov : ndarray\n        The inverse of `covariance`.\n\n    Methods\n    -------\n    evaluate\n    __call__\n    integrate_gaussian\n    integrate_box_1d\n    integrate_box\n    integrate_kde\n    pdf\n    logpdf\n    resample\n    set_bandwidth\n    covariance_factor\n\n    Notes\n    -----\n    Bandwidth selection strongly influences the estimate obtained from the KDE\n    (much more so than the actual shape of the kernel).  Bandwidth selection\n    can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n    methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n    uses a rule of thumb, the default is Scott's Rule.\n\n    Scott's Rule [1]_, implemented as `scotts_factor`, is::\n\n        n**(-1./(d+4)),\n\n    with ``n`` the number of data points and ``d`` the number of dimensions.\n    In the case of unequally weighted points, `scotts_factor` becomes::\n\n        neff**(-1./(d+4)),\n\n    with ``neff`` the effective number of datapoints.\n    Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n\n        (n * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    or in the case of unequally weighted points::\n\n        (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    Good general descriptions of kernel density estimation can be found in [1]_\n    and [2]_, the mathematics for this multi-dimensional implementation can be\n    found in [1]_.\n\n    With a set of weighted samples, the effective number of datapoints ``neff``\n    is defined by::\n\n        neff = sum(weights)^2 / sum(weights^2)\n\n    as detailed in [5]_.\n\n    References\n    ----------\n    .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n           Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n    .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n           Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n           Chapman and Hall, London, 1986.\n    .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n           Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n    .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n           conditional density estimation\", Computational Statistics & Data\n           Analysis, Vol. 36, pp. 279-298, 2001.\n    .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n           Series A (General), 132, 272\n\n    Examples\n    --------\n    Generate some random two-dimensional data:\n\n    >>> from scipy import stats\n    >>> def measure(n):\n    ...     \"Measurement model, return two coupled measurements.\"\n    ...     m1 = np.random.normal(size=n)\n    ...     m2 = np.random.normal(scale=0.5, size=n)\n    ...     return m1+m2, m1-m2\n\n    >>> m1, m2 = measure(2000)\n    >>> xmin = m1.min()\n    >>> xmax = m1.max()\n    >>> ymin = m2.min()\n    >>> ymax = m2.max()\n\n    Perform a kernel density estimate on the data:\n\n    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n    >>> values = np.vstack([m1, m2])\n    >>> kernel = stats.gaussian_kde(values)\n    >>> Z = np.reshape(kernel(positions).T, X.shape)\n\n    Plot the results:\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n    ...           extent=[xmin, xmax, ymin, ymax])\n    >>> ax.plot(m1, m2, 'k.', markersize=2)\n    >>> ax.set_xlim([xmin, xmax])\n    >>> ax.set_ylim([ymin, ymax])\n    >>> plt.show()\n\n    ",
        "klass": "scipy.stats.kde.gaussian_kde",
        "module": "scipy"
    },
    {
        "base_classes": [
            "zmq.backend.cython.context.Context",
            "zmq.sugar.attrsettr.AttributeSetter"
        ],
        "class_docstring": "Create a zmq Context\n\n    A zmq Context creates sockets via its ``ctx.socket`` method.\n    ",
        "klass": "zmq.Context",
        "module": "zmq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A stateful poll interface that mirrors Python's built-in poll.",
        "klass": "zmq.Poller",
        "module": "zmq"
    },
    {
        "base_classes": [
            "zmq.backend.cython.socket.Socket",
            "zmq.sugar.attrsettr.AttributeSetter"
        ],
        "class_docstring": "The ZMQ socket object\n\n    To create a Socket, first create a Context::\n\n        ctx = zmq.Context.instance()\n\n    then call ``ctx.socket(socket_type)``::\n\n        s = ctx.socket(zmq.ROUTER)\n\n    ",
        "klass": "zmq.Socket",
        "module": "zmq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Deprecated zmq.Stopwatch implementation\n\n    You can use Python's builtin timers (time.monotonic, etc.).\n    ",
        "klass": "zmq.Stopwatch",
        "module": "zmq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    ",
        "klass": "threading.Event",
        "module": "threading"
    },
    {
        "base_classes": [
            "zmq.devices.basedevice.BackgroundDevice"
        ],
        "class_docstring": "A Device that will be run in a background Thread.\n\n    See Device for details.\n    ",
        "klass": "zmq.devices.ThreadDevice",
        "module": "zmq"
    },
    {
        "base_classes": [
            "zmq.devices.proxydevice.ProxyBase",
            "zmq.devices.basedevice.ThreadDevice"
        ],
        "class_docstring": "Proxy in a Thread. See Proxy for more.",
        "klass": "zmq.devices.ThreadProxy",
        "module": "zmq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A utility class to register callbacks when a zmq socket sends and receives\n    \n    For use with zmq.eventloop.ioloop\n\n    There are three main methods\n    \n    Methods:\n    \n    * **on_recv(callback, copy=True):**\n        register a callback to be run every time the socket has something to receive\n    * **on_send(callback):**\n        register a callback to be run every time you call send\n    * **send(self, msg, flags=0, copy=False, callback=None):**\n        perform a send that will trigger the callback\n        if callback is passed, on_send is also called.\n        \n        There are also send_multipart(), send_json(), send_pyobj()\n    \n    Three other methods for deactivating the callbacks:\n    \n    * **stop_on_recv():**\n        turn off the recv callback\n    * **stop_on_send():**\n        turn off the send callback\n    \n    which simply call ``on_<evt>(None)``.\n    \n    The entire socket interface, excluding direct recv methods, is also\n    provided, primarily through direct-linking the methods.\n    e.g.\n    \n    >>> stream.bind is stream.socket.bind\n    True\n    \n    ",
        "klass": "zmq.eventloop.zmqstream.ZMQStream",
        "module": "zmq"
    },
    {
        "base_classes": [
            "zmq.utils.win32._allow_interrupt"
        ],
        "class_docstring": "Utility for fixing CTRL-C events on Windows.\n\n    On Windows, the Python interpreter intercepts CTRL-C events in order to\n    translate them into ``KeyboardInterrupt`` exceptions.  It (presumably)\n    does this by setting a flag in its \"console control handler\" and\n    checking it later at a convenient location in the interpreter.\n\n    However, when the Python interpreter is blocked waiting for the ZMQ\n    poll operation to complete, it must wait for ZMQ's ``select()``\n    operation to complete before translating the CTRL-C event into the\n    ``KeyboardInterrupt`` exception.\n\n    The only way to fix this seems to be to add our own \"console control\n    handler\" and perform some application-defined operation that will\n    unblock the ZMQ polling operation in order to force ZMQ to pass control\n    back to the Python interpreter.\n\n    This context manager performs all that Windows-y stuff, providing you\n    with a hook that is called when a CTRL-C event is intercepted.  This\n    hook allows you to unblock your ZMQ poll operation immediately, which\n    will then result in the expected ``KeyboardInterrupt`` exception.\n\n    Without this context manager, your ZMQ-based application will not\n    respond normally to CTRL-C events on Windows.  If a CTRL-C event occurs\n    while blocked on ZMQ socket polling, the translation to a\n    ``KeyboardInterrupt`` exception will be delayed until the I/O completes\n    and control returns to the Python interpreter (this may never happen if\n    you use an infinite timeout).\n\n    A no-op implementation is provided on non-Win32 systems to avoid the\n    application from having to conditionally use it.\n\n    Example usage:\n\n    .. sourcecode:: python\n\n       def stop_my_application():\n           # ...\n\n       with allow_interrupt(stop_my_application):\n           # main polling loop.\n\n    In a typical ZMQ application, you would use the \"self pipe trick\" to\n    send message to a ``PAIR`` socket in order to interrupt your blocking\n    socket polling operation.\n\n    In a Tornado event loop, you can use the ``IOLoop.stop`` method to\n    unblock your I/O loop.\n    ",
        "klass": "zmq.utils.win32.allow_interrupt",
        "module": "zmq"
    },
    {
        "base_classes": [
            "marshmallow.base.SchemaABC"
        ],
        "class_docstring": "Base schema class with which to define custom schemas.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import datetime as dt\n        from dataclasses import dataclass\n\n        from marshmallow import Schema, fields\n\n        @dataclass\n        class Album:\n            title: str\n            release_date: dt.date\n\n        class AlbumSchema(Schema):\n            title = fields.Str()\n            release_date = fields.Date()\n\n        album = Album(\"Beggars Banquet\", dt.date(1968, 12, 6))\n        schema = AlbumSchema()\n        data = schema.dump(album)\n        data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}\n\n    :param only: Whitelist of the declared fields to select when\n        instantiating the Schema. If None, all fields are used. Nested fields\n        can be represented with dot delimiters.\n    :param exclude: Blacklist of the declared fields to exclude\n        when instantiating the Schema. If a field appears in both `only` and\n        `exclude`, it is not used. Nested fields can be represented with dot\n        delimiters.\n    :param many: Should be set to `True` if ``obj`` is a collection\n        so that the object will be serialized to a list.\n    :param context: Optional context passed to :class:`fields.Method` and\n        :class:`fields.Function` fields.\n    :param load_only: Fields to skip during serialization (write-only fields)\n    :param dump_only: Fields to skip during deserialization (read-only fields)\n    :param partial: Whether to ignore missing fields and not require\n        any fields declared. Propagates down to ``Nested`` fields as well. If\n        its value is an iterable, only missing fields listed in that iterable\n        will be ignored. Use dot delimiters to specify nested fields.\n    :param unknown: Whether to exclude, include, or raise an error for unknown\n        fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n\n    .. versionchanged:: 3.0.0\n        `prefix` parameter removed.\n\n    .. versionchanged:: 2.0.0\n        `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of\n        `marshmallow.decorators.validates_schema`,\n        `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.\n        `__accessor__` and `__error_handler__` are deprecated. Implement the\n        `handle_error` and `get_attribute` methods instead.\n    ",
        "klass": "marshmallow.Schema",
        "module": "marshmallow"
    },
    {
        "base_classes": [
            "octavia.certificates.common.cert.Cert"
        ],
        "class_docstring": "Representation of a Cert based on the Barbican CertificateContainer.",
        "klass": "octavia.certificates.common.barbican.BarbicanCert",
        "module": "octavia"
    },
    {
        "base_classes": [
            "octavia.certificates.common.cert.Cert"
        ],
        "class_docstring": "Representation of a Cert for local storage.",
        "klass": "octavia.certificates.common.local.LocalCert",
        "module": "octavia"
    },
    {
        "base_classes": [
            "dask.callbacks.Callback"
        ],
        "class_docstring": " Use cache for computation\n\n    Examples\n    --------\n\n    >>> cache = Cache(1e9)          # doctest: +SKIP\n\n    The cache can be used locally as a context manager around ``compute`` or\n    ``get`` calls:\n\n    >>> with cache:                 # doctest: +SKIP\n    ...     result = x.compute()\n\n    You can also register a cache globally, so that it works for all\n    computations:\n\n    >>> cache.register()            # doctest: +SKIP\n    >>> cache.unregister()          # doctest: +SKIP\n    ",
        "klass": "dask.cache.Cache",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Base class for using the callback mechanism\n\n    Create a callback with functions of the following signatures:\n\n    >>> def start(dsk):\n    ...     pass\n    >>> def start_state(dsk, state):\n    ...     pass\n    >>> def pretask(key, dsk, state):\n    ...     pass\n    >>> def posttask(key, result, dsk, state, worker_id):\n    ...     pass\n    >>> def finish(dsk, state, failed):\n    ...     pass\n\n    You may then construct a callback object with any number of them\n\n    >>> cb = Callback(pretask=pretask, finish=finish)  # doctest: +SKIP\n\n    And use it either as a context manager over a compute/get call\n\n    >>> with cb:  # doctest: +SKIP\n    ...     x.compute()  # doctest: +SKIP\n\n    Or globally with the ``register`` method\n\n    >>> cb.register()  # doctest: +SKIP\n    >>> cb.unregister()  # doctest: +SKIP\n\n    Alternatively subclass the ``Callback`` class with your own methods.\n\n    >>> class PrintKeys(Callback):\n    ...     def _pretask(self, key, dask, state):\n    ...         print(\"Computing: {0}!\".format(repr(key)))\n\n    >>> with PrintKeys():  # doctest: +SKIP\n    ...     x.compute()  # doctest: +SKIP\n    ",
        "klass": "dask.callbacks.Callback",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Temporarily set configuration values within a context manager\n\n    Parameters\n    ----------\n    arg : mapping or None, optional\n        A mapping of configuration key-value pairs to set.\n    **kwargs :\n        Additional key-value pairs to set. If ``arg`` is provided, values set\n        in ``arg`` will be applied before those in ``kwargs``.\n        Double-underscores (``__``) in keyword arguments will be replaced with\n        ``.``, allowing nested values to be easily set.\n\n    Examples\n    --------\n    >>> import dask\n\n    Set ``'foo.bar'`` in a context, by providing a mapping.\n\n    >>> with dask.config.set({'foo.bar': 123}):\n    ...     pass\n\n    Set ``'foo.bar'`` in a context, by providing a keyword argument.\n\n    >>> with dask.config.set(foo__bar=123):\n    ...     pass\n\n    Set ``'foo.bar'`` globally.\n\n    >>> dask.config.set(foo__bar=123)  # doctest: +SKIP\n\n    See Also\n    --------\n    dask.config.get\n    ",
        "klass": "dask.config.set",
        "module": "dask"
    },
    {
        "base_classes": [
            "dask.callbacks.Callback"
        ],
        "class_docstring": "A profiler for dask execution at the scheduler cache level.\n\n    Records the following information for each task:\n        1. Key\n        2. Task\n        3. Size metric\n        4. Cache entry time in seconds since the epoch\n        5. Cache exit time in seconds since the epoch\n\n    Examples\n    --------\n\n    >>> from operator import add, mul\n    >>> from dask.threaded import get\n    >>> dsk = {'x': 1, 'y': (add, 'x', 10), 'z': (mul, 'y', 2)}\n    >>> with CacheProfiler() as prof:\n    ...     get(dsk, 'z')\n    22\n\n    >>> prof.results    # doctest: +SKIP\n    [CacheData('y', (add, 'x', 10), 1, 1435352238.48039, 1435352238.480655),\n     CacheData('z', (mul, 'y', 2), 1, 1435352238.480657, 1435352238.480803)]\n\n    The default is to count each task (``metric`` is 1 for all tasks). Other\n    functions may used as a metric instead through the ``metric`` keyword. For\n    example, the ``nbytes`` function found in ``cachey`` can be used to measure\n    the number of bytes in the cache.\n\n    >>> from cachey import nbytes    # doctest: +SKIP\n    >>> with CacheProfiler(metric=nbytes) as prof:  # doctest: +SKIP\n    ...     get(dsk, 'z')\n\n    The profiling results can be visualized in a bokeh plot using the\n    ``visualize`` method. Note that this requires bokeh to be installed.\n\n    >>> prof.visualize() # doctest: +SKIP\n\n    You can activate the profiler globally\n\n    >>> prof.register()  # doctest: +SKIP\n\n    If you use the profiler globally you will need to clear out old results\n    manually.\n\n    >>> prof.clear()\n\n    ",
        "klass": "dask.diagnostics.CacheProfiler",
        "module": "dask"
    },
    {
        "base_classes": [
            "dask.callbacks.Callback"
        ],
        "class_docstring": "A progress bar for dask.\n\n    Parameters\n    ----------\n    minimum : int, optional\n        Minimum time threshold in seconds before displaying a progress bar.\n        Default is 0 (always display)\n    width : int, optional\n        Width of the bar\n    dt : float, optional\n        Update resolution in seconds, default is 0.1 seconds\n\n    Examples\n    --------\n\n    Below we create a progress bar with a minimum threshold of 1 second before\n    displaying. For cheap computations nothing is shown:\n\n    >>> with ProgressBar(minimum=1.0):      # doctest: +SKIP\n    ...     out = some_fast_computation.compute()\n\n    But for expensive computations a full progress bar is displayed:\n\n    >>> with ProgressBar(minimum=1.0):      # doctest: +SKIP\n    ...     out = some_slow_computation.compute()\n    [########################################] | 100% Completed | 10.4 s\n\n    The duration of the last computation is available as an attribute\n\n    >>> pbar = ProgressBar()\n    >>> with pbar:                          # doctest: +SKIP\n    ...     out = some_computation.compute()\n    [########################################] | 100% Completed | 10.4 s\n    >>> pbar.last_duration                  # doctest: +SKIP\n    10.4\n\n    You can also register a progress bar so that it displays for all\n    computations:\n\n    >>> pbar = ProgressBar()                # doctest: +SKIP\n    >>> pbar.register()                     # doctest: +SKIP\n    >>> some_slow_computation.compute()     # doctest: +SKIP\n    [########################################] | 100% Completed | 10.4 s\n    ",
        "klass": "dask.diagnostics.ProgressBar",
        "module": "dask"
    },
    {
        "base_classes": [
            "dask.callbacks.Callback"
        ],
        "class_docstring": "A profiler for resource use.\n\n    Records the following each timestep\n        1. Time in seconds since the epoch\n        2. Memory usage in MB\n        3. % CPU usage\n\n    Examples\n    --------\n\n    >>> from operator import add, mul\n    >>> from dask.threaded import get\n    >>> dsk = {'x': 1, 'y': (add, 'x', 10), 'z': (mul, 'y', 2)}\n    >>> with ResourceProfiler() as prof:  # doctest: +SKIP\n    ...     get(dsk, 'z')\n    22\n\n    These results can be visualized in a bokeh plot using the ``visualize``\n    method. Note that this requires bokeh to be installed.\n\n    >>> prof.visualize() # doctest: +SKIP\n\n    You can activate the profiler globally\n\n    >>> prof.register()  # doctest: +SKIP\n\n    If you use the profiler globally you will need to clear out old results\n    manually.\n\n    >>> prof.clear()  # doctest: +SKIP\n\n    Note that when used as a context manager data will be collected throughout\n    the duration of the enclosed block. In contrast, when registered globally\n    data will only be collected while a dask scheduler is active.\n    ",
        "klass": "dask.diagnostics.ResourceProfiler",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Traverser interface for tasks.\n\n    Class for storing the state while performing a preorder-traversal of a\n    task.\n\n    Parameters\n    ----------\n    term : task\n        The task to be traversed\n\n    Attributes\n    ----------\n    term\n        The current element in the traversal\n    current\n        The head of the current element in the traversal. This is simply `head`\n        applied to the attribute `term`.\n    ",
        "klass": "dask.rewrite.Traverser",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(keepends=True)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(keepends=True)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print(''.join(result), end=\"\")\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    ",
        "klass": "difflib.Differ",
        "module": "difflib"
    },
    {
        "base_classes": [
            "hypothesis._settings.settings"
        ],
        "class_docstring": "A settings object controls a variety of parameters that are used in\n    falsification. These may control both the falsification strategy and the\n    details of the data that is generated.\n\n    Default values are picked up from the settings.default object and\n    changes made there will be picked up in newly created settings.\n    ",
        "klass": "hypothesis.settings",
        "module": "hypothesis"
    },
    {
        "base_classes": [
            "numbers.Rational"
        ],
        "class_docstring": "This class implements rational numbers.\n\n    In the two-argument form of the constructor, Fraction(8, 6) will\n    produce a rational number equivalent to 4/3. Both arguments must\n    be Rational. The numerator defaults to 0 and the denominator\n    defaults to 1 so that Fraction(3) == 3 and Fraction() == 0.\n\n    Fractions can also be constructed from:\n\n      - numeric strings similar to those accepted by the\n        float constructor (for example, '-2.3' or '1e10')\n\n      - strings of the form '123/456'\n\n      - float and Decimal instances\n\n      - other Rational instances (including integers)\n\n    ",
        "klass": "fractions.Fraction",
        "module": "fractions"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The context affects almost all operations and controls rounding,\nOver/Underflow, raising of exceptions and much more.  A new context\ncan be constructed as follows:\n\n    >>> c = Context(prec=28, Emin=-425000000, Emax=425000000,\n    ...             rounding=ROUND_HALF_EVEN, capitals=1, clamp=1,\n    ...             traps=[InvalidOperation, DivisionByZero, Overflow],\n    ...             flags=[])\n    >>>\n\n\n",
        "klass": "decimal.Context",
        "module": "decimal"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Construct a new Decimal object. 'value' can be an integer, string, tuple,\nor another Decimal object. If no value is given, return Decimal('0'). The\ncontext does not affect the conversion and is only passed to determine if\nthe InvalidOperation trap is active.\n\n",
        "klass": "decimal.Decimal",
        "module": "decimal"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "methodcaller(name, ...) --> methodcaller object\n\nReturn a callable object that calls the given method on its operand.\nAfter f = methodcaller('name'), the call f(r) returns r.name().\nAfter g = methodcaller('name', 'date', foo=1), the call g(r) returns\nr.name('date', foo=1).",
        "klass": "operator.methodcaller",
        "module": "operator"
    },
    {
        "base_classes": [
            "logging.Handler"
        ],
        "class_docstring": "\n    These are used to capture entries logged to the Python logging\n    framework and make assertions about what was logged.\n\n    :param names: A string (or tuple of strings) containing the dotted name(s)\n                  of loggers to capture. By default, the root logger is\n                  captured.\n\n    :param install: If `True`, the :class:`LogCapture` will be\n                    installed as part of its instantiation.\n\n    :param propagate: If specified, any captured loggers will have their\n                      `propagate` attribute set to the supplied value. This can\n                      be used to prevent propagation from a child logger to a\n                      parent logger that has configured handlers.\n\n    :param attributes:\n\n      The sequence of attribute names to return for each record or a callable\n      that extracts a row from a record.\n\n      If a sequence of attribute names, those attributes will be taken from the\n      :class:`~logging.LogRecord`. If an attribute is callable, the value\n      used will be the result of calling it. If an attribute is missing,\n      ``None`` will be used in its place.\n\n      If a callable, it will be called with the :class:`~logging.LogRecord`\n      and the value returned will be used as the row..\n\n    :param recursive_check:\n\n      If ``True``, log messages will be compared recursively by\n      :meth:`LogCapture.check`.\n    ",
        "klass": "testfixtures.LogCapture",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A context manager for capturing output to the\n    :attr:`sys.stdout` and :attr:`sys.stderr` streams.\n\n    :param separate: If ``True``, ``stdout`` and ``stderr`` will be captured\n                     separately and their expected values must be passed to\n                     :meth:`~OutputCapture.compare`.\n\n    :param fd: If ``True``, the underlying file descriptors will be captured,\n               rather than just the attributes on :mod:`sys`. This allows\n               you to capture things like subprocesses that write directly\n               to the file descriptors, but is more invasive, so only use it\n               when you need it.\n\n    :param strip_whitespace:\n        When ``True``, which is the default, leading and training whitespace\n        is trimmed from both the expected and actual values when comparing.\n\n    .. note:: If ``separate`` is passed as ``True``,\n              :attr:`OutputCapture.captured` will be an empty string.\n    ",
        "klass": "testfixtures.OutputCapture",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\nA context manager that uses a :class:`Replacer` to replace a single target.\n\n:param target: A string containing the dotted-path to the\n               object to be replaced. This path may specify a\n               module in a package, an attribute of a module,\n               or any attribute of something contained within\n               a module.\n\n:param replacement: The object to use as a replacement.\n\n:param strict: When `True`, an exception will be raised if an\n               attempt is made to replace an object that does\n               not exist.\n",
        "klass": "testfixtures.Replace",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    These are used to manage the mocking out of objects so that units\n    of code can be tested without having to rely on their normal\n    dependencies.\n    ",
        "klass": "testfixtures.Replacer",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This context manager is used to assert that an exception is raised\n    within the context it is managing.\n    \n\n    :param exception: This can be one of the following:\n\n                      * `None`, indicating that an exception must be\n                        raised, but the type is unimportant.\n\n                      * An exception class, indicating that the type\n                        of the exception is important but not the\n                        parameters it is created with.\n\n                      * An exception instance, indicating that an\n                        exception exactly matching the one supplied\n                        should be raised.\n\n    :param unless: Can be passed a boolean that, when ``True`` indicates that\n                   no exception is expected. This is useful when checking\n                   that exceptions are only raised on certain versions of\n                   Python.\n",
        "klass": "testfixtures.ShouldRaise",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class representing a temporary directory on disk.\n\n    :param ignore: A sequence of strings containing regular expression\n                   patterns that match filenames that should be\n                   ignored by the :class:`TempDirectory` listing and\n                   checking methods.\n\n    :param create: If `True`, the temporary directory will be created\n                   as part of class instantiation.\n\n    :param path: If passed, this should be a string containing a\n                 physical path to use as the temporary directory. When\n                 passed, :class:`TempDirectory` will not create a new\n                 directory to use.\n\n    :param encoding: A default encoding to use for :meth:`read` and\n                     :meth:`write` operations when the ``encoding`` parameter\n                     is not passed to those methods.\n    ",
        "klass": "testfixtures.tempdirectory.TempDirectory",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class representing a temporary directory on disk.\n\n    :param ignore: A sequence of strings containing regular expression\n                   patterns that match filenames that should be\n                   ignored by the :class:`TempDirectory` listing and\n                   checking methods.\n\n    :param create: If `True`, the temporary directory will be created\n                   as part of class instantiation.\n\n    :param path: If passed, this should be a string containing a\n                 physical path to use as the temporary directory. When\n                 passed, :class:`TempDirectory` will not create a new\n                 directory to use.\n\n    :param encoding: A default encoding to use for :meth:`read` and\n                     :meth:`write` operations when the ``encoding`` parameter\n                     is not passed to those methods.\n    ",
        "klass": "testfixtures.TempDirectory",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "unittest.mock.CallableMixin",
            "unittest.mock.NonCallableMock"
        ],
        "class_docstring": "\n    Create a new `Mock` object. `Mock` takes several optional arguments\n    that specify the behaviour of the Mock object:\n\n    * `spec`: This can be either a list of strings or an existing object (a\n      class or instance) that acts as the specification for the mock object. If\n      you pass in an object then a list of strings is formed by calling dir on\n      the object (excluding unsupported magic attributes and methods). Accessing\n      any attribute not in this list will raise an `AttributeError`.\n\n      If `spec` is an object (rather than a list of strings) then\n      `mock.__class__` returns the class of the spec object. This allows mocks\n      to pass `isinstance` tests.\n\n    * `spec_set`: A stricter variant of `spec`. If used, attempting to *set*\n      or get an attribute on the mock that isn't on the object passed as\n      `spec_set` will raise an `AttributeError`.\n\n    * `side_effect`: A function to be called whenever the Mock is called. See\n      the `side_effect` attribute. Useful for raising exceptions or\n      dynamically changing return values. The function is called with the same\n      arguments as the mock, and unless it returns `DEFAULT`, the return\n      value of this function is used as the return value.\n\n      If `side_effect` is an iterable then each call to the mock will return\n      the next value from the iterable. If any of the members of the iterable\n      are exceptions they will be raised instead of returned.\n\n    * `return_value`: The value returned when the mock is called. By default\n      this is a new Mock (created on first access). See the\n      `return_value` attribute.\n\n    * `wraps`: Item for the mock object to wrap. If `wraps` is not None then\n      calling the Mock will pass the call through to the wrapped object\n      (returning the real result). Attribute access on the mock will return a\n      Mock object that wraps the corresponding attribute of the wrapped object\n      (so attempting to access an attribute that doesn't exist will raise an\n      `AttributeError`).\n\n      If the mock has an explicit `return_value` set then calls are not passed\n      to the wrapped object and the `return_value` is returned instead.\n\n    * `name`: If the mock has a name then it will be used in the repr of the\n      mock. This can be useful for debugging. The name is propagated to child\n      mocks.\n\n    Mocks can also be called with arbitrary keyword arguments. These will be\n    used to set attributes on the mock after it is created.\n    ",
        "klass": "unittest.mock.Mock",
        "module": "unittest"
    },
    {
        "base_classes": [
            "unittest.mock.Mock"
        ],
        "class_docstring": "\n    A mock intended to be used as a property, or other descriptor, on a class.\n    `PropertyMock` provides `__get__` and `__set__` methods so you can specify\n    a return value when it is fetched.\n\n    Fetching a `PropertyMock` instance from an object calls the mock, with\n    no args. Setting it calls the mock with the value being set.\n    ",
        "klass": "unittest.mock.PropertyMock",
        "module": "unittest"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A specialised mock for testing use of :class:`subprocess.Popen`.\n    An instance of this class can be used in place of the\n    :class:`subprocess.Popen` and is often inserted where it's needed using\n    :func:`unittest.mock.patch` or a :class:`~testfixtures.Replacer`.\n    ",
        "klass": "testfixtures.popen.MockPopen",
        "module": "testfixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The user has the attributes\n      login, realm and resolver.\n    Usually a user can be found via \"login@realm\".\n    \n    A user object with an empty login and realm should not exist,\n    whereas a user object could have an empty resolver.\n    ",
        "klass": "privacyidea.lib.user.User",
        "module": "privacyidea"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Store a sequence of fields, reading multipart/form-data.\n\n    This class provides naming, typing, files stored on disk, and\n    more.  At the top level, it is accessible like a dictionary, whose\n    keys are the field names.  (Note: None can occur as a field name.)\n    The items are either a Python list (if there's multiple values) or\n    another FieldStorage or MiniFieldStorage object.  If it's a single\n    object, it has the following attributes:\n\n    name: the field name, if specified; otherwise None\n\n    filename: the filename, if specified; otherwise None; this is the\n        client side filename, *not* the file name on which it is\n        stored (that's a temporary file you don't deal with)\n\n    value: the value as a *string*; for file uploads, this\n        transparently reads the file every time you request the value\n        and returns *bytes*\n\n    file: the file(-like) object from which you can read the data *as\n        bytes* ; None if the data is stored a simple string\n\n    type: the content-type, or None if not specified\n\n    type_options: dictionary of options specified on the content-type\n        line\n\n    disposition: content-disposition, or None if not specified\n\n    disposition_options: dictionary of corresponding options\n\n    headers: a dictionary(-like) object (sometimes email.message.Message or a\n        subclass thereof) containing *all* headers\n\n    The class is subclassable, mostly for the purpose of overriding\n    the make_file() method, which is called internally to come up with\n    a file open for reading and writing.  This makes it possible to\n    override the default choice of storing all files in a temporary\n    directory and unlinking them as soon as they have been opened.\n\n    ",
        "klass": "cgi.FieldStorage",
        "module": "cgi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The :class:`FileStorage` class is a thin wrapper over incoming files.\n    It is used by the request object to represent uploaded files.  All the\n    attributes of the wrapper stream are proxied by the file storage so\n    it's possible to do ``storage.read()`` instead of the long form\n    ``storage.stream.read()``.\n    ",
        "klass": "werkzeug.datastructures.FileStorage",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "bs4.element.Tag"
        ],
        "class_docstring": "\n    This class defines the basic interface called by the tree builders.\n\n    These methods will be called by the parser:\n      reset()\n      feed(markup)\n\n    The tree builder may call these methods from its feed() implementation:\n      handle_starttag(name, attrs) # See note about return value\n      handle_endtag(name)\n      handle_data(data) # Appends to the current data node\n      endData(containerClass) # Ends the current data node\n\n    No matter how complicated the underlying parser is, you should be\n    able to build a tree using 'start tag' events, 'end tag' events,\n    'data' events, and \"done with data\" events.\n\n    If you encounter an empty-element tag (aka a self-closing tag,\n    like HTML's <br> tag), call handle_starttag and then\n    handle_endtag.\n    ",
        "klass": "bs4.BeautifulSoup",
        "module": "bs4"
    },
    {
        "base_classes": [
            "sqlalchemy.orm.session._SessionClassMethods"
        ],
        "class_docstring": "A configurable :class:`.Session` factory.\n\n    The :class:`.sessionmaker` factory generates new\n    :class:`.Session` objects when called, creating them given\n    the configurational arguments established here.\n\n    e.g.::\n\n        # global scope\n        Session = sessionmaker(autoflush=False)\n\n        # later, in a local scope, create and use a session:\n        sess = Session()\n\n    Any keyword arguments sent to the constructor itself will override the\n    \"configured\" keywords::\n\n        Session = sessionmaker()\n\n        # bind an individual session to a connection\n        sess = Session(bind=connection)\n\n    The class also includes a method :meth:`.configure`, which can\n    be used to specify additional keyword arguments to the factory, which\n    will take effect for subsequent :class:`.Session` objects generated.\n    This is usually used to associate one or more :class:`.Engine` objects\n    with an existing :class:`.sessionmaker` factory before it is first\n    used::\n\n        # application starts\n        Session = sessionmaker()\n\n        # ... later\n        engine = create_engine('sqlite:///foo.db')\n        Session.configure(bind=engine)\n\n        sess = Session()\n\n    .. seealso:\n\n        :ref:`session_getting` - introductory text on creating\n        sessions using :class:`.sessionmaker`.\n\n    ",
        "klass": "sqlalchemy.orm.session.sessionmaker",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "privacyidea.lib.token.TokenClass"
        ],
        "class_docstring": "\n    This Token does use a fixed Password as the OTP value.\n    In addition, the OTP PIN can be used with this token.\n    This Token can be used for a scenario like losttoken\n    ",
        "klass": "privacyidea.lib.tokens.passwordtoken.PasswordTokenClass",
        "module": "privacyidea"
    },
    {
        "base_classes": [
            "privacyidea.lib.tokens.passwordtoken.PasswordTokenClass"
        ],
        "class_docstring": "\n    Token to implement a registration code.\n    It can be used to create a registration code or a \"TAN\" which can be used\n    once by a user to authenticate somewhere. After this registration code is\n    used, the token is automatically deleted.\n\n    The idea is to provide a workflow, where the user can get a registration code\n    by e.g. postal mail and then use this code as the initial first factor to\n    authenticate to the UI to enroll real tokens.\n\n    A registration code can be created by an administrative task with the\n    token/init api like this:\n\n      **Example Authentication Request**:\n\n        .. sourcecode:: http\n\n           POST /token/init HTTP/1.1\n           Host: example.com\n           Accept: application/json\n\n           type=registration\n           user=cornelius\n           realm=realm1\n\n      **Example response**:\n\n           .. sourcecode:: http\n\n               HTTP/1.1 200 OK\n               Content-Type: application/json\n\n               {\n                  \"detail\": {\n                    \"registrationcode\": \"12345808124095097608\"\n                  },\n                  \"id\": 1,\n                  \"jsonrpc\": \"2.0\",\n                  \"result\": {\n                    \"status\": true,\n                    \"value\": true\n                  },\n                  \"version\": \"privacyIDEA unknown\"\n                }\n\n    ",
        "klass": "privacyidea.lib.tokens.registrationtoken.RegistrationTokenClass",
        "module": "privacyidea"
    },
    {
        "base_classes": [
            "privacyidea.lib.token.TokenClass"
        ],
        "class_docstring": "\n    The Remote token forwards an authentication request to another privacyIDEA\n    server. The request can be forwarded to a user on the other server or to\n    a serial number on the other server. The PIN can be checked on the local\n    privacyIDEA server or on the remote server.\n\n    Using the Remote token you can assign one physical token to many\n    different users.\n    ",
        "klass": "privacyidea.lib.tokens.remotetoken.RemoteTokenClass",
        "module": "privacyidea"
    },
    {
        "base_classes": [
            "privacyidea.lib.token.TokenClass"
        ],
        "class_docstring": "\n    This is a simple pass token.\n    It does have no OTP component. The OTP checking will always\n    succeed. Of course, an OTP PIN can be used.\n    ",
        "klass": "privacyidea.lib.tokens.spasstoken.SpassTokenClass",
        "module": "privacyidea"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An instance of this class represents a loaded dll/shared\n    library, exporting functions using the standard C calling\n    convention (named 'cdecl' on Windows).\n\n    The exported functions can be accessed as attributes, or by\n    indexing with the function name.  Examples:\n\n    <obj>.qsort -> callable object\n    <obj>['qsort'] -> callable object\n\n    Calling the functions releases the Python GIL during the call and\n    reacquires it afterwards.\n    ",
        "klass": "ctypes.CDLL",
        "module": "ctypes"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "time([hour[, minute[, second[, microsecond[, tzinfo]]]]]) --> a time object\n\nAll arguments are optional. tzinfo may be None, or an instance of\na tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "datetime.time",
        "module": "datetime"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Activation Key entity.",
        "klass": "nailgun.entities.ActivationKey",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Architecture entity.",
        "klass": "nailgun.entities.Architecture",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Compute Profile entity.",
        "klass": "nailgun.entities.ComputeProfile",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Content View entity.",
        "klass": "nailgun.entities.ContentView",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Domain entity.",
        "klass": "nailgun.entities.Domain",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Environment entity.",
        "klass": "nailgun.entities.Environment",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Filter entity.",
        "klass": "nailgun.entities.Filter",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entities.ContentCredential"
        ],
        "class_docstring": "A representation of a GPG Key entity.",
        "klass": "nailgun.entities.GPGKey",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntityUpdateMixin",
            "nailgun.entity_mixins.EntitySearchMixin"
        ],
        "class_docstring": "A representation of a Host entity.",
        "klass": "nailgun.entities.Host",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Host Group entity.",
        "klass": "nailgun.entities.HostGroup",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Location entity.",
        "klass": "nailgun.entities.Location",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Operating System entity.\n\n    ``major`` is listed as a string field in the API docs, but only numeric\n    values are accepted, and they may be no longer than 5 digits long. Also see\n    `Bugzilla #1122261 <https://bugzilla.redhat.com/show_bug.cgi?id=1122261>`_.\n\n    ``title`` field is valid despite not being listed in the API docs. This may\n    be changed in future as both ``title`` and ``description`` fields share\n    similar purpose. See `Bugzilla #1290359\n    <https://bugzilla.redhat.com/show_bug.cgi?id=1290359>`_ for more details.\n    ",
        "klass": "nailgun.entities.OperatingSystem",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of an Organization entity.",
        "klass": "nailgun.entities.Organization",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Product entity.",
        "klass": "nailgun.entities.Product",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a RHCI deployment entity.",
        "klass": "nailgun.entities.RHCIDeployment",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Repository entity.",
        "klass": "nailgun.entities.Repository",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Role entity.",
        "klass": "nailgun.entities.Role",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Setting entity.",
        "klass": "nailgun.entities.Setting",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Smart Proxy entity.",
        "klass": "nailgun.entities.SmartProxy",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a Subnet entity.",
        "klass": "nailgun.entities.Subnet",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "nailgun.entity_mixins.Entity",
            "nailgun.entity_mixins.EntityCreateMixin",
            "nailgun.entity_mixins.EntityDeleteMixin",
            "nailgun.entity_mixins.EntityReadMixin",
            "nailgun.entity_mixins.EntitySearchMixin",
            "nailgun.entity_mixins.EntityUpdateMixin"
        ],
        "class_docstring": "A representation of a User entity.\n\n    The LDAP authentication source with an ID of 1 is internal. It is nearly\n    guaranteed to exist and be functioning. Thus, ``auth_source`` is set to \"1\"\n    by default for a practical reason: it is much easier to use internal\n    authentication than to spawn LDAP authentication servers for each new user.\n\n    ",
        "klass": "nailgun.entities.User",
        "module": "nailgun"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents the result of an asynchronous computation.",
        "klass": "concurrent.futures.Future",
        "module": "concurrent"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Task Signature.\n\n    Class that wraps the arguments and execution options\n    for a single task invocation.\n\n    Used as the parts in a :class:`group` and other constructs,\n    or to pass tasks around as callbacks while being compatible\n    with serializers with a strict type subset.\n\n    Signatures can also be created from tasks:\n\n    - Using the ``.signature()`` method that has the same signature\n      as ``Task.apply_async``:\n\n        .. code-block:: pycon\n\n            >>> add.signature(args=(1,), kwargs={'kw': 2}, options={})\n\n    - or the ``.s()`` shortcut that works for star arguments:\n\n        .. code-block:: pycon\n\n            >>> add.s(1, kw=2)\n\n    - the ``.s()`` shortcut does not allow you to specify execution options\n      but there's a chaning `.set` method that returns the signature:\n\n        .. code-block:: pycon\n\n            >>> add.s(2, 2).set(countdown=10).set(expires=30).delay()\n\n    Note:\n        You should use :func:`~celery.signature` to create new signatures.\n        The ``Signature`` class is the type returned by that function and\n        should be used for ``isinstance`` checks for signatures.\n\n    See Also:\n        :ref:`guide-canvas` for the complete guide.\n\n    Arguments:\n        task (Union[Type[celery.app.task.Task], str]): Either a task\n            class/instance, or the name of a task.\n        args (Tuple): Positional arguments to apply.\n        kwargs (Dict): Keyword arguments to apply.\n        options (Dict): Additional options to :meth:`Task.apply_async`.\n\n    Note:\n        If the first argument is a :class:`dict`, the other\n        arguments will be ignored and the values in the dict will be used\n        instead::\n\n            >>> s = signature('tasks.add', args=(2, 2))\n            >>> signature(s)\n            {'task': 'tasks.add', args=(2, 2), kwargs={}, options={}}\n    ",
        "klass": "celery.canvas.Signature",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.canvas._chain"
        ],
        "class_docstring": "Chain tasks together.\n\n    Each tasks follows one another,\n    by being applied as a callback of the previous task.\n\n    Note:\n        If called with only one argument, then that argument must\n        be an iterable of tasks to chain: this allows us\n        to use generator expressions.\n\n    Example:\n        This is effectively :math:`((2 + 2) + 4)`:\n\n        .. code-block:: pycon\n\n            >>> res = chain(add.s(2, 2), add.s(4))()\n            >>> res.get()\n            8\n\n        Calling a chain will return the result of the last task in the chain.\n        You can get to the other tasks by following the ``result.parent``'s:\n\n        .. code-block:: pycon\n\n            >>> res.parent.get()\n            4\n\n        Using a generator expression:\n\n        .. code-block:: pycon\n\n            >>> lazy_chain = chain(add.s(i) for i in range(10))\n            >>> res = lazy_chain(3)\n\n    Arguments:\n        *tasks (Signature): List of task signatures to chain.\n            If only one argument is passed and that argument is\n            an iterable, then that'll be used as the list of signatures\n            to chain instead.  This means that you can use a generator\n            expression.\n\n    Returns:\n        ~celery.chain: A lazy signature that can be called to apply the first\n            task in the chain.  When that task succeeed the next task in the\n            chain is applied, and so on.\n    ",
        "klass": "celery.canvas.chain",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.canvas._chain"
        ],
        "class_docstring": "Chain tasks together.\n\n    Each tasks follows one another,\n    by being applied as a callback of the previous task.\n\n    Note:\n        If called with only one argument, then that argument must\n        be an iterable of tasks to chain: this allows us\n        to use generator expressions.\n\n    Example:\n        This is effectively :math:`((2 + 2) + 4)`:\n\n        .. code-block:: pycon\n\n            >>> res = chain(add.s(2, 2), add.s(4))()\n            >>> res.get()\n            8\n\n        Calling a chain will return the result of the last task in the chain.\n        You can get to the other tasks by following the ``result.parent``'s:\n\n        .. code-block:: pycon\n\n            >>> res.parent.get()\n            4\n\n        Using a generator expression:\n\n        .. code-block:: pycon\n\n            >>> lazy_chain = chain(add.s(i) for i in range(10))\n            >>> res = lazy_chain(3)\n\n    Arguments:\n        *tasks (Signature): List of task signatures to chain.\n            If only one argument is passed and that argument is\n            an iterable, then that'll be used as the list of signatures\n            to chain instead.  This means that you can use a generator\n            expression.\n\n    Returns:\n        ~celery.chain: A lazy signature that can be called to apply the first\n            task in the chain.  When that task succeeed the next task in the\n            chain is applied, and so on.\n    ",
        "klass": "celery.chain",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.canvas.Signature"
        ],
        "class_docstring": "Barrier synchronization primitive.\n\n    A chord consists of a header and a body.\n\n    The header is a group of tasks that must complete before the callback is\n    called.  A chord is essentially a callback for a group of tasks.\n\n    The body is applied with the return values of all the header\n    tasks as a list.\n\n    Example:\n\n        The chord:\n\n        .. code-block:: pycon\n\n            >>> res = chord([add.s(2, 2), add.s(4, 4)])(sum_task.s())\n\n        is effectively :math:`\\Sigma ((2 + 2) + (4 + 4))`:\n\n        .. code-block:: pycon\n\n            >>> res.get()\n            12\n    ",
        "klass": "celery.canvas.chord",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.canvas.Signature"
        ],
        "class_docstring": "Barrier synchronization primitive.\n\n    A chord consists of a header and a body.\n\n    The header is a group of tasks that must complete before the callback is\n    called.  A chord is essentially a callback for a group of tasks.\n\n    The body is applied with the return values of all the header\n    tasks as a list.\n\n    Example:\n\n        The chord:\n\n        .. code-block:: pycon\n\n            >>> res = chord([add.s(2, 2), add.s(4, 4)])(sum_task.s())\n\n        is effectively :math:`\\Sigma ((2 + 2) + (4 + 4))`:\n\n        .. code-block:: pycon\n\n            >>> res.get()\n            12\n    ",
        "klass": "celery.chord",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.canvas.Signature"
        ],
        "class_docstring": "Creates a group of tasks to be executed in parallel.\n\n    A group is lazy so you must call it to take action and evaluate\n    the group.\n\n    Note:\n        If only one argument is passed, and that argument is an iterable\n        then that'll be used as the list of tasks instead: this\n        allows us to use ``group`` with generator expressions.\n\n    Example:\n        >>> lazy_group = group([add.s(2, 2), add.s(4, 4)])\n        >>> promise = lazy_group()  # <-- evaluate: returns lazy result.\n        >>> promise.get()  # <-- will wait for the task to return\n        [4, 8]\n\n    Arguments:\n        *tasks (List[Signature]): A list of signatures that this group will\n            call. If there's only one argument, and that argument is an\n            iterable, then that'll define the list of signatures instead.\n        **options (Any): Execution options applied to all tasks\n            in the group.\n\n    Returns:\n        ~celery.group: signature that when called will then call all of the\n            tasks in the group (and return a :class:`GroupResult` instance\n            that can be used to inspect the state of the group).\n    ",
        "klass": "celery.group",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Message consumer.\n\n    Arguments:\n        channel (kombu.Connection, ChannelT): see :attr:`channel`.\n        queues (Sequence[kombu.Queue]): see :attr:`queues`.\n        no_ack (bool): see :attr:`no_ack`.\n        auto_declare (bool): see :attr:`auto_declare`\n        callbacks (Sequence[Callable]): see :attr:`callbacks`.\n        on_message (Callable): See :attr:`on_message`\n        on_decode_error (Callable): see :attr:`on_decode_error`.\n        prefetch_count (int): see :attr:`prefetch_count`.\n    ",
        "klass": "kombu.messaging.Consumer",
        "module": "kombu"
    },
    {
        "base_classes": [
            "logging.FileHandler"
        ],
        "class_docstring": "\n    A handler for logging to a file, which watches the file\n    to see if it has changed while in use. This can happen because of\n    usage of programs such as newsyslog and logrotate which perform\n    log file rotation. This handler, intended for use under Unix,\n    watches the file to see if it has changed since the last emit.\n    (A file has changed if its device or inode have changed.)\n    If it has changed, the old file stream is closed, and the file\n    opened to get a new stream.\n\n    This handler is not appropriate for use under Windows, because\n    under Windows open files cannot be moved or renamed - logging\n    opens the files with exclusive locks - and so there is no need\n    for such a handler. Furthermore, ST_INO is not supported under\n    Windows; stat always returns zero for this value.\n\n    This handler is based on a suggestion and patch by Chad J.\n    Schroeder.\n    ",
        "klass": "logging.handlers.WatchedFileHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Terminal colored text.\n\n    Example:\n        >>> c = colored(enabled=True)\n        >>> print(str(c.red('the quick '), c.blue('brown ', c.bold('fox ')),\n        ...       c.magenta(c.underline('jumps over')),\n        ...       c.yellow(' the lazy '),\n        ...       c.green('dog ')))\n    ",
        "klass": "celery.utils.term.colored",
        "module": "celery"
    },
    {
        "base_classes": [
            "collections.abc.MutableSequence"
        ],
        "class_docstring": "A more or less complete user-defined wrapper around list objects.",
        "klass": "collections.UserList",
        "module": "collections"
    },
    {
        "base_classes": [
            "celery.backends.base.BaseKeyValueStoreBackend",
            "celery.backends.base.SyncBackendMixin"
        ],
        "class_docstring": "Result backend base class for key/value stores.",
        "klass": "celery.backends.base.KeyValueStoreBackend",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.result.ResultSet"
        ],
        "class_docstring": "Like :class:`ResultSet`, but with an associated id.\n\n    This type is returned by :class:`~celery.group`.\n\n    It enables inspection of the tasks state and return values as\n    a single entity.\n\n    Arguments:\n        id (str): The id of the group.\n        results (Sequence[AsyncResult]): List of result instances.\n        parent (ResultBase): Parent result of this group.\n    ",
        "klass": "celery.result.GroupResult",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for command-line applications.\n\n    Arguments:\n        app (Celery): The app to use.\n        get_app (Callable): Fucntion returning the current app\n            when no app provided.\n    ",
        "klass": "celery.bin.base.Command",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Return a count object whose .__next__() method returns consecutive values.\n\nEquivalent to:\n    def count(firstval=0, step=1):\n        x = firstval\n        while 1:\n            yield x\n            x += step",
        "klass": "itertools.count",
        "module": "itertools"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Start the beat periodic task scheduler.\n\n    Examples:\n        .. code-block:: console\n\n            $ celery beat -l info\n            $ celery beat -s /var/run/celery/beat-schedule --detach\n            $ celery beat -S django\n\n    The last example requires the :pypi:`django-celery-beat` extension\n    package found on PyPI.\n    ",
        "klass": "celery.bin.beat.beat",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Base class for commands.",
        "klass": "celery.bin.celery.CeleryCommand",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Show help screen and exit.",
        "klass": "celery.bin.celery.help",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.control._RemoteControl"
        ],
        "class_docstring": "Inspect the worker at runtime.\n\n    Availability: RabbitMQ (AMQP) and Redis transports.\n\n    Examples:\n        .. code-block:: console\n\n            $ celery inspect active --timeout=5\n            $ celery inspect scheduled -d worker1@example.com\n            $ celery inspect revoked -d w1@e.com,w2@e.com\n    ",
        "klass": "celery.bin.celery.inspect",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Get info from broker.\n\n    Note:\n       For RabbitMQ the management plugin is required.\n\n    Example:\n        .. code-block:: console\n\n            $ celery list bindings\n    ",
        "klass": "celery.bin.list.list_",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Get info from broker.\n\n    Note:\n       For RabbitMQ the management plugin is required.\n\n    Example:\n        .. code-block:: console\n\n            $ celery list bindings\n    ",
        "klass": "celery.bin.celery.list_",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Migrate tasks from one broker to another.\n\n    Warning:\n        This command is experimental, make sure you have a backup of\n        the tasks before you continue.\n\n    Example:\n        .. code-block:: console\n\n            $ celery migrate amqp://A.example.com amqp://guest@B.example.com//\n            $ celery migrate redis://localhost amqp://guest@localhost//\n    ",
        "klass": "celery.bin.migrate.migrate",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Migrate tasks from one broker to another.\n\n    Warning:\n        This command is experimental, make sure you have a backup of\n        the tasks before you continue.\n\n    Example:\n        .. code-block:: console\n\n            $ celery migrate amqp://A.example.com amqp://guest@B.example.com//\n            $ celery migrate redis://localhost amqp://guest@localhost//\n    ",
        "klass": "celery.bin.celery.migrate",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Start worker instance.\n\n    Examples:\n        .. code-block:: console\n\n            $ celery worker --app=proj -l info\n            $ celery worker -A proj -l info -Q hipri,lopri\n\n            $ celery worker -A proj --concurrency=4\n            $ celery worker -A proj --concurrency=1000 -P eventlet\n            $ celery worker --autoscale=10,0\n    ",
        "klass": "celery.bin.worker.worker",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.bin.base.Command"
        ],
        "class_docstring": "Start worker instance.\n\n    Examples:\n        .. code-block:: console\n\n            $ celery worker --app=proj -l info\n            $ celery worker -A proj -l info -Q hipri,lopri\n\n            $ celery worker -A proj --concurrency=4\n            $ celery worker -A proj --concurrency=1000 -P eventlet\n            $ celery worker --autoscale=10,0\n    ",
        "klass": "celery.bin.celery.worker",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Barrier.\n\n    Synchronization primitive to call a callback after a list\n    of promises have been fulfilled.\n\n    Example:\n\n    .. code-block:: python\n\n        # Request supports the .then() method.\n        p1 = http.Request('http://a')\n        p2 = http.Request('http://b')\n        p3 = http.Request('http://c')\n        requests = [p1, p2, p3]\n\n        def all_done():\n            pass  # all requests complete\n\n        b = barrier(requests).then(all_done)\n\n        # oops, we forgot we want another request\n        b.add(http.Request('http://d'))\n\n    Note that you cannot add new promises to a barrier after\n    the barrier is fulfilled.\n    ",
        "klass": "vine.synchronization.barrier",
        "module": "vine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Promise of future evaluation.\n\n    This is a special implementation of promises in that it can\n    be used both for \"promise of a value\" and lazy evaluation.\n    The biggest upside for this is that everything in a promise can also be\n    a promise, e.g. filters, callbacks and errbacks can all be promises.\n\n    Usage examples:\n\n    .. code-block:: python\n\n        >>> from __future__ import print_statement  # noqa\n        >>> p = promise()\n        >>> p.then(promise(print, ('OK',)))  # noqa\n        >>> p.on_error = promise(print, ('ERROR',))  # noqa\n        >>> p(20)\n        OK, 20\n        >>> p.then(promise(print, ('hello',)))  # noqa\n        hello, 20\n\n\n        >>> p.throw(KeyError('foo'))\n        ERROR, KeyError('foo')\n\n\n        >>> p2 = promise()\n        >>> p2.then(print)  # noqa\n        >>> p2.cancel()\n        >>> p(30)\n\n    Example:\n\n    .. code-block:: python\n\n        from vine import promise, wrap\n\n        class Protocol(object):\n\n            def __init__(self):\n                self.buffer = []\n\n            def receive_message(self):\n                return self.read_header().then(\n                    self.read_body).then(\n                        wrap(self.prepare_body))\n\n            def read(self, size, callback=None):\n                callback = callback or promise()\n                tell_eventloop_to_read(size, callback)\n                return callback\n\n            def read_header(self, callback=None):\n                return self.read(4, callback)\n\n            def read_body(self, header, callback=None):\n                body_size, = unpack('>L', header)\n                return self.read(body_size, callback)\n\n            def prepare_body(self, value):\n                self.buffer.append(value)\n    ",
        "klass": "vine.promises.promise",
        "module": "vine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Task pool.",
        "klass": "celery.concurrency.base.BasePool",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.concurrency.base.BasePool"
        ],
        "class_docstring": "Eventlet Task Pool.",
        "klass": "celery.concurrency.eventlet.TaskPool",
        "module": "celery"
    },
    {
        "base_classes": [
            "kombu.asynchronous.timer.Timer"
        ],
        "class_docstring": "Eventlet Timer.",
        "klass": "celery.concurrency.eventlet.Timer",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.concurrency.base.BasePool"
        ],
        "class_docstring": "GEvent Pool.",
        "klass": "celery.concurrency.gevent.TaskPool",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.concurrency.base.BasePool"
        ],
        "class_docstring": "Solo task pool (blocking, inline, fast).",
        "klass": "celery.concurrency.solo.TaskPool",
        "module": "celery"
    },
    {
        "base_classes": [
            "celery.result.ResultBase"
        ],
        "class_docstring": "Query task state.\n\n    Arguments:\n        id (str): See :attr:`id`.\n        backend (Backend): See :attr:`backend`.\n    ",
        "klass": "celery.result.AsyncResult",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Record event snapshots.",
        "klass": "celery.events.snapshot.Polaroid",
        "module": "celery"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "Timer thread.\n\n    Note:\n        This is only used for transports not supporting AsyncIO.\n    ",
        "klass": "celery.utils.timer2.Timer",
        "module": "celery"
    },
    {
        "base_classes": [
            "queue.Queue"
        ],
        "class_docstring": "Variant of Queue that retrieves most recently added entries first.",
        "klass": "queue.LifoQueue",
        "module": "queue"
    },
    {
        "base_classes": [
            "_io.StringIO"
        ],
        "class_docstring": "StringIO that takes bytes or str.",
        "klass": "celery.five.WhateverIO",
        "module": "vine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "map(func, *iterables) --> map object\n\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.",
        "klass": "map",
        "module": "map"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Task request variables (Task.request).",
        "klass": "celery.task.base.Context",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Async timer implementation.",
        "klass": "kombu.asynchronous.timer.Timer",
        "module": "kombu"
    },
    {
        "base_classes": [
            "celery.utils.threads.bgThread"
        ],
        "class_docstring": "Background thread to autoscale pool workers.",
        "klass": "celery.worker.autoscale.Autoscaler",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Asynchronous Bounded Semaphore.\n\n    Lax means that the value will stay within the specified\n    range even if released more times than it was acquired.\n\n    Example:\n        >>> from future import print_statement as printf\n        # ^ ignore: just fooling stupid pyflakes\n\n        >>> x = LaxBoundedSemaphore(2)\n\n        >>> x.acquire(printf, 'HELLO 1')\n        HELLO 1\n\n        >>> x.acquire(printf, 'HELLO 2')\n        HELLO 2\n\n        >>> x.acquire(printf, 'HELLO 3')\n        >>> x._waiters   # private, do not access directly\n        [print, ('HELLO 3',)]\n\n        >>> x.release()\n        HELLO 3\n    ",
        "klass": "kombu.asynchronous.semaphore.LaxBoundedSemaphore",
        "module": "kombu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Timer sending heartbeats at regular intervals.\n\n    Arguments:\n        timer (kombu.asynchronous.timer.Timer): Timer to use.\n        eventer (celery.events.EventDispatcher): Event dispatcher\n            to use.\n        interval (float): Time in seconds between sending\n            heartbeats.  Default is 2 seconds.\n    ",
        "klass": "celery.worker.heartbeat.Heart",
        "module": "celery"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "repeat(object [,times]) -> create an iterator which returns the object\nfor the specified number of times.  If not specified, returns the object\nendlessly.",
        "klass": "itertools.repeat",
        "module": "itertools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    DataSource(destpath='.')\n\n    A generic data source file (file, http, ftp, ...).\n\n    DataSources can be local files or remote files/URLs.  The files may\n    also be compressed or uncompressed. DataSource hides some of the\n    low-level details of downloading the file, allowing you to simply pass\n    in a valid file path (or URL) and obtain a file object.\n\n    Parameters\n    ----------\n    destpath : str or None, optional\n        Path to the directory where the source file gets downloaded to for\n        use.  If `destpath` is None, a temporary directory will be created.\n        The default path is the current directory.\n\n    Notes\n    -----\n    URLs require a scheme string (``http://``) to be used, without it they\n    will fail::\n\n        >>> repos = np.DataSource()\n        >>> repos.exists('www.google.com/index.html')\n        False\n        >>> repos.exists('http://www.google.com/index.html')\n        True\n\n    Temporary directories are deleted when the DataSource is deleted.\n\n    Examples\n    --------\n    ::\n\n        >>> ds = np.DataSource('/home/guido')\n        >>> urlname = 'http://www.google.com/'\n        >>> gfile = ds.open('http://www.google.com/')\n        >>> ds.abspath(urlname)\n        '/home/guido/www.google.com/index.html'\n\n        >>> ds = np.DataSource(None)  # use with temporary file\n        >>> ds.open('/home/guido/foobar.txt')\n        <open file '/home/guido.foobar.txt', mode 'r' at 0x91d4430>\n        >>> ds.abspath('/home/guido/foobar.txt')\n        '/tmp/.../home/guido/foobar.txt'\n\n    ",
        "klass": "numpy.DataSource",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "\n    chararray(shape, itemsize=1, unicode=False, buffer=None, offset=0,\n              strides=None, order=None)\n\n    Provides a convenient view on arrays of string and unicode values.\n\n    .. note::\n       The `chararray` class exists for backwards compatibility with\n       Numarray, it is not recommended for new development. Starting from numpy\n       1.4, if one needs arrays of strings, it is recommended to use arrays of\n       `dtype` `object_`, `string_` or `unicode_`, and use the free functions\n       in the `numpy.char` module for fast vectorized string operations.\n\n    Versus a regular NumPy array of type `str` or `unicode`, this\n    class adds the following functionality:\n\n      1) values automatically have whitespace removed from the end\n         when indexed\n\n      2) comparison operators automatically remove whitespace from the\n         end when comparing values\n\n      3) vectorized string operations are provided as methods\n         (e.g. `.endswith`) and infix operators (e.g. ``\"+\", \"*\", \"%\"``)\n\n    chararrays should be created using `numpy.char.array` or\n    `numpy.char.asarray`, rather than this constructor directly.\n\n    This constructor creates the array, using `buffer` (with `offset`\n    and `strides`) if it is not ``None``. If `buffer` is ``None``, then\n    constructs a new array with `strides` in \"C order\", unless both\n    ``len(shape) >= 2`` and ``order='Fortran'``, in which case `strides`\n    is in \"Fortran order\".\n\n    Methods\n    -------\n    astype\n    argsort\n    copy\n    count\n    decode\n    dump\n    dumps\n    encode\n    endswith\n    expandtabs\n    fill\n    find\n    flatten\n    getfield\n    index\n    isalnum\n    isalpha\n    isdecimal\n    isdigit\n    islower\n    isnumeric\n    isspace\n    istitle\n    isupper\n    item\n    join\n    ljust\n    lower\n    lstrip\n    nonzero\n    put\n    ravel\n    repeat\n    replace\n    reshape\n    resize\n    rfind\n    rindex\n    rjust\n    rsplit\n    rstrip\n    searchsorted\n    setfield\n    setflags\n    sort\n    split\n    splitlines\n    squeeze\n    startswith\n    strip\n    swapaxes\n    swapcase\n    take\n    title\n    tofile\n    tolist\n    tostring\n    translate\n    transpose\n    upper\n    view\n    zfill\n\n    Parameters\n    ----------\n    shape : tuple\n        Shape of the array.\n    itemsize : int, optional\n        Length of each array element, in number of characters. Default is 1.\n    unicode : bool, optional\n        Are the array elements of type unicode (True) or string (False).\n        Default is False.\n    buffer : int, optional\n        Memory address of the start of the array data.  Default is None,\n        in which case a new array is created.\n    offset : int, optional\n        Fixed stride displacement from the beginning of an axis?\n        Default is 0. Needs to be >=0.\n    strides : array_like of ints, optional\n        Strides for the array (see `ndarray.strides` for full description).\n        Default is None.\n    order : {'C', 'F'}, optional\n        The order in which the array data is stored in memory: 'C' ->\n        \"row major\" order (the default), 'F' -> \"column major\"\n        (Fortran) order.\n\n    Examples\n    --------\n    >>> charar = np.chararray((3, 3))\n    >>> charar[:] = 'a'\n    >>> charar\n    chararray([[b'a', b'a', b'a'],\n               [b'a', b'a', b'a'],\n               [b'a', b'a', b'a']], dtype='|S1')\n\n    >>> charar = np.chararray(charar.shape, itemsize=5)\n    >>> charar[:] = 'abc'\n    >>> charar\n    chararray([[b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc'],\n               [b'abc', b'abc', b'abc']], dtype='|S5')\n\n    ",
        "klass": "numpy.chararray",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.floating",
            "float"
        ],
        "class_docstring": "Double-precision floating-point number type, compatible with Python `float`\n    and C ``double``.\n    Character code: ``'d'``.\n    Canonical name: ``np.double``.\n    Alias: ``np.float_``.\n    Alias *on this platform*: ``np.float64``: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.",
        "klass": "numpy.float64",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "dtype(obj, align=False, copy=False)\n\n    Create a data type object.\n\n    A numpy array is homogeneous, and contains elements described by a\n    dtype object. A dtype object can be constructed from different\n    combinations of fundamental numeric types.\n\n    Parameters\n    ----------\n    obj\n        Object to be converted to a data type object.\n    align : bool, optional\n        Add padding to the fields to match what a C compiler would output\n        for a similar C-struct. Can be ``True`` only if `obj` is a dictionary\n        or a comma-separated string. If a struct dtype is being created,\n        this also sets a sticky alignment flag ``isalignedstruct``.\n    copy : bool, optional\n        Make a new copy of the data-type object. If ``False``, the result\n        may just be a reference to a built-in data-type object.\n\n    See also\n    --------\n    result_type\n\n    Examples\n    --------\n    Using array-scalar type:\n\n    >>> np.dtype(np.int16)\n    dtype('int16')\n\n    Structured type, one field name 'f1', containing int16:\n\n    >>> np.dtype([('f1', np.int16)])\n    dtype([('f1', '<i2')])\n\n    Structured type, one field named 'f1', in itself containing a structured\n    type with one field:\n\n    >>> np.dtype([('f1', [('f1', np.int16)])])\n    dtype([('f1', [('f1', '<i2')])])\n\n    Structured type, two fields: the first field contains an unsigned int, the\n    second an int32:\n\n    >>> np.dtype([('f1', np.uint64), ('f2', np.int32)])\n    dtype([('f1', '<u8'), ('f2', '<i4')])\n\n    Using array-protocol type strings:\n\n    >>> np.dtype([('a','f8'),('b','S10')])\n    dtype([('a', '<f8'), ('b', 'S10')])\n\n    Using comma-separated field formats.  The shape is (2,3):\n\n    >>> np.dtype(\"i4, (2,3)f8\")\n    dtype([('f0', '<i4'), ('f1', '<f8', (2, 3))])\n\n    Using tuples.  ``int`` is a fixed type, 3 the field's shape.  ``void``\n    is a flexible type, here of size 10:\n\n    >>> np.dtype([('hello',(np.int64,3)),('world',np.void,10)])\n    dtype([('hello', '<i8', (3,)), ('world', 'V10')])\n\n    Subdivide ``int16`` into 2 ``int8``'s, called x and y.  0 and 1 are\n    the offsets in bytes:\n\n    >>> np.dtype((np.int16, {'x':(np.int8,0), 'y':(np.int8,1)}))\n    dtype((numpy.int16, [('x', 'i1'), ('y', 'i1')]))\n\n    Using dictionaries.  Two fields named 'gender' and 'age':\n\n    >>> np.dtype({'names':['gender','age'], 'formats':['S1',np.uint8]})\n    dtype([('gender', 'S1'), ('age', 'u1')])\n\n    Offsets in bytes, here 0 and 25:\n\n    >>> np.dtype({'surname':('S25',0),'age':(np.uint8,25)})\n    dtype([('surname', 'S25'), ('age', 'u1')])",
        "klass": "numpy.dtype",
        "module": "numpy"
    },
    {
        "base_classes": [
            "contextlib.ContextDecorator"
        ],
        "class_docstring": "\n    errstate(**kwargs)\n\n    Context manager for floating-point error handling.\n\n    Using an instance of `errstate` as a context manager allows statements in\n    that context to execute with a known error handling behavior. Upon entering\n    the context the error handling is set with `seterr` and `seterrcall`, and\n    upon exiting it is reset to what it was before.\n\n    ..  versionchanged:: 1.17.0\n        `errstate` is also usable as a function decorator, saving\n        a level of indentation if an entire function is wrapped.\n        See :py:class:`contextlib.ContextDecorator` for more information.\n\n    Parameters\n    ----------\n    kwargs : {divide, over, under, invalid}\n        Keyword arguments. The valid keywords are the possible floating-point\n        exceptions. Each keyword should have a string value that defines the\n        treatment for the particular error. Possible values are\n        {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.\n\n    See Also\n    --------\n    seterr, geterr, seterrcall, geterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> olderr = np.seterr(all='ignore')  # Set error handling to known state.\n\n    >>> np.arange(3) / 0.\n    array([nan, inf, inf])\n    >>> with np.errstate(divide='warn'):\n    ...     np.arange(3) / 0.\n    array([nan, inf, inf])\n\n    >>> np.sqrt(-1)\n    nan\n    >>> with np.errstate(invalid='raise'):\n    ...     np.sqrt(-1)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 2, in <module>\n    FloatingPointError: invalid value encountered in sqrt\n\n    Outside the context the error handling behavior has not changed:\n\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'ignore'), ('invalid', 'ignore'), ('over', 'ignore'), ('under', 'ignore')])\n\n    ",
        "klass": "numpy.errstate",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    finfo(dtype)\n\n    Machine limits for floating point types.\n\n    Attributes\n    ----------\n    bits : int\n        The number of bits occupied by the type.\n    eps : float\n        The smallest representable positive number such that\n        ``1.0 + eps != 1.0``.  Type of `eps` is an appropriate floating\n        point type.\n    epsneg : floating point number of the appropriate type\n        The smallest representable positive number such that\n        ``1.0 - epsneg != 1.0``.\n    iexp : int\n        The number of bits in the exponent portion of the floating point\n        representation.\n    machar : MachAr\n        The object which calculated these parameters and holds more\n        detailed information.\n    machep : int\n        The exponent that yields `eps`.\n    max : floating point number of the appropriate type\n        The largest representable number.\n    maxexp : int\n        The smallest positive power of the base (2) that causes overflow.\n    min : floating point number of the appropriate type\n        The smallest representable number, typically ``-max``.\n    minexp : int\n        The most negative power of the base (2) consistent with there\n        being no leading 0's in the mantissa.\n    negep : int\n        The exponent that yields `epsneg`.\n    nexp : int\n        The number of bits in the exponent including its sign and bias.\n    nmant : int\n        The number of bits in the mantissa.\n    precision : int\n        The approximate number of decimal digits to which this kind of\n        float is precise.\n    resolution : floating point number of the appropriate type\n        The approximate decimal resolution of this type, i.e.,\n        ``10**-precision``.\n    tiny : float\n        The smallest positive usable number.  Type of `tiny` is an\n        appropriate floating point type.\n\n    Parameters\n    ----------\n    dtype : float, dtype, or instance\n        Kind of floating point data-type about which to get information.\n\n    See Also\n    --------\n    MachAr : The implementation of the tests that produce this information.\n    iinfo : The equivalent for integer data types.\n\n    Notes\n    -----\n    For developers of NumPy: do not instantiate this at the module level.\n    The initial calculation of these parameters is expensive and negatively\n    impacts import times.  These objects are cached, so calling ``finfo()``\n    repeatedly inside your functions is not a problem.\n\n    ",
        "klass": "numpy.finfo",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.floating"
        ],
        "class_docstring": "Single-precision floating-point number type, compatible with C ``float``.\n    Character code: ``'f'``.\n    Canonical name: ``np.single``.\n    Alias *on this platform*: ``np.float32``: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.",
        "klass": "numpy.float32",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    iinfo(type)\n\n    Machine limits for integer types.\n\n    Attributes\n    ----------\n    bits : int\n        The number of bits occupied by the type.\n    min : int\n        The smallest integer expressible by the type.\n    max : int\n        The largest integer expressible by the type.\n\n    Parameters\n    ----------\n    int_type : integer type, dtype, or instance\n        The kind of integer data type to get information about.\n\n    See Also\n    --------\n    finfo : The equivalent for floating point data types.\n\n    Examples\n    --------\n    With types:\n\n    >>> ii16 = np.iinfo(np.int16)\n    >>> ii16.min\n    -32768\n    >>> ii16.max\n    32767\n    >>> ii32 = np.iinfo(np.int32)\n    >>> ii32.min\n    -2147483648\n    >>> ii32.max\n    2147483647\n\n    With instances:\n\n    >>> ii32 = np.iinfo(np.int32(10))\n    >>> ii32.min\n    -2147483648\n    >>> ii32.max\n    2147483647\n\n    ",
        "klass": "numpy.iinfo",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.signedinteger"
        ],
        "class_docstring": "Signed integer type, compatible with Python `int` anc C ``long``.\n    Character code: ``'l'``.\n    Canonical name: ``np.int_``.\n    Alias *on this platform*: ``np.int64``: 64-bit signed integer (-9223372036854775808 to 9223372036854775807).\n    Alias *on this platform*: ``np.intp``: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.",
        "klass": "numpy.int64",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "\n    matrix(data, dtype=None, copy=True)\n\n    .. note:: It is no longer recommended to use this class, even for linear\n              algebra. Instead use regular arrays. The class may be removed\n              in the future.\n\n    Returns a matrix from an array-like object, or from a string of data.\n    A matrix is a specialized 2-D array that retains its 2-D nature\n    through operations.  It has certain special operators, such as ``*``\n    (matrix multiplication) and ``**`` (matrix power).\n\n    Parameters\n    ----------\n    data : array_like or string\n       If `data` is a string, it is interpreted as a matrix with commas\n       or spaces separating columns, and semicolons separating rows.\n    dtype : data-type\n       Data-type of the output matrix.\n    copy : bool\n       If `data` is already an `ndarray`, then this flag determines\n       whether the data is copied (the default), or whether a view is\n       constructed.\n\n    See Also\n    --------\n    array\n\n    Examples\n    --------\n    >>> a = np.matrix('1 2; 3 4')\n    >>> a\n    matrix([[1, 2],\n            [3, 4]])\n\n    >>> np.matrix([[1, 2], [3, 4]])\n    matrix([[1, 2],\n            [3, 4]])\n\n    ",
        "klass": "numpy.matrix",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "Create a memory-map to an array stored in a *binary* file on disk.\n\n    Memory-mapped files are used for accessing small segments of large files\n    on disk, without reading the entire file into memory.  NumPy's\n    memmap's are array-like objects.  This differs from Python's ``mmap``\n    module, which uses file-like objects.\n\n    This subclass of ndarray has some unpleasant interactions with\n    some operations, because it doesn't quite fit properly as a subclass.\n    An alternative to using this subclass is to create the ``mmap``\n    object yourself, then create an ndarray with ndarray.__new__ directly,\n    passing the object created in its 'buffer=' parameter.\n\n    This class may at some point be turned into a factory function\n    which returns a view into an mmap buffer.\n\n    Delete the memmap instance to close the memmap file.\n\n\n    Parameters\n    ----------\n    filename : str, file-like object, or pathlib.Path instance\n        The file name or file object to be used as the array data buffer.\n    dtype : data-type, optional\n        The data-type used to interpret the file contents.\n        Default is `uint8`.\n    mode : {'r+', 'r', 'w+', 'c'}, optional\n        The file is opened in this mode:\n\n        +------+-------------------------------------------------------------+\n        | 'r'  | Open existing file for reading only.                        |\n        +------+-------------------------------------------------------------+\n        | 'r+' | Open existing file for reading and writing.                 |\n        +------+-------------------------------------------------------------+\n        | 'w+' | Create or overwrite existing file for reading and writing.  |\n        +------+-------------------------------------------------------------+\n        | 'c'  | Copy-on-write: assignments affect data in memory, but       |\n        |      | changes are not saved to disk.  The file on disk is         |\n        |      | read-only.                                                  |\n        +------+-------------------------------------------------------------+\n\n        Default is 'r+'.\n    offset : int, optional\n        In the file, array data starts at this offset. Since `offset` is\n        measured in bytes, it should normally be a multiple of the byte-size\n        of `dtype`. When ``mode != 'r'``, even positive offsets beyond end of\n        file are valid; The file will be extended to accommodate the\n        additional data. By default, ``memmap`` will start at the beginning of\n        the file, even if ``filename`` is a file pointer ``fp`` and\n        ``fp.tell() != 0``.\n    shape : tuple, optional\n        The desired shape of the array. If ``mode == 'r'`` and the number\n        of remaining bytes after `offset` is not a multiple of the byte-size\n        of `dtype`, you must specify `shape`. By default, the returned array\n        will be 1-D with the number of elements determined by file size\n        and data-type.\n    order : {'C', 'F'}, optional\n        Specify the order of the ndarray memory layout:\n        :term:`row-major`, C-style or :term:`column-major`,\n        Fortran-style.  This only has an effect if the shape is\n        greater than 1-D.  The default order is 'C'.\n\n    Attributes\n    ----------\n    filename : str or pathlib.Path instance\n        Path to the mapped file.\n    offset : int\n        Offset position in the file.\n    mode : str\n        File mode.\n\n    Methods\n    -------\n    flush\n        Flush any changes in memory to file on disk.\n        When you delete a memmap object, flush is called first to write\n        changes to disk before removing the object.\n\n\n    See also\n    --------\n    lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n\n    Notes\n    -----\n    The memmap object can be used anywhere an ndarray is accepted.\n    Given a memmap ``fp``, ``isinstance(fp, numpy.ndarray)`` returns\n    ``True``.\n    \n    Memory-mapped files cannot be larger than 2GB on 32-bit systems.\n\n    When a memmap causes a file to be created or extended beyond its\n    current size in the filesystem, the contents of the new part are\n    unspecified. On systems with POSIX filesystem semantics, the extended\n    part will be filled with zero bytes.\n\n    Examples\n    --------\n    >>> data = np.arange(12, dtype='float32')\n    >>> data.resize((3,4))\n\n    This example uses a temporary file so that doctest doesn't write\n    files to your directory. You would use a 'normal' filename.\n\n    >>> from tempfile import mkdtemp\n    >>> import os.path as path\n    >>> filename = path.join(mkdtemp(), 'newfile.dat')\n\n    Create a memmap with dtype and shape that matches our data:\n\n    >>> fp = np.memmap(filename, dtype='float32', mode='w+', shape=(3,4))\n    >>> fp\n    memmap([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]], dtype=float32)\n\n    Write data to memmap array:\n\n    >>> fp[:] = data[:]\n    >>> fp\n    memmap([[  0.,   1.,   2.,   3.],\n            [  4.,   5.,   6.,   7.],\n            [  8.,   9.,  10.,  11.]], dtype=float32)\n\n    >>> fp.filename == path.abspath(filename)\n    True\n\n    Deletion flushes memory changes to disk before removing the object:\n\n    >>> del fp\n\n    Load the memmap and verify data was stored:\n\n    >>> newfp = np.memmap(filename, dtype='float32', mode='r', shape=(3,4))\n    >>> newfp\n    memmap([[  0.,   1.,   2.,   3.],\n            [  4.,   5.,   6.,   7.],\n            [  8.,   9.,  10.,  11.]], dtype=float32)\n\n    Read-only memmap:\n\n    >>> fpr = np.memmap(filename, dtype='float32', mode='r', shape=(3,4))\n    >>> fpr.flags.writeable\n    False\n\n    Copy-on-write memmap:\n\n    >>> fpc = np.memmap(filename, dtype='float32', mode='c', shape=(3,4))\n    >>> fpc.flags.writeable\n    True\n\n    It's possible to assign to copy-on-write array, but values are only\n    written into the memory copy of the array, and not written to disk:\n\n    >>> fpc\n    memmap([[  0.,   1.,   2.,   3.],\n            [  4.,   5.,   6.,   7.],\n            [  8.,   9.,  10.,  11.]], dtype=float32)\n    >>> fpc[0,:] = 0\n    >>> fpc\n    memmap([[  0.,   0.,   0.,   0.],\n            [  4.,   5.,   6.,   7.],\n            [  8.,   9.,  10.,  11.]], dtype=float32)\n\n    File on disk is unchanged:\n\n    >>> fpr\n    memmap([[  0.,   1.,   2.,   3.],\n            [  4.,   5.,   6.,   7.],\n            [  8.,   9.,  10.,  11.]], dtype=float32)\n\n    Offset into a memmap:\n\n    >>> fpo = np.memmap(filename, dtype='float32', mode='r', offset=16)\n    >>> fpo\n    memmap([  4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.], dtype=float32)\n\n    ",
        "klass": "numpy.memmap",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An N-dimensional iterator object to index arrays.\n\n    Given the shape of an array, an `ndindex` instance iterates over\n    the N-dimensional index of the array. At each iteration a tuple\n    of indices is returned, the last dimension is iterated over first.\n\n    Parameters\n    ----------\n    `*args` : ints\n      The size of each dimension of the array.\n\n    See Also\n    --------\n    ndenumerate, flatiter\n\n    Examples\n    --------\n    >>> for index in np.ndindex(3, 2, 1):\n    ...     print(index)\n    (0, 0, 0)\n    (0, 1, 0)\n    (1, 0, 0)\n    (1, 1, 0)\n    (2, 0, 0)\n    (2, 1, 0)\n\n    ",
        "klass": "numpy.ndindex",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Efficient multi-dimensional iterator object to iterate over arrays.\n    To get started using this object, see the\n    :ref:`introductory guide to array iteration <arrays.nditer>`.\n\n    Parameters\n    ----------\n    op : ndarray or sequence of array_like\n        The array(s) to iterate over.\n\n    flags : sequence of str, optional\n          Flags to control the behavior of the iterator.\n\n          * ``buffered`` enables buffering when required.\n          * ``c_index`` causes a C-order index to be tracked.\n          * ``f_index`` causes a Fortran-order index to be tracked.\n          * ``multi_index`` causes a multi-index, or a tuple of indices\n            with one per iteration dimension, to be tracked.\n          * ``common_dtype`` causes all the operands to be converted to\n            a common data type, with copying or buffering as necessary.\n          * ``copy_if_overlap`` causes the iterator to determine if read\n            operands have overlap with write operands, and make temporary\n            copies as necessary to avoid overlap. False positives (needless\n            copying) are possible in some cases.\n          * ``delay_bufalloc`` delays allocation of the buffers until\n            a reset() call is made. Allows ``allocate`` operands to\n            be initialized before their values are copied into the buffers.\n          * ``external_loop`` causes the ``values`` given to be\n            one-dimensional arrays with multiple values instead of\n            zero-dimensional arrays.\n          * ``grow_inner`` allows the ``value`` array sizes to be made\n            larger than the buffer size when both ``buffered`` and\n            ``external_loop`` is used.\n          * ``ranged`` allows the iterator to be restricted to a sub-range\n            of the iterindex values.\n          * ``refs_ok`` enables iteration of reference types, such as\n            object arrays.\n          * ``reduce_ok`` enables iteration of ``readwrite`` operands\n            which are broadcasted, also known as reduction operands.\n          * ``zerosize_ok`` allows `itersize` to be zero.\n    op_flags : list of list of str, optional\n          This is a list of flags for each operand. At minimum, one of\n          ``readonly``, ``readwrite``, or ``writeonly`` must be specified.\n\n          * ``readonly`` indicates the operand will only be read from.\n          * ``readwrite`` indicates the operand will be read from and written to.\n          * ``writeonly`` indicates the operand will only be written to.\n          * ``no_broadcast`` prevents the operand from being broadcasted.\n          * ``contig`` forces the operand data to be contiguous.\n          * ``aligned`` forces the operand data to be aligned.\n          * ``nbo`` forces the operand data to be in native byte order.\n          * ``copy`` allows a temporary read-only copy if required.\n          * ``updateifcopy`` allows a temporary read-write copy if required.\n          * ``allocate`` causes the array to be allocated if it is None\n            in the ``op`` parameter.\n          * ``no_subtype`` prevents an ``allocate`` operand from using a subtype.\n          * ``arraymask`` indicates that this operand is the mask to use\n            for selecting elements when writing to operands with the\n            'writemasked' flag set. The iterator does not enforce this,\n            but when writing from a buffer back to the array, it only\n            copies those elements indicated by this mask.\n          * ``writemasked`` indicates that only elements where the chosen\n            ``arraymask`` operand is True will be written to.\n          * ``overlap_assume_elementwise`` can be used to mark operands that are\n            accessed only in the iterator order, to allow less conservative\n            copying when ``copy_if_overlap`` is present.\n    op_dtypes : dtype or tuple of dtype(s), optional\n        The required data type(s) of the operands. If copying or buffering\n        is enabled, the data will be converted to/from their original types.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Controls the iteration order. 'C' means C order, 'F' means\n        Fortran order, 'A' means 'F' order if all the arrays are Fortran\n        contiguous, 'C' order otherwise, and 'K' means as close to the\n        order the array elements appear in memory as possible. This also\n        affects the element memory order of ``allocate`` operands, as they\n        are allocated to be compatible with iteration order.\n        Default is 'K'.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur when making a copy\n        or buffering.  Setting this to 'unsafe' is not recommended,\n        as it can adversely affect accumulations.\n\n        * 'no' means the data types should not be cast at all.\n        * 'equiv' means only byte-order changes are allowed.\n        * 'safe' means only casts which can preserve values are allowed.\n        * 'same_kind' means only safe casts or casts within a kind,\n          like float64 to float32, are allowed.\n        * 'unsafe' means any data conversions may be done.\n    op_axes : list of list of ints, optional\n        If provided, is a list of ints or None for each operands.\n        The list of axes for an operand is a mapping from the dimensions\n        of the iterator to the dimensions of the operand. A value of\n        -1 can be placed for entries, causing that dimension to be\n        treated as `newaxis`.\n    itershape : tuple of ints, optional\n        The desired shape of the iterator. This allows ``allocate`` operands\n        with a dimension mapped by op_axes not corresponding to a dimension\n        of a different operand to get a value not equal to 1 for that\n        dimension.\n    buffersize : int, optional\n        When buffering is enabled, controls the size of the temporary\n        buffers. Set to 0 for the default value.\n\n    Attributes\n    ----------\n    dtypes : tuple of dtype(s)\n        The data types of the values provided in `value`. This may be\n        different from the operand data types if buffering is enabled.\n        Valid only before the iterator is closed.\n    finished : bool\n        Whether the iteration over the operands is finished or not.\n    has_delayed_bufalloc : bool\n        If True, the iterator was created with the ``delay_bufalloc`` flag,\n        and no reset() function was called on it yet.\n    has_index : bool\n        If True, the iterator was created with either the ``c_index`` or\n        the ``f_index`` flag, and the property `index` can be used to\n        retrieve it.\n    has_multi_index : bool\n        If True, the iterator was created with the ``multi_index`` flag,\n        and the property `multi_index` can be used to retrieve it.\n    index\n        When the ``c_index`` or ``f_index`` flag was used, this property\n        provides access to the index. Raises a ValueError if accessed\n        and ``has_index`` is False.\n    iterationneedsapi : bool\n        Whether iteration requires access to the Python API, for example\n        if one of the operands is an object array.\n    iterindex : int\n        An index which matches the order of iteration.\n    itersize : int\n        Size of the iterator.\n    itviews\n        Structured view(s) of `operands` in memory, matching the reordered\n        and optimized iterator access pattern. Valid only before the iterator\n        is closed.\n    multi_index\n        When the ``multi_index`` flag was used, this property\n        provides access to the index. Raises a ValueError if accessed\n        accessed and ``has_multi_index`` is False.\n    ndim : int\n        The dimensions of the iterator.\n    nop : int\n        The number of iterator operands.\n    operands : tuple of operand(s)\n        The array(s) to be iterated over. Valid only before the iterator is\n        closed.\n    shape : tuple of ints\n        Shape tuple, the shape of the iterator.\n    value\n        Value of ``operands`` at current iteration. Normally, this is a\n        tuple of array scalars, but if the flag ``external_loop`` is used,\n        it is a tuple of one dimensional arrays.\n\n    Notes\n    -----\n    `nditer` supersedes `flatiter`.  The iterator implementation behind\n    `nditer` is also exposed by the NumPy C API.\n\n    The Python exposure supplies two iteration interfaces, one which follows\n    the Python iterator protocol, and another which mirrors the C-style\n    do-while pattern.  The native Python approach is better in most cases, but\n    if you need the coordinates or index of an iterator, use the C-style pattern.\n\n    Examples\n    --------\n    Here is how we might write an ``iter_add`` function, using the\n    Python iterator protocol:\n\n    >>> def iter_add_py(x, y, out=None):\n    ...     addop = np.add\n    ...     it = np.nditer([x, y, out], [],\n    ...                 [['readonly'], ['readonly'], ['writeonly','allocate']])\n    ...     with it:\n    ...         for (a, b, c) in it:\n    ...             addop(a, b, out=c)\n    ...     return it.operands[2]\n\n    Here is the same function, but following the C-style pattern:\n\n    >>> def iter_add(x, y, out=None):\n    ...    addop = np.add\n    ...    it = np.nditer([x, y, out], [],\n    ...                [['readonly'], ['readonly'], ['writeonly','allocate']])\n    ...    with it:\n    ...        while not it.finished:\n    ...            addop(it[0], it[1], out=it[2])\n    ...            it.iternext()\n    ...        return it.operands[2]\n\n    Here is an example outer product function:\n\n    >>> def outer_it(x, y, out=None):\n    ...     mulop = np.multiply\n    ...     it = np.nditer([x, y, out], ['external_loop'],\n    ...             [['readonly'], ['readonly'], ['writeonly', 'allocate']],\n    ...             op_axes=[list(range(x.ndim)) + [-1] * y.ndim,\n    ...                      [-1] * x.ndim + list(range(y.ndim)),\n    ...                      None])\n    ...     with it:\n    ...         for (a, b, c) in it:\n    ...             mulop(a, b, out=c)\n    ...         return it.operands[2]\n\n    >>> a = np.arange(2)+1\n    >>> b = np.arange(3)+1\n    >>> outer_it(a,b)\n    array([[1, 2, 3],\n           [2, 4, 6]])\n\n    Here is an example function which operates like a \"lambda\" ufunc:\n\n    >>> def luf(lamdaexpr, *args, **kwargs):\n    ...    '''luf(lambdaexpr, op1, ..., opn, out=None, order='K', casting='safe', buffersize=0)'''\n    ...    nargs = len(args)\n    ...    op = (kwargs.get('out',None),) + args\n    ...    it = np.nditer(op, ['buffered','external_loop'],\n    ...            [['writeonly','allocate','no_broadcast']] +\n    ...                            [['readonly','nbo','aligned']]*nargs,\n    ...            order=kwargs.get('order','K'),\n    ...            casting=kwargs.get('casting','safe'),\n    ...            buffersize=kwargs.get('buffersize',0))\n    ...    while not it.finished:\n    ...        it[0] = lamdaexpr(*it[1:])\n    ...        it.iternext()\n    ...        return it.operands[0]\n\n    >>> a = np.arange(5)\n    >>> b = np.ones(5)\n    >>> luf(lambda i,j:i*i + j/2, a, b)\n    array([  0.5,   1.5,   4.5,   9.5,  16.5])\n\n    If operand flags `\"writeonly\"` or `\"readwrite\"` are used the operands may\n    be views into the original data with the `WRITEBACKIFCOPY` flag. In this case\n    nditer must be used as a context manager or the nditer.close\n    method must be called before using the result. The temporary\n    data will be written back to the original data when the `__exit__`\n    function is called but not before:\n\n    >>> a = np.arange(6, dtype='i4')[::-2]\n    >>> with np.nditer(a, [],\n    ...        [['writeonly', 'updateifcopy']],\n    ...        casting='unsafe',\n    ...        op_dtypes=[np.dtype('f4')]) as i:\n    ...    x = i.operands[0]\n    ...    x[:] = [-1, -2, -3]\n    ...    # a still unchanged here\n    >>> a, x\n    (array([-1, -2, -3], dtype=int32), array([-1., -2., -3.], dtype=float32))\n\n    It is important to note that once the iterator is exited, dangling\n    references (like `x` in the example) may or may not share data with\n    the original data `a`. If writeback semantics were active, i.e. if\n    `x.base.flags.writebackifcopy` is `True`, then exiting the iterator\n    will sever the connection between `x` and `a`, writing to `x` will\n    no longer write to `a`. If writeback semantics are not active, then\n    `x.data` will still point at some part of `a.data`, and writing to\n    one will affect the other.",
        "klass": "numpy.nditer",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "Construct an ndarray that allows field access using attributes.\n\n    Arrays may have a data-types containing fields, analogous\n    to columns in a spread sheet.  An example is ``[(x, int), (y, float)]``,\n    where each entry in the array is a pair of ``(int, float)``.  Normally,\n    these attributes are accessed using dictionary lookups such as ``arr['x']``\n    and ``arr['y']``.  Record arrays allow the fields to be accessed as members\n    of the array, using ``arr.x`` and ``arr.y``.\n\n    Parameters\n    ----------\n    shape : tuple\n        Shape of output array.\n    dtype : data-type, optional\n        The desired data-type.  By default, the data-type is determined\n        from `formats`, `names`, `titles`, `aligned` and `byteorder`.\n    formats : list of data-types, optional\n        A list containing the data-types for the different columns, e.g.\n        ``['i4', 'f8', 'i4']``.  `formats` does *not* support the new\n        convention of using types directly, i.e. ``(int, float, int)``.\n        Note that `formats` must be a list, not a tuple.\n        Given that `formats` is somewhat limited, we recommend specifying\n        `dtype` instead.\n    names : tuple of str, optional\n        The name of each column, e.g. ``('x', 'y', 'z')``.\n    buf : buffer, optional\n        By default, a new array is created of the given shape and data-type.\n        If `buf` is specified and is an object exposing the buffer interface,\n        the array will use the memory from the existing buffer.  In this case,\n        the `offset` and `strides` keywords are available.\n\n    Other Parameters\n    ----------------\n    titles : tuple of str, optional\n        Aliases for column names.  For example, if `names` were\n        ``('x', 'y', 'z')`` and `titles` is\n        ``('x_coordinate', 'y_coordinate', 'z_coordinate')``, then\n        ``arr['x']`` is equivalent to both ``arr.x`` and ``arr.x_coordinate``.\n    byteorder : {'<', '>', '='}, optional\n        Byte-order for all fields.\n    aligned : bool, optional\n        Align the fields in memory as the C-compiler would.\n    strides : tuple of ints, optional\n        Buffer (`buf`) is interpreted according to these strides (strides\n        define how many bytes each array element, row, column, etc.\n        occupy in memory).\n    offset : int, optional\n        Start reading buffer (`buf`) from this offset onwards.\n    order : {'C', 'F'}, optional\n        Row-major (C-style) or column-major (Fortran-style) order.\n\n    Returns\n    -------\n    rec : recarray\n        Empty array of the given shape and type.\n\n    See Also\n    --------\n    rec.fromrecords : Construct a record array from data.\n    record : fundamental data-type for `recarray`.\n    format_parser : determine a data-type from formats, names, titles.\n\n    Notes\n    -----\n    This constructor can be compared to ``empty``: it creates a new record\n    array but does not fill it with data.  To create a record array from data,\n    use one of the following methods:\n\n    1. Create a standard ndarray and convert it to a record array,\n       using ``arr.view(np.recarray)``\n    2. Use the `buf` keyword.\n    3. Use `np.rec.fromrecords`.\n\n    Examples\n    --------\n    Create an array with two fields, ``x`` and ``y``:\n\n    >>> x = np.array([(1.0, 2), (3.0, 4)], dtype=[('x', '<f8'), ('y', '<i8')])\n    >>> x\n    array([(1., 2), (3., 4)], dtype=[('x', '<f8'), ('y', '<i8')])\n\n    >>> x['x']\n    array([1., 3.])\n\n    View the array as a record array:\n\n    >>> x = x.view(np.recarray)\n\n    >>> x.x\n    array([1., 3.])\n\n    >>> x.y\n    array([2, 4])\n\n    Create a new, empty record array:\n\n    >>> np.recarray((2,),\n    ... dtype=[('x', int), ('y', float), ('z', int)]) #doctest: +SKIP\n    rec.array([(-1073741821, 1.2249118382103472e-301, 24547520),\n           (3471280, 1.2134086255804012e-316, 0)],\n          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])\n\n    ",
        "klass": "numpy.recarray",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.unsignedinteger"
        ],
        "class_docstring": "Unsigned integer type, compatible with C ``unsigned char``.\n    Character code: ``'B'``.\n    Canonical name: ``np.ubyte``.\n    Alias *on this platform*: ``np.uint8``: 8-bit unsigned integer (0 to 255).",
        "klass": "numpy.uint8",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.unsignedinteger"
        ],
        "class_docstring": "Unsigned integer type, compatible with C ``unsigned long``.\n    Character code: ``'L'``.\n    Canonical name: ``np.uint``.\n    Alias *on this platform*: ``np.uint64``: 64-bit unsigned integer (0 to 18446744073709551615).\n    Alias *on this platform*: ``np.uintp``: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.",
        "klass": "numpy.uint64",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.unsignedinteger"
        ],
        "class_docstring": "Unsigned integer type, compatible with C ``unsigned short``.\n    Character code: ``'H'``.\n    Canonical name: ``np.ushort``.\n    Alias *on this platform*: ``np.uint16``: 16-bit unsigned integer (0 to 65535).",
        "klass": "numpy.uint16",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.unsignedinteger"
        ],
        "class_docstring": "Unsigned integer type, compatible with C ``unsigned int``.\n    Character code: ``'I'``.\n    Canonical name: ``np.uintc``.\n    Alias *on this platform*: ``np.uint32``: 32-bit unsigned integer (0 to 4294967295).",
        "klass": "numpy.uint32",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n              signature=None)\n\n    Generalized function class.\n\n    Define a vectorized function which takes a nested sequence of objects or\n    numpy arrays as inputs and returns a single numpy array or a tuple of numpy\n    arrays. The vectorized function evaluates `pyfunc` over successive tuples\n    of the input arrays like the python map function, except it uses the\n    broadcasting rules of numpy.\n\n    The data type of the output of `vectorized` is determined by calling\n    the function with the first element of the input.  This can be avoided\n    by specifying the `otypes` argument.\n\n    Parameters\n    ----------\n    pyfunc : callable\n        A python function or method.\n    otypes : str or list of dtypes, optional\n        The output data type. It must be specified as either a string of\n        typecode characters or a list of data type specifiers. There should\n        be one data type specifier for each output.\n    doc : str, optional\n        The docstring for the function. If `None`, the docstring will be the\n        ``pyfunc.__doc__``.\n    excluded : set, optional\n        Set of strings or integers representing the positional or keyword\n        arguments for which the function will not be vectorized.  These will be\n        passed directly to `pyfunc` unmodified.\n\n        .. versionadded:: 1.7.0\n\n    cache : bool, optional\n       If `True`, then cache the first function call that determines the number\n       of outputs if `otypes` is not provided.\n\n        .. versionadded:: 1.7.0\n\n    signature : string, optional\n        Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for\n        vectorized matrix-vector multiplication. If provided, ``pyfunc`` will\n        be called with (and expected to return) arrays with shapes given by the\n        size of corresponding core dimensions. By default, ``pyfunc`` is\n        assumed to take scalars as input and output.\n\n        .. versionadded:: 1.12.0\n\n    Returns\n    -------\n    vectorized : callable\n        Vectorized function.\n\n    See Also\n    --------\n    frompyfunc : Takes an arbitrary Python function and returns a ufunc\n\n    Notes\n    -----\n    The `vectorize` function is provided primarily for convenience, not for\n    performance. The implementation is essentially a for loop.\n\n    If `otypes` is not specified, then a call to the function with the\n    first argument will be used to determine the number of outputs.  The\n    results of this call will be cached if `cache` is `True` to prevent\n    calling the function twice.  However, to implement the cache, the\n    original function must be wrapped which will slow down subsequent\n    calls, so only do this if your function is expensive.\n\n    The new keyword argument interface and `excluded` argument support\n    further degrades performance.\n\n    References\n    ----------\n    .. [1] NumPy Reference, section `Generalized Universal Function API\n           <https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html>`_.\n\n    Examples\n    --------\n    >>> def myfunc(a, b):\n    ...     \"Return a-b if a>b, otherwise return a+b\"\n    ...     if a > b:\n    ...         return a - b\n    ...     else:\n    ...         return a + b\n\n    >>> vfunc = np.vectorize(myfunc)\n    >>> vfunc([1, 2, 3, 4], 2)\n    array([3, 4, 1, 2])\n\n    The docstring is taken from the input function to `vectorize` unless it\n    is specified:\n\n    >>> vfunc.__doc__\n    'Return a-b if a>b, otherwise return a+b'\n    >>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')\n    >>> vfunc.__doc__\n    'Vectorized `myfunc`'\n\n    The output type is determined by evaluating the first element of the input,\n    unless it is specified:\n\n    >>> out = vfunc([1, 2, 3, 4], 2)\n    >>> type(out[0])\n    <class 'numpy.int64'>\n    >>> vfunc = np.vectorize(myfunc, otypes=[float])\n    >>> out = vfunc([1, 2, 3, 4], 2)\n    >>> type(out[0])\n    <class 'numpy.float64'>\n\n    The `excluded` argument can be used to prevent vectorizing over certain\n    arguments.  This can be useful for array-like arguments of a fixed length\n    such as the coefficients for a polynomial as in `polyval`:\n\n    >>> def mypolyval(p, x):\n    ...     _p = list(p)\n    ...     res = _p.pop(0)\n    ...     while _p:\n    ...         res = res*x + _p.pop(0)\n    ...     return res\n    >>> vpolyval = np.vectorize(mypolyval, excluded=['p'])\n    >>> vpolyval(p=[1, 2, 3], x=[0, 1])\n    array([3, 6])\n\n    Positional arguments may also be excluded by specifying their position:\n\n    >>> vpolyval.excluded.add(0)\n    >>> vpolyval([1, 2, 3], x=[0, 1])\n    array([3, 6])\n\n    The `signature` argument allows for vectorizing functions that act on\n    non-scalar arrays of fixed length. For example, you can use it for a\n    vectorized calculation of Pearson correlation coefficient and its p-value:\n\n    >>> import scipy.stats\n    >>> pearsonr = np.vectorize(scipy.stats.pearsonr,\n    ...                 signature='(n),(n)->(),()')\n    >>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])\n    (array([ 1., -1.]), array([ 0.,  0.]))\n\n    Or for a vectorized convolution:\n\n    >>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')\n    >>> convolve(np.eye(4), [1, 2, 1])\n    array([[1., 2., 1., 0., 0., 0.],\n           [0., 1., 2., 1., 0., 0.],\n           [0., 0., 1., 2., 1., 0.],\n           [0., 0., 0., 1., 2., 1.]])\n\n    ",
        "klass": "numpy.vectorize",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    RandomState(seed=None)\n\n    Container for the slow Mersenne Twister pseudo-random number generator.\n    Consider using a different BitGenerator with the Generator container\n    instead.\n\n    `RandomState` and `Generator` expose a number of methods for generating\n    random numbers drawn from a variety of probability distributions. In\n    addition to the distribution-specific arguments, each method takes a\n    keyword argument `size` that defaults to ``None``. If `size` is ``None``,\n    then a single value is generated and returned. If `size` is an integer,\n    then a 1-D array filled with generated values is returned. If `size` is a\n    tuple, then an array with that shape is filled and returned.\n\n    **Compatibility Guarantee**\n\n    A fixed bit generator using a fixed seed and a fixed series of calls to\n    'RandomState' methods using the same parameters will always produce the\n    same results up to roundoff error except when the values were incorrect.\n    `RandomState` is effectively frozen and will only receive updates that\n    are required by changes in the the internals of Numpy. More substantial\n    changes, including algorithmic improvements, are reserved for\n    `Generator`.\n\n    Parameters\n    ----------\n    seed : {None, int, array_like, BitGenerator}, optional\n        Random seed used to initialize the pseudo-random number generator or\n        an instantized BitGenerator.  If an integer or array, used as a seed for\n        the MT19937 BitGenerator. Values can be any integer between 0 and\n        2**32 - 1 inclusive, an array (or other sequence) of such integers,\n        or ``None`` (the default).  If `seed` is ``None``, then the `MT19937`\n        BitGenerator is initialized by reading data from ``/dev/urandom``\n        (or the Windows analogue) if available or seed from the clock\n        otherwise.\n\n    Notes\n    -----\n    The Python stdlib module \"random\" also contains a Mersenne Twister\n    pseudo-random number generator with a number of methods that are similar\n    to the ones available in `RandomState`. `RandomState`, besides being\n    NumPy-aware, has the advantage that it provides a much larger number\n    of probability distributions to choose from.\n\n    See Also\n    --------\n    Generator\n    MT19937\n    :ref:`bit_generator`\n\n    ",
        "klass": "numpy.random.mtrand.RandomState",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    RandomState(seed=None)\n\n    Container for the slow Mersenne Twister pseudo-random number generator.\n    Consider using a different BitGenerator with the Generator container\n    instead.\n\n    `RandomState` and `Generator` expose a number of methods for generating\n    random numbers drawn from a variety of probability distributions. In\n    addition to the distribution-specific arguments, each method takes a\n    keyword argument `size` that defaults to ``None``. If `size` is ``None``,\n    then a single value is generated and returned. If `size` is an integer,\n    then a 1-D array filled with generated values is returned. If `size` is a\n    tuple, then an array with that shape is filled and returned.\n\n    **Compatibility Guarantee**\n\n    A fixed bit generator using a fixed seed and a fixed series of calls to\n    'RandomState' methods using the same parameters will always produce the\n    same results up to roundoff error except when the values were incorrect.\n    `RandomState` is effectively frozen and will only receive updates that\n    are required by changes in the the internals of Numpy. More substantial\n    changes, including algorithmic improvements, are reserved for\n    `Generator`.\n\n    Parameters\n    ----------\n    seed : {None, int, array_like, BitGenerator}, optional\n        Random seed used to initialize the pseudo-random number generator or\n        an instantized BitGenerator.  If an integer or array, used as a seed for\n        the MT19937 BitGenerator. Values can be any integer between 0 and\n        2**32 - 1 inclusive, an array (or other sequence) of such integers,\n        or ``None`` (the default).  If `seed` is ``None``, then the `MT19937`\n        BitGenerator is initialized by reading data from ``/dev/urandom``\n        (or the Windows analogue) if available or seed from the clock\n        otherwise.\n\n    Notes\n    -----\n    The Python stdlib module \"random\" also contains a Mersenne Twister\n    pseudo-random number generator with a number of methods that are similar\n    to the ones available in `RandomState`. `RandomState`, besides being\n    NumPy-aware, has the advantage that it provides a much larger number\n    of probability distributions to choose from.\n\n    See Also\n    --------\n    Generator\n    MT19937\n    :ref:`bit_generator`\n\n    ",
        "klass": "autograd.numpy.random.RandomState",
        "module": "numpy"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "Thread that periodically calls a given function.\n\n    :type interval: int or float\n    :param interval: Seconds between calls to the function.\n\n    :type function: function\n    :param function: The function to call.\n\n    :type args: list\n    :param args: The args passed in while calling `function`.\n\n    :type kwargs: dict\n    :param args: The kwargs passed in while calling `function`.\n    ",
        "klass": "opencensus.common.schedule.PeriodicTask",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.common.transports.base.Transport"
        ],
        "class_docstring": "Asynchronous transport that uses a background thread.\n\n    :type exporter: :class:`~opencensus.trace.base_exporter.Exporter` or\n                    :class:`~opencensus.stats.base_exporter.StatsExporter`\n    :param exporter: Instance of Exporter object.\n\n    :type grace_period: float\n    :param grace_period: The amount of time to wait for pending data to\n                         be submitted when the process is shutting down.\n\n    :type max_batch_size: int\n    :param max_batch_size: The maximum number of items to send at a time\n                           in the background thread.\n\n    :type wait_period: int\n    :param wait_period: The amount of time to wait before sending the next\n                        batch of data.\n    ",
        "klass": "opencensus.common.transports.async_.AsyncTransport",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A background thread that exports batches of data.\n\n    :type exporter: :class:`~opencensus.trace.base_exporter.Exporter` or\n                    :class:`~opencensus.stats.base_exporter.StatsExporter`\n    :param exporter: Instance of Exporter object.\n\n    :type grace_period: float\n    :param grace_period: The amount of time to wait for pending data to\n                         be submitted when the process is shutting down.\n\n    :type max_batch_size: int\n    :param max_batch_size: The maximum number of items to send at a time\n                           in the background thread.\n\n    :type wait_period: int\n    :param wait_period: The amount of time to wait before sending the next\n                        batch of data.\n    ",
        "klass": "opencensus.common.transports.async_._Worker",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for transport.\n\n    Subclasses of :class:`Transport` must override :meth:`export`.\n    ",
        "klass": "opencensus.common.transports.base.Transport",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.gauge.GaugePointDouble"
        ],
        "class_docstring": "A `GaugePointDouble` that cannot decrease.",
        "klass": "opencensus.metrics.export.cumulative.CumulativePointDouble",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.gauge.GaugePointLong"
        ],
        "class_docstring": "A `GaugePointLong` that cannot decrease.",
        "klass": "opencensus.metrics.export.cumulative.CumulativePointLong",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.cumulative.DoubleCumulativeMixin",
            "opencensus.metrics.export.gauge.DerivedGauge"
        ],
        "class_docstring": "Records derived cumulative float-valued measurements.",
        "klass": "opencensus.metrics.export.cumulative.DerivedDoubleCumulative",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.cumulative.LongCumulativeMixin",
            "opencensus.metrics.export.gauge.DerivedGauge"
        ],
        "class_docstring": "Records derived cumulative int-valued measurements.",
        "klass": "opencensus.metrics.export.cumulative.DerivedLongCumulative",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.cumulative.DoubleCumulativeMixin",
            "opencensus.metrics.export.gauge.Gauge"
        ],
        "class_docstring": "Records cumulative float-valued measurements.",
        "klass": "opencensus.metrics.export.cumulative.DoubleCumulative",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.metrics.export.cumulative.LongCumulativeMixin",
            "opencensus.metrics.export.gauge.Gauge"
        ],
        "class_docstring": "Records cumulative int-valued measurements.",
        "klass": "opencensus.metrics.export.cumulative.LongCumulative",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Container class for MetricProducers to be used by exporters.\n\n    :type metric_producers: iterable(class: 'MetricProducer')\n    :param metric_producers: Optional initial metric producers.\n    ",
        "klass": "opencensus.metrics.export.metric_producer.MetricProducerManager",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.common.schedule.PeriodicTask"
        ],
        "class_docstring": "Thread that periodically calls a given function.\n\n    :type interval: int or float\n    :param interval: Seconds between calls to the function.\n\n    :type function: function\n    :param function: The function to call.\n\n    :type args: list\n    :param args: The args passed in while calling `function`.\n\n    :type kwargs: dict\n    :param args: The kwargs passed in while calling `function`.\n    ",
        "klass": "opencensus.metrics.transport.PeriodicMetricTask",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "View Data is the aggregated data for a particular view\n\n    :type view:\n    :param view: The view associated with this view data\n\n    :type start_time: datetime\n    :param start_time: the start time for this view data\n\n    :type end_time: datetime\n    :param end_time: the end time for this view data\n\n    ",
        "klass": "opencensus.stats.view_data.ViewData",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "View Manager allows the registering of Views for collecting stats\n    and receiving stats data as View Data",
        "klass": "opencensus.stats.view_manager.ViewManager",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for opencensus trace request exporters.\n\n    Subclasses of :class:`Exporter` must override :meth:`export`.\n    ",
        "klass": "opencensus.trace.base_exporter.Exporter",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for Opencensus spans.\n    Subclasses of :class:`BaseSpan` must implement the below methods.\n    ",
        "klass": "opencensus.trace.base_span.BaseSpan",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.trace.base_exporter.Exporter"
        ],
        "class_docstring": "A exporter to export the spans data to python logging. Also can use\n    handlers like CloudLoggingHandler to log to Stackdriver Logging API.\n\n    :type handler: :class:`logging.handler`\n    :param handler: the handler to attach to the global handler\n\n    :type transport: :class:`type`\n    :param transport: Class for creating new transport objects. It should\n                      extend from the base_exporter :class:`.Transport` type\n                      and implement :meth:`.Transport.export`. Defaults to\n                      :class:`.SyncTransport`. The other option is\n                      :class:`.AsyncTransport`.\n\n    Example:\n\n    .. code-block:: python\n\n        import google.cloud.logging\n        from google.cloud.logging.handlers import CloudLoggingHandler\n        from opencensus.trace import logging_exporter\n\n        client = google.cloud.logging.Client()\n        cloud_handler = CloudLoggingHandler(client)\n        exporter = logging_exporter.LoggingExporter(handler=cloud_handler)\n\n        exporter.export(your_spans_list)\n\n    Or initialize a context tracer with the logging exporter, then the traces\n    will be exported to logging when finished.\n    ",
        "klass": "opencensus.trace.logging_exporter.LoggingExporter",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Propagator for the B3 HTTP header format.\n\n    See: https://github.com/openzipkin/b3-propagation\n    ",
        "klass": "opencensus.trace.propagation.b3_format.B3FormatPropagator",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This propagator contains the method for serializing and deserializing\n    SpanContext using a binary format.\n\n    See: https://github.com/census-instrumentation/opencensus-specs/blob/\n         master/encodings/BinaryEncoding.md\n\n    Example:\n        [SpanContext]\n            trace_id: hex string with length 32.\n                e.g. 'a0b72ca15c1a4bd18962d0ac59dc90b9'\n            span_id: hex string with length 16.\n                e.g. 'a0b72ca15c1a4bd1'\n            enabled (trace option): bool.\n                e.g. True\n        [Binary Format]\n            trace_id: Bytes with length 16.\n                e.g. b'\u00a0\u00b7,\u00a1\\\u001aK\u00d1\u0089b\u00d0\u00acY\u00dc\u0090\u00b9'\n            span_id: Bytes with length 8.\n                e.g. b'\u0000\u00f0g\u00aa\u000b\u00a9\u0002\u00b7'\n            trace_option: Byte with length 1.\n                e.g. b'\u0001'\n    ",
        "klass": "opencensus.trace.propagation.binary_format.BinaryFormatPropagator",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class is for converting the trace header in google cloud format\n    and generate a SpanContext, or converting a SpanContext to a google cloud\n    format header. Later we will add implementation for supporting other\n    format like binary format and zipkin, opencensus format.\n    ",
        "klass": "opencensus.trace.propagation.google_cloud_format.GoogleCloudFormatPropagator",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The Tracer is for tracing a request for web applications.\n\n    :type span_context: :class:`~opencensus.trace.span_context.SpanContext`\n    :param span_context: SpanContext encapsulates the current context within\n                         the request's trace.\n\n    :type sampler: :class:`~opencensus.trace.samplers.base.Sampler`\n    :param sampler: Instances of Sampler objects. Defaults to\n                    :class:`.ProbabilitySampler`. Other options include\n                    :class:`.AlwaysOnSampler` and :class:`.AlwaysOffSampler`.\n\n    :type exporter: :class:`~opencensus.trace.base_exporter.exporter`\n    :param exporter: Instances of exporter objects. Default to\n                     :class:`.Printexporter`. The rest options are\n                     :class:`.Fileexporter`, :class:`.Printexporter`,\n                     :class:`.Loggingexporter`, :class:`.Zipkinexporter`,\n                     :class:`.GoogleCloudexporter`\n    ",
        "klass": "opencensus.trace.tracer.Tracer",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for Opencensus tracers.\n\n    Subclasses of :class:`Tracer` must implement the below methods.\n    ",
        "klass": "opencensus.trace.tracers.base.Tracer",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "opencensus.trace.tracers.base.Tracer"
        ],
        "class_docstring": "The interface for tracing a request context.\n\n    :type span_context: :class:`~opencensus.trace.span_context.SpanContext`\n    :param span_context: SpanContext encapsulates the current context within\n                         the request's trace.\n    ",
        "klass": "opencensus.trace.tracers.context_tracer.ContextTracer",
        "module": "opencensus"
    },
    {
        "base_classes": [
            "suds.UnicodeMixin"
        ],
        "class_docstring": "\n    A lightweight web services client.\n    I{(2nd generation)} API.\n    @ivar wsdl: The WSDL object.\n    @type wsdl:L{Definitions}\n    @ivar service: The service proxy used to invoke operations.\n    @type service: L{Service}\n    @ivar factory: The factory used to create objects.\n    @type factory: L{Factory}\n    @ivar sd: The service definition\n    @type sd: L{ServiceDefinition}\n    @ivar messages: The last sent/received messages.\n    @type messages: str[2]\n    ",
        "klass": "suds.client.Client",
        "module": "suds"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "Raises *exception* in the current greenthread after *timeout* seconds.\n\n    When *exception* is omitted or ``None``, the :class:`Timeout` instance\n    itself is raised. If *seconds* is None, the timer is not scheduled, and is\n    only useful if you're planning to raise it directly.\n\n    Timeout objects are context managers, and so can be used in with statements.\n    When used in a with statement, if *exception* is ``False``, the timeout is\n    still raised, but the context manager suppresses it, so the code outside the\n    with-block won't see it.\n    ",
        "klass": "eventlet.timeout.Timeout",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.schema.SchemaItem"
        ],
        "class_docstring": "A collection of :class:`.Table` objects and their associated schema\n    constructs.\n\n    Holds a collection of :class:`.Table` objects as well as\n    an optional binding to an :class:`.Engine` or\n    :class:`.Connection`.  If bound, the :class:`.Table` objects\n    in the collection and their columns may participate in implicit SQL\n    execution.\n\n    The :class:`.Table` objects themselves are stored in the\n    :attr:`.MetaData.tables` dictionary.\n\n    :class:`.MetaData` is a thread-safe object for read operations.\n    Construction of new tables within a single :class:`.MetaData` object,\n    either explicitly or via reflection, may not be completely thread-safe.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    ",
        "klass": "sqlalchemy.sql.schema.MetaData",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.SchemaItem",
            "sqlalchemy.sql.selectable.TableClause",
            "migrate.changeset.schema.ChangesetTable"
        ],
        "class_docstring": "Represent a table in a database.\n\n    e.g.::\n\n        mytable = Table(\"mytable\", metadata,\n                        Column('mytable_id', Integer, primary_key=True),\n                        Column('value', String(50))\n                   )\n\n    The :class:`.Table` object constructs a unique instance of itself based\n    on its name and optional schema name within the given\n    :class:`.MetaData` object. Calling the :class:`.Table`\n    constructor with the same name and same :class:`.MetaData` argument\n    a second time will return the *same* :class:`.Table` object - in this way\n    the :class:`.Table` constructor acts as a registry function.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    Constructor arguments are as follows:\n\n    :param name: The name of this table as represented in the database.\n\n        The table name, along with the value of the ``schema`` parameter,\n        forms a key which uniquely identifies this :class:`.Table` within\n        the owning :class:`.MetaData` collection.\n        Additional calls to :class:`.Table` with the same name, metadata,\n        and schema name will return the same :class:`.Table` object.\n\n        Names which contain no upper case characters\n        will be treated as case insensitive names, and will not be quoted\n        unless they are a reserved word or contain special characters.\n        A name with any number of upper case characters is considered\n        to be case sensitive, and will be sent as quoted.\n\n        To enable unconditional quoting for the table name, specify the flag\n        ``quote=True`` to the constructor, or use the :class:`.quoted_name`\n        construct to specify the name.\n\n    :param metadata: a :class:`.MetaData` object which will contain this\n        table.  The metadata is used as a point of association of this table\n        with other tables which are referenced via foreign key.  It also\n        may be used to associate this table with a particular\n        :class:`.Connectable`.\n\n    :param \\*args: Additional positional arguments are used primarily\n        to add the list of :class:`.Column` objects contained within this\n        table. Similar to the style of a CREATE TABLE statement, other\n        :class:`.SchemaItem` constructs may be added here, including\n        :class:`.PrimaryKeyConstraint`, and :class:`.ForeignKeyConstraint`.\n\n    :param autoload: Defaults to False, unless :paramref:`.Table.autoload_with`\n        is set in which case it defaults to True; :class:`.Column` objects\n        for this table should be reflected from the database, possibly\n        augmenting or replacing existing :class:`.Column` objects that were\n        explicitly specified.\n\n        .. versionchanged:: 1.0.0 setting the :paramref:`.Table.autoload_with`\n           parameter implies that :paramref:`.Table.autoload` will default\n           to True.\n\n        .. seealso::\n\n            :ref:`metadata_reflection_toplevel`\n\n    :param autoload_replace: Defaults to ``True``; when using\n        :paramref:`.Table.autoload`\n        in conjunction with :paramref:`.Table.extend_existing`, indicates\n        that :class:`.Column` objects present in the already-existing\n        :class:`.Table` object should be replaced with columns of the same\n        name retrieved from the autoload process.   When ``False``, columns\n        already present under existing names will be omitted from the\n        reflection process.\n\n        Note that this setting does not impact :class:`.Column` objects\n        specified programmatically within the call to :class:`.Table` that\n        also is autoloading; those :class:`.Column` objects will always\n        replace existing columns of the same name when\n        :paramref:`.Table.extend_existing` is ``True``.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.extend_existing`\n\n    :param autoload_with: An :class:`.Engine` or :class:`.Connection` object\n        with which this :class:`.Table` object will be reflected; when\n        set to a non-None value, it implies that :paramref:`.Table.autoload`\n        is ``True``.   If left unset, but :paramref:`.Table.autoload` is\n        explicitly set to ``True``, an autoload operation will attempt to\n        proceed by locating an :class:`.Engine` or :class:`.Connection` bound\n        to the underlying :class:`.MetaData` object.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n    :param extend_existing: When ``True``, indicates that if this\n        :class:`.Table` is already present in the given :class:`.MetaData`,\n        apply further arguments within the constructor to the existing\n        :class:`.Table`.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        :paramref:`.Table.extend_existing` will also work in conjunction\n        with :paramref:`.Table.autoload` to run a new reflection\n        operation against the database, even if a :class:`.Table`\n        of the same name is already present in the target\n        :class:`.MetaData`; newly reflected :class:`.Column` objects\n        and other options will be added into the state of the\n        :class:`.Table`, potentially overwriting existing columns\n        and options of the same name.\n\n        As is always the case with :paramref:`.Table.autoload`,\n        :class:`.Column` objects can be specified in the same :class:`.Table`\n        constructor, which will take precedence.  Below, the existing\n        table ``mytable`` will be augmented with :class:`.Column` objects\n        both reflected from the database, as well as the given :class:`.Column`\n        named \"y\"::\n\n            Table(\"mytable\", metadata,\n                        Column('y', Integer),\n                        extend_existing=True,\n                        autoload=True,\n                        autoload_with=engine\n                    )\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.autoload_replace`\n\n            :paramref:`.Table.keep_existing`\n\n\n    :param implicit_returning: True by default - indicates that\n        RETURNING can be used by default to fetch newly inserted primary key\n        values, for backends which support this.  Note that\n        create_engine() also provides an implicit_returning flag.\n\n    :param include_columns: A list of strings indicating a subset of\n        columns to be loaded via the ``autoload`` operation; table columns who\n        aren't present in this list will not be represented on the resulting\n        ``Table`` object. Defaults to ``None`` which indicates all columns\n        should be reflected.\n\n    :param resolve_fks: Whether or not to reflect :class:`.Table` objects\n        related to this one via :class:`.ForeignKey` objects, when\n        :paramref:`.Table.autoload` or :paramref:`.Table.autoload_with` is\n        specified.   Defaults to True.  Set to False to disable reflection of\n        related tables as :class:`.ForeignKey` objects are encountered; may be\n        used either to save on SQL calls or to avoid issues with related tables\n        that can't be accessed. Note that if a related table is already present\n        in the :class:`.MetaData` collection, or becomes present later, a\n        :class:`.ForeignKey` object associated with this :class:`.Table` will\n        resolve to that table normally.\n\n        .. versionadded:: 1.3\n\n        .. seealso::\n\n            :paramref:`.MetaData.reflect.resolve_fks`\n\n\n    :param info: Optional data dictionary which will be populated into the\n        :attr:`.SchemaItem.info` attribute of this object.\n\n    :param keep_existing: When ``True``, indicates that if this Table\n        is already present in the given :class:`.MetaData`, ignore\n        further arguments within the constructor to the existing\n        :class:`.Table`, and return the :class:`.Table` object as\n        originally created. This is to allow a function that wishes\n        to define a new :class:`.Table` on first call, but on\n        subsequent calls will return the same :class:`.Table`,\n        without any of the declarations (particularly constraints)\n        being applied a second time.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        .. seealso::\n\n            :paramref:`.Table.extend_existing`\n\n    :param listeners: A list of tuples of the form ``(<eventname>, <fn>)``\n        which will be passed to :func:`.event.listen` upon construction.\n        This alternate hook to :func:`.event.listen` allows the establishment\n        of a listener function specific to this :class:`.Table` before\n        the \"autoload\" process begins.  Particularly useful for\n        the :meth:`.DDLEvents.column_reflect` event::\n\n            def listen_for_reflect(table, column_info):\n                \"handle the column reflection event\"\n                # ...\n\n            t = Table(\n                'sometable',\n                autoload=True,\n                listeners=[\n                    ('column_reflect', listen_for_reflect)\n                ])\n\n    :param mustexist: When ``True``, indicates that this Table must already\n        be present in the given :class:`.MetaData` collection, else\n        an exception is raised.\n\n    :param prefixes:\n        A list of strings to insert after CREATE in the CREATE TABLE\n        statement.  They will be separated by spaces.\n\n    :param quote: Force quoting of this table's name on or off, corresponding\n        to ``True`` or ``False``.  When left at its default of ``None``,\n        the column identifier will be quoted according to whether the name is\n        case sensitive (identifiers with at least one upper case character are\n        treated as case sensitive), or if it's a reserved word.  This flag\n        is only needed to force quoting of a reserved word which is not known\n        by the SQLAlchemy dialect.\n\n    :param quote_schema: same as 'quote' but applies to the schema identifier.\n\n    :param schema: The schema name for this table, which is required if\n        the table resides in a schema other than the default selected schema\n        for the engine's database connection.  Defaults to ``None``.\n\n        If the owning :class:`.MetaData` of this :class:`.Table` specifies its\n        own :paramref:`.MetaData.schema` parameter, then that schema name will\n        be applied to this :class:`.Table` if the schema parameter here is set\n        to ``None``.  To set a blank schema name on a :class:`.Table` that\n        would otherwise use the schema set on the owning :class:`.MetaData`,\n        specify the special symbol :attr:`.BLANK_SCHEMA`.\n\n        .. versionadded:: 1.0.14  Added the :attr:`.BLANK_SCHEMA` symbol to\n           allow a :class:`.Table` to have a blank schema name even when the\n           parent :class:`.MetaData` specifies :paramref:`.MetaData.schema`.\n\n        The quoting rules for the schema name are the same as those for the\n        ``name`` parameter, in that quoting is applied for reserved words or\n        case-sensitive names; to enable unconditional quoting for the schema\n        name, specify the flag ``quote_schema=True`` to the constructor, or use\n        the :class:`.quoted_name` construct to specify the name.\n\n    :param useexisting: the same as :paramref:`.Table.extend_existing`.\n\n    :param comment: Optional string that will render an SQL comment on table\n         creation.\n\n         .. versionadded:: 1.2 Added the :paramref:`.Table.comment` parameter\n            to :class:`.Table`.\n\n    :param \\**kw: Additional keyword arguments not mentioned above are\n        dialect specific, and passed in the form ``<dialectname>_<argname>``.\n        See the documentation regarding an individual dialect at\n        :ref:`dialect_toplevel` for detail on documented arguments.\n\n    ",
        "klass": "sqlalchemy.sql.schema.Table",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "migrate.changeset.constraint.ConstraintChangeset",
            "sqlalchemy.sql.schema.ForeignKeyConstraint"
        ],
        "class_docstring": "Construct ForeignKeyConstraint\n\n    Migrate's additional parameters:\n\n    :param columns: Columns in constraint\n    :param refcolumns: Columns that this FK reffers to in another table.\n    :param table: If columns are passed as strings, this kw is required\n    :type table: Table instance\n    :type columns: list of strings or Column instances\n    :type refcolumns: list of strings or Column instances\n    ",
        "klass": "migrate.changeset.constraint.ForeignKeyConstraint",
        "module": "migrate"
    },
    {
        "base_classes": [
            "migrate.changeset.constraint.ConstraintChangeset",
            "sqlalchemy.sql.schema.UniqueConstraint"
        ],
        "class_docstring": "Construct UniqueConstraint\n\n    Migrate's additional parameters:\n\n    :param cols: Columns in constraint.\n    :param table: If columns are passed as strings, this kw is required\n    :type table: Table instance\n    :type cols: strings or Column instances\n\n    .. versionadded:: 0.6.0\n    ",
        "klass": "migrate.changeset.constraint.UniqueConstraint",
        "module": "migrate"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The central template object.  This class represents a compiled template\n    and is used to evaluate it.\n\n    Normally the template object is generated from an :class:`Environment` but\n    it also has a constructor that makes it possible to create a template\n    instance directly using the constructor.  It takes the same arguments as\n    the environment constructor but it's not possible to specify a loader.\n\n    Every template object has a few methods and members that are guaranteed\n    to exist.  However it's important that a template object should be\n    considered immutable.  Modifications on the object are not supported.\n\n    Template objects created from the constructor rather than an environment\n    do have an `environment` attribute that points to a temporary environment\n    that is probably shared with other templates created with the constructor\n    and compatible settings.\n\n    >>> template = Template('Hello {{ name }}!')\n    >>> template.render(name='John Doe') == u'Hello John Doe!'\n    True\n    >>> stream = template.stream(name='John Doe')\n    >>> next(stream) == u'Hello John Doe!'\n    True\n    >>> next(stream)\n    Traceback (most recent call last):\n        ...\n    StopIteration\n    ",
        "klass": "jinja2.environment.Template",
        "module": "jinja2"
    },
    {
        "base_classes": [
            "neo.core.container.Container"
        ],
        "class_docstring": "\n    Main container gathering all the data, whether discrete or continous, for a\n    given recording session.\n\n    A block is not necessarily temporally homogeneous, in contrast to :class:`Segment`.\n\n    *Usage*::\n\n        >>> from neo.core import (Block, Segment, ChannelIndex,\n        ...                       AnalogSignal)\n        >>> from quantities import nA, kHz\n        >>> import numpy as np\n        >>>\n        >>> # create a Block with 3 Segment and 2 ChannelIndex objects\n        ,,, blk = Block()\n        >>> for ind in range(3):\n        ...     seg = Segment(name='segment %d' % ind, index=ind)\n        ...     blk.segments.append(seg)\n        ...\n        >>> for ind in range(2):\n        ...     chx = ChannelIndex(name='Array probe %d' % ind,\n        ...                        index=np.arange(64))\n        ...     blk.channel_indexes.append(chx)\n        ...\n        >>> # Populate the Block with AnalogSignal objects\n        ... for seg in blk.segments:\n        ...     for chx in blk.channel_indexes:\n        ...         a = AnalogSignal(np.random.randn(10000, 64)*nA,\n        ...                          sampling_rate=10*kHz)\n        ...         chx.analogsignals.append(a)\n        ...         seg.analogsignals.append(a)\n\n    *Required attributes/properties*:\n        None\n\n    *Recommended attributes/properties*:\n        :name: (str) A label for the dataset.\n        :description: (str) Text description.\n        :file_origin: (str) Filesystem path or URL of the original data file.\n        :file_datetime: (datetime) The creation date and time of the original\n            data file.\n        :rec_datetime: (datetime) The date and time of the original recording.\n\n    *Properties available on this object*:\n        :list_units: descends through hierarchy and returns a list of\n            :class:`Unit` objects existing in the block. This shortcut exists\n            because a common analysis case is analyzing all neurons that\n            you recorded in a session.\n\n    Note: Any other additional arguments are assumed to be user-specific\n    metadata and stored in :attr:`annotations`.\n\n    *Container of*:\n        :class:`Segment`\n        :class:`ChannelIndex`\n\n    ",
        "klass": "neo.Block",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.io.baseio.BaseIO"
        ],
        "class_docstring": "\n    Class for reading and writing NIX files.\n    ",
        "klass": "neo.io.nixio.NixIO",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.core.container.Container"
        ],
        "class_docstring": "\n    Main container gathering all the data, whether discrete or continous, for a\n    given recording session.\n\n    A block is not necessarily temporally homogeneous, in contrast to :class:`Segment`.\n\n    *Usage*::\n\n        >>> from neo.core import (Block, Segment, ChannelIndex,\n        ...                       AnalogSignal)\n        >>> from quantities import nA, kHz\n        >>> import numpy as np\n        >>>\n        >>> # create a Block with 3 Segment and 2 ChannelIndex objects\n        ,,, blk = Block()\n        >>> for ind in range(3):\n        ...     seg = Segment(name='segment %d' % ind, index=ind)\n        ...     blk.segments.append(seg)\n        ...\n        >>> for ind in range(2):\n        ...     chx = ChannelIndex(name='Array probe %d' % ind,\n        ...                        index=np.arange(64))\n        ...     blk.channel_indexes.append(chx)\n        ...\n        >>> # Populate the Block with AnalogSignal objects\n        ... for seg in blk.segments:\n        ...     for chx in blk.channel_indexes:\n        ...         a = AnalogSignal(np.random.randn(10000, 64)*nA,\n        ...                          sampling_rate=10*kHz)\n        ...         chx.analogsignals.append(a)\n        ...         seg.analogsignals.append(a)\n\n    *Required attributes/properties*:\n        None\n\n    *Recommended attributes/properties*:\n        :name: (str) A label for the dataset.\n        :description: (str) Text description.\n        :file_origin: (str) Filesystem path or URL of the original data file.\n        :file_datetime: (datetime) The creation date and time of the original\n            data file.\n        :rec_datetime: (datetime) The date and time of the original recording.\n\n    *Properties available on this object*:\n        :list_units: descends through hierarchy and returns a list of\n            :class:`Unit` objects existing in the block. This shortcut exists\n            because a common analysis case is analyzing all neurons that\n            you recorded in a session.\n\n    Note: Any other additional arguments are assumed to be user-specific\n    metadata and stored in :attr:`annotations`.\n\n    *Container of*:\n        :class:`Segment`\n        :class:`ChannelIndex`\n\n    ",
        "klass": "neo.core.Block",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.io.proxyobjects.BaseProxy"
        ],
        "class_docstring": "\n    This object mimic AnalogSignal except that it does not\n    have the signals array itself. All attributes and annotations are here.\n\n    The goal is to postpone the loading of data into memory\n    when reading a file with the new lazy load system based\n    on neo.rawio.\n\n    This object must not be constructed directly but is given\n    neo.io when lazy=True instead of a true AnalogSignal.\n\n    The AnalogSignalProxy is able to load:\n      * only a slice of time\n      * only a subset of channels\n      * have an internal raw magnitude identic to the file (int16) with\n        a pq.CompoundUnit().\n\n    Usage:\n    >>> proxy_anasig = AnalogSignalProxy(rawio=self.reader,\n                                                                global_channel_indexes=None,\n                                                                block_index=0,\n                                                                seg_index=0)\n    >>> anasig = proxy_anasig.load()\n    >>> slice_of_anasig = proxy_anasig.load(time_slice=(1.*pq.s, 2.*pq.s))\n    >>> some_channel_of_anasig = proxy_anasig.load(channel_indexes=[0,5,10])\n\n    ",
        "klass": "neo.io.proxyobjects.AnalogSignalProxy",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.io.proxyobjects._EventOrEpoch"
        ],
        "class_docstring": "\n    This object mimic Epoch except that it does not\n    have the times nor labels nor durations.\n    All other attributes and annotations are here.\n\n    The goal is to postpone the loading of data into memory\n    when reading a file with the new lazy load system based\n    on neo.rawio.\n\n    This object must not be constructed directly but is given\n    neo.io when lazy=True instead of a true Epoch.\n\n    The EpochProxy is able to load:\n      * only a slice of time\n\n    Usage:\n    >>> proxy_epoch = EpochProxy(rawio=self.reader, event_channel_index=0,\n                        block_index=0, seg_index=0,)\n    >>> epoch = proxy_epoch.load()\n    >>> slice_of_epoch = proxy_epoch.load(time_slice=(1.*pq.s, 2.*pq.s))\n\n    ",
        "klass": "neo.io.proxyobjects.EpochProxy",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.io.proxyobjects._EventOrEpoch"
        ],
        "class_docstring": "\n    This object mimic Event except that it does not\n    have the times nor labels.\n    All other attributes and annotations are here.\n\n    The goal is to postpone the loading of data into memory\n    when reading a file with the new lazy load system based\n    on neo.rawio.\n\n    This object must not be constructed directly but is given\n    neo.io when lazy=True instead of a true Event.\n\n    The EventProxy is able to load:\n      * only a slice of time\n\n    Usage:\n    >>> proxy_event = EventProxy(rawio=self.reader, event_channel_index=0,\n                        block_index=0, seg_index=0,)\n    >>> event = proxy_event.load()\n    >>> slice_of_event = proxy_event.load(time_slice=(1.*pq.s, 2.*pq.s))\n\n    ",
        "klass": "neo.io.proxyobjects.EventProxy",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.io.proxyobjects.BaseProxy"
        ],
        "class_docstring": "\n    This object mimic SpikeTrain except that it does not\n    have the spike time nor waveforms.\n    All attributes and annotations are here.\n\n    The goal is to postpone the loading of data into memory\n    when reading a file with the new lazy load system based\n    on neo.rawio.\n\n    This object must not be constructed directly but is given\n    neo.io when lazy=True instead of a true SpikeTrain.\n\n    The SpikeTrainProxy is able to load:\n      * only a slice of time\n      * load wveforms or not.\n      * have an internal raw magnitude identic to the file (generally the ticks\n        of clock in int64) or the rescale to seconds.\n\n    Usage:\n    >>> proxy_sptr = SpikeTrainProxy(rawio=self.reader, unit_channel=0,\n                        block_index=0, seg_index=0,)\n    >>> sptr = proxy_sptr.load()\n    >>> slice_of_sptr = proxy_sptr.load(time_slice=(1.*pq.s, 2.*pq.s))\n\n    ",
        "klass": "neo.io.proxyobjects.SpikeTrainProxy",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.rawio.baserawio.BaseRawIO"
        ],
        "class_docstring": "\n    Class for reading data in from a file set recorded by the Blackrock\n    (Cerebus) recording system.\n\n    Upon initialization, the class is linked to the available set of Blackrock\n    files.\n\n    Note: This routine will handle files according to specification 2.1, 2.2,\n    and 2.3. Recording pauses that may occur in file specifications 2.2 and\n    2.3 are automatically extracted and the data set is split into different\n    segments.\n\n    The Blackrock data format consists not of a single file, but a set of\n    different files. This constructor associates itself with a set of files\n    that constitute a common data set. By default, all files belonging to\n    the file set have the same base name, but different extensions.\n    However, by using the override parameters, individual filenames can\n    be set.\n\n    Args:\n        filename (string):\n            File name (without extension) of the set of Blackrock files to\n            associate with. Any .nsX or .nev, .sif, or .ccf extensions are\n            ignored when parsing this parameter.\n        nsx_override (string):\n            File name of the .nsX files (without extension). If None,\n            filename is used.\n            Default: None.\n        nev_override (string):\n            File name of the .nev file (without extension). If None,\n            filename is used.\n            Default: None.\n        nsx_to_load (int, list, 'max', 'all' (=None)) default None:\n            IDs of nsX file from which to load data, e.g., if set to\n            5 only data from the ns5 file are loaded.\n            If 'all', then all nsX will be loaded.\n            Contrary to previsous version of the IO  (<0.7), nsx_to_load\n            must be set at the init before parse_header().\n\n    Examples:\n        >>> reader = BlackrockRawIO(filename='FileSpec2.3001', nsx_to_load=5)\n        >>> reader.parse_header()\n\n            Inspect a set of file consisting of files FileSpec2.3001.ns5 and\n            FileSpec2.3001.nev\n\n        >>> print(reader)\n\n            Display all informations about signal channels, units, segment size....\n    ",
        "klass": "neo.rawio.blackrockrawio.BlackrockRawIO",
        "module": "neo"
    },
    {
        "base_classes": [
            "neo.rawio.baserawio.BaseRawIO"
        ],
        "class_docstring": "\n    OpenEphys GUI software offers several data formats, see\n    https://open-ephys.atlassian.net/wiki/spaces/OEW/pages/491632/Data+format\n\n    This class implements the legacy OpenEphys format here\n    https://open-ephys.atlassian.net/wiki/spaces/OEW/pages/65667092/Open+Ephys+format\n\n    The OpenEphys group already proposes some tools here:\n    https://github.com/open-ephys/analysis-tools/blob/master/OpenEphys.py\n    but (i) there is no package at PyPI and (ii) those tools read everything in memory.\n\n    The format is directory based with several files:\n        * .continuous\n        * .events\n        * .spikes\n\n    This implementation is based on:\n      * this code https://github.com/open-ephys/analysis-tools/blob/master/Python3/OpenEphys.py\n        written by Dan Denman and Josh Siegle\n      * a previous PR by Cristian Tatarau at Charit\u00e9 Berlin\n\n    In contrast to previous code for reading this format, here all data use memmap so it should\n    be super fast and light compared to legacy code.\n\n    When the acquisition is stopped and restarted then files are named ``*_2``, ``*_3``.\n    In that case this class creates a new Segment. Note that timestamps are reset in this\n    situation.\n\n    Limitation :\n      * Works only if all continuous channels have the same sampling rate, which is a reasonable\n        hypothesis.\n      * When the recording is stopped and restarted all continuous files will contain gaps.\n        Ideally this would lead to a new Segment but this use case is not implemented due to its\n        complexity.\n        Instead it will raise an error.\n\n    Special cases:\n      * Normaly all continuous files have the same first timestamp and length. In situations\n        where it is not the case all files are clipped to the smallest one so that they are all\n        aligned,\n        and a warning is emitted.\n    ",
        "klass": "neo.rawio.openephysrawio.OpenEphysRawIO",
        "module": "neo"
    },
    {
        "base_classes": [
            "ipaddress._BaseV4",
            "ipaddress._BaseNetwork"
        ],
        "class_docstring": "This class represents and manipulates 32-bit IPv4 network + addresses..\n\n    Attributes: [examples for IPv4Network('192.0.2.0/27')]\n        .network_address: IPv4Address('192.0.2.0')\n        .hostmask: IPv4Address('0.0.0.31')\n        .broadcast_address: IPv4Address('192.0.2.32')\n        .netmask: IPv4Address('255.255.255.224')\n        .prefixlen: 27\n\n    ",
        "klass": "ipaddress.IPv4Network",
        "module": "ipaddress"
    },
    {
        "base_classes": [
            "ipaddress._BaseV6",
            "ipaddress._BaseNetwork"
        ],
        "class_docstring": "This class represents and manipulates 128-bit IPv6 networks.\n\n    Attributes: [examples for IPv6('2001:db8::1000/124')]\n        .network_address: IPv6Address('2001:db8::1000')\n        .hostmask: IPv6Address('::f')\n        .broadcast_address: IPv6Address('2001:db8::100f')\n        .netmask: IPv6Address('ffff:ffff:ffff:ffff:ffff:ffff:ffff:fff0')\n        .prefixlen: 124\n\n    ",
        "klass": "ipaddress.IPv6Network",
        "module": "ipaddress"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Simple doubly linked list of objects with LinkableItem inferface",
        "klass": "pykit.adt.LinkedList",
        "module": "pykit"
    },
    {
        "base_classes": [
            "requests.exceptions.RequestException"
        ],
        "class_docstring": "An HTTP error occurred.",
        "klass": "requests.exceptions.HTTPError",
        "module": "requests"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The :class:`Response <Response>` object, which contains a\n    server's response to an HTTP request.\n    ",
        "klass": "requests.Response",
        "module": "requests"
    },
    {
        "base_classes": [
            "requests.sessions.SessionRedirectMixin"
        ],
        "class_docstring": "A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('https://httpbin.org/get')\n      <Response [200]>\n\n    Or as a context manager::\n\n      >>> with requests.Session() as s:\n      >>>     s.get('https://httpbin.org/get')\n      <Response [200]>\n    ",
        "klass": "requests.sessions.Session",
        "module": "requests"
    },
    {
        "base_classes": [
            "requests.sessions.SessionRedirectMixin"
        ],
        "class_docstring": "A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('https://httpbin.org/get')\n      <Response [200]>\n\n    Or as a context manager::\n\n      >>> with requests.Session() as s:\n      >>>     s.get('https://httpbin.org/get')\n      <Response [200]>\n    ",
        "klass": "requests.Session",
        "module": "requests"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "A case-insensitive ``dict``-like object.\n\n    Implements all methods and operations of\n    ``MutableMapping`` as well as dict's ``copy``. Also\n    provides ``lower_items``.\n\n    All keys are expected to be strings. The structure remembers the\n    case of the last key to be set, and ``iter(instance)``,\n    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``\n    will contain case-sensitive keys. However, querying and contains\n    testing is case insensitive::\n\n        cid = CaseInsensitiveDict()\n        cid['Accept'] = 'application/json'\n        cid['aCCEPT'] == 'application/json'  # True\n        list(cid) == ['Accept']  # True\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header, regardless\n    of how the header name was originally stored.\n\n    If the constructor, ``.update``, or equality comparison\n    operations are given keys that have equal ``.lower()``s, the\n    behavior is undefined.\n    ",
        "klass": "requests.structures.CaseInsensitiveDict",
        "module": "requests"
    },
    {
        "base_classes": [
            "urllib3.request.RequestMethods"
        ],
        "class_docstring": "\n    Allows for arbitrary requests while transparently keeping track of\n    necessary connection pools for you.\n\n    :param num_pools:\n        Number of connection pools to cache before discarding the least\n        recently used pool.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param \\**connection_pool_kw:\n        Additional parameters are used to create fresh\n        :class:`urllib3.connectionpool.ConnectionPool` instances.\n\n    Example::\n\n        >>> manager = PoolManager(num_pools=2)\n        >>> r = manager.request('GET', 'http://google.com/')\n        >>> r = manager.request('GET', 'http://google.com/mail')\n        >>> r = manager.request('GET', 'http://yahoo.com/')\n        >>> len(manager.pools)\n        2\n\n    ",
        "klass": "urllib3.poolmanager.PoolManager",
        "module": "urllib3"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Telnet interface class.\n\n    An instance of this class represents a connection to a telnet\n    server.  The instance is initially not connected; the open()\n    method must be used to establish a connection.  Alternatively, the\n    host name and optional port number can be passed to the\n    constructor, too.\n\n    Don't try to reopen an already connected instance.\n\n    This class has many read_*() methods.  Note that some of them\n    raise EOFError when the end of the connection is read, because\n    they can return an empty string for other reasons.  See the\n    individual doc strings.\n\n    read_until(expected, [timeout])\n        Read until the expected string has been seen, or a timeout is\n        hit (default is no timeout); may block.\n\n    read_all()\n        Read all data until EOF; may block.\n\n    read_some()\n        Read at least one byte or EOF; may block.\n\n    read_very_eager()\n        Read all data available already queued or on the socket,\n        without blocking.\n\n    read_eager()\n        Read either data already queued or some data available on the\n        socket, without blocking.\n\n    read_lazy()\n        Read all data in the raw queue (processing it first), without\n        doing any socket I/O.\n\n    read_very_lazy()\n        Reads all data in the cooked queue, without doing any socket\n        I/O.\n\n    read_sb_data()\n        Reads available data between SB ... SE sequence. Don't block.\n\n    set_option_negotiation_callback(callback)\n        Each time a telnet option is read on the input flow, this callback\n        (if set) is called with the following parameters :\n        callback(telnet socket, command, option)\n            option will be chr(0) when there is no option.\n        No other action is done afterwards by telnetlib.\n\n    ",
        "klass": "telnetlib.Telnet",
        "module": "telnetlib"
    },
    {
        "base_classes": [
            "nipype.pipeline.engine.base.EngineBase"
        ],
        "class_docstring": "Controls the setup and execution of a pipeline of processes.",
        "klass": "nipype.Workflow",
        "module": "nipype"
    },
    {
        "base_classes": [
            "numpy.polynomial._polybase.ABCPolyBase"
        ],
        "class_docstring": "A Legendre series class.\n\n    The Legendre class provides the standard Python numerical methods\n    '+', '-', '*', '//', '%', 'divmod', '**', and '()' as well as the\n    attributes and methods listed in the `ABCPolyBase` documentation.\n\n    Parameters\n    ----------\n    coef : array_like\n        Legendre coefficients in order of increasing degree, i.e.,\n        ``(1, 2, 3)`` gives ``1*P_0(x) + 2*P_1(x) + 3*P_2(x)``.\n    domain : (2,) array_like, optional\n        Domain to use. The interval ``[domain[0], domain[1]]`` is mapped\n        to the interval ``[window[0], window[1]]`` by shifting and scaling.\n        The default value is [-1, 1].\n    window : (2,) array_like, optional\n        Window, see `domain` for its use. The default value is [-1, 1].\n\n        .. versionadded:: 1.6.0\n\n    ",
        "klass": "numpy.polynomial.legendre.Legendre",
        "module": "numpy"
    },
    {
        "base_classes": [
            "nibabel.nifti1.Nifti1Pair",
            "nibabel.filebasedimages.SerializableImage"
        ],
        "class_docstring": " Class for single file NIfTI1 format image\n    ",
        "klass": "nibabel.nifti1.Nifti1Image",
        "module": "nibabel"
    },
    {
        "base_classes": [
            "nipype.pipeline.engine.base.EngineBase"
        ],
        "class_docstring": "\n    Wraps interface objects for use in pipeline\n\n    A Node creates a sandbox-like directory for executing the underlying\n    interface. It will copy or link inputs into this directory to ensure that\n    input data are not overwritten. A hash of the input state is used to\n    determine if the Node inputs have changed and whether the node needs to be\n    re-executed.\n\n    Examples\n    --------\n\n    >>> from nipype import Node\n    >>> from nipype.interfaces import spm\n    >>> realign = Node(spm.Realign(), 'realign')\n    >>> realign.inputs.in_files = 'functional.nii'\n    >>> realign.inputs.register_to_mean = True\n    >>> realign.run() # doctest: +SKIP\n\n    ",
        "klass": "nipype.pipeline.engine.Node",
        "module": "nipype"
    },
    {
        "base_classes": [
            "nipype.pipeline.engine.base.EngineBase"
        ],
        "class_docstring": "Controls the setup and execution of a pipeline of processes.",
        "klass": "nipype.pipeline.engine.Workflow",
        "module": "nipype"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Class with methods to open, read, write, close, list zip files.\n\n    z = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=True,\n                compresslevel=None)\n\n    file: Either the path to the file, or a file-like object.\n          If it is a path, the file will be opened and closed by ZipFile.\n    mode: The mode can be either read 'r', write 'w', exclusive create 'x',\n          or append 'a'.\n    compression: ZIP_STORED (no compression), ZIP_DEFLATED (requires zlib),\n                 ZIP_BZIP2 (requires bz2) or ZIP_LZMA (requires lzma).\n    allowZip64: if True ZipFile will create files with ZIP64 extensions when\n                needed, otherwise it will raise an exception when this would\n                be necessary.\n    compresslevel: None (default for the given compression type) or an integer\n                   specifying the level to pass to the compressor.\n                   When using ZIP_STORED or ZIP_LZMA this keyword has no effect.\n                   When using ZIP_DEFLATED integers 0 through 9 are accepted.\n                   When using ZIP_BZIP2 integers 1 through 9 are accepted.\n\n    ",
        "klass": "zipfile.ZipFile",
        "module": "zipfile"
    },
    {
        "base_classes": [
            "_compression.BaseStream"
        ],
        "class_docstring": "A file object providing transparent bzip2 (de)compression.\n\n    A BZ2File can act as a wrapper for an existing file object, or refer\n    directly to a named file on disk.\n\n    Note that BZ2File provides a *binary* file interface - data read is\n    returned as bytes, and data to be written should be given as bytes.\n    ",
        "klass": "bz2.BZ2File",
        "module": "bz2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The main top-level class that you instantiate once, or once per module.\n\n    Example usage:\n\n        ffi = FFI()\n        ffi.cdef(\"\"\"\n            int printf(const char *, ...);\n        \"\"\")\n\n        C = ffi.dlopen(None)   # standard library\n        -or-\n        C = ffi.verify()  # use a C compiler: verify the decl above is right\n\n        C.printf(\"hello, %s!\\n\", ffi.new(\"char[]\", \"world\"))\n    ",
        "klass": "cffi.api.FFI",
        "module": "cffi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The main top-level class that you instantiate once, or once per module.\n\n    Example usage:\n\n        ffi = FFI()\n        ffi.cdef(\"\"\"\n            int printf(const char *, ...);\n        \"\"\")\n\n        C = ffi.dlopen(None)   # standard library\n        -or-\n        C = ffi.verify()  # use a C compiler: verify the decl above is right\n\n        C.printf(\"hello, %s!\\n\", ffi.new(\"char[]\", \"world\"))\n    ",
        "klass": "cffi.FFI",
        "module": "cffi"
    },
    {
        "base_classes": [
            "abc.ABC"
        ],
        "class_docstring": "An abstract class for Home Assistant entities.",
        "klass": "homeassistant.helpers.entity.Entity",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Record the time when the with-block is entered.\n\n    Add all states that have changed since the start time to the return list\n    when with-block is exited.\n\n    Must be run within the event loop.\n    ",
        "klass": "homeassistant.helpers.state.AsyncTrackStates",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "homeassistant.helpers.entity.Entity"
        ],
        "class_docstring": "Mixin class for restoring previous entity state.",
        "klass": "homeassistant.helpers.restore_state.RestoreEntity",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "aiohttp.streams.AsyncStreamReaderMixin"
        ],
        "class_docstring": "An enhancement of asyncio.StreamReader.\n\n    Supports asynchronous iteration by line, chunk or as available::\n\n        async for line in reader:\n            ...\n        async for chunk in reader.iter_chunked(1024):\n            ...\n        async for slice in reader.iter_any():\n            ...\n\n    ",
        "klass": "aiohttp.streams.StreamReader",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "homeassistant.config_entries.ConfigFlow"
        ],
        "class_docstring": "Config flow for IPMA component.",
        "klass": "homeassistant.components.ipma.config_flow.IpmaFlowHandler",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "homeassistant.config_entries.ConfigFlow"
        ],
        "class_docstring": "Config flow for Met component.",
        "klass": "homeassistant.components.met.config_flow.MetFlowHandler",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a script.",
        "klass": "homeassistant.helpers.script.Script",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "homeassistant.config_entries.ConfigFlow"
        ],
        "class_docstring": "Config flow for SMHI component.",
        "klass": "homeassistant.components.smhi.config_flow.SmhiFlowHandler",
        "module": "homeassistant"
    },
    {
        "base_classes": [
            "email.mime.nonmultipart.MIMENonMultipart"
        ],
        "class_docstring": "Class for generating application/* MIME documents.",
        "klass": "email.mime.application.MIMEApplication",
        "module": "email"
    },
    {
        "base_classes": [
            "email.mime.nonmultipart.MIMENonMultipart"
        ],
        "class_docstring": "Class for generating image/* type MIME documents.",
        "klass": "email.mime.image.MIMEImage",
        "module": "email"
    },
    {
        "base_classes": [
            "email.mime.base.MIMEBase"
        ],
        "class_docstring": "Base class for MIME multipart/* type messages.",
        "klass": "email.mime.multipart.MIMEMultipart",
        "module": "email"
    },
    {
        "base_classes": [
            "email.mime.nonmultipart.MIMENonMultipart"
        ],
        "class_docstring": "Class for generating text/* type MIME documents.",
        "klass": "email.mime.text.MIMEText",
        "module": "email"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    ",
        "klass": "threading.Timer",
        "module": "threading"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "timeout context manager.\n\n    Useful in cases when you want to apply timeout logic around block\n    of code or in cases when asyncio.wait_for is not suitable. For example:\n\n    >>> with timeout(0.001):\n    ...     async with aiohttp.get('https://github.com') as r:\n    ...         await r.text()\n\n\n    timeout - value in seconds or None to disable timeout logic\n    loop - asyncio compatible event loop\n    ",
        "klass": "async_timeout.timeout",
        "module": "async_timeout"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "set() -> new empty set object\nset(iterable) -> new set object\n\nBuild an unordered collection of unique elements.",
        "klass": "set",
        "module": "set"
    },
    {
        "base_classes": [
            "py._path.local.PosixPath"
        ],
        "class_docstring": " object oriented interface to os.path and other local filesystem\n        related information.\n    ",
        "klass": "py._path.local.LocalPath",
        "module": "py"
    },
    {
        "base_classes": [
            "cinder.utils.ComparableMixin"
        ],
        "class_docstring": "This class represents an API Version Request.\n\n    This class includes convenience methods for manipulation\n    and comparison of version numbers as needed to implement\n    API microversions.\n    ",
        "klass": "cinder.api.openstack.api_version_request.APIVersionRequest",
        "module": "cinder"
    },
    {
        "base_classes": [
            "oslo_context.context.RequestContext"
        ],
        "class_docstring": "Security context and request information.\n\n    Represents the user taking a given action within the system.\n\n    ",
        "klass": "cinder.context.RequestContext",
        "module": "cinder"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tooz coordination wrapper.\n\n    Coordination member id is created from concatenated\n    `prefix` and `agent_id` parameters.\n\n    :param str agent_id: Agent identifier\n    :param str prefix: Used to provide member identifier with a\n        meaningful prefix.\n    ",
        "klass": "cinder.coordination.Coordinator",
        "module": "cinder"
    },
    {
        "base_classes": [
            "cinder.objects.base.CinderPersistentObject",
            "cinder.objects.base.CinderObject",
            "cinder.objects.base.CinderComparableObject"
        ],
        "class_docstring": "Cluster Versioned Object.\n\n    Method get_by_id supports as additional named arguments:\n        - get_services: If we want to load all services from this cluster.\n        - services_summary: If we want to load num_nodes and num_down_nodes\n                            fields.\n        - is_up: Boolean value to filter based on the cluster's up status.\n        - read_deleted: Filtering based on delete status. Default value \"no\".\n        - Any other cluster field will be used as a filter.\n    ",
        "klass": "cinder.objects.Cluster",
        "module": "cinder"
    },
    {
        "base_classes": [
            "cinder.db.base.Base"
        ],
        "class_docstring": "API for interacting volume transfers.",
        "klass": "cinder.transfer.api.API",
        "module": "cinder"
    },
    {
        "base_classes": [
            "cinder.volume.throttling.Throttle"
        ],
        "class_docstring": "Throttle disk I/O bandwidth using blkio cgroups.",
        "klass": "cinder.volume.throttling.BlkioCgroup",
        "module": "cinder"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for throttling disk I/O bandwidth",
        "klass": "cinder.volume.throttling.Throttle",
        "module": "cinder"
    },
    {
        "base_classes": [
            "cherrypy._cperror.CherryPyException"
        ],
        "class_docstring": "Exception used to return an HTTP error code (4xx-5xx) to the client.\n\n    This exception can be used to automatically send a response using a\n    http status code, with an appropriate error page. It takes an optional\n    ``status`` argument (which must be between 400 and 599); it defaults to 500\n    (\"Internal Server Error\"). It also takes an optional ``message`` argument,\n    which will be returned in the response body. See\n    `RFC2616 <http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4>`_\n    for a complete list of available error codes and when to use them.\n\n    Examples::\n\n        raise cherrypy.HTTPError(403)\n        raise cherrypy.HTTPError(\n            \"403 Forbidden\", \"You are not allowed to access this resource.\")\n    ",
        "klass": "cherrypy.HTTPError",
        "module": "cherrypy"
    },
    {
        "base_classes": [
            "http.client.HTTPConnection"
        ],
        "class_docstring": "This class allows communication via SSL.",
        "klass": "http.client.HTTPSConnection",
        "module": "http"
    },
    {
        "base_classes": [
            "http.client.HTTPConnection"
        ],
        "class_docstring": "This class allows communication via SSL.",
        "klass": "cherrypy._cpcompat.HTTPSConnection",
        "module": "http"
    },
    {
        "base_classes": [
            "configparser.RawConfigParser"
        ],
        "class_docstring": "ConfigParser implementing interpolation.",
        "klass": "configparser.ConfigParser",
        "module": "configparser"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "ConfigParser that does not do interpolation.",
        "klass": "configparser.RawConfigParser",
        "module": "configparser"
    },
    {
        "base_classes": [
            "configparser.ConfigParser"
        ],
        "class_docstring": "ConfigParser alias for backwards compatibility purposes.",
        "klass": "configparser.SafeConfigParser",
        "module": "configparser"
    },
    {
        "base_classes": [
            "mock.mock.MagicMixin",
            "mock.mock.Mock"
        ],
        "class_docstring": "\n    MagicMock is a subclass of Mock with default implementations\n    of most of the magic methods. You can use MagicMock without having to\n    configure the magic methods yourself.\n\n    If you use the `spec` or `spec_set` arguments then *only* magic\n    methods that exist in the spec will be created.\n\n    Attributes and the return value of a `MagicMock` will also be `MagicMocks`.\n    ",
        "klass": "mock.mock.MagicMock",
        "module": "mock"
    },
    {
        "base_classes": [
            "mock.mock.MagicMixin",
            "mock.mock.Mock"
        ],
        "class_docstring": "\n    MagicMock is a subclass of Mock with default implementations\n    of most of the magic methods. You can use MagicMock without having to\n    configure the magic methods yourself.\n\n    If you use the `spec` or `spec_set` arguments then *only* magic\n    methods that exist in the spec will be created.\n\n    Attributes and the return value of a `MagicMock` will also be `MagicMocks`.\n    ",
        "klass": "mock.MagicMock",
        "module": "mock"
    },
    {
        "base_classes": [
            "mock.mock.CallableMixin",
            "mock.mock.NonCallableMock"
        ],
        "class_docstring": "\n    Create a new `Mock` object. `Mock` takes several optional arguments\n    that specify the behaviour of the Mock object:\n\n    * `spec`: This can be either a list of strings or an existing object (a\n      class or instance) that acts as the specification for the mock object. If\n      you pass in an object then a list of strings is formed by calling dir on\n      the object (excluding unsupported magic attributes and methods). Accessing\n      any attribute not in this list will raise an `AttributeError`.\n\n      If `spec` is an object (rather than a list of strings) then\n      `mock.__class__` returns the class of the spec object. This allows mocks\n      to pass `isinstance` tests.\n\n    * `spec_set`: A stricter variant of `spec`. If used, attempting to *set*\n      or get an attribute on the mock that isn't on the object passed as\n      `spec_set` will raise an `AttributeError`.\n\n    * `side_effect`: A function to be called whenever the Mock is called. See\n      the `side_effect` attribute. Useful for raising exceptions or\n      dynamically changing return values. The function is called with the same\n      arguments as the mock, and unless it returns `DEFAULT`, the return\n      value of this function is used as the return value.\n\n      Alternatively `side_effect` can be an exception class or instance. In\n      this case the exception will be raised when the mock is called.\n\n      If `side_effect` is an iterable then each call to the mock will return\n      the next value from the iterable. If any of the members of the iterable\n      are exceptions they will be raised instead of returned.\n\n    * `return_value`: The value returned when the mock is called. By default\n      this is a new Mock (created on first access). See the\n      `return_value` attribute.\n\n    * `wraps`: Item for the mock object to wrap. If `wraps` is not None then\n      calling the Mock will pass the call through to the wrapped object\n      (returning the real result). Attribute access on the mock will return a\n      Mock object that wraps the corresponding attribute of the wrapped object\n      (so attempting to access an attribute that doesn't exist will raise an\n      `AttributeError`).\n\n      If the mock has an explicit `return_value` set then calls are not passed\n      to the wrapped object and the `return_value` is returned instead.\n\n    * `name`: If the mock has a name then it will be used in the repr of the\n      mock. This can be useful for debugging. The name is propagated to child\n      mocks.\n\n    Mocks can also be called with arbitrary keyword arguments. These will be\n    used to set attributes on the mock after it is created.\n    ",
        "klass": "mock.mock.Mock",
        "module": "mock"
    },
    {
        "base_classes": [
            "mock.mock.CallableMixin",
            "mock.mock.NonCallableMock"
        ],
        "class_docstring": "\n    Create a new `Mock` object. `Mock` takes several optional arguments\n    that specify the behaviour of the Mock object:\n\n    * `spec`: This can be either a list of strings or an existing object (a\n      class or instance) that acts as the specification for the mock object. If\n      you pass in an object then a list of strings is formed by calling dir on\n      the object (excluding unsupported magic attributes and methods). Accessing\n      any attribute not in this list will raise an `AttributeError`.\n\n      If `spec` is an object (rather than a list of strings) then\n      `mock.__class__` returns the class of the spec object. This allows mocks\n      to pass `isinstance` tests.\n\n    * `spec_set`: A stricter variant of `spec`. If used, attempting to *set*\n      or get an attribute on the mock that isn't on the object passed as\n      `spec_set` will raise an `AttributeError`.\n\n    * `side_effect`: A function to be called whenever the Mock is called. See\n      the `side_effect` attribute. Useful for raising exceptions or\n      dynamically changing return values. The function is called with the same\n      arguments as the mock, and unless it returns `DEFAULT`, the return\n      value of this function is used as the return value.\n\n      Alternatively `side_effect` can be an exception class or instance. In\n      this case the exception will be raised when the mock is called.\n\n      If `side_effect` is an iterable then each call to the mock will return\n      the next value from the iterable. If any of the members of the iterable\n      are exceptions they will be raised instead of returned.\n\n    * `return_value`: The value returned when the mock is called. By default\n      this is a new Mock (created on first access). See the\n      `return_value` attribute.\n\n    * `wraps`: Item for the mock object to wrap. If `wraps` is not None then\n      calling the Mock will pass the call through to the wrapped object\n      (returning the real result). Attribute access on the mock will return a\n      Mock object that wraps the corresponding attribute of the wrapped object\n      (so attempting to access an attribute that doesn't exist will raise an\n      `AttributeError`).\n\n      If the mock has an explicit `return_value` set then calls are not passed\n      to the wrapped object and the `return_value` is returned instead.\n\n    * `name`: If the mock has a name then it will be used in the repr of the\n      mock. This can be useful for debugging. The name is propagated to child\n      mocks.\n\n    Mocks can also be called with arbitrary keyword arguments. These will be\n    used to set attributes on the mock after it is created.\n    ",
        "klass": "mock.Mock",
        "module": "mock"
    },
    {
        "base_classes": [
            "mock.mock.Mock"
        ],
        "class_docstring": "\n    A mock intended to be used as a property, or other descriptor, on a class.\n    `PropertyMock` provides `__get__` and `__set__` methods so you can specify\n    a return value when it is fetched.\n\n    Fetching a `PropertyMock` instance from an object calls the mock, with\n    no args. Setting it calls the mock with the value being set.\n    ",
        "klass": "mock.PropertyMock",
        "module": "mock"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "super() -> same as super(__class__, <first argument>)\nsuper(type) -> unbound super object\nsuper(type, obj) -> bound super object; requires isinstance(obj, type)\nsuper(type, type2) -> bound super object; requires issubclass(type2, type)\nTypical use to call a cooperative superclass method:\nclass C(B):\n    def meth(self, arg):\n        super().meth(arg)\nThis works for class methods too:\nclass C(B):\n    @classmethod\n    def cmeth(cls, arg):\n        super().cmeth(arg)\n",
        "klass": "super",
        "module": "super"
    },
    {
        "base_classes": [
            "tqdm.utils.Comparable"
        ],
        "class_docstring": "\n    Decorate an iterable object, returning an iterator which acts exactly\n    like the original iterable, but prints a dynamically updating\n    progressbar every time a value is requested.\n    ",
        "klass": "tqdm.tqdm",
        "module": "tqdm"
    },
    {
        "base_classes": [
            "tqdm.utils.Comparable"
        ],
        "class_docstring": "\n    Decorate an iterable object, returning an iterator which acts exactly\n    like the original iterable, but prints a dynamically updating\n    progressbar every time a value is requested.\n    ",
        "klass": "tqdm.auto.tqdm",
        "module": "tqdm"
    },
    {
        "base_classes": [
            "tkinter.Misc",
            "tkinter.Wm"
        ],
        "class_docstring": "Toplevel widget of Tk which represents mostly the main window\n    of an application. It has an associated Tcl interpreter.",
        "klass": "tkinter.Tk",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "pymc3.model.Factor",
            "pymc3.memoize.WithMemoization"
        ],
        "class_docstring": "Encapsulates the variables and likelihood factors of a model.\n\n    Model class can be used for creating class based models. To create\n    a class based model you should inherit from :class:`~.Model` and\n    override :meth:`~.__init__` with arbitrary definitions (do not\n    forget to call base class :meth:`__init__` first).\n\n    Parameters\n    ----------\n    name : str\n        name that will be used as prefix for names of all random\n        variables defined within model\n    model : Model\n        instance of Model that is supposed to be a parent for the new\n        instance. If ``None``, context will be used. All variables\n        defined within instance will be passed to the parent instance.\n        So that 'nested' model contributes to the variables and\n        likelihood factors of parent model.\n    theano_config : dict\n        A dictionary of theano config values that should be set\n        temporarily in the model context. See the documentation\n        of theano for a complete list. Set config key\n        ``compute_test_value`` to `raise` if it is None.\n\n    Examples\n    --------\n\n    How to define a custom model\n\n    .. code-block:: python\n\n        class CustomModel(Model):\n            # 1) override init\n            def __init__(self, mean=0, sigma=1, name='', model=None):\n                # 2) call super's init first, passing model and name\n                # to it name will be prefix for all variables here if\n                # no name specified for model there will be no prefix\n                super().__init__(name, model)\n                # now you are in the context of instance,\n                # `modelcontext` will return self you can define\n                # variables in several ways note, that all variables\n                # will get model's name prefix\n\n                # 3) you can create variables with Var method\n                self.Var('v1', Normal.dist(mu=mean, sigma=sd))\n                # this will create variable named like '{prefix_}v1'\n                # and assign attribute 'v1' to instance created\n                # variable can be accessed with self.v1 or self['v1']\n\n                # 4) this syntax will also work as we are in the\n                # context of instance itself, names are given as usual\n                Normal('v2', mu=mean, sigma=sd)\n\n                # something more complex is allowed, too\n                half_cauchy = HalfCauchy('sd', beta=10, testval=1.)\n                Normal('v3', mu=mean, sigma=half_cauchy)\n\n                # Deterministic variables can be used in usual way\n                Deterministic('v3_sq', self.v3 ** 2)\n\n                # Potentials too\n                Potential('p1', tt.constant(1))\n\n        # After defining a class CustomModel you can use it in several\n        # ways\n\n        # I:\n        #   state the model within a context\n        with Model() as model:\n            CustomModel()\n            # arbitrary actions\n\n        # II:\n        #   use new class as entering point in context\n        with CustomModel() as model:\n            Normal('new_normal_var', mu=1, sigma=0)\n\n        # III:\n        #   just get model instance with all that was defined in it\n        model = CustomModel()\n\n        # IV:\n        #   use many custom models within one context\n        with Model() as model:\n            CustomModel(mean=1, name='first')\n            CustomModel(mean=2, name='second')\n    ",
        "klass": "pymc3.Model",
        "module": "pymc3"
    },
    {
        "base_classes": [
            "pandas.core.base.IndexOpsMixin",
            "pandas.core.generic.NDFrame"
        ],
        "class_docstring": "\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    copy : bool, default False\n        Copy input data.\n    ",
        "klass": "pandas.core.series.Series",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Module component with similar interface to numpy.random\n    (numpy.random.RandomState).\n\n    Parameters\n    ----------\n    seed : int or list of 6 int\n        A default seed to initialize the random state.\n        If a single int is given, it will be replicated 6 times.\n        The first 3 values of the seed must all be less than M1 = 2147483647,\n        and not all 0; and the last 3 values must all be less than\n        M2 = 2147462579, and not all 0.\n\n    ",
        "klass": "theano.sandbox.rng_mrg.MRG_RandomStreams",
        "module": "theano"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a specific locale.\n\n    >>> locale = Locale('en', 'US')\n    >>> repr(locale)\n    \"Locale('en', territory='US')\"\n    >>> locale.display_name\n    u'English (United States)'\n\n    A `Locale` object can also be instantiated from a raw locale string:\n\n    >>> locale = Locale.parse('en-US', sep='-')\n    >>> repr(locale)\n    \"Locale('en', territory='US')\"\n\n    `Locale` objects provide access to a collection of locale data, such as\n    territory and language names, number and date format patterns, and more:\n\n    >>> locale.number_symbols['decimal']\n    u'.'\n\n    If a locale is requested for which no locale data is available, an\n    `UnknownLocaleError` is raised:\n\n    >>> Locale.parse('en_XX')\n    Traceback (most recent call last):\n        ...\n    UnknownLocaleError: unknown locale 'en_XX'\n\n    For more information see :rfc:`3066`.\n    ",
        "klass": "babel.core.Locale",
        "module": "babel"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a specific locale.\n\n    >>> locale = Locale('en', 'US')\n    >>> repr(locale)\n    \"Locale('en', territory='US')\"\n    >>> locale.display_name\n    u'English (United States)'\n\n    A `Locale` object can also be instantiated from a raw locale string:\n\n    >>> locale = Locale.parse('en-US', sep='-')\n    >>> repr(locale)\n    \"Locale('en', territory='US')\"\n\n    `Locale` objects provide access to a collection of locale data, such as\n    territory and language names, number and date format patterns, and more:\n\n    >>> locale.number_symbols['decimal']\n    u'.'\n\n    If a locale is requested for which no locale data is available, an\n    `UnknownLocaleError` is raised:\n\n    >>> Locale.parse('en_XX')\n    Traceback (most recent call last):\n        ...\n    UnknownLocaleError: unknown locale 'en_XX'\n\n    For more information see :rfc:`3066`.\n    ",
        "klass": "babel.Locale",
        "module": "babel"
    },
    {
        "base_classes": [
            "_io._BufferedIOBase"
        ],
        "class_docstring": "Buffered I/O implementation using an in-memory bytes buffer.",
        "klass": "babel._compat.BytesIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a message catalog.",
        "klass": "babel.messages.catalog.Catalog",
        "module": "babel"
    },
    {
        "base_classes": [
            "babel.messages.frontend.Command"
        ],
        "class_docstring": "Catalog compilation command for use in ``setup.py`` scripts.\n\n    If correctly installed, this command is available to Setuptools-using\n    setup scripts automatically. For projects using plain old ``distutils``,\n    the command needs to be registered explicitly in ``setup.py``::\n\n        from babel.messages.frontend import compile_catalog\n\n        setup(\n            ...\n            cmdclass = {'compile_catalog': compile_catalog}\n        )\n\n    .. versionadded:: 0.9\n    ",
        "klass": "babel.messages.frontend.compile_catalog",
        "module": "babel"
    },
    {
        "base_classes": [
            "babel.messages.frontend.Command"
        ],
        "class_docstring": "Message extraction command for use in ``setup.py`` scripts.\n\n    If correctly installed, this command is available to Setuptools-using\n    setup scripts automatically. For projects using plain old ``distutils``,\n    the command needs to be registered explicitly in ``setup.py``::\n\n        from babel.messages.frontend import extract_messages\n\n        setup(\n            ...\n            cmdclass = {'extract_messages': extract_messages}\n        )\n    ",
        "klass": "babel.messages.frontend.extract_messages",
        "module": "babel"
    },
    {
        "base_classes": [
            "babel.support.NullTranslations",
            "gettext.GNUTranslations"
        ],
        "class_docstring": "An extended translation catalog class.",
        "klass": "babel.support.Translations",
        "module": "babel"
    },
    {
        "base_classes": [
            "mne.epochs.BaseEpochs"
        ],
        "class_docstring": "Epochs extracted from a Raw instance.\n\n    Parameters\n    ----------\n    raw : Raw object\n        An instance of Raw.\n    events : array of int, shape (n_events, 3)\n        The events typically returned by the read_events function.\n        If some events don't match the events of interest as specified\n        by event_id, they will be marked as 'IGNORED' in the drop log.\n    event_id : int | list of int | dict | None\n        The id of the event to consider. If dict,\n        the keys can later be used to access associated events. Example:\n        dict(auditory=1, visual=3). If int, a dict will be created with\n        the id as string. If a list, all events with the IDs specified\n        in the list are used. If None, all events will be used with\n        and a dict is created with string integer names corresponding\n        to the event id integers.\n    tmin : float\n        Start time before event. If nothing is provided, defaults to -0.2\n    tmax : float\n        End time after event. If nothing is provided, defaults to 0.5\n    baseline : None or tuple of length 2 (default (None, 0))\n        The time interval to apply baseline correction. If None do not apply\n        it. If baseline is (a, b) the interval is between \"a (s)\" and \"b (s)\".\n        If a is None the beginning of the data is used and if b is None then b\n        is set to the end of the interval. If baseline is equal to (None, None)\n        all the time interval is used. Correction is applied by computing mean\n        of the baseline period and subtracting it from the data. The baseline\n        (a, b) includes both endpoints, i.e. all timepoints t such that\n        a <= t <= b.\n    picks : str | list | slice | None\n        Channels to include. Slices and lists of integers will be\n        interpreted as channel indices. In lists, channel *type* strings\n        (e.g., ``['meg', 'eeg']``) will pick channels of those\n        types, channel *name* strings (e.g., ``['MEG0111', 'MEG2623']``\n        will pick the given channels. Can also be the string values\n        \"all\" to pick all channels, or \"data\" to pick data channels.\n        None (default) will pick all channels.\n    preload : boolean\n        Load all epochs from disk when creating the object\n        or wait before accessing each epoch (more memory\n        efficient but can be slower).\n    reject : dict | None\n        Rejection parameters based on peak-to-peak amplitude.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg'.\n        If reject is None then no rejection is done. Example::\n\n            reject = dict(grad=4000e-13, # T / m (gradiometers)\n                          mag=4e-12, # T (magnetometers)\n                          eeg=40e-6, # V (EEG channels)\n                          eog=250e-6 # V (EOG channels)\n                          )\n\n    flat : dict | None\n        Rejection parameters based on flatness of signal.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg', and values\n        are floats that set the minimum acceptable peak-to-peak amplitude.\n        If flat is None then no rejection is done.\n    proj : bool | 'delayed'\n        Apply SSP projection vectors. If proj is 'delayed' and reject is not\n        None the single epochs will be projected before the rejection\n        decision, but used in unprojected state if they are kept.\n        This way deciding which projection vectors are good can be postponed\n        to the evoked stage without resulting in lower epoch counts and\n        without producing results different from early SSP application\n        given comparable parameters. Note that in this case baselining,\n        detrending and temporal decimation will be postponed.\n        If proj is False no projections will be applied which is the\n        recommended value if SSPs are not used for cleaning the data.\n    decim : int\n        Factor by which to downsample the data from the raw file upon import.\n        Warning: This simply selects every nth sample, data is not filtered\n        here. If data is not properly filtered, aliasing artifacts may occur.\n    reject_tmin : scalar | None\n        Start of the time window used to reject epochs (with the default None,\n        the window will start with tmin).\n    reject_tmax : scalar | None\n        End of the time window used to reject epochs (with the default None,\n        the window will end with tmax).\n    detrend : int | None\n        If 0 or 1, the data channels (MEG and EEG) will be detrended when\n        loaded. 0 is a constant (DC) detrend, 1 is a linear detrend. None\n        is no detrending. Note that detrending is performed before baseline\n        correction. If no DC offset is preferred (zeroth order detrending),\n        either turn off baseline correction, as this may introduce a DC\n        shift, or set baseline correction to use the entire time interval\n        (will yield equivalent results but be slower).\n    on_missing : str\n        What to do if one or several event ids are not found in the recording.\n        Valid keys are 'error' | 'warning' | 'ignore'\n        Default is 'error'. If on_missing is 'warning' it will proceed but\n        warn, if 'ignore' it will proceed silently. Note.\n        If none of the event ids are found in the data, an error will be\n        automatically generated irrespective of this parameter.\n    reject_by_annotation : bool\n        Whether to reject based on annotations. If True (default), epochs\n        overlapping with segments whose description begins with ``'bad'`` are\n        rejected. If False, no rejection based on annotations is performed.\n    metadata : instance of pandas.DataFrame | None\n        A :class:`pandas.DataFrame` specifying more complex metadata about\n        events. If given, ``len(metadata)`` must equal ``len(events)``.\n        The DataFrame may have values of type (str | int | float).\n        If metadata is given, then pandas-style queries may be used to select\n        subsets of data, see :meth:`mne.Epochs.__getitem__`.\n        When a subset of the epochs is created in this (or any other\n        supported) manner, the metadata object is subsetted in the same manner.\n        MNE will modify the row indices to match ``epochs.selection``.\n\n        .. versionadded:: 0.16\n    event_repeated : str\n        How to handle duplicates in `events[:, 0]`. Can be 'error' (default),\n        to raise an error, 'drop' to only retain the row occurring first in the\n        `events`, or 'merge' to combine the coinciding events (=duplicates)\n        into a new event (see Notes for details).\n\n        .. versionadded:: 0.19\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Attributes\n    ----------\n    info : instance of Info\n        Measurement info.\n    event_id : dict\n        Names of conditions corresponding to event_ids.\n    ch_names : list of string\n        List of channel names.\n    selection : array\n        List of indices of selected events (not dropped or ignored etc.). For\n        example, if the original event array had 4 events and the second event\n        has been dropped, this attribute would be np.array([0, 2, 3]).\n    preload : bool\n        Indicates whether epochs are in memory.\n    drop_log : list of list\n        A list of the same length as the event array used to initialize the\n        Epochs object. If the i-th original event is still part of the\n        selection, drop_log[i] will be an empty list; otherwise it will be\n        a list of the reasons the event is not longer in the selection, e.g.:\n\n        'IGNORED' if it isn't part of the current subset defined by the user;\n        'NO_DATA' or 'TOO_SHORT' if epoch didn't contain enough data;\n        names of channels that exceeded the amplitude threshold;\n        'EQUALIZED_COUNTS' (see equalize_event_counts);\n        or 'USER' for user-defined reasons (see drop method).\n    filename : str\n        The filename of the object.\n    times :  ndarray\n        Time vector in seconds. Goes from `tmin` to `tmax`. Time interval\n        between consecutive time samples is equal to the inverse of the\n        sampling frequency.\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    See Also\n    --------\n    mne.epochs.combine_event_ids\n    mne.Epochs.equalize_event_counts\n\n    Notes\n    -----\n    When accessing data, Epochs are detrended, baseline-corrected, and\n    decimated, then projectors are (optionally) applied.\n\n    For indexing and slicing using ``epochs[...]``, see\n    :meth:`mne.Epochs.__getitem__`.\n\n    All methods for iteration over objects (using :meth:`mne.Epochs.__iter__`,\n    :meth:`mne.Epochs.iter_evoked` or :meth:`mne.Epochs.next`) use the same\n    internal state.\n\n    If `event_repeated` is set to \"merge\", the coinciding events (duplicates)\n    will be merged into a single event_id and assigned a new id_number as\n    follows: `event_id['{event_id_1}/{event_id_2}/...'] = new_id_number`.\n    For example with the event_id {'aud': 1, 'vis': 2} and the events\n    [[0, 0, 1], [0, 0, 2]], the \"merge\" behavior will update both event_id and\n    events to be: {'aud/vis': 3} and [[0, 0, 3], ] respectively.\n\n    ",
        "klass": "mne.epochs.Epochs",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.epochs.BaseEpochs"
        ],
        "class_docstring": "Epochs extracted from a Raw instance.\n\n    Parameters\n    ----------\n    raw : Raw object\n        An instance of Raw.\n    events : array of int, shape (n_events, 3)\n        The events typically returned by the read_events function.\n        If some events don't match the events of interest as specified\n        by event_id, they will be marked as 'IGNORED' in the drop log.\n    event_id : int | list of int | dict | None\n        The id of the event to consider. If dict,\n        the keys can later be used to access associated events. Example:\n        dict(auditory=1, visual=3). If int, a dict will be created with\n        the id as string. If a list, all events with the IDs specified\n        in the list are used. If None, all events will be used with\n        and a dict is created with string integer names corresponding\n        to the event id integers.\n    tmin : float\n        Start time before event. If nothing is provided, defaults to -0.2\n    tmax : float\n        End time after event. If nothing is provided, defaults to 0.5\n    baseline : None or tuple of length 2 (default (None, 0))\n        The time interval to apply baseline correction. If None do not apply\n        it. If baseline is (a, b) the interval is between \"a (s)\" and \"b (s)\".\n        If a is None the beginning of the data is used and if b is None then b\n        is set to the end of the interval. If baseline is equal to (None, None)\n        all the time interval is used. Correction is applied by computing mean\n        of the baseline period and subtracting it from the data. The baseline\n        (a, b) includes both endpoints, i.e. all timepoints t such that\n        a <= t <= b.\n    picks : str | list | slice | None\n        Channels to include. Slices and lists of integers will be\n        interpreted as channel indices. In lists, channel *type* strings\n        (e.g., ``['meg', 'eeg']``) will pick channels of those\n        types, channel *name* strings (e.g., ``['MEG0111', 'MEG2623']``\n        will pick the given channels. Can also be the string values\n        \"all\" to pick all channels, or \"data\" to pick data channels.\n        None (default) will pick all channels.\n    preload : boolean\n        Load all epochs from disk when creating the object\n        or wait before accessing each epoch (more memory\n        efficient but can be slower).\n    reject : dict | None\n        Rejection parameters based on peak-to-peak amplitude.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg'.\n        If reject is None then no rejection is done. Example::\n\n            reject = dict(grad=4000e-13, # T / m (gradiometers)\n                          mag=4e-12, # T (magnetometers)\n                          eeg=40e-6, # V (EEG channels)\n                          eog=250e-6 # V (EOG channels)\n                          )\n\n    flat : dict | None\n        Rejection parameters based on flatness of signal.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg', and values\n        are floats that set the minimum acceptable peak-to-peak amplitude.\n        If flat is None then no rejection is done.\n    proj : bool | 'delayed'\n        Apply SSP projection vectors. If proj is 'delayed' and reject is not\n        None the single epochs will be projected before the rejection\n        decision, but used in unprojected state if they are kept.\n        This way deciding which projection vectors are good can be postponed\n        to the evoked stage without resulting in lower epoch counts and\n        without producing results different from early SSP application\n        given comparable parameters. Note that in this case baselining,\n        detrending and temporal decimation will be postponed.\n        If proj is False no projections will be applied which is the\n        recommended value if SSPs are not used for cleaning the data.\n    decim : int\n        Factor by which to downsample the data from the raw file upon import.\n        Warning: This simply selects every nth sample, data is not filtered\n        here. If data is not properly filtered, aliasing artifacts may occur.\n    reject_tmin : scalar | None\n        Start of the time window used to reject epochs (with the default None,\n        the window will start with tmin).\n    reject_tmax : scalar | None\n        End of the time window used to reject epochs (with the default None,\n        the window will end with tmax).\n    detrend : int | None\n        If 0 or 1, the data channels (MEG and EEG) will be detrended when\n        loaded. 0 is a constant (DC) detrend, 1 is a linear detrend. None\n        is no detrending. Note that detrending is performed before baseline\n        correction. If no DC offset is preferred (zeroth order detrending),\n        either turn off baseline correction, as this may introduce a DC\n        shift, or set baseline correction to use the entire time interval\n        (will yield equivalent results but be slower).\n    on_missing : str\n        What to do if one or several event ids are not found in the recording.\n        Valid keys are 'error' | 'warning' | 'ignore'\n        Default is 'error'. If on_missing is 'warning' it will proceed but\n        warn, if 'ignore' it will proceed silently. Note.\n        If none of the event ids are found in the data, an error will be\n        automatically generated irrespective of this parameter.\n    reject_by_annotation : bool\n        Whether to reject based on annotations. If True (default), epochs\n        overlapping with segments whose description begins with ``'bad'`` are\n        rejected. If False, no rejection based on annotations is performed.\n    metadata : instance of pandas.DataFrame | None\n        A :class:`pandas.DataFrame` specifying more complex metadata about\n        events. If given, ``len(metadata)`` must equal ``len(events)``.\n        The DataFrame may have values of type (str | int | float).\n        If metadata is given, then pandas-style queries may be used to select\n        subsets of data, see :meth:`mne.Epochs.__getitem__`.\n        When a subset of the epochs is created in this (or any other\n        supported) manner, the metadata object is subsetted in the same manner.\n        MNE will modify the row indices to match ``epochs.selection``.\n\n        .. versionadded:: 0.16\n    event_repeated : str\n        How to handle duplicates in `events[:, 0]`. Can be 'error' (default),\n        to raise an error, 'drop' to only retain the row occurring first in the\n        `events`, or 'merge' to combine the coinciding events (=duplicates)\n        into a new event (see Notes for details).\n\n        .. versionadded:: 0.19\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Attributes\n    ----------\n    info : instance of Info\n        Measurement info.\n    event_id : dict\n        Names of conditions corresponding to event_ids.\n    ch_names : list of string\n        List of channel names.\n    selection : array\n        List of indices of selected events (not dropped or ignored etc.). For\n        example, if the original event array had 4 events and the second event\n        has been dropped, this attribute would be np.array([0, 2, 3]).\n    preload : bool\n        Indicates whether epochs are in memory.\n    drop_log : list of list\n        A list of the same length as the event array used to initialize the\n        Epochs object. If the i-th original event is still part of the\n        selection, drop_log[i] will be an empty list; otherwise it will be\n        a list of the reasons the event is not longer in the selection, e.g.:\n\n        'IGNORED' if it isn't part of the current subset defined by the user;\n        'NO_DATA' or 'TOO_SHORT' if epoch didn't contain enough data;\n        names of channels that exceeded the amplitude threshold;\n        'EQUALIZED_COUNTS' (see equalize_event_counts);\n        or 'USER' for user-defined reasons (see drop method).\n    filename : str\n        The filename of the object.\n    times :  ndarray\n        Time vector in seconds. Goes from `tmin` to `tmax`. Time interval\n        between consecutive time samples is equal to the inverse of the\n        sampling frequency.\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    See Also\n    --------\n    mne.epochs.combine_event_ids\n    mne.Epochs.equalize_event_counts\n\n    Notes\n    -----\n    When accessing data, Epochs are detrended, baseline-corrected, and\n    decimated, then projectors are (optionally) applied.\n\n    For indexing and slicing using ``epochs[...]``, see\n    :meth:`mne.Epochs.__getitem__`.\n\n    All methods for iteration over objects (using :meth:`mne.Epochs.__iter__`,\n    :meth:`mne.Epochs.iter_evoked` or :meth:`mne.Epochs.next`) use the same\n    internal state.\n\n    If `event_repeated` is set to \"merge\", the coinciding events (duplicates)\n    will be merged into a single event_id and assigned a new id_number as\n    follows: `event_id['{event_id_1}/{event_id_2}/...'] = new_id_number`.\n    For example with the event_id {'aud': 1, 'vis': 2} and the events\n    [[0, 0, 1], [0, 0, 2]], the \"merge\" behavior will update both event_id and\n    events to be: {'aud/vis': 3} and [[0, 0, 3], ] respectively.\n\n    ",
        "klass": "mne.Epochs",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Return successive r-length combinations of elements in the iterable.\n\ncombinations(range(4), 3) --> (0,1,2), (0,1,3), (0,2,3), (1,2,3)",
        "klass": "itertools.combinations",
        "module": "itertools"
    },
    {
        "base_classes": [
            "optparse.OptionContainer"
        ],
        "class_docstring": "\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    ",
        "klass": "optparse.OptionParser",
        "module": "optparse"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin",
            "mne.fixes.BaseEstimator"
        ],
        "class_docstring": "M/EEG signal decomposition using the Common Spatial Patterns (CSP).\n\n    This object can be used as a supervised decomposition to estimate\n    spatial filters for feature extraction in a 2 class decoding problem.\n    CSP in the context of EEG was first described in [1]; a comprehensive\n    tutorial on CSP can be found in [2]. Multiclass solving is implemented\n    from [3].\n\n    Parameters\n    ----------\n    n_components : int, default 4\n        The number of components to decompose M/EEG signals.\n        This number should be set by cross-validation.\n    reg : float | str | None (default None)\n        If not None (same as ``'empirical'``, default), allow\n        regularization for covariance estimation.\n        If float, shrinkage is used (0 <= shrinkage <= 1).\n        For str options, ``reg`` will be passed to ``method`` to\n        :func:`mne.compute_covariance`.\n    log : None | bool (default None)\n        If transform_into == 'average_power' and log is None or True, then\n        applies a log transform to standardize the features, else the features\n        are z-scored. If transform_into == 'csp_space', then log must be None.\n    cov_est : 'concat' | 'epoch', default 'concat'\n        If 'concat', covariance matrices are estimated on concatenated epochs\n        for each class.\n        If 'epoch', covariance matrices are estimated on each epoch separately\n        and then averaged over each class.\n    transform_into : {'average_power', 'csp_space'}\n        If 'average_power' then self.transform will return the average power of\n        each spatial filter. If 'csp_space' self.transform will return the data\n        in CSP space. Defaults to 'average_power'.\n    norm_trace : bool\n        Normalize class covariance by its trace. Defaults to False. Trace\n        normalization is a step of the original CSP algorithm [1]_ to eliminate\n        magnitude variations in the EEG between individuals. It is not applied\n        in more recent work [2]_, [3]_ and can have a negative impact on\n        patterns ordering.\n    cov_method_params : dict | None\n        Parameters to pass to :func:`mne.compute_covariance`.\n\n        .. versionadded:: 0.16\n    \n    rank : None | dict | 'info' | 'full'\n        This controls the rank computation that can be read from the\n        measurement info or estimated from the data. See ``Notes``\n        of :func:`mne.compute_rank` for details.The default is None.\n\n        .. versionadded:: 0.17\n\n    Attributes\n    ----------\n    filters_ :  ndarray, shape (n_channels, n_channels)\n        If fit, the CSP components used to decompose the data, else None.\n    patterns_ : ndarray, shape (n_channels, n_channels)\n        If fit, the CSP patterns used to restore M/EEG signals, else None.\n    mean_ : ndarray, shape (n_components,)\n        If fit, the mean squared power for each component.\n    std_ : ndarray, shape (n_components,)\n        If fit, the std squared power for each component.\n\n    See Also\n    --------\n    mne.preprocessing.Xdawn, SPoC\n\n    References\n    ----------\n    .. [1] Zoltan J. Koles, Michael S. Lazar, Steven Z. Zhou. Spatial Patterns\n           Underlying Population Differences in the Background EEG. Brain\n           Topography 2(4), 275-284, 1990.\n    .. [2] Benjamin Blankertz, Ryota Tomioka, Steven Lemm, Motoaki Kawanabe,\n           Klaus-Robert M\u00fcller. Optimizing Spatial Filters for Robust EEG\n           Single-Trial Analysis. IEEE Signal Processing Magazine 25(1), 41-56,\n           2008.\n    .. [3] Grosse-Wentrup, Moritz, and Martin Buss. Multiclass common spatial\n           patterns and information theoretic feature extraction. IEEE\n           Transactions on Biomedical Engineering, Vol 55, no. 8, 2008.\n    ",
        "klass": "mne.decoding.csp.CSP",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin",
            "mne.decoding.mixin.EstimatorMixin"
        ],
        "class_docstring": "Transformer to compute event-matched spatial filters.\n\n    This version of EMS [1]_ operates on the entire time course. No time\n    window needs to be specified. The result is a spatial filter at each\n    time point and a corresponding time course. Intuitively, the result\n    gives the similarity between the filter at each time point and the\n    data vector (sensors) at that time point.\n\n    .. note : EMS only works for binary classification.\n\n    Attributes\n    ----------\n    filters_ : ndarray, shape (n_channels, n_times)\n        The set of spatial filters.\n    classes_ : ndarray, shape (n_classes,)\n        The target classes.\n\n    References\n    ----------\n    .. [1] Aaron Schurger, Sebastien Marti, and Stanislas Dehaene, \"Reducing\n           multi-sensor data to a single time course that reveals experimental\n           effects\", BMC Neuroscience 2013, 14:122\n    ",
        "klass": "mne.decoding.EMS",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin"
        ],
        "class_docstring": "Estimator to filter RtEpochs.\n\n    Applies a zero-phase low-pass, high-pass, band-pass, or band-stop\n    filter to the channels selected by \"picks\".\n\n    l_freq and h_freq are the frequencies below which and above which,\n    respectively, to filter out of the data. Thus the uses are:\n\n        - l_freq < h_freq: band-pass filter\n        - l_freq > h_freq: band-stop filter\n        - l_freq is not None, h_freq is None: low-pass filter\n        - l_freq is None, h_freq is not None: high-pass filter\n\n    If n_jobs > 1, more memory is required as \"len(picks) * n_times\"\n    additional time points need to be temporarily stored in memory.\n\n    Parameters\n    ----------\n    info : instance of Info\n        Measurement info.\n    \n    l_freq : float | None\n        For FIR filters, the lower pass-band edge; for IIR filters, the upper\n        cutoff frequency. If None the data are only low-passed.\n    \n    h_freq : float | None\n        For FIR filters, the upper pass-band edge; for IIR filters, the upper\n        cutoff frequency. If None the data are only low-passed.\n    picks : str | list | slice | None\n        Channels to include. Slices and lists of integers will be\n        interpreted as channel indices. In lists, channel *type* strings\n        (e.g., ``['meg', 'eeg']``) will pick channels of those\n        types, channel *name* strings (e.g., ``['MEG0111', 'MEG2623']``\n        will pick the given channels. Can also be the string values\n        \"all\" to pick all channels, or \"data\" to pick data channels.\n        None (default) will pick good data channels.\n    \n    filter_length : str | int\n        Length of the FIR filter to use (if applicable):\n    \n        * **'auto' (default)**: The filter length is chosen based\n          on the size of the transition regions (6.6 times the reciprocal\n          of the shortest transition band for fir_window='hamming'\n          and fir_design=\"firwin2\", and half that for \"firwin\").\n        * **str**: A human-readable time in\n          units of \"s\" or \"ms\" (e.g., \"10s\" or \"5500ms\") will be\n          converted to that number of samples if ``phase=\"zero\"``, or\n          the shortest power-of-two length at least that duration for\n          ``phase=\"zero-double\"``.\n        * **int**: Specified length in samples. For fir_design=\"firwin\",\n          this should not be used.\n    \n    \n    l_trans_bandwidth : float | str\n        Width of the transition band at the low cut-off frequency in Hz\n        (high pass or cutoff 1 in bandpass). Can be \"auto\"\n        (default) to use a multiple of ``l_freq``::\n    \n            min(max(l_freq * 0.25, 2), l_freq)\n    \n        Only used for ``method='fir'``.\n    \n    h_trans_bandwidth : float | str\n        Width of the transition band at the high cut-off frequency in Hz\n        (low pass or cutoff 2 in bandpass). Can be \"auto\"\n        (default in 0.14) to use a multiple of ``h_freq``::\n    \n            min(max(h_freq * 0.25, 2.), info['sfreq'] / 2. - h_freq)\n    \n        Only used for ``method='fir'``.\n    n_jobs : int | str\n        Number of jobs to run in parallel.\n        Can be 'cuda' if ``cupy`` is installed properly and method='fir'.\n    method : str\n        'fir' will use overlap-add FIR filtering, 'iir' will use IIR\n        forward-backward filtering (via filtfilt).\n    iir_params : dict | None\n        Dictionary of parameters to use for IIR filtering.\n        See mne.filter.construct_iir_filter for details. If iir_params\n        is None and method=\"iir\", 4th order Butterworth will be used.\n    \n    fir_design : str\n        Can be \"firwin\" (default) to use :func:`scipy.signal.firwin`,\n        or \"firwin2\" to use :func:`scipy.signal.firwin2`. \"firwin\" uses\n        a time-domain design technique that generally gives improved\n        attenuation using fewer samples than \"firwin2\".\n    \n        .. versionadded:: 0.15\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    See Also\n    --------\n    TemporalFilter\n\n    Notes\n    -----\n    This is primarily meant for use in conjunction with\n    :class:`mne_realtime.RtEpochs`. In general it is not recommended in a\n    normal processing pipeline as it may result in edge artifacts. Use with\n    caution.\n    ",
        "klass": "mne.decoding.FilterEstimator",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin"
        ],
        "class_docstring": "Compute power spectral density (PSD) using a multi-taper method.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling frequency.\n    fmin : float\n        The lower frequency of interest.\n    fmax : float\n        The upper frequency of interest.\n    bandwidth : float\n        The bandwidth of the multi taper windowing function in Hz.\n    adaptive : bool\n        Use adaptive weights to combine the tapered spectra into PSD\n        (slow, use n_jobs >> 1 to speed up computation).\n    low_bias : bool\n        Only use tapers with more than 90% spectral concentration within\n        bandwidth.\n    n_jobs : int\n        Number of parallel jobs to use (only used if adaptive=True).\n    normalization : str\n        Either \"full\" or \"length\" (default). If \"full\", the PSD will\n        be normalized by the sampling rate as well as the length of\n        the signal (as in nitime).\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    See Also\n    --------\n    mne.time_frequency.psd_multitaper\n    ",
        "klass": "mne.decoding.PSDEstimator",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin",
            "mne.fixes.BaseEstimator"
        ],
        "class_docstring": "Standardize channel data.\n\n    This class scales data for each channel. It differs from scikit-learn\n    classes (e.g., :class:`sklearn.preprocessing.StandardScaler`) in that\n    it scales each *channel* by estimating \u03bc and \u03c3 using data from all\n    time points and epochs, as opposed to standardizing each *feature*\n    (i.e., each time point for each channel) by estimating using \u03bc and \u03c3\n    using data from all epochs.\n\n    Parameters\n    ----------\n    info : instance of Info | None\n        The measurement info. Only necessary if ``scalings`` is a dict or\n        None.\n    scalings : dict, string, default None.\n        Scaling method to be applied to data channel wise.\n\n        * if scalings is None (default), scales mag by 1e15, grad by 1e13,\n          and eeg by 1e6.\n        * if scalings is :class:`dict`, keys are channel types and values\n          are scale factors.\n        * if ``scalings=='median'``,\n          :class:`sklearn.preprocessing.RobustScaler`\n          is used (requires sklearn version 0.17+).\n        * if ``scalings=='mean'``,\n          :class:`sklearn.preprocessing.StandardScaler`\n          is used.\n\n    with_mean : boolean, default True\n        If True, center the data using mean (or median) before scaling.\n        Ignored for channel-type scaling.\n    with_std : boolean, default True\n        If True, scale the data to unit variance (``scalings='mean'``),\n        quantile range (``scalings='median``), or using channel type\n        if ``scalings`` is a dict or None).\n    ",
        "klass": "mne.decoding.Scaler",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin",
            "mne.fixes.BaseEstimator"
        ],
        "class_docstring": "Use unsupervised spatial filtering across time and samples.\n\n    Parameters\n    ----------\n    estimator : instance of sklearn.base.BaseEstimator\n        Estimator using some decomposition algorithm.\n    average : bool, default False\n        If True, the estimator is fitted on the average across samples\n        (e.g. epochs).\n    ",
        "klass": "mne.decoding.UnsupervisedSpatialFilter",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.decoding.mixin.TransformerMixin"
        ],
        "class_docstring": "Transform n-dimensional array into 2D array of n_samples by n_features.\n\n    This class reshapes an n-dimensional array into an n_samples * n_features\n    array, usable by the estimators and transformers of scikit-learn.\n\n    Examples\n    --------\n    clf = make_pipeline(SpatialFilter(), _XdawnTransformer(), Vectorizer(),\n                        LogisticRegression())\n\n    Attributes\n    ----------\n    features_shape_ : tuple\n         Stores the original shape of data.\n    ",
        "klass": "mne.decoding.Vectorizer",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.io.base.BaseRaw"
        ],
        "class_docstring": "Raw data in FIF format.\n\n    Parameters\n    ----------\n    fname : str | file-like\n        The raw filename to load. For files that have automatically been split,\n        the split part will be automatically loaded. Filenames should end\n        with raw.fif, raw.fif.gz, raw_sss.fif, raw_sss.fif.gz, raw_tsss.fif,\n        raw_tsss.fif.gz, or _meg.fif. If a file-like object is provided,\n        preloading must be used.\n\n        .. versionchanged:: 0.18\n           Support for file-like objects.\n    allow_maxshield : bool | str (default False)\n        If True, allow loading of data that has been recorded with internal\n        active compensation (MaxShield). Data recorded with MaxShield should\n        generally not be loaded directly, but should first be processed using\n        SSS/tSSS to remove the compensation signals that may also affect brain\n        activity. Can also be \"yes\" to load without eliciting a warning.\n    \n    preload : bool or str (default False)\n        Preload data into memory for data manipulation and faster indexing.\n        If True, the data will be preloaded into memory (fast, requires\n        large amount of memory). If preload is a string, preload is the\n        file name of a memory-mapped file which is used to store the data\n        on the hard drive (slower, requires less memory).\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Attributes\n    ----------\n    info : dict\n        :class:`Measurement info <mne.Info>`.\n    ch_names : list of string\n        List of channels' names.\n    n_times : int\n        Total number of time points in the raw file.\n    times :  ndarray\n        Time vector in seconds. Starts from 0, independently of `first_samp`\n        value. Time interval between consecutive time samples is equal to the\n        inverse of the sampling frequency.\n    preload : bool\n        Indicates whether raw data are in memory.\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    ",
        "klass": "mne.io.Raw",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.io.base.BaseRaw"
        ],
        "class_docstring": "Raw object from numpy array.\n\n    Parameters\n    ----------\n    data : array, shape (n_channels, n_times)\n        The channels' time series. See notes for proper units of measure.\n    info : instance of Info\n        Info dictionary. Consider using :func:`mne.create_info` to populate\n        this structure. This may be modified in place by the class.\n    first_samp : int\n        First sample offset used during recording (default 0).\n\n        .. versionadded:: 0.12\n    copy : {'data', 'info', 'both', 'auto', None}\n        Determines what gets copied on instantiation. \"auto\" (default)\n        will copy info, and copy \"data\" only if necessary to get to\n        double floating point precision.\n\n        .. versionadded:: 0.18\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Notes\n    -----\n    Proper units of measure:\n    * V: eeg, eog, seeg, emg, ecg, bio, ecog\n    * T: mag\n    * T/m: grad\n    * M: hbo, hbr\n    * Am: dipole\n    * AU: misc\n\n    See Also\n    --------\n    mne.EpochsArray\n    mne.EvokedArray\n    mne.create_info\n    ",
        "klass": "mne.io.RawArray",
        "module": "mne"
    },
    {
        "base_classes": [
            "_compression.BaseStream"
        ],
        "class_docstring": "The GzipFile class simulates most of the methods of a file object with\n    the exception of the truncate() method.\n\n    This class only supports opening files in binary mode. If you need to open a\n    compressed file in text mode, use the gzip.open() function.\n\n    ",
        "klass": "gzip.GzipFile",
        "module": "gzip"
    },
    {
        "base_classes": [
            "scipy.sparse.compressed._cs_matrix"
        ],
        "class_docstring": "\n    Compressed Sparse Column matrix\n\n    This can be instantiated in several ways:\n\n        csc_matrix(D)\n            with a dense matrix or rank-2 ndarray D\n\n        csc_matrix(S)\n            with another sparse matrix S (equivalent to S.tocsc())\n\n        csc_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n        csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n            where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n            relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n\n        csc_matrix((data, indices, indptr), [shape=(M, N)])\n            is the standard CSC representation where the row indices for\n            column i are stored in ``indices[indptr[i]:indptr[i+1]]``\n            and their corresponding values are stored in\n            ``data[indptr[i]:indptr[i+1]]``.  If the shape parameter is\n            not supplied, the matrix dimensions are inferred from\n            the index arrays.\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        Data array of the matrix\n    indices\n        CSC format index array\n    indptr\n        CSC format index pointer array\n    has_sorted_indices\n        Whether indices are sorted\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Advantages of the CSC format\n        - efficient arithmetic operations CSC + CSC, CSC * CSC, etc.\n        - efficient column slicing\n        - fast matrix vector products (CSR, BSR may be faster)\n\n    Disadvantages of the CSC format\n      - slow row slicing operations (consider CSR)\n      - changes to the sparsity structure are expensive (consider LIL or DOK)\n\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy.sparse import csc_matrix\n    >>> csc_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> row = np.array([0, 2, 2, 0, 1, 2])\n    >>> col = np.array([0, 0, 1, 2, 2, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csc_matrix((data, (row, col)), shape=(3, 3)).toarray()\n    array([[1, 0, 4],\n           [0, 0, 5],\n           [2, 3, 6]])\n\n    >>> indptr = np.array([0, 2, 3, 6])\n    >>> indices = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n    array([[1, 0, 4],\n           [0, 0, 5],\n           [2, 3, 6]])\n\n    ",
        "klass": "scipy.sparse.csc.csc_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "mne.channels.channels.ContainsMixin"
        ],
        "class_docstring": "M/EEG signal decomposition using Independent Component Analysis (ICA).\n\n    This object estimates independent components from :class:`mne.io.Raw`,\n    :class:`mne.Epochs`, or :class:`mne.Evoked` objects. Components can\n    optionally be removed (for artifact repair) prior to signal reconstruction.\n\n    .. warning:: ICA is sensitive to low-frequency drifts and therefore\n                 requires the data to be high-pass filtered prior to fitting.\n                 Typically, a cutoff frequency of 1 Hz is recommended.\n\n    Parameters\n    ----------\n    n_components : int | float | None\n        Number of principal components (from the pre-whitening PCA step) that\n        are passed to the ICA algorithm during fitting. If :class:`int`, must\n        not be larger than ``max_pca_components``. If :class:`float` between 0\n        and 1, the number of components with cumulative explained variance less\n        than ``n_components`` will be used. If ``None``, ``max_pca_components``\n        will be used. Defaults to ``None``; the actual number used when\n        executing the :meth:`ICA.fit` method will be stored in the attribute\n        ``n_components_`` (note the trailing underscore).\n    max_pca_components : int | None\n        Number of principal components (from the pre-whitening PCA step) that\n        are retained for later use (i.e., for signal reconstruction in\n        :meth:`ICA.apply`; see the ``n_pca_components`` parameter). If\n        ``None``, no dimensionality reduction occurs and ``max_pca_components``\n        will equal the number of channels in the :class:`mne.io.Raw`,\n        :class:`mne.Epochs`, or :class:`mne.Evoked` object passed to\n        :meth:`ICA.fit`.\n    n_pca_components : int | float | None\n        Total number of components (ICA + PCA) used for signal reconstruction\n        in :meth:`ICA.apply`. At minimum, at least ``n_components`` will be\n        used (unless modified by ``ICA.include`` or ``ICA.exclude``). If\n        ``n_pca_components > n_components``, additional PCA components will be\n        incorporated. If :class:`float` between 0 and 1, the number is chosen\n        as the number of *PCA* components with cumulative explained variance\n        less than ``n_components`` (without accounting for ``ICA.include`` or\n        ``ICA.exclude``). If :class:`int` or :class:`float`, ``n_components_ \u2264\n        n_pca_components \u2264 max_pca_components`` must hold. If ``None``,\n        ``max_pca_components`` will be used. Defaults to ``None``.\n    noise_cov : None | instance of Covariance\n        Noise covariance used for pre-whitening. If None (default), channels\n        are scaled to unit variance (\"z-standardized\") prior to the whitening\n        by PCA.\n    \n    random_state : None | int | instance of ~numpy.random.RandomState\n        If ``random_state`` is an :class:`int`, it will be used as a seed for\n        :class:`~numpy.random.RandomState`. If ``None``, the seed will be\n        obtained from the operating system (see\n        :class:`~numpy.random.RandomState` for details). Default is\n        ``None``.\n        As estimation can be non-deterministic it can be useful to fix the\n        random state to have reproducible results.\n    method : {'fastica', 'infomax', 'picard'}\n        The ICA method to use in the fit method. Use the fit_params argument to\n        set additional parameters. Specifically, if you want Extended Infomax,\n        set method='infomax' and fit_params=dict(extended=True) (this also\n        works for method='picard'). Defaults to 'fastica'. For reference, see\n        [1]_, [2]_, [3]_ and [4]_.\n    fit_params : dict | None\n        Additional parameters passed to the ICA estimator as specified by\n        `method`.\n    max_iter : int\n        Maximum number of iterations during fit. Defaults to 200.\n    allow_ref_meg : bool\n        Allow ICA on MEG reference channels. Defaults to False.\n\n        .. versionadded:: 0.18\n    \n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Attributes\n    ----------\n    current_fit : str\n        Flag informing about which data type (raw or epochs) was used for the\n        fit.\n    ch_names : list-like\n        Channel names resulting from initial picking.\n    n_components_ : int\n        If fit, the actual number of PCA components used for ICA decomposition.\n    pre_whitener_ : ndarray, shape (n_channels, 1)\n        If fit, array used to pre-whiten the data prior to PCA.\n    pca_components_ : ndarray, shape (`max_pca_components`, n_channels)\n        If fit, the PCA components.\n    pca_mean_ : ndarray, shape (n_channels,)\n        If fit, the mean vector used to center the data before doing the PCA.\n    pca_explained_variance_ : ndarray, shape (`max_pca_components`,)\n        If fit, the variance explained by each PCA component.\n    mixing_matrix_ : ndarray, shape (`n_components_`, `n_components_`)\n        If fit, the whitened mixing matrix to go back from ICA space to PCA\n        space.\n        It is, in combination with the `pca_components_`, used by\n        :meth:`ICA.apply` and :meth:`ICA.get_components` to re-mix/project\n        a subset of the ICA components into the observed channel space.\n        The former method also removes the pre-whitening (z-scaling) and the\n        de-meaning.\n    unmixing_matrix_ : ndarray, shape (`n_components_`, `n_components_`)\n        If fit, the whitened matrix to go from PCA space to ICA space.\n        Used, in combination with the `pca_components_`, by the methods\n        :meth:`ICA.get_sources` and :meth:`ICA.apply` to unmix the observed data.\n    exclude : array-like of int\n        List or np.array of sources indices to exclude when re-mixing the data\n        in the :meth:`ICA.apply` method, i.e. artifactual ICA components.\n        The components identified manually and by the various automatic\n        artifact detection methods should be (manually) appended\n        (e.g. ``ica.exclude.extend(eog_inds)``).\n        (There is also an `exclude` parameter in the :meth:`ICA.apply` method.)\n        To scrap all marked components, set this attribute to an empty list.\n    info : None | instance of Info\n        The measurement info copied from the object fitted.\n    n_samples_ : int\n        The number of samples used on fit.\n    labels_ : dict\n        A dictionary of independent component indices, grouped by types of\n        independent components. This attribute is set by some of the artifact\n        detection functions.\n\n    Notes\n    -----\n    A trailing ``_`` in an attribute name signifies that the attribute was\n    added to the object during fitting, consistent with standard scikit-learn\n    practice.\n\n    Prior to fitting and applying the ICA, data is whitened (de-correlated and\n    scaled to unit variance, also called sphering transformation) by means of\n    a Principle Component Analysis (PCA). In addition to the whitening, this\n    step introduces the option to reduce the dimensionality of the data, both\n    prior to fitting the ICA (with the ``max_pca_components`` parameter) and\n    prior to reconstructing the sensor signals (with the ``n_pca_components``\n    parameter). In this way, we separate the question of how many ICA\n    components to estimate from the question of how much to reduce the\n    dimensionality of the signal. For example: by setting high values for\n    ``max_pca_components`` and ``n_pca_components``, relatively little\n    dimensionality reduction will occur when the signal is reconstructed,\n    regardless of the value of ``n_components`` (the number of ICA components\n    estimated).\n\n    .. note:: Commonly used for reasons of i) computational efficiency and\n              ii) additional noise reduction, it is a matter of current debate\n              whether pre-ICA dimensionality reduction could decrease the\n              reliability and stability of the ICA, at least for EEG data and\n              especially during preprocessing [5]_. (But see also [6]_ for a\n              possibly confounding effect of the different whitening/sphering\n              methods used in this paper (ZCA vs. PCA).)\n              On the other hand, for rank-deficient data such as EEG data after\n              average reference or interpolation, it is recommended to reduce\n              the dimensionality (by 1 for average reference and 1 for each\n              interpolated channel) for optimal ICA performance (see the\n              `EEGLAB wiki <eeglab_wiki_>`_).\n\n    Caveat! If supplying a noise covariance, keep track of the projections\n    available in the cov or in the raw object. For example, if you are\n    interested in EOG or ECG artifacts, EOG and ECG projections should be\n    temporally removed before fitting ICA, for example::\n\n        >> projs, raw.info['projs'] = raw.info['projs'], []\n        >> ica.fit(raw)\n        >> raw.info['projs'] = projs\n\n    Methods currently implemented are FastICA (default), Infomax, and Picard.\n    Standard Infomax can be quite sensitive to differences in floating point\n    arithmetic. Extended Infomax seems to be more stable in this respect,\n    enhancing reproducibility and stability of results; use Extended Infomax\n    via ``method='infomax', fit_params=dict(extended=True)``. Allowed entries\n    in ``fit_params`` are determined by the various algorithm implementations:\n    see :class:`~sklearn.decomposition.FastICA`, :func:`~picard.picard`,\n    :func:`~mne.preprocessing.infomax`.\n\n    Reducing the tolerance (set in `fit_params`) speeds up estimation at the\n    cost of consistency of the obtained results. It is difficult to directly\n    compare tolerance levels between Infomax and Picard, but for Picard and\n    FastICA a good rule of thumb is ``tol_fastica == tol_picard ** 2``.\n\n    .. _eeglab_wiki: https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA#Issue:_ICA_returns_near-identical_components_with_opposite_polarities\n\n    References\n    ----------\n    .. [1] Hyv\u00e4rinen, A., 1999. Fast and robust fixed-point algorithms for\n           independent component analysis. IEEE transactions on Neural\n           Networks, 10(3), pp.626-634.\n\n    .. [2] Bell, A.J., Sejnowski, T.J., 1995. An information-maximization\n           approach to blind separation and blind deconvolution. Neural\n           computation, 7(6), pp.1129-1159.\n\n    .. [3] Lee, T.W., Girolami, M., Sejnowski, T.J., 1999. Independent\n           component analysis using an extended infomax algorithm for mixed\n           subgaussian and supergaussian sources. Neural computation, 11(2),\n           pp.417-441.\n\n    .. [4] Ablin P, Cardoso J, Gramfort A, 2018. Faster Independent Component\n           Analysis by Preconditioning With Hessian Approximations.\n           IEEE Transactions on Signal Processing 66:4040\u20134049\n\n    .. [5] Artoni, F., Delorme, A., und Makeig, S, 2018. Applying Dimension\n           Reduction to EEG Data by Principal Component Analysis Reduces the\n           Quality of Its Subsequent Independent Component Decomposition.\n           NeuroImage 175, pp.176\u2013187.\n\n    .. [6] Montoya-Mart\u00ednez, J., Cardoso, J.-F., Gramfort, A, 2017. Caveats\n           with stochastic gradient and maximum likelihood based ICA for EEG.\n           LVA-ICA International Conference, Feb 2017, Grenoble, France.\n           `\u3008hal-01451432\u3009 <hal-01451432_>`_\n\n    .. _hal-01451432: https://hal.archives-ouvertes.fr/hal-01451432/document\n    ",
        "klass": "mne.preprocessing.ICA",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.preprocessing.xdawn._XdawnTransformer"
        ],
        "class_docstring": "Implementation of the Xdawn Algorithm.\n\n    Xdawn [1]_ [2]_ is a spatial filtering method designed to improve the\n    signal to signal + noise ratio (SSNR) of the ERP responses. Xdawn was\n    originally designed for P300 evoked potential by enhancing the target\n    response with respect to the non-target response. This implementation\n    is a generalization to any type of ERP.\n\n    Parameters\n    ----------\n    n_components : int, (default 2)\n        The number of components to decompose the signals.\n    signal_cov : None | Covariance | ndarray, shape (n_channels, n_channels)\n        (default None). The signal covariance used for whitening of the data.\n        if None, the covariance is estimated from the epochs signal.\n    correct_overlap : 'auto' or bool (default 'auto')\n        Compute the independent evoked responses per condition, while\n        correcting for event overlaps if any. If 'auto', then\n        overlapp_correction = True if the events do overlap.\n    reg : float | str | None (default None)\n        If not None (same as ``'empirical'``, default), allow\n        regularization for covariance estimation.\n        If float, shrinkage is used (0 <= shrinkage <= 1).\n        For str options, ``reg`` will be passed as ``method`` to\n        :func:`mne.compute_covariance`.\n\n    Attributes\n    ----------\n    filters_ : dict of ndarray\n        If fit, the Xdawn components used to decompose the data for each event\n        type, else empty.\n    patterns_ : dict of ndarray\n        If fit, the Xdawn patterns used to restore the signals for each event\n        type, else empty.\n    evokeds_ : dict of Evoked\n        If fit, the evoked response for each event type.\n    event_id_ : dict\n        The event id.\n    correct_overlap_ : bool\n        Whether overlap correction was applied.\n\n    Notes\n    -----\n    .. versionadded:: 0.10\n\n    See Also\n    --------\n    mne.decoding.CSP, mne.decoding.SPoC\n\n    References\n    ----------\n    .. [1] Rivet, B., Souloumiac, A., Attina, V., & Gibert, G. (2009). xDAWN\n           algorithm to enhance evoked potentials: application to\n           brain-computer interface. Biomedical Engineering, IEEE Transactions\n           on, 56(8), 2035-2043.\n\n    .. [2] Rivet, B., Cecotti, H., Souloumiac, A., Maby, E., & Mattout, J.\n           (2011, August). Theoretical analysis of xDAWN algorithm:\n           application to an efficient sensor selection in a P300 BCI. In\n           Signal Processing Conference, 2011 19th European (pp. 1382-1386).\n           IEEE.\n    ",
        "klass": "mne.preprocessing.xdawn.Xdawn",
        "module": "mne"
    },
    {
        "base_classes": [
            "mne.fixes.BaseEstimator",
            "mne.decoding.mixin.TransformerMixin"
        ],
        "class_docstring": "Implementation of the Xdawn Algorithm compatible with scikit-learn.\n\n    Xdawn is a spatial filtering method designed to improve the signal\n    to signal + noise ratio (SSNR) of the event related responses. Xdawn was\n    originally designed for P300 evoked potential by enhancing the target\n    response with respect to the non-target response. This implementation is a\n    generalization to any type of event related response.\n\n    .. note:: _XdawnTransformer does not correct for epochs overlap. To correct\n              overlaps see ``Xdawn``.\n\n    Parameters\n    ----------\n    n_components : int (default 2)\n        The number of components to decompose the signals.\n    reg : float | str | None (default None)\n        If not None (same as ``'empirical'``, default), allow\n        regularization for covariance estimation.\n        If float, shrinkage is used (0 <= shrinkage <= 1).\n        For str options, ``reg`` will be passed to ``method`` to\n        :func:`mne.compute_covariance`.\n    signal_cov : None | Covariance | array, shape (n_channels, n_channels)\n        The signal covariance used for whitening of the data.\n        if None, the covariance is estimated from the epochs signal.\n    method_params : dict | None\n        Parameters to pass to :func:`mne.compute_covariance`.\n\n        .. versionadded:: 0.16\n\n    Attributes\n    ----------\n    classes_ : array, shape (n_classes)\n        The event indices of the classes.\n    filters_ : array, shape (n_channels, n_channels)\n        The Xdawn components used to decompose the data for each event type.\n    patterns_ : array, shape (n_channels, n_channels)\n        The Xdawn patterns used to restore the signals for each event type.\n    ",
        "klass": "mne.preprocessing.xdawn._XdawnTransformer",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to generate simulated Source Estimates.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        Source space.\n    tstep : float\n        Time step between successive samples in data. Default is 0.001 sec.\n    duration : float | None\n        Time interval during which the simulation takes place in seconds.\n        If None, it is computed using existing events and waveform lengths.\n\n    Attributes\n    ----------\n    duration : float\n        The duration of the simulation in seconds.\n    n_times : int\n        The number of time samples of the simulation.\n    ",
        "klass": "mne.simulation.source.SourceSimulator",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to generate simulated Source Estimates.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        Source space.\n    tstep : float\n        Time step between successive samples in data. Default is 0.001 sec.\n    duration : float | None\n        Time interval during which the simulation takes place in seconds.\n        If None, it is computed using existing events and waveform lengths.\n\n    Attributes\n    ----------\n    duration : float\n        The duration of the simulation in seconds.\n    n_times : int\n        The number of time samples of the simulation.\n    ",
        "klass": "mne.simulation.SourceSimulator",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Warp surfaces via spherical harmonic smoothing and thin-plate splines.\n\n    Notes\n    -----\n    This class can be used to warp data from a source subject to\n    a destination subject, as described in [1]_. The procedure is:\n\n        1. Perform a spherical harmonic approximation to the source and\n           destination surfaces, which smooths them and allows arbitrary\n           interpolation.\n        2. Choose a set of matched points on the two surfaces.\n        3. Use thin-plate spline warping (common in 2D image manipulation)\n           to generate transformation coefficients.\n        4. Warp points from the source subject (which should be inside the\n           original surface) to the destination subject.\n\n    .. versionadded:: 0.14\n\n    References\n    ----------\n    .. [1] Darvas F, Ermer JJ, Mosher JC, Leahy RM (2006). \"Generic head\n           models for atlas-based EEG source analysis.\"\n           Human Brain Mapping 27:129-143\n    ",
        "klass": "mne.transforms._SphericalSurfaceWarp",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Temporarily set sys.argv.",
        "klass": "mne.utils.ArgvSetter",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Add more meaningful message to errors generated by ETS Toolkit.",
        "klass": "mne.utils.ETSContext",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Generate a command-line progressbar.\n\n    Parameters\n    ----------\n    max_value : int | iterable\n        Maximum value of process (e.g. number of samples to process, bytes to\n        download, etc.). If an iterable is given, then `max_value` will be set\n        to the length of this iterable.\n    initial_value : int\n        Initial value of process, useful when resuming process from a specific\n        value, defaults to 0.\n    mesg : str\n        Message to include at end of progress bar.\n    max_chars : int | str\n        Number of characters to use for progress bar itself.\n        This does not include characters used for the message or percent\n        complete. Can be \"auto\" (default) to try to set a sane value based\n        on the terminal width.\n    progress_character : char\n        Character in the progress bar that indicates the portion completed.\n    spinner : bool\n        Show a spinner.  Useful for long-running processes that may not\n        increment the progress bar very often.  This provides the user with\n        feedback that the progress has not stalled.\n    max_total_width : int | str\n        Maximum total message width. Can use \"auto\" (default) to try to set\n        a sane value based on the current terminal width.\n    verbose_bool : bool | 'auto'\n        If True, show progress. 'auto' will use the current MNE verbose level.\n\n    Example\n    -------\n    >>> progress = ProgressBar(13000)\n    >>> progress.update(3000) # doctest: +SKIP\n    [.........                               ] 23.07692 |\n    >>> progress.update(6000) # doctest: +SKIP\n    [..................                      ] 46.15385 |\n\n    >>> progress = ProgressBar(13000, spinner=True)\n    >>> progress.update(3000) # doctest: +SKIP\n    [.........                               ] 23.07692 |\n    >>> progress.update(6000) # doctest: +SKIP\n    [..................                      ] 46.15385 /\n    ",
        "klass": "mne.utils.ProgressBar",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Store logging.\n\n    This will remove all other logging handlers, and return the handler to\n    stdout when complete.\n    ",
        "klass": "mne.utils.catch_logging",
        "module": "mne"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context handler for logging level.\n\n    Parameters\n    ----------\n    level : int\n        The level to use.\n    ",
        "klass": "mne.utils.use_log_level",
        "module": "mne"
    },
    {
        "base_classes": [
            "networkx.es.multidigraph.MultiDiGraph"
        ],
        "class_docstring": "An extension to :class:`networkx.MultiDiGraph` to represent BEL.",
        "klass": "pybel.BELGraph",
        "module": "pybel"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Build and runs analytical pipelines on BEL graphs.\n\n    Example usage:\n\n    >>> from pybel import BELGraph\n    >>> from pybel.struct.pipeline import Pipeline\n    >>> from pybel.struct.mutation import enrich_protein_and_rna_origins, prune_protein_rna_origins\n    >>> graph = BELGraph()\n    >>> example = Pipeline()\n    >>> example.append(enrich_protein_and_rna_origins)\n    >>> example.append(prune_protein_rna_origins)\n    >>> result = example.run(graph)\n\n    ",
        "klass": "pybel.Pipeline",
        "module": "pybel"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "\n    Token to exactly match a specified string as a keyword, that is, it must be\n    immediately followed by a non-keyword character.  Compare with C{L{Literal}}:\n     - C{Literal(\"if\")} will match the leading C{'if'} in C{'ifAndOnlyIf'}.\n     - C{Keyword(\"if\")} will not; it will only match the leading C{'if'} in C{'if x=1'}, or C{'if(y==2)'}\n    Accepts two optional constructor arguments in addition to the keyword string:\n     - C{identChars} is a string of characters that would be valid identifier characters,\n          defaulting to all alphanumerics + \"_\" and \"$\"\n     - C{caseless} allows case-insensitive matching, default is C{False}.\n       \n    Example::\n        Keyword(\"start\").parseString(\"start\")  # -> ['start']\n        Keyword(\"start\").parseString(\"starting\")  # -> Exception\n\n    For case-insensitive matching, use L{CaselessKeyword}.\n    ",
        "klass": "pyparsing.Keyword",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.TokenConverter"
        ],
        "class_docstring": "Converter to return the matched tokens as a list - useful for\n    returning tokens of :class:`ZeroOrMore` and :class:`OneOrMore` expressions.\n\n    Example::\n\n        ident = Word(alphas)\n        num = Word(nums)\n        term = ident | num\n        func = ident + Optional(delimitedList(term))\n        print(func.parseString(\"fn a, b, 100\"))  # -> ['fn', 'a', 'b', '100']\n\n        func = ident + Group(Optional(delimitedList(term)))\n        print(func.parseString(\"fn a, b, 100\"))  # -> ['fn', ['a', 'b', '100']]\n    ",
        "klass": "pyparsing.Group",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The ProgressBar class which updates and prints the bar.\n\n    A common way of using it is like:\n    >>> pbar = ProgressBar().start()\n    >>> for i in range(100):\n    ...    # do something\n    ...    pbar.update(i+1)\n    ...\n    >>> pbar.finish()\n\n    You can also use a ProgressBar as an iterator:\n    >>> progress = ProgressBar()\n    >>> for i in progress(some_iterable):\n    ...    # do something\n    ...\n\n    Since the progress bar is incredibly customizable you can specify\n    different widgets of any type in any order. You can even write your own\n    widgets! However, since there are already a good number of widgets you\n    should probably play around with them before moving on to create your own\n    widgets.\n\n    The term_width parameter represents the current terminal width. If the\n    parameter is set to an integer then the progress bar will use that,\n    otherwise it will attempt to determine the terminal width falling back to\n    80 columns if the width cannot be determined.\n\n    When implementing a widget's update method you are passed a reference to\n    the current progress bar. As a result, you have access to the\n    ProgressBar's methods and attributes. Although there is nothing preventing\n    you from changing the ProgressBar you should treat it as read only.\n\n    Useful methods and attributes include (Public API):\n     - currval: current progress (0 <= currval <= maxval)\n     - maxval: maximum (and final) value\n     - finished: True if the bar has finished (reached 100%)\n     - start_time: the time when start() method of ProgressBar was called\n     - seconds_elapsed: seconds elapsed since start_time and last call to\n                        update\n     - percentage(): progress in percent [0..100]\n    ",
        "klass": "progressbar.ProgressBar",
        "module": "progressbar"
    },
    {
        "base_classes": [
            "googleads.common.CommonClient"
        ],
        "class_docstring": "A central location to set headers and create web service clients.\n\n  Attributes:\n    oauth2_client: A googleads.oauth2.GoogleOAuth2Client used to authorize your\n        requests.\n    application_name: An arbitrary string which will be used to identify your\n        application\n    network_code: A string identifying the network code of the network you are\n        accessing. All requests other than some NetworkService calls require\n        this header to be set.\n  ",
        "klass": "googleads.ad_manager.AdManagerClient",
        "module": "googleads"
    },
    {
        "base_classes": [
            "googleads.common.CommonClient"
        ],
        "class_docstring": "A central location to set headers and create web service clients.\n\n  Attributes:\n    developer_token: A string containing your AdWords API developer token.\n    oauth2_client: A googleads.oauth2.GoogleOAuth2Client used to authorize your\n        requests.\n    user_agent: An arbitrary string which will be used to identify your\n        application\n    client_customer_id: A string identifying which AdWords customer you want to\n        act as.\n    validate_only: A boolean indicating if you want your request to be validated\n        but not actually executed.\n    partial_failure: A boolean indicating if you want your mutate calls\n        containing several operations, some of which fail and some of which\n        succeed, to result in a complete failure with no changes made or a\n        partial failure with some changes made. Only certain services respect\n        this header.\n  ",
        "klass": "googleads.adwords.AdWordsClient",
        "module": "googleads"
    },
    {
        "base_classes": [
            "pymunk._pickle.PickleMixin",
            "object"
        ],
        "class_docstring": "Simple bounding box.\n\n    Stored as left, bottom, right, top values.\n    ",
        "klass": "pymunk.BB",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "pymunk.shapes.Shape"
        ],
        "class_docstring": "A circle shape defined by a radius\n\n    This is the fastest and simplest collision shape\n    ",
        "klass": "pymunk.Circle",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "pymunk.shapes.Shape"
        ],
        "class_docstring": "A convex polygon shape\n\n    Slowest, but most flexible collision shape.\n    ",
        "klass": "pymunk.Poly",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "pymunk.shapes.Shape"
        ],
        "class_docstring": "A line segment shape between two points\n\n    Meant mainly as a static shape. Can be beveled in order to give them a\n    thickness.\n    ",
        "klass": "pymunk.Segment",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "pymunk._pickle.PickleMixin",
            "object"
        ],
        "class_docstring": "Spaces are the basic unit of simulation. You add rigid bodies, shapes\n    and joints to it and then step them all forward together through time.\n\n    A Space can be copied and pickled. Note that any post step callbacks are \n    not copied. Also note that some internal collision cache data is not copied,\n    which can make the simulation a bit unstable the first few steps of the \n    fresh copy.\n\n    Custom properties set on the space will also be copied/pickled.\n\n    Any collision handlers will also be copied/pickled. Note that depending on \n    the pickle protocol used there are some restrictions on what functions can \n    be copied/pickled.\n\n    Example::\n\n    >>> import pymunk, pickle\n    >>> space = pymunk.Space()\n    >>> space2 = space.copy()\n    >>> space3 = pickle.loads(pickle.dumps(space))\n    ",
        "klass": "pymunk.Space",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "2d vector class, supports vector and scalar operators, and also \n    provides some high level functions.\n    ",
        "klass": "pymunk.vec2d.Vec2d",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "2d vector class, supports vector and scalar operators, and also \n    provides some high level functions.\n    ",
        "klass": "pymunk.Vec2d",
        "module": "pymunk"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates and manages an isolated environment to install build deps\n    ",
        "klass": "pip._internal.build_env.BuildEnvironment",
        "module": "pip"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Helper class that owns and cleans up a temporary directory.\n\n    This class can be used as a context manager or as an OO representation of a\n    temporary directory.\n\n    Attributes:\n        path\n            Location to the created temporary directory\n        delete\n            Whether the directory should be deleted when exiting\n            (when used as a contextmanager)\n\n    Methods:\n        cleanup()\n            Deletes the temporary directory\n\n    When used as a context manager, if the delete attribute is True, on\n    exiting the context the temporary directory is deleted.\n    ",
        "klass": "pip._internal.utils.temp_dir.TempDirectory",
        "module": "pip"
    },
    {
        "base_classes": [
            "pip._internal.cli.parser.CustomOptionParser"
        ],
        "class_docstring": "Custom option parser which updates its defaults by checking the\n    configuration files and environmental variables",
        "klass": "pip._internal.cli.parser.ConfigOptionParser",
        "module": "pip"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A feed-style parser of email.",
        "klass": "email.feedparser.FeedParser",
        "module": "email"
    },
    {
        "base_classes": [
            "pip._vendor.requests.adapters.BaseAdapter"
        ],
        "class_docstring": "The built-in HTTP Adapter for urllib3.\n\n    Provides a general-case interface for Requests sessions to contact HTTP and\n    HTTPS urls by implementing the Transport Adapter interface. This class will\n    usually be created by the :class:`Session <Session>` class under the\n    covers.\n\n    :param pool_connections: The number of urllib3 connection pools to cache.\n    :param pool_maxsize: The maximum number of connections to save in the pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups, socket\n        connections and connection timeouts, never to requests where data has\n        made it to the server. By default, Requests does not retry failed\n        connections. If you need granular control over the conditions under\n        which we retry a request, import urllib3's ``Retry`` class and pass\n        that instead.\n    :param pool_block: Whether the connection pool should block for connections.\n\n    Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> a = requests.adapters.HTTPAdapter(max_retries=3)\n      >>> s.mount('http://', a)\n    ",
        "klass": "pip._vendor.requests.adapters.HTTPAdapter",
        "module": "pip"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The ``UniversalDetector`` class underlies the ``chardet.detect`` function\n    and coordinates all of the different charset probers.\n\n    To get a ``dict`` containing an encoding and its confidence, you can simply\n    run:\n\n    .. code::\n\n            u = UniversalDetector()\n            u.feed(some_bytes)\n            u.close()\n            detected = u.result\n\n    ",
        "klass": "pip._vendor.chardet.universaldetector.UniversalDetector",
        "module": "pip"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": " A ChainMap groups multiple dicts (or other mappings) together\n    to create a single, updateable view.\n\n    The underlying mappings are stored in a list.  That list is public and can\n    be accessed or updated using the *maps* attribute.  There is no other\n    state.\n\n    Lookups search the underlying mappings successively until a key is found.\n    In contrast, writes, updates, and deletions only operate on the first\n    mapping.\n\n    ",
        "klass": "collections.ChainMap",
        "module": "collections"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "filter(function or None, iterable) --> filter object\n\nReturn an iterator yielding those items of iterable for which function(item)\nis true. If function is None, return the items that are true.",
        "klass": "filter",
        "module": "filter"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "Common base class for all non-exit exceptions.",
        "klass": "Exception",
        "module": "Exception"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A toolbox for evolution that contains the evolutionary operators. At\n    first the toolbox contains a :meth:`~deap.toolbox.clone` method that\n    duplicates any element it is passed as argument, this method defaults to\n    the :func:`copy.deepcopy` function. and a :meth:`~deap.toolbox.map`\n    method that applies the function given as first argument to every items\n    of the iterables given as next arguments, this method defaults to the\n    :func:`map` function. You may populate the toolbox with any other\n    function by using the :meth:`~deap.base.Toolbox.register` method.\n\n    Concrete usages of the toolbox are shown for initialization in the\n    :ref:`creating-types` tutorial and for tools container in the\n    :ref:`next-step` tutorial.\n    ",
        "klass": "deap.base.Toolbox",
        "module": "deap"
    },
    {
        "base_classes": [
            "deap.gp.PrimitiveSetTyped"
        ],
        "class_docstring": "Class same as :class:`~deap.gp.PrimitiveSetTyped`, except there is no\n    definition of type.\n    ",
        "klass": "deap.gp.PrimitiveSet",
        "module": "deap"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class that contains the primitives that can be used to solve a\n    Strongly Typed GP problem. The set also defined the researched\n    function return type, and input arguments type and number.\n    ",
        "klass": "deap.gp.PrimitiveSetTyped",
        "module": "deap"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Dictionary of :class:`Statistics` object allowing to compute\n    statistics on multiple keys using a single call to :meth:`compile`. It\n    takes a set of key-value pairs associating a statistics object to a\n    unique name. This name can then be used to retrieve the statistics object.\n\n    The following code computes statistics simultaneously on the length and\n    the first value of the provided objects.\n    ::\n\n        >>> from operator import itemgetter\n        >>> import numpy\n        >>> len_stats = Statistics(key=len)\n        >>> itm0_stats = Statistics(key=itemgetter(0))\n        >>> mstats = MultiStatistics(length=len_stats, item=itm0_stats)\n        >>> mstats.register(\"mean\", numpy.mean, axis=0)\n        >>> mstats.register(\"max\", numpy.max, axis=0)\n        >>> mstats.compile([[0.0, 1.0, 1.0, 5.0], [2.0, 5.0]])  # doctest: +SKIP\n        {'length': {'mean': 3.0, 'max': 4}, 'item': {'mean': 1.0, 'max': 2.0}}\n    ",
        "klass": "deap.tools.MultiStatistics",
        "module": "deap"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Object that compiles statistics on a list of arbitrary objects.\n    When created the statistics object receives a *key* argument that\n    is used to get the values on which the function will be computed.\n    If not provided the *key* argument defaults to the identity function.\n\n    The value returned by the key may be a multi-dimensional object, i.e.:\n    a tuple or a list, as long as the statistical function registered\n    support it. So for example, statistics can be computed directly on\n    multi-objective fitnesses when using numpy statistical function.\n\n    :param key: A function to access the values on which to compute the\n                statistics, optional.\n\n    ::\n\n        >>> import numpy\n        >>> s = Statistics()\n        >>> s.register(\"mean\", numpy.mean)\n        >>> s.register(\"max\", max)\n        >>> s.compile([1, 2, 3, 4])     # doctest: +SKIP\n        {'max': 4, 'mean': 2.5}\n        >>> s.compile([5, 6, 7, 8])     # doctest: +SKIP\n        {'mean': 6.5, 'max': 8}\n    ",
        "klass": "deap.tools.Statistics",
        "module": "deap"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the app resource.",
        "klass": "solum.api.handlers.app_handler.AppHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the assembly resource.",
        "klass": "solum.api.handlers.assembly_handler.AssemblyHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the component resource.",
        "klass": "solum.api.handlers.component_handler.ComponentHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the extension resource.",
        "klass": "solum.api.handlers.extension_handler.ExtensionHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the infrastructure stack resource.",
        "klass": "solum.api.handlers.infrastructure_handler.InfrastructureStackHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the languagepack resource.",
        "klass": "solum.api.handlers.language_pack_handler.LanguagePackHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the operation resource.",
        "klass": "solum.api.handlers.operation_handler.OperationHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the pipeline resource.",
        "klass": "solum.api.handlers.pipeline_handler.PipelineHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the sensor resource.",
        "klass": "solum.api.handlers.sensor_handler.SensorHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the service resource.",
        "klass": "solum.api.handlers.service_handler.ServiceHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "solum.api.handlers.handler.Handler"
        ],
        "class_docstring": "Fulfills a request on the workflow resource.",
        "klass": "solum.api.handlers.workflow_handler.WorkflowHandler",
        "module": "solum"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "solum.objects.execution.Execution"
        ],
        "class_docstring": "Represent an execution in sqlalchemy.",
        "klass": "solum.objects.sqlalchemy.execution.Execution",
        "module": "solum"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "solum.objects.extension.Extension"
        ],
        "class_docstring": "Represent an extension in sqlalchemy.",
        "klass": "solum.objects.sqlalchemy.extension.Extension",
        "module": "solum"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "solum.objects.plan.Plan"
        ],
        "class_docstring": "Represent a plan in sqlalchemy.",
        "klass": "solum.objects.sqlalchemy.plan.Plan",
        "module": "solum"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "solum.objects.sensor.Sensor"
        ],
        "class_docstring": "Represent an sensor in sqlalchemy.",
        "klass": "solum.objects.sqlalchemy.sensor.Sensor",
        "module": "solum"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "solum.objects.workflow.Workflow"
        ],
        "class_docstring": "Represent an app workflow in sqlalchemy.",
        "klass": "solum.objects.sqlalchemy.workflow.Workflow",
        "module": "solum"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An `EventAccumulator` takes an event generator, and accumulates the values.\n\n  The `EventAccumulator` is intended to provide a convenient Python interface\n  for loading Event data written during a TensorFlow run. TensorFlow writes out\n  `Event` protobuf objects, which have a timestamp and step number, and often\n  contain a `Summary`. Summaries can have different kinds of data like an image,\n  a scalar value, or a histogram. The Summaries also have a tag, which we use to\n  organize logically related data. The `EventAccumulator` supports retrieving\n  the `Event` and `Summary` data by its tag.\n\n  Calling `Tags()` gets a map from `tagType` (e.g. `'images'`,\n  `'compressedHistograms'`, `'scalars'`, etc) to the associated tags for those\n  data types. Then, various functional endpoints (eg\n  `Accumulator.Scalars(tag)`) allow for the retrieval of all data\n  associated with that tag.\n\n  The `Reload()` method synchronously loads all of the data written so far.\n\n  Histograms, audio, and images are very large, so storing all of them is not\n  recommended.\n\n  Fields:\n    audios: A reservoir.Reservoir of audio summaries.\n    compressed_histograms: A reservoir.Reservoir of compressed\n        histogram summaries.\n    histograms: A reservoir.Reservoir of histogram summaries.\n    images: A reservoir.Reservoir of image summaries.\n    most_recent_step: Step of last Event proto added. This should only\n        be accessed from the thread that calls Reload. This is -1 if\n        nothing has been loaded yet.\n    most_recent_wall_time: Timestamp of last Event proto added. This is\n        a float containing seconds from the UNIX epoch, or -1 if\n        nothing has been loaded yet. This should only be accessed from\n        the thread that calls Reload.\n    path: A file path to a directory containing tf events files, or a single\n        tf events file. The accumulator will load events from this path.\n    scalars: A reservoir.Reservoir of scalar summaries.\n    tensors: A reservoir.Reservoir of tensor summaries.\n\n  @@Tensors\n  ",
        "klass": "tensorboard.backend.event_processing.event_accumulator.EventAccumulator",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An `EventMultiplexer` manages access to multiple `EventAccumulator`s.\n\n  Each `EventAccumulator` is associated with a `run`, which is a self-contained\n  TensorFlow execution. The `EventMultiplexer` provides methods for extracting\n  information about events from multiple `run`s.\n\n  Example usage for loading specific runs from files:\n\n  ```python\n  x = EventMultiplexer({'run1': 'path/to/run1', 'run2': 'path/to/run2'})\n  x.Reload()\n  ```\n\n  Example usage for loading a directory where each subdirectory is a run\n\n  ```python\n  (eg:) /parent/directory/path/\n        /parent/directory/path/run1/\n        /parent/directory/path/run1/events.out.tfevents.1001\n        /parent/directory/path/run1/events.out.tfevents.1002\n\n        /parent/directory/path/run2/\n        /parent/directory/path/run2/events.out.tfevents.9232\n\n        /parent/directory/path/run3/\n        /parent/directory/path/run3/events.out.tfevents.9232\n  x = EventMultiplexer().AddRunsFromDirectory('/parent/directory/path')\n  (which is equivalent to:)\n  x = EventMultiplexer({'run1': '/parent/directory/path/run1', 'run2':...}\n  ```\n\n  If you would like to watch `/parent/directory/path`, wait for it to be created\n    (if necessary) and then periodically pick up new runs, use\n    `AutoloadingMultiplexer`\n  @@Tensors\n  ",
        "klass": "tensorboard.backend.event_processing.event_multiplexer.EventMultiplexer",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An `EventMultiplexer` manages access to multiple `EventAccumulator`s.\n\n  Each `EventAccumulator` is associated with a `run`, which is a self-contained\n  TensorFlow execution. The `EventMultiplexer` provides methods for extracting\n  information about events from multiple `run`s.\n\n  Example usage for loading specific runs from files:\n\n  ```python\n  x = EventMultiplexer({'run1': 'path/to/run1', 'run2': 'path/to/run2'})\n  x.Reload()\n  ```\n\n  Example usage for loading a directory where each subdirectory is a run\n\n  ```python\n  (eg:) /parent/directory/path/\n        /parent/directory/path/run1/\n        /parent/directory/path/run1/events.out.tfevents.1001\n        /parent/directory/path/run1/events.out.tfevents.1002\n\n        /parent/directory/path/run2/\n        /parent/directory/path/run2/events.out.tfevents.9232\n\n        /parent/directory/path/run3/\n        /parent/directory/path/run3/events.out.tfevents.9232\n  x = EventMultiplexer().AddRunsFromDirectory('/parent/directory/path')\n  (which is equivalent to:)\n  x = EventMultiplexer({'run1': '/parent/directory/path/run1', 'run2':...}\n  ```\n\n  If you would like to watch `/parent/directory/path`, wait for it to be created\n    (if necessary) and then periodically pick up new runs, use\n    `AutoloadingMultiplexer`\n  @@Tensors\n  ",
        "klass": "tensorboard.backend.event_processing.plugin_event_multiplexer.EventMultiplexer",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A map-to-arrays container, with deterministic Reservoir Sampling.\n\n  Items are added with an associated key. Items may be retrieved by key, and\n  a list of keys can also be retrieved. If size is not zero, then it dictates\n  the maximum number of items that will be stored with each key. Once there are\n  more items for a given key, they are replaced via reservoir sampling, such\n  that each item has an equal probability of being included in the sample.\n\n  Deterministic means that for any given seed and bucket size, the sequence of\n  values that are kept for any given tag will always be the same, and that this\n  is independent of any insertions on other tags. That is:\n\n  >>> separate_reservoir = reservoir.Reservoir(10)\n  >>> interleaved_reservoir = reservoir.Reservoir(10)\n  >>> for i in xrange(100):\n  >>>   separate_reservoir.AddItem('key1', i)\n  >>> for i in xrange(100):\n  >>>   separate_reservoir.AddItem('key2', i)\n  >>> for i in xrange(100):\n  >>>   interleaved_reservoir.AddItem('key1', i)\n  >>>   interleaved_reservoir.AddItem('key2', i)\n\n  separate_reservoir and interleaved_reservoir will be in identical states.\n\n  See: https://en.wikipedia.org/wiki/Reservoir_sampling\n\n  Adding items has amortized O(1) runtime.\n\n  Fields:\n    always_keep_last: Whether the latest seen sample is always at the\n      end of the reservoir. Defaults to True.\n    size: An integer of the maximum number of samples.\n  ",
        "klass": "tensorboard.backend.event_processing.reservoir.Reservoir",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A container for items from a stream, that implements reservoir sampling.\n\n  It always stores the most recent item as its final item.\n  ",
        "klass": "tensorboard.backend.event_processing.reservoir._ReservoirBucket",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An object that stores some headers.  It has a dict-like interface\n    but is ordered and can store the same keys multiple times.\n\n    This data structure is useful if you want a nicer way to handle WSGI\n    headers which are stored as tuples in a list.\n\n    From Werkzeug 0.3 onwards, the :exc:`KeyError` raised by this class is\n    also a subclass of the :class:`~exceptions.BadRequest` HTTP exception\n    and will render a page for a ``400 BAD REQUEST`` if caught in a\n    catch-all for HTTP exceptions.\n\n    Headers is mostly compatible with the Python :class:`wsgiref.headers.Headers`\n    class, with the exception of `__getitem__`.  :mod:`wsgiref` will return\n    `None` for ``headers['missing']``, whereas :class:`Headers` will raise\n    a :class:`KeyError`.\n\n    To create a new :class:`Headers` object pass it a list or dict of headers\n    which are used as default values.  This does not reuse the list passed\n    to the constructor for internal usage.\n\n    :param defaults: The list of default values for the :class:`Headers`.\n\n    .. versionchanged:: 0.9\n       This data structure now stores unicode values similar to how the\n       multi dicts do it.  The main difference is that bytes can be set as\n       well which will automatically be latin1 decoded.\n\n    .. versionchanged:: 0.9\n       The :meth:`linked` function was removed without replacement as it\n       was an API that does not support the changes to the encoding model.\n    ",
        "klass": "werkzeug.datastructures.Headers",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Writes `Event` protocol buffers to an event file.\n\n    The `EventFileWriter` class creates an event file in the specified directory,\n    and asynchronously writes Event protocol buffers to the file. The Event file\n    is encoded using the tfrecord format, which is similar to RecordIO.\n    ",
        "klass": "tensorboard.summary.writer.event_file_writer.EventFileWriter",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Write encoded protobuf to a file with packing defined in tensorflow",
        "klass": "tensorboard.summary.writer.record_writer.RecordWriter",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Writes bytes to a file.",
        "klass": "tensorboard.summary.writer.event_file_writer._AsyncWriter",
        "module": "tensorboard"
    },
    {
        "base_classes": [
            "seaborn.axisgrid.Grid"
        ],
        "class_docstring": "Multi-plot grid for plotting conditional relationships.",
        "klass": "seaborn.FacetGrid",
        "module": "seaborn"
    },
    {
        "base_classes": [
            "seaborn.axisgrid.Grid"
        ],
        "class_docstring": "Multi-plot grid for plotting conditional relationships.",
        "klass": "seaborn.apionly.FacetGrid",
        "module": "seaborn"
    },
    {
        "base_classes": [
            "matplotlib.colors.Colormap"
        ],
        "class_docstring": "\n    Colormap objects based on lookup tables using linear segments.\n\n    The lookup table is generated using linear interpolation for each\n    primary color, with the 0-1 domain divided into any number of\n    segments.\n    ",
        "klass": "matplotlib.colors.LinearSegmentedColormap",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.SchemaItem",
            "sqlalchemy.sql.elements.ColumnClause"
        ],
        "class_docstring": "Represents a column in a database table.",
        "klass": "sqlalchemy.Column",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.type_api.Emulated",
            "sqlalchemy.sql.sqltypes.String",
            "sqlalchemy.sql.sqltypes.SchemaType"
        ],
        "class_docstring": "Generic Enum Type.\n\n    The :class:`.Enum` type provides a set of possible string values\n    which the column is constrained towards.\n\n    The :class:`.Enum` type will make use of the backend's native \"ENUM\"\n    type if one is available; otherwise, it uses a VARCHAR datatype and\n    produces a CHECK constraint.  Use of the backend-native enum type\n    can be disabled using the :paramref:`.Enum.native_enum` flag, and\n    the production of the CHECK constraint is configurable using the\n    :paramref:`.Enum.create_constraint` flag.\n\n    The :class:`.Enum` type also provides in-Python validation of string\n    values during both read and write operations.  When reading a value\n    from the database in a result set, the string value is always checked\n    against the list of possible values and a ``LookupError`` is raised\n    if no match is found.  When passing a value to the database as a\n    plain string within a SQL statement, if the\n    :paramref:`.Enum.validate_strings` parameter is\n    set to True, a ``LookupError`` is raised for any string value that's\n    not located in the given list of possible values; note that this\n    impacts usage of LIKE expressions with enumerated values (an unusual\n    use case).\n\n    .. versionchanged:: 1.1 the :class:`.Enum` type now provides in-Python\n       validation of input values as well as on data being returned by\n       the database.\n\n    The source of enumerated values may be a list of string values, or\n    alternatively a PEP-435-compliant enumerated class.  For the purposes\n    of the :class:`.Enum` datatype, this class need only provide a\n    ``__members__`` method.\n\n    When using an enumerated class, the enumerated objects are used\n    both for input and output, rather than strings as is the case with\n    a plain-string enumerated type::\n\n        import enum\n        class MyEnum(enum.Enum):\n            one = 1\n            two = 2\n            three = 3\n\n        t = Table(\n            'data', MetaData(),\n            Column('value', Enum(MyEnum))\n        )\n\n        connection.execute(t.insert(), {\"value\": MyEnum.two})\n        assert connection.scalar(t.select()) is MyEnum.two\n\n    Above, the string names of each element, e.g. \"one\", \"two\", \"three\",\n    are persisted to the database; the values of the Python Enum, here\n    indicated as integers, are **not** used; the value of each enum can\n    therefore be any kind of Python object whether or not it is persistable.\n\n    In order to persist the values and not the names, the\n    :paramref:`.Enum.values_callable` parameter may be used.   The value of\n    this parameter is a user-supplied callable, which  is intended to be used\n    with a PEP-435-compliant enumerated class and  returns a list of string\n    values to be persisted.   For a simple enumeration that uses string values,\n    a callable such as  ``lambda x: [e.value for e in x]`` is sufficient.\n\n    .. versionadded:: 1.1 - support for PEP-435-style enumerated\n       classes.\n\n\n    .. seealso::\n\n        :class:`.postgresql.ENUM` - PostgreSQL-specific type,\n        which has additional functionality.\n\n        :class:`.mysql.ENUM` - MySQL-specific type\n\n    ",
        "klass": "sqlalchemy.Enum",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.ColumnCollectionMixin",
            "sqlalchemy.sql.schema.SchemaItem"
        ],
        "class_docstring": "A table-level INDEX.\n\n    Defines a composite (one or more column) INDEX.\n\n    E.g.::\n\n        sometable = Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100))\n                    )\n\n        Index(\"some_index\", sometable.c.name)\n\n    For a no-frills, single column index, adding\n    :class:`.Column` also supports ``index=True``::\n\n        sometable = Table(\"sometable\", metadata,\n                        Column(\"name\", String(50), index=True)\n                    )\n\n    For a composite index, multiple columns can be specified::\n\n        Index(\"some_index\", sometable.c.name, sometable.c.address)\n\n    Functional indexes are supported as well, typically by using the\n    :data:`.func` construct in conjunction with table-bound\n    :class:`.Column` objects::\n\n        Index(\"some_index\", func.lower(sometable.c.name))\n\n    An :class:`.Index` can also be manually associated with a :class:`.Table`,\n    either through inline declaration or using\n    :meth:`.Table.append_constraint`.  When this approach is used, the names\n    of the indexed columns can be specified as strings::\n\n        Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100)),\n                        Index(\"some_index\", \"name\", \"address\")\n                )\n\n    To support functional or expression-based indexes in this form, the\n    :func:`.text` construct may be used::\n\n        from sqlalchemy import text\n\n        Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100)),\n                        Index(\"some_index\", text(\"lower(name)\"))\n                )\n\n    .. versionadded:: 0.9.5 the :func:`.text` construct may be used to\n       specify :class:`.Index` expressions, provided the :class:`.Index`\n       is explicitly associated with the :class:`.Table`.\n\n\n    .. seealso::\n\n        :ref:`schema_indexes` - General information on :class:`.Index`.\n\n        :ref:`postgresql_indexes` - PostgreSQL-specific options available for\n        the :class:`.Index` construct.\n\n        :ref:`mysql_indexes` - MySQL-specific options available for the\n        :class:`.Index` construct.\n\n        :ref:`mssql_indexes` - MSSQL-specific options available for the\n        :class:`.Index` construct.\n\n    ",
        "klass": "sqlalchemy.Index",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.schema.SchemaItem"
        ],
        "class_docstring": "A collection of :class:`.Table` objects and their associated schema\n    constructs.\n\n    Holds a collection of :class:`.Table` objects as well as\n    an optional binding to an :class:`.Engine` or\n    :class:`.Connection`.  If bound, the :class:`.Table` objects\n    in the collection and their columns may participate in implicit SQL\n    execution.\n\n    The :class:`.Table` objects themselves are stored in the\n    :attr:`.MetaData.tables` dictionary.\n\n    :class:`.MetaData` is a thread-safe object for read operations.\n    Construction of new tables within a single :class:`.MetaData` object,\n    either explicitly or via reflection, may not be completely thread-safe.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    ",
        "klass": "sqlalchemy.MetaData",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.schema.DefaultGenerator"
        ],
        "class_docstring": "Represents a named database sequence.\n\n    The :class:`.Sequence` object represents the name and configurational\n    parameters of a database sequence.   It also represents\n    a construct that can be \"executed\" by a SQLAlchemy :class:`.Engine`\n    or :class:`.Connection`, rendering the appropriate \"next value\" function\n    for the target database and returning a result.\n\n    The :class:`.Sequence` is typically associated with a primary key column::\n\n        some_table = Table(\n            'some_table', metadata,\n            Column('id', Integer, Sequence('some_table_seq'),\n            primary_key=True)\n        )\n\n    When CREATE TABLE is emitted for the above :class:`.Table`, if the\n    target platform supports sequences, a CREATE SEQUENCE statement will\n    be emitted as well.   For platforms that don't support sequences,\n    the :class:`.Sequence` construct is ignored.\n\n    .. seealso::\n\n        :class:`.CreateSequence`\n\n        :class:`.DropSequence`\n\n    ",
        "klass": "sqlalchemy.Sequence",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.SchemaItem",
            "sqlalchemy.sql.selectable.TableClause"
        ],
        "class_docstring": "Represent a table in a database.\n\n    e.g.::\n\n        mytable = Table(\"mytable\", metadata,\n                        Column('mytable_id', Integer, primary_key=True),\n                        Column('value', String(50))\n                   )\n\n    The :class:`.Table` object constructs a unique instance of itself based\n    on its name and optional schema name within the given\n    :class:`.MetaData` object. Calling the :class:`.Table`\n    constructor with the same name and same :class:`.MetaData` argument\n    a second time will return the *same* :class:`.Table` object - in this way\n    the :class:`.Table` constructor acts as a registry function.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    Constructor arguments are as follows:\n\n    :param name: The name of this table as represented in the database.\n\n        The table name, along with the value of the ``schema`` parameter,\n        forms a key which uniquely identifies this :class:`.Table` within\n        the owning :class:`.MetaData` collection.\n        Additional calls to :class:`.Table` with the same name, metadata,\n        and schema name will return the same :class:`.Table` object.\n\n        Names which contain no upper case characters\n        will be treated as case insensitive names, and will not be quoted\n        unless they are a reserved word or contain special characters.\n        A name with any number of upper case characters is considered\n        to be case sensitive, and will be sent as quoted.\n\n        To enable unconditional quoting for the table name, specify the flag\n        ``quote=True`` to the constructor, or use the :class:`.quoted_name`\n        construct to specify the name.\n\n    :param metadata: a :class:`.MetaData` object which will contain this\n        table.  The metadata is used as a point of association of this table\n        with other tables which are referenced via foreign key.  It also\n        may be used to associate this table with a particular\n        :class:`.Connectable`.\n\n    :param \\*args: Additional positional arguments are used primarily\n        to add the list of :class:`.Column` objects contained within this\n        table. Similar to the style of a CREATE TABLE statement, other\n        :class:`.SchemaItem` constructs may be added here, including\n        :class:`.PrimaryKeyConstraint`, and :class:`.ForeignKeyConstraint`.\n\n    :param autoload: Defaults to False, unless :paramref:`.Table.autoload_with`\n        is set in which case it defaults to True; :class:`.Column` objects\n        for this table should be reflected from the database, possibly\n        augmenting or replacing existing :class:`.Column` objects that were\n        explicitly specified.\n\n        .. versionchanged:: 1.0.0 setting the :paramref:`.Table.autoload_with`\n           parameter implies that :paramref:`.Table.autoload` will default\n           to True.\n\n        .. seealso::\n\n            :ref:`metadata_reflection_toplevel`\n\n    :param autoload_replace: Defaults to ``True``; when using\n        :paramref:`.Table.autoload`\n        in conjunction with :paramref:`.Table.extend_existing`, indicates\n        that :class:`.Column` objects present in the already-existing\n        :class:`.Table` object should be replaced with columns of the same\n        name retrieved from the autoload process.   When ``False``, columns\n        already present under existing names will be omitted from the\n        reflection process.\n\n        Note that this setting does not impact :class:`.Column` objects\n        specified programmatically within the call to :class:`.Table` that\n        also is autoloading; those :class:`.Column` objects will always\n        replace existing columns of the same name when\n        :paramref:`.Table.extend_existing` is ``True``.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.extend_existing`\n\n    :param autoload_with: An :class:`.Engine` or :class:`.Connection` object\n        with which this :class:`.Table` object will be reflected; when\n        set to a non-None value, it implies that :paramref:`.Table.autoload`\n        is ``True``.   If left unset, but :paramref:`.Table.autoload` is\n        explicitly set to ``True``, an autoload operation will attempt to\n        proceed by locating an :class:`.Engine` or :class:`.Connection` bound\n        to the underlying :class:`.MetaData` object.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n    :param extend_existing: When ``True``, indicates that if this\n        :class:`.Table` is already present in the given :class:`.MetaData`,\n        apply further arguments within the constructor to the existing\n        :class:`.Table`.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        :paramref:`.Table.extend_existing` will also work in conjunction\n        with :paramref:`.Table.autoload` to run a new reflection\n        operation against the database, even if a :class:`.Table`\n        of the same name is already present in the target\n        :class:`.MetaData`; newly reflected :class:`.Column` objects\n        and other options will be added into the state of the\n        :class:`.Table`, potentially overwriting existing columns\n        and options of the same name.\n\n        As is always the case with :paramref:`.Table.autoload`,\n        :class:`.Column` objects can be specified in the same :class:`.Table`\n        constructor, which will take precedence.  Below, the existing\n        table ``mytable`` will be augmented with :class:`.Column` objects\n        both reflected from the database, as well as the given :class:`.Column`\n        named \"y\"::\n\n            Table(\"mytable\", metadata,\n                        Column('y', Integer),\n                        extend_existing=True,\n                        autoload=True,\n                        autoload_with=engine\n                    )\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.autoload_replace`\n\n            :paramref:`.Table.keep_existing`\n\n\n    :param implicit_returning: True by default - indicates that\n        RETURNING can be used by default to fetch newly inserted primary key\n        values, for backends which support this.  Note that\n        create_engine() also provides an implicit_returning flag.\n\n    :param include_columns: A list of strings indicating a subset of\n        columns to be loaded via the ``autoload`` operation; table columns who\n        aren't present in this list will not be represented on the resulting\n        ``Table`` object. Defaults to ``None`` which indicates all columns\n        should be reflected.\n\n    :param resolve_fks: Whether or not to reflect :class:`.Table` objects\n        related to this one via :class:`.ForeignKey` objects, when\n        :paramref:`.Table.autoload` or :paramref:`.Table.autoload_with` is\n        specified.   Defaults to True.  Set to False to disable reflection of\n        related tables as :class:`.ForeignKey` objects are encountered; may be\n        used either to save on SQL calls or to avoid issues with related tables\n        that can't be accessed. Note that if a related table is already present\n        in the :class:`.MetaData` collection, or becomes present later, a\n        :class:`.ForeignKey` object associated with this :class:`.Table` will\n        resolve to that table normally.\n\n        .. versionadded:: 1.3\n\n        .. seealso::\n\n            :paramref:`.MetaData.reflect.resolve_fks`\n\n\n    :param info: Optional data dictionary which will be populated into the\n        :attr:`.SchemaItem.info` attribute of this object.\n\n    :param keep_existing: When ``True``, indicates that if this Table\n        is already present in the given :class:`.MetaData`, ignore\n        further arguments within the constructor to the existing\n        :class:`.Table`, and return the :class:`.Table` object as\n        originally created. This is to allow a function that wishes\n        to define a new :class:`.Table` on first call, but on\n        subsequent calls will return the same :class:`.Table`,\n        without any of the declarations (particularly constraints)\n        being applied a second time.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        .. seealso::\n\n            :paramref:`.Table.extend_existing`\n\n    :param listeners: A list of tuples of the form ``(<eventname>, <fn>)``\n        which will be passed to :func:`.event.listen` upon construction.\n        This alternate hook to :func:`.event.listen` allows the establishment\n        of a listener function specific to this :class:`.Table` before\n        the \"autoload\" process begins.  Particularly useful for\n        the :meth:`.DDLEvents.column_reflect` event::\n\n            def listen_for_reflect(table, column_info):\n                \"handle the column reflection event\"\n                # ...\n\n            t = Table(\n                'sometable',\n                autoload=True,\n                listeners=[\n                    ('column_reflect', listen_for_reflect)\n                ])\n\n    :param mustexist: When ``True``, indicates that this Table must already\n        be present in the given :class:`.MetaData` collection, else\n        an exception is raised.\n\n    :param prefixes:\n        A list of strings to insert after CREATE in the CREATE TABLE\n        statement.  They will be separated by spaces.\n\n    :param quote: Force quoting of this table's name on or off, corresponding\n        to ``True`` or ``False``.  When left at its default of ``None``,\n        the column identifier will be quoted according to whether the name is\n        case sensitive (identifiers with at least one upper case character are\n        treated as case sensitive), or if it's a reserved word.  This flag\n        is only needed to force quoting of a reserved word which is not known\n        by the SQLAlchemy dialect.\n\n    :param quote_schema: same as 'quote' but applies to the schema identifier.\n\n    :param schema: The schema name for this table, which is required if\n        the table resides in a schema other than the default selected schema\n        for the engine's database connection.  Defaults to ``None``.\n\n        If the owning :class:`.MetaData` of this :class:`.Table` specifies its\n        own :paramref:`.MetaData.schema` parameter, then that schema name will\n        be applied to this :class:`.Table` if the schema parameter here is set\n        to ``None``.  To set a blank schema name on a :class:`.Table` that\n        would otherwise use the schema set on the owning :class:`.MetaData`,\n        specify the special symbol :attr:`.BLANK_SCHEMA`.\n\n        .. versionadded:: 1.0.14  Added the :attr:`.BLANK_SCHEMA` symbol to\n           allow a :class:`.Table` to have a blank schema name even when the\n           parent :class:`.MetaData` specifies :paramref:`.MetaData.schema`.\n\n        The quoting rules for the schema name are the same as those for the\n        ``name`` parameter, in that quoting is applied for reserved words or\n        case-sensitive names; to enable unconditional quoting for the schema\n        name, specify the flag ``quote_schema=True`` to the constructor, or use\n        the :class:`.quoted_name` construct to specify the name.\n\n    :param useexisting: the same as :paramref:`.Table.extend_existing`.\n\n    :param comment: Optional string that will render an SQL comment on table\n         creation.\n\n         .. versionadded:: 1.2 Added the :paramref:`.Table.comment` parameter\n            to :class:`.Table`.\n\n    :param \\**kw: Additional keyword arguments not mentioned above are\n        dialect specific, and passed in the form ``<dialectname>_<argname>``.\n        See the documentation regarding an individual dialect at\n        :ref:`dialect_toplevel` for detail on documented arguments.\n\n    ",
        "klass": "sqlalchemy.Table",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.type_api.NativeForEmulated",
            "sqlalchemy.sql.sqltypes.Enum"
        ],
        "class_docstring": "PostgreSQL ENUM type.\n\n    This is a subclass of :class:`.types.Enum` which includes\n    support for PG's ``CREATE TYPE`` and ``DROP TYPE``.\n\n    When the builtin type :class:`.types.Enum` is used and the\n    :paramref:`.Enum.native_enum` flag is left at its default of\n    True, the PostgreSQL backend will use a :class:`.postgresql.ENUM`\n    type as the implementation, so the special create/drop rules\n    will be used.\n\n    The create/drop behavior of ENUM is necessarily intricate, due to the\n    awkward relationship the ENUM type has in relationship to the\n    parent table, in that it may be \"owned\" by just a single table, or\n    may be shared among many tables.\n\n    When using :class:`.types.Enum` or :class:`.postgresql.ENUM`\n    in an \"inline\" fashion, the ``CREATE TYPE`` and ``DROP TYPE`` is emitted\n    corresponding to when the :meth:`.Table.create` and :meth:`.Table.drop`\n    methods are called::\n\n        table = Table('sometable', metadata,\n            Column('some_enum', ENUM('a', 'b', 'c', name='myenum'))\n        )\n\n        table.create(engine)  # will emit CREATE ENUM and CREATE TABLE\n        table.drop(engine)  # will emit DROP TABLE and DROP ENUM\n\n    To use a common enumerated type between multiple tables, the best\n    practice is to declare the :class:`.types.Enum` or\n    :class:`.postgresql.ENUM` independently, and associate it with the\n    :class:`.MetaData` object itself::\n\n        my_enum = ENUM('a', 'b', 'c', name='myenum', metadata=metadata)\n\n        t1 = Table('sometable_one', metadata,\n            Column('some_enum', myenum)\n        )\n\n        t2 = Table('sometable_two', metadata,\n            Column('some_enum', myenum)\n        )\n\n    When this pattern is used, care must still be taken at the level\n    of individual table creates.  Emitting CREATE TABLE without also\n    specifying ``checkfirst=True`` will still cause issues::\n\n        t1.create(engine) # will fail: no such type 'myenum'\n\n    If we specify ``checkfirst=True``, the individual table-level create\n    operation will check for the ``ENUM`` and create if not exists::\n\n        # will check if enum exists, and emit CREATE TYPE if not\n        t1.create(engine, checkfirst=True)\n\n    When using a metadata-level ENUM type, the type will always be created\n    and dropped if either the metadata-wide create/drop is called::\n\n        metadata.create_all(engine)  # will emit CREATE TYPE\n        metadata.drop_all(engine)  # will emit DROP TYPE\n\n    The type can also be created and dropped directly::\n\n        my_enum.create(engine)\n        my_enum.drop(engine)\n\n    .. versionchanged:: 1.0.0 The PostgreSQL :class:`.postgresql.ENUM` type\n       now behaves more strictly with regards to CREATE/DROP.  A metadata-level\n       ENUM type will only be created and dropped at the metadata level,\n       not the table level, with the exception of\n       ``table.create(checkfirst=True)``.\n       The ``table.drop()`` call will now emit a DROP TYPE for a table-level\n       enumerated type.\n\n    ",
        "klass": "sqlalchemy.dialects.postgresql.ENUM",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "ORM-level SQL construction object.\n\n    :class:`.Query` is the source of all SELECT statements generated by the\n    ORM, both those formulated by end-user query operations as well as by\n    high level internal operations such as related collection loading.  It\n    features a generative interface whereby successive calls return a new\n    :class:`.Query` object, a copy of the former with additional\n    criteria and options associated with it.\n\n    :class:`.Query` objects are normally initially generated using the\n    :meth:`~.Session.query` method of :class:`.Session`, and in\n    less common cases by instantiating the :class:`.Query` directly and\n    associating with a :class:`.Session` using the :meth:`.Query.with_session`\n    method.\n\n    For a full walkthrough of :class:`.Query` usage, see the\n    :ref:`ormtutorial_toplevel`.\n\n    ",
        "klass": "sqlalchemy.orm.query.Query",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.orm.session._SessionClassMethods"
        ],
        "class_docstring": "Manages persistence operations for ORM-mapped objects.\n\n    The Session's usage paradigm is described at :doc:`/orm/session`.\n\n\n    ",
        "klass": "sqlalchemy.orm.session.Session",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "ORM-level SQL construction object.\n\n    :class:`.Query` is the source of all SELECT statements generated by the\n    ORM, both those formulated by end-user query operations as well as by\n    high level internal operations such as related collection loading.  It\n    features a generative interface whereby successive calls return a new\n    :class:`.Query` object, a copy of the former with additional\n    criteria and options associated with it.\n\n    :class:`.Query` objects are normally initially generated using the\n    :meth:`~.Session.query` method of :class:`.Session`, and in\n    less common cases by instantiating the :class:`.Query` directly and\n    associating with a :class:`.Session` using the :meth:`.Query.with_session`\n    method.\n\n    For a full walkthrough of :class:`.Query` usage, see the\n    :ref:`ormtutorial_toplevel`.\n\n    ",
        "klass": "sqlalchemy.orm.Query",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.orm.session._SessionClassMethods"
        ],
        "class_docstring": "Manages persistence operations for ORM-mapped objects.\n\n    The Session's usage paradigm is described at :doc:`/orm/session`.\n\n\n    ",
        "klass": "sqlalchemy.orm.Session",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Provides scoped management of :class:`.Session` objects.\n\n    See :ref:`unitofwork_contextual` for a tutorial.\n\n    ",
        "klass": "sqlalchemy.orm.scoped_session",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.orm.session._SessionClassMethods"
        ],
        "class_docstring": "A configurable :class:`.Session` factory.\n\n    The :class:`.sessionmaker` factory generates new\n    :class:`.Session` objects when called, creating them given\n    the configurational arguments established here.\n\n    e.g.::\n\n        # global scope\n        Session = sessionmaker(autoflush=False)\n\n        # later, in a local scope, create and use a session:\n        sess = Session()\n\n    Any keyword arguments sent to the constructor itself will override the\n    \"configured\" keywords::\n\n        Session = sessionmaker()\n\n        # bind an individual session to a connection\n        sess = Session(bind=connection)\n\n    The class also includes a method :meth:`.configure`, which can\n    be used to specify additional keyword arguments to the factory, which\n    will take effect for subsequent :class:`.Session` objects generated.\n    This is usually used to associate one or more :class:`.Engine` objects\n    with an existing :class:`.sessionmaker` factory before it is first\n    used::\n\n        # application starts\n        Session = sessionmaker()\n\n        # ... later\n        engine = create_engine('sqlite:///foo.db')\n        Session.configure(bind=engine)\n\n        sess = Session()\n\n    .. seealso:\n\n        :ref:`session_getting` - introductory text on creating\n        sessions using :class:`.sessionmaker`.\n\n    ",
        "klass": "sqlalchemy.orm.sessionmaker",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.ColumnCollectionMixin",
            "sqlalchemy.sql.schema.SchemaItem"
        ],
        "class_docstring": "A table-level INDEX.\n\n    Defines a composite (one or more column) INDEX.\n\n    E.g.::\n\n        sometable = Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100))\n                    )\n\n        Index(\"some_index\", sometable.c.name)\n\n    For a no-frills, single column index, adding\n    :class:`.Column` also supports ``index=True``::\n\n        sometable = Table(\"sometable\", metadata,\n                        Column(\"name\", String(50), index=True)\n                    )\n\n    For a composite index, multiple columns can be specified::\n\n        Index(\"some_index\", sometable.c.name, sometable.c.address)\n\n    Functional indexes are supported as well, typically by using the\n    :data:`.func` construct in conjunction with table-bound\n    :class:`.Column` objects::\n\n        Index(\"some_index\", func.lower(sometable.c.name))\n\n    An :class:`.Index` can also be manually associated with a :class:`.Table`,\n    either through inline declaration or using\n    :meth:`.Table.append_constraint`.  When this approach is used, the names\n    of the indexed columns can be specified as strings::\n\n        Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100)),\n                        Index(\"some_index\", \"name\", \"address\")\n                )\n\n    To support functional or expression-based indexes in this form, the\n    :func:`.text` construct may be used::\n\n        from sqlalchemy import text\n\n        Table(\"sometable\", metadata,\n                        Column(\"name\", String(50)),\n                        Column(\"address\", String(100)),\n                        Index(\"some_index\", text(\"lower(name)\"))\n                )\n\n    .. versionadded:: 0.9.5 the :func:`.text` construct may be used to\n       specify :class:`.Index` expressions, provided the :class:`.Index`\n       is explicitly associated with the :class:`.Table`.\n\n\n    .. seealso::\n\n        :ref:`schema_indexes` - General information on :class:`.Index`.\n\n        :ref:`postgresql_indexes` - PostgreSQL-specific options available for\n        the :class:`.Index` construct.\n\n        :ref:`mysql_indexes` - MySQL-specific options available for the\n        :class:`.Index` construct.\n\n        :ref:`mssql_indexes` - MSSQL-specific options available for the\n        :class:`.Index` construct.\n\n    ",
        "klass": "sqlalchemy.schema.Index",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.schema.SchemaItem"
        ],
        "class_docstring": "A collection of :class:`.Table` objects and their associated schema\n    constructs.\n\n    Holds a collection of :class:`.Table` objects as well as\n    an optional binding to an :class:`.Engine` or\n    :class:`.Connection`.  If bound, the :class:`.Table` objects\n    in the collection and their columns may participate in implicit SQL\n    execution.\n\n    The :class:`.Table` objects themselves are stored in the\n    :attr:`.MetaData.tables` dictionary.\n\n    :class:`.MetaData` is a thread-safe object for read operations.\n    Construction of new tables within a single :class:`.MetaData` object,\n    either explicitly or via reflection, may not be completely thread-safe.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    ",
        "klass": "sqlalchemy.schema.MetaData",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.base.DialectKWArgs",
            "sqlalchemy.sql.schema.SchemaItem",
            "sqlalchemy.sql.selectable.TableClause"
        ],
        "class_docstring": "Represent a table in a database.\n\n    e.g.::\n\n        mytable = Table(\"mytable\", metadata,\n                        Column('mytable_id', Integer, primary_key=True),\n                        Column('value', String(50))\n                   )\n\n    The :class:`.Table` object constructs a unique instance of itself based\n    on its name and optional schema name within the given\n    :class:`.MetaData` object. Calling the :class:`.Table`\n    constructor with the same name and same :class:`.MetaData` argument\n    a second time will return the *same* :class:`.Table` object - in this way\n    the :class:`.Table` constructor acts as a registry function.\n\n    .. seealso::\n\n        :ref:`metadata_describing` - Introduction to database metadata\n\n    Constructor arguments are as follows:\n\n    :param name: The name of this table as represented in the database.\n\n        The table name, along with the value of the ``schema`` parameter,\n        forms a key which uniquely identifies this :class:`.Table` within\n        the owning :class:`.MetaData` collection.\n        Additional calls to :class:`.Table` with the same name, metadata,\n        and schema name will return the same :class:`.Table` object.\n\n        Names which contain no upper case characters\n        will be treated as case insensitive names, and will not be quoted\n        unless they are a reserved word or contain special characters.\n        A name with any number of upper case characters is considered\n        to be case sensitive, and will be sent as quoted.\n\n        To enable unconditional quoting for the table name, specify the flag\n        ``quote=True`` to the constructor, or use the :class:`.quoted_name`\n        construct to specify the name.\n\n    :param metadata: a :class:`.MetaData` object which will contain this\n        table.  The metadata is used as a point of association of this table\n        with other tables which are referenced via foreign key.  It also\n        may be used to associate this table with a particular\n        :class:`.Connectable`.\n\n    :param \\*args: Additional positional arguments are used primarily\n        to add the list of :class:`.Column` objects contained within this\n        table. Similar to the style of a CREATE TABLE statement, other\n        :class:`.SchemaItem` constructs may be added here, including\n        :class:`.PrimaryKeyConstraint`, and :class:`.ForeignKeyConstraint`.\n\n    :param autoload: Defaults to False, unless :paramref:`.Table.autoload_with`\n        is set in which case it defaults to True; :class:`.Column` objects\n        for this table should be reflected from the database, possibly\n        augmenting or replacing existing :class:`.Column` objects that were\n        explicitly specified.\n\n        .. versionchanged:: 1.0.0 setting the :paramref:`.Table.autoload_with`\n           parameter implies that :paramref:`.Table.autoload` will default\n           to True.\n\n        .. seealso::\n\n            :ref:`metadata_reflection_toplevel`\n\n    :param autoload_replace: Defaults to ``True``; when using\n        :paramref:`.Table.autoload`\n        in conjunction with :paramref:`.Table.extend_existing`, indicates\n        that :class:`.Column` objects present in the already-existing\n        :class:`.Table` object should be replaced with columns of the same\n        name retrieved from the autoload process.   When ``False``, columns\n        already present under existing names will be omitted from the\n        reflection process.\n\n        Note that this setting does not impact :class:`.Column` objects\n        specified programmatically within the call to :class:`.Table` that\n        also is autoloading; those :class:`.Column` objects will always\n        replace existing columns of the same name when\n        :paramref:`.Table.extend_existing` is ``True``.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.extend_existing`\n\n    :param autoload_with: An :class:`.Engine` or :class:`.Connection` object\n        with which this :class:`.Table` object will be reflected; when\n        set to a non-None value, it implies that :paramref:`.Table.autoload`\n        is ``True``.   If left unset, but :paramref:`.Table.autoload` is\n        explicitly set to ``True``, an autoload operation will attempt to\n        proceed by locating an :class:`.Engine` or :class:`.Connection` bound\n        to the underlying :class:`.MetaData` object.\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n    :param extend_existing: When ``True``, indicates that if this\n        :class:`.Table` is already present in the given :class:`.MetaData`,\n        apply further arguments within the constructor to the existing\n        :class:`.Table`.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        :paramref:`.Table.extend_existing` will also work in conjunction\n        with :paramref:`.Table.autoload` to run a new reflection\n        operation against the database, even if a :class:`.Table`\n        of the same name is already present in the target\n        :class:`.MetaData`; newly reflected :class:`.Column` objects\n        and other options will be added into the state of the\n        :class:`.Table`, potentially overwriting existing columns\n        and options of the same name.\n\n        As is always the case with :paramref:`.Table.autoload`,\n        :class:`.Column` objects can be specified in the same :class:`.Table`\n        constructor, which will take precedence.  Below, the existing\n        table ``mytable`` will be augmented with :class:`.Column` objects\n        both reflected from the database, as well as the given :class:`.Column`\n        named \"y\"::\n\n            Table(\"mytable\", metadata,\n                        Column('y', Integer),\n                        extend_existing=True,\n                        autoload=True,\n                        autoload_with=engine\n                    )\n\n        .. seealso::\n\n            :paramref:`.Table.autoload`\n\n            :paramref:`.Table.autoload_replace`\n\n            :paramref:`.Table.keep_existing`\n\n\n    :param implicit_returning: True by default - indicates that\n        RETURNING can be used by default to fetch newly inserted primary key\n        values, for backends which support this.  Note that\n        create_engine() also provides an implicit_returning flag.\n\n    :param include_columns: A list of strings indicating a subset of\n        columns to be loaded via the ``autoload`` operation; table columns who\n        aren't present in this list will not be represented on the resulting\n        ``Table`` object. Defaults to ``None`` which indicates all columns\n        should be reflected.\n\n    :param resolve_fks: Whether or not to reflect :class:`.Table` objects\n        related to this one via :class:`.ForeignKey` objects, when\n        :paramref:`.Table.autoload` or :paramref:`.Table.autoload_with` is\n        specified.   Defaults to True.  Set to False to disable reflection of\n        related tables as :class:`.ForeignKey` objects are encountered; may be\n        used either to save on SQL calls or to avoid issues with related tables\n        that can't be accessed. Note that if a related table is already present\n        in the :class:`.MetaData` collection, or becomes present later, a\n        :class:`.ForeignKey` object associated with this :class:`.Table` will\n        resolve to that table normally.\n\n        .. versionadded:: 1.3\n\n        .. seealso::\n\n            :paramref:`.MetaData.reflect.resolve_fks`\n\n\n    :param info: Optional data dictionary which will be populated into the\n        :attr:`.SchemaItem.info` attribute of this object.\n\n    :param keep_existing: When ``True``, indicates that if this Table\n        is already present in the given :class:`.MetaData`, ignore\n        further arguments within the constructor to the existing\n        :class:`.Table`, and return the :class:`.Table` object as\n        originally created. This is to allow a function that wishes\n        to define a new :class:`.Table` on first call, but on\n        subsequent calls will return the same :class:`.Table`,\n        without any of the declarations (particularly constraints)\n        being applied a second time.\n\n        If :paramref:`.Table.extend_existing` or\n        :paramref:`.Table.keep_existing` are not set, and the given name\n        of the new :class:`.Table` refers to a :class:`.Table` that is\n        already present in the target :class:`.MetaData` collection, and\n        this :class:`.Table` specifies additional columns or other constructs\n        or flags that modify the table's state, an\n        error is raised.  The purpose of these two mutually-exclusive flags\n        is to specify what action should be taken when a :class:`.Table`\n        is specified that matches an existing :class:`.Table`, yet specifies\n        additional constructs.\n\n        .. seealso::\n\n            :paramref:`.Table.extend_existing`\n\n    :param listeners: A list of tuples of the form ``(<eventname>, <fn>)``\n        which will be passed to :func:`.event.listen` upon construction.\n        This alternate hook to :func:`.event.listen` allows the establishment\n        of a listener function specific to this :class:`.Table` before\n        the \"autoload\" process begins.  Particularly useful for\n        the :meth:`.DDLEvents.column_reflect` event::\n\n            def listen_for_reflect(table, column_info):\n                \"handle the column reflection event\"\n                # ...\n\n            t = Table(\n                'sometable',\n                autoload=True,\n                listeners=[\n                    ('column_reflect', listen_for_reflect)\n                ])\n\n    :param mustexist: When ``True``, indicates that this Table must already\n        be present in the given :class:`.MetaData` collection, else\n        an exception is raised.\n\n    :param prefixes:\n        A list of strings to insert after CREATE in the CREATE TABLE\n        statement.  They will be separated by spaces.\n\n    :param quote: Force quoting of this table's name on or off, corresponding\n        to ``True`` or ``False``.  When left at its default of ``None``,\n        the column identifier will be quoted according to whether the name is\n        case sensitive (identifiers with at least one upper case character are\n        treated as case sensitive), or if it's a reserved word.  This flag\n        is only needed to force quoting of a reserved word which is not known\n        by the SQLAlchemy dialect.\n\n    :param quote_schema: same as 'quote' but applies to the schema identifier.\n\n    :param schema: The schema name for this table, which is required if\n        the table resides in a schema other than the default selected schema\n        for the engine's database connection.  Defaults to ``None``.\n\n        If the owning :class:`.MetaData` of this :class:`.Table` specifies its\n        own :paramref:`.MetaData.schema` parameter, then that schema name will\n        be applied to this :class:`.Table` if the schema parameter here is set\n        to ``None``.  To set a blank schema name on a :class:`.Table` that\n        would otherwise use the schema set on the owning :class:`.MetaData`,\n        specify the special symbol :attr:`.BLANK_SCHEMA`.\n\n        .. versionadded:: 1.0.14  Added the :attr:`.BLANK_SCHEMA` symbol to\n           allow a :class:`.Table` to have a blank schema name even when the\n           parent :class:`.MetaData` specifies :paramref:`.MetaData.schema`.\n\n        The quoting rules for the schema name are the same as those for the\n        ``name`` parameter, in that quoting is applied for reserved words or\n        case-sensitive names; to enable unconditional quoting for the schema\n        name, specify the flag ``quote_schema=True`` to the constructor, or use\n        the :class:`.quoted_name` construct to specify the name.\n\n    :param useexisting: the same as :paramref:`.Table.extend_existing`.\n\n    :param comment: Optional string that will render an SQL comment on table\n         creation.\n\n         .. versionadded:: 1.2 Added the :paramref:`.Table.comment` parameter\n            to :class:`.Table`.\n\n    :param \\**kw: Additional keyword arguments not mentioned above are\n        dialect specific, and passed in the form ``<dialectname>_<argname>``.\n        See the documentation regarding an individual dialect at\n        :ref:`dialect_toplevel` for detail on documented arguments.\n\n    ",
        "klass": "sqlalchemy.schema.Table",
        "module": "sqlalchemy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)",
        "klass": "dict",
        "module": "dict"
    },
    {
        "base_classes": [
            "xmlrpc.server.SimpleXMLRPCServer",
            "xmlrpc.server.XMLRPCDocGenerator"
        ],
        "class_docstring": "XML-RPC and HTML documentation server.\n\n    Adds the ability to serve server documentation to the capabilities\n    of SimpleXMLRPCServer.\n    ",
        "klass": "xmlrpc.server.DocXMLRPCServer",
        "module": "xmlrpc"
    },
    {
        "base_classes": [
            "socketserver.TCPServer",
            "xmlrpc.server.SimpleXMLRPCDispatcher"
        ],
        "class_docstring": "Simple XML-RPC server.\n\n    Simple XML-RPC server that allows functions and a single instance\n    to be installed to handle requests. The default implementation\n    attempts to dispatch XML-RPC calls to the functions or instance\n    installed in the server. Override the _dispatch method inherited\n    from SimpleXMLRPCDispatcher to change this behavior.\n    ",
        "klass": "xmlrpc.server.SimpleXMLRPCServer",
        "module": "xmlrpc"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Assembles a causal analysis graph from INDRA Statements.\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statement.Statements]]\n        A list of INDRA Statements to be assembled. Currently supports\n        Influence Statements.\n\n    Attributes\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to be assembled.\n    CAG : nx.MultiDiGraph\n        A networkx MultiDiGraph object representing the causal analysis graph.\n    ",
        "klass": "indra.assemblers.cag.CAGAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class assembles a CX network from a set of INDRA Statements.\n\n    The CX format is an aspect oriented data mode for networks.\n    The format is defined at http://www.home.ndexbio.org/data-model/.\n    The CX format is the standard for NDEx and is compatible with\n    CytoScape via the CyNDEx plugin.\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statements.Statement]]\n        A list of INDRA Statements to be assembled.\n    network_name : Optional[str]\n        The name of the network to be assembled. Default: indra_assembled\n\n    Attributes\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to be assembled.\n    network_name : str\n        The name of the network to be assembled.\n    cx : dict\n        The structure of the CX network that is assembled.\n    ",
        "klass": "indra.assemblers.cx.CxAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This assembler generates English sentences from INDRA Statements.\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statements.Statement]]\n        A list of INDRA Statements to be added to the assembler.\n\n    Attributes\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to assemble.\n    model : str\n        The assembled sentences as a single string.\n    ",
        "klass": "indra.assemblers.english.assembler.EnglishAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The core component of Jinja is the `Environment`.  It contains\n    important shared variables like configuration, filters, tests,\n    globals and others.  Instances of this class may be modified if\n    they are not shared and if no template was loaded so far.\n    Modifications on environments after the first template was loaded\n    will lead to surprising effects and undefined behavior.\n\n    Here are the possible initialization parameters:\n\n        `block_start_string`\n            The string marking the beginning of a block.  Defaults to ``'{%'``.\n\n        `block_end_string`\n            The string marking the end of a block.  Defaults to ``'%}'``.\n\n        `variable_start_string`\n            The string marking the beginning of a print statement.\n            Defaults to ``'{{'``.\n\n        `variable_end_string`\n            The string marking the end of a print statement.  Defaults to\n            ``'}}'``.\n\n        `comment_start_string`\n            The string marking the beginning of a comment.  Defaults to ``'{#'``.\n\n        `comment_end_string`\n            The string marking the end of a comment.  Defaults to ``'#}'``.\n\n        `line_statement_prefix`\n            If given and a string, this will be used as prefix for line based\n            statements.  See also :ref:`line-statements`.\n\n        `line_comment_prefix`\n            If given and a string, this will be used as prefix for line based\n            comments.  See also :ref:`line-statements`.\n\n            .. versionadded:: 2.2\n\n        `trim_blocks`\n            If this is set to ``True`` the first newline after a block is\n            removed (block, not variable tag!).  Defaults to `False`.\n\n        `lstrip_blocks`\n            If this is set to ``True`` leading spaces and tabs are stripped\n            from the start of a line to a block.  Defaults to `False`.\n\n        `newline_sequence`\n            The sequence that starts a newline.  Must be one of ``'\\r'``,\n            ``'\\n'`` or ``'\\r\\n'``.  The default is ``'\\n'`` which is a\n            useful default for Linux and OS X systems as well as web\n            applications.\n\n        `keep_trailing_newline`\n            Preserve the trailing newline when rendering templates.\n            The default is ``False``, which causes a single newline,\n            if present, to be stripped from the end of the template.\n\n            .. versionadded:: 2.7\n\n        `extensions`\n            List of Jinja extensions to use.  This can either be import paths\n            as strings or extension classes.  For more information have a\n            look at :ref:`the extensions documentation <jinja-extensions>`.\n\n        `optimized`\n            should the optimizer be enabled?  Default is ``True``.\n\n        `undefined`\n            :class:`Undefined` or a subclass of it that is used to represent\n            undefined values in the template.\n\n        `finalize`\n            A callable that can be used to process the result of a variable\n            expression before it is output.  For example one can convert\n            ``None`` implicitly into an empty string here.\n\n        `autoescape`\n            If set to ``True`` the XML/HTML autoescaping feature is enabled by\n            default.  For more details about autoescaping see\n            :class:`~jinja2.utils.Markup`.  As of Jinja 2.4 this can also\n            be a callable that is passed the template name and has to\n            return ``True`` or ``False`` depending on autoescape should be\n            enabled by default.\n\n            .. versionchanged:: 2.4\n               `autoescape` can now be a function\n\n        `loader`\n            The template loader for this environment.\n\n        `cache_size`\n            The size of the cache.  Per default this is ``400`` which means\n            that if more than 400 templates are loaded the loader will clean\n            out the least recently used template.  If the cache size is set to\n            ``0`` templates are recompiled all the time, if the cache size is\n            ``-1`` the cache will not be cleaned.\n\n            .. versionchanged:: 2.8\n               The cache size was increased to 400 from a low 50.\n\n        `auto_reload`\n            Some loaders load templates from locations where the template\n            sources may change (ie: file system or database).  If\n            ``auto_reload`` is set to ``True`` (default) every time a template is\n            requested the loader checks if the source changed and if yes, it\n            will reload the template.  For higher performance it's possible to\n            disable that.\n\n        `bytecode_cache`\n            If set to a bytecode cache object, this object will provide a\n            cache for the internal Jinja bytecode so that templates don't\n            have to be parsed if they were not changed.\n\n            See :ref:`bytecode-cache` for more information.\n\n        `enable_async`\n            If set to true this enables async template execution which allows\n            you to take advantage of newer Python features.  This requires\n            Python 3.6 or later.\n    ",
        "klass": "jinja2.environment.Environment",
        "module": "jinja2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Assembler creating index cards from a set of INDRA Statements.\n\n    Parameters\n    ----------\n    statements : list\n        A list of INDRA statements to be assembled.\n    pmc_override : Optional[str]\n        A PMC ID to assign to the index card.\n\n    Attributes\n    ----------\n    statements : list\n        A list of INDRA statements to be assembled.\n    ",
        "klass": "indra.assemblers.index_card.IndexCardAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Return successive r-length permutations of elements in the iterable.\n\npermutations(range(3), 2) --> (0,1), (0,2), (1,0), (1,2), (2,0), (2,1)",
        "klass": "itertools.permutations",
        "module": "itertools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Assembler creating a PySB model from a set of INDRA Statements.\n\n    Parameters\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to be assembled.\n\n    Attributes\n    ----------\n    policies : dict\n        A dictionary of policies that defines assembly policies for Statement\n        types. It is assigned in the constructor.\n    statements : list[indra.statements.Statement]\n        A list of INDRA statements to be assembled.\n    model : pysb.Model\n        A PySB model object that is assembled by this class.\n    agent_set : BaseAgentSet\n        A set of BaseAgents used during the assembly process.\n    ",
        "klass": "indra.assemblers.pysb.PysbAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class assembles an SBGN model from a set of INDRA Statements.\n\n    The Systems Biology Graphical Notation (SBGN) is a widely used\n    graphical notation standard for systems biology models.\n    This assembler creates SBGN models following the Process Desctiption (PD)\n    standard, documented at:\n    https://github.com/sbgn/process-descriptions/blob/master/UserManual/sbgn_PD-level1-user-public.pdf.\n    For more information on SBGN, see: http://sbgn.github.io/sbgn/\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statements.Statement]]\n        A list of INDRA Statements to be assembled.\n\n    Attributes\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to be assembled.\n    sbgn : lxml.etree.ElementTree\n        The structure of the SBGN model that is assembled, represented as an\n        XML ElementTree.\n    ",
        "klass": "indra.assemblers.sbgn.SBGNAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The SIF assembler assembles INDRA Statements into a networkx graph.\n\n    This graph can then be exported into SIF (simple interaction format) or\n    a Boolean network.\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statements.Statement]]\n        A list of INDRA Statements to be added to the assembler's list\n        of Statements.\n\n    Attributes\n    ----------\n    graph : networkx.DiGraph\n        A networkx graph that is assembled by this assembler.\n    ",
        "klass": "indra.assemblers.sif.SifAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Assembles Statements into a set of tabular files for export or curation.\n\n    Currently designed for use with \"raw\" Statements, i.e., Statements with a\n    single evidence entry. Exports Statements into a single tab-separated file\n    with the following columns:\n\n    *INDEX*\n        A 1-indexed integer identifying the statement.\n    *UUID*\n        The UUID of the Statement.\n    *TYPE*\n        Statement type, given by the name of the class in indra.statements.\n    *STR*\n        String representation of the Statement. Contains most relevant\n        information for curation including any additional statement data\n        beyond the Statement type and Agents.\n    *AG_A_TEXT*\n        For Statements extracted from text, the text in the sentence\n        corresponding to the first agent (i.e., the 'TEXT' entry in the\n        db_refs dictionary). For all other Statements, the Agent name is\n        given. Empty field if the Agent is None.\n    *AG_A_LINKS*\n        Groundings for the first agent given as a comma-separated list of\n        identifiers.org links. Empty if the Agent is None.\n    *AG_A_STR*\n        String representation of the first agent, including additional\n        agent context (e.g. modification, mutation, location, and bound\n        conditions). Empty if the Agent is None.\n    *AG_B_TEXT, AG_B_LINKS, AG_B_STR*\n        As above for the second agent. Note that the Agent may be None (and\n        these fields left empty) if the Statement consists only of a single\n        Agent (e.g., SelfModification, ActiveForm, or Translocation statement).\n    *PMID*\n        PMID of the first entry in the evidence list for the Statement.\n    *TEXT*\n        Evidence text for the Statement.\n    *IS_HYP*\n        Whether the Statement represents a \"hypothesis\", as flagged by some\n        reading systems and recorded in the `evidence.epistemics['hypothesis']`\n        field.\n    *IS_DIRECT*\n        Whether the Statement represents a direct physical interactions,\n        as recorded by the `evidence.epistemics['direct']` field.\n\n    In addition, if the `add_curation_cols` flag is set when calling\n    :py:meth:`TsvAssembler.make_model`, the following additional (empty)\n    columns will be added, to be filled out by curators:\n\n    *AG_A_IDS_CORRECT*\n        Correctness of Agent A grounding.\n    *AG_A_STATE_CORRECT*\n        Correctness of Agent A context (e.g., modification, bound, and other\n        conditions).\n    *AG_B_IDS_CORRECT, AG_B_STATE_CORRECT*\n        As above, for Agent B.\n    *EVENT_CORRECT*\n        Whether the event is supported by the evidence text if the entities\n        (Agents A and B) are considered as placeholders (i.e.,\n        ignoring the correctness of their grounding).\n    *RES_CORRECT*\n        For Modification statements, whether the amino acid residue indicated\n        by the Statement is supported by the evidence.\n    *POS_CORRECT*\n        For Modification statements, whether the amino acid position indicated\n        by the Statement is supported by the evidence.\n    *SUBJ_ACT_CORRECT*\n        For Activation/Inhibition Statements, whether the activity indicated\n        for the subject (Agent A) is supported by the evidence.\n    *OBJ_ACT_CORRECT*\n        For Activation/Inhibition Statements, whether the activity indicated\n        for the object (Agent B) is supported by the evidence.\n    *HYP_CORRECT*\n        Whether the Statement is correctly flagged as a hypothesis.\n    *HYP_CORRECT*\n        Whether the Statement is correctly flagged as direct.\n\n    Parameters\n    ----------\n    stmts : Optional[list[indra.statements.Statement]]\n        A list of INDRA Statements to be assembled.\n\n    Attributes\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to be assembled.\n    ",
        "klass": "indra.assemblers.tsv.TsvAssembler",
        "module": "indra"
    },
    {
        "base_classes": [
            "rdflib.term.Node"
        ],
        "class_docstring": "An RDF Graph\n\n    The constructor accepts one argument, the 'store'\n    that will be used to store the graph data (see the 'store'\n    package for stores currently shipped with rdflib).\n\n    Stores can be context-aware or unaware.  Unaware stores take up\n    (some) less space but cannot support features that require\n    context, such as true merging/demerging of sub-graphs and\n    provenance.\n\n    The Graph constructor can take an identifier which identifies the Graph\n    by name.  If none is given, the graph is assigned a BNode for its\n    identifier.\n    For more on named graphs, see: http://www.w3.org/2004/03/trix/\n\n    ",
        "klass": "rdflib.graph.Graph",
        "module": "rdflib"
    },
    {
        "base_classes": [
            "collections.abc.Hashable",
            "collections.abc.Mapping",
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Represents a Composition, which is essentially a {element:amount} mapping\n    type. Composition is written to be immutable and hashable,\n    unlike a standard Python dict.\n\n    Note that the key can be either an Element or a Specie. Elements and Specie\n    are treated differently. i.e., a Fe2+ is not the same as a Fe3+ Specie and\n    would be put in separate keys. This differentiation is deliberate to\n    support using Composition to determine the fraction of a particular Specie.\n\n    Works almost completely like a standard python dictionary, except that\n    __getitem__ is overridden to return 0 when an element is not found.\n    (somewhat like a defaultdict, except it is immutable).\n\n    Also adds more convenience methods relevant to compositions, e.g.,\n    get_fraction.\n\n    It should also be noted that many Composition related functionality takes\n    in a standard string as a convenient input. For example,\n    even though the internal representation of a Fe2O3 composition is\n    {Element(\"Fe\"): 2, Element(\"O\"): 3}, you can obtain the amount of Fe\n    simply by comp[\"Fe\"] instead of the more verbose comp[Element(\"Fe\")].\n\n    >>> comp = Composition(\"LiFePO4\")\n    >>> comp.get_atomic_fraction(Element(\"Li\"))\n    0.14285714285714285\n    >>> comp.num_atoms\n    7.0\n    >>> comp.reduced_formula\n    'LiFePO4'\n    >>> comp.formula\n    'Li1 Fe1 P1 O4'\n    >>> comp.get_wt_fraction(Element(\"Li\"))\n    0.04399794666951898\n    >>> comp.num_atoms\n    7.0\n    ",
        "klass": "pymatgen.core.composition.Composition",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    A lattice object.  Essentially a matrix with conversion matrices. In\n    general, it is assumed that length units are in Angstroms and angles are in\n    degrees unless otherwise stated.\n    ",
        "klass": "pymatgen.core.lattice.Lattice",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    A lattice object.  Essentially a matrix with conversion matrices. In\n    general, it is assumed that length units are in Angstroms and angles are in\n    degrees unless otherwise stated.\n    ",
        "klass": "pymatgen.Lattice",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.core.sites.Site",
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Extension of generic Site object to periodic systems.\n    PeriodicSite includes a lattice system.\n    ",
        "klass": "pymatgen.core.sites.PeriodicSite",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.core.structure.IStructure",
            "collections.abc.MutableSequence"
        ],
        "class_docstring": "\n    Mutable version of structure.\n    ",
        "klass": "pymatgen.core.structure.Structure",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.core.structure.IStructure",
            "collections.abc.MutableSequence"
        ],
        "class_docstring": "\n    Mutable version of structure.\n    ",
        "klass": "pymatgen.Structure",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.alchemy.filters.AbstractStructureFilter"
        ],
        "class_docstring": "\n    This filter removes exact duplicate structures from the transmuter.\n    ",
        "klass": "pymatgen.alchemy.filters.RemoveDuplicatesFilter",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.alchemy.filters.AbstractStructureFilter"
        ],
        "class_docstring": "\n    This filter removes structures that have certain species that are too close\n    together.\n\n    Args:\n        specie_and_min_dist_dict: A species string to float mapping. For\n            example, {\"Na+\": 1} means that all Na+ ions must be at least 1\n            Angstrom away from each other. Multiple species criteria can be\n            applied. Note that the testing is done based on the actual object\n            . If you have a structure with Element, you must use {\"Na\":1}\n            instead to filter based on Element and not Specie.\n\n    ",
        "klass": "pymatgen.alchemy.filters.SpecieProximityFilter",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Object for representing the data in a POSCAR or CONTCAR file.\n    Please note that this current implementation. Most attributes can be set\n    directly.\n\n    .. attribute:: structure\n\n        Associated Structure.\n\n    .. attribute:: comment\n\n        Optional comment string.\n\n    .. attribute:: true_names\n\n        Boolean indication whether Poscar contains actual real names parsed\n        from either a POTCAR or the POSCAR itself.\n\n    .. attribute:: selective_dynamics\n\n        Selective dynamics attribute for each site if available. A Nx3 array of\n        booleans.\n\n    .. attribute:: velocities\n\n        Velocities for each site (typically read in from a CONTCAR). A Nx3\n        array of floats.\n\n    .. attribute:: predictor_corrector\n\n        Predictor corrector coordinates and derivatives for each site; i.e.\n        a list of three 1x3 arrays for each site (typically read in from a MD\n        CONTCAR).\n\n    .. attribute:: predictor_corrector_preamble\n\n        Predictor corrector preamble contains the predictor-corrector key,\n        POTIM, and thermostat parameters that precede the site-specic predictor\n        corrector data in MD CONTCAR\n\n    .. attribute:: temperature\n\n        Temperature of velocity Maxwell-Boltzmann initialization. Initialized\n        to -1 (MB hasn\"t been performed).\n    ",
        "klass": "pymatgen.io.vasp.inputs.Poscar",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Class used to store the full Voronoi of a given structure.\n    ",
        "klass": "pymatgen.analysis.chemenv.coordination_environments.voronoi.DetailedVoronoiContainer",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.analysis.chemenv.coordination_environments.chemenv_strategies.WeightedNbSetChemenvStrategy"
        ],
        "class_docstring": "\n    MultiWeightsChemenvStrategy\n    ",
        "klass": "pymatgen.analysis.chemenv.coordination_environments.chemenv_strategies.MultiWeightsChemenvStrategy",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.analysis.chemenv.coordination_environments.chemenv_strategies.AbstractChemenvStrategy"
        ],
        "class_docstring": "\n    Simplest ChemenvStrategy using fixed angle and distance parameters for the definition of neighbors in the\n    Voronoi approach. The coordination environment is then given as the one with the lowest continuous symmetry measure\n    ",
        "klass": "pymatgen.analysis.chemenv.coordination_environments.chemenv_strategies.SimplestChemenvStrategy",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    One-dimensional smoothing spline fit to a given set of data points.\n\n    Fits a spline y = spl(x) of degree `k` to the provided `x`, `y` data.  `s`\n    specifies the number of knots by specifying a smoothing condition.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        1-D array of independent input data. Must be increasing.\n    y : (N,) array_like\n        1-D array of dependent input data, of the same length as `x`.\n    w : (N,) array_like, optional\n        Weights for spline fitting.  Must be positive.  If None (default),\n        weights are all equal.\n    bbox : (2,) array_like, optional\n        2-sequence specifying the boundary of the approximation interval. If\n        None (default), ``bbox=[x[0], x[-1]]``.\n    k : int, optional\n        Degree of the smoothing spline.  Must be <= 5.\n        Default is k=3, a cubic spline.\n    s : float or None, optional\n        Positive smoothing factor used to choose the number of knots.  Number\n        of knots will be increased until the smoothing condition is satisfied::\n\n            sum((w[i] * (y[i]-spl(x[i])))**2, axis=0) <= s\n\n        If None (default), ``s = len(w)`` which should be a good value if\n        ``1/w[i]`` is an estimate of the standard deviation of ``y[i]``.\n        If 0, spline will interpolate through all data points.\n    ext : int or str, optional\n        Controls the extrapolation mode for elements\n        not in the interval defined by the knot sequence.\n\n        * if ext=0 or 'extrapolate', return the extrapolated value.\n        * if ext=1 or 'zeros', return 0\n        * if ext=2 or 'raise', raise a ValueError\n        * if ext=3 of 'const', return the boundary value.\n\n        The default value is 0.\n\n    check_finite : bool, optional\n        Whether to check that the input arrays contain only finite numbers.\n        Disabling may give a performance gain, but may result in problems\n        (crashes, non-termination or non-sensical results) if the inputs\n        do contain infinities or NaNs.\n        Default is False.\n\n    See Also\n    --------\n    InterpolatedUnivariateSpline : Subclass with smoothing forced to 0\n    LSQUnivariateSpline : Subclass in which knots are user-selected instead of\n        being set by smoothing condition\n    splrep : An older, non object-oriented wrapping of FITPACK\n    splev, sproot, splint, spalde\n    BivariateSpline : A similar class for two-dimensional spline interpolation\n\n    Notes\n    -----\n    The number of data points must be larger than the spline degree `k`.\n\n    **NaN handling**: If the input arrays contain ``nan`` values, the result\n    is not useful, since the underlying spline fitting routines cannot deal\n    with ``nan`` . A workaround is to use zero weights for not-a-number\n    data points:\n\n    >>> from scipy.interpolate import UnivariateSpline\n    >>> x, y = np.array([1, 2, 3, 4]), np.array([1, np.nan, 3, 4])\n    >>> w = np.isnan(y)\n    >>> y[w] = 0.\n    >>> spl = UnivariateSpline(x, y, w=~w)\n\n    Notice the need to replace a ``nan`` by a numerical value (precise value\n    does not matter as long as the corresponding weight is zero.)\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.interpolate import UnivariateSpline\n    >>> x = np.linspace(-3, 3, 50)\n    >>> y = np.exp(-x**2) + 0.1 * np.random.randn(50)\n    >>> plt.plot(x, y, 'ro', ms=5)\n\n    Use the default value for the smoothing parameter:\n\n    >>> spl = UnivariateSpline(x, y)\n    >>> xs = np.linspace(-3, 3, 1000)\n    >>> plt.plot(xs, spl(xs), 'g', lw=3)\n\n    Manually change the amount of smoothing:\n\n    >>> spl.set_smoothing_factor(0.5)\n    >>> plt.plot(xs, spl(xs), 'b', lw=3)\n    >>> plt.show()\n\n    ",
        "klass": "scipy.interpolate.fitpack2.UnivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "ExpBase"
        ],
        "class_docstring": "\n    The exponential function, :math:`e^x`.\n\n    See Also\n    ========\n\n    log\n    ",
        "klass": "exp",
        "module": "exp"
    },
    {
        "base_classes": [
            "pymatgen.analysis.energy_models.EnergyModel"
        ],
        "class_docstring": "\n    Wrapper around EwaldSum to calculate the electrostatic energy.\n    ",
        "klass": "pymatgen.analysis.energy_models.EwaldElectrostaticModel",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.analysis.energy_models.EnergyModel"
        ],
        "class_docstring": "\n    A very simple Ising model, with r^2 decay.\n    ",
        "klass": "pymatgen.analysis.energy_models.IsingModel",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.analysis.energy_models.EnergyModel"
        ],
        "class_docstring": "\n    Sets the energy to the -ve of the spacegroup number. Higher symmetry =>\n    lower \"energy\".\n\n    Args have same meaning as in\n    :class:`pymatgen.symmetry.finder.SpacegroupAnalyzer`.\n    ",
        "klass": "pymatgen.analysis.energy_models.SymmetryModel",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "matplotlib.artist.Artist"
        ],
        "class_docstring": "\n    A line - the line can have both a solid linestyle connecting all\n    the vertices, and a marker at each vertex.  Additionally, the\n    drawing of the solid line is influenced by the drawstyle, e.g., one\n    can create \"stepped\" lines in various styles.\n    ",
        "klass": "matplotlib.lines.Line2D",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Class to check whether the connection tables of the two molecules are the\n    same. The atom in the two molecule must be paired accordingly.\n\n    Args:\n        bond_length_cap: The ratio of the elongation of the bond to be\n            acknowledged. If the distance between two atoms is less than (\n            empirical covalent bond length) X (1 + bond_length_cap), the bond\n            between the two atoms will be acknowledged.\n        covalent_radius: The covalent radius of the atoms.\n            dict (element symbol -> radius)\n        priority_bonds: The bonds that are known to be existed in the initial\n            molecule. Such bonds will be acknowledged in a loose criteria.\n            The index should start from 0.\n        priority_cap: The ratio of the elongation of the bond to be\n            acknowledged for the priority bonds.\n    ",
        "klass": "pymatgen.analysis.molecule_structure_comparator.MoleculeStructureComparator",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The Borg Queen controls the drones to assimilate data in an entire\n    directory tree. Uses multiprocessing to speed up things considerably. It\n    also contains convenience methods to save and load data between sessions.\n\n    Args:\n        drone (Drone): An implementation of\n            :class:`pymatgen.apps.borg.hive.AbstractDrone` to use for\n            assimilation.\n        rootpath (str): The root directory to start assimilation. Leave it\n            as None if you want to do assimilation later, or is using the\n            BorgQueen to load previously assimilated data.\n        ndrones (int): Number of drones to parallelize over.\n            Typical machines today have up to four processors. Note that you\n            won't see a 100% improvement with two drones over one, but you\n            will definitely see a significant speedup of at least 50% or so.\n            If you are running this over a server with far more processors,\n            the speedup will be even greater.\n    ",
        "klass": "pymatgen.apps.borg.queen.BorgQueen",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    .. note::\n\n        With effect from Python 3.2, tempfile.TemporaryDirectory already\n        implements much of the functionality of ScratchDir. However, it does\n        not provide options for copying of files to and from (though it is\n        possible to do this with other methods provided by shutil).\n\n    Creates a \"with\" context manager that automatically handles creation of\n    temporary directories (utilizing Python's build in temp directory\n    functions) and cleanup when done. This improves on Python's built in\n    functions by allowing for truly temporary workspace that are deleted\n    when it is done. The way it works is as follows:\n\n    1. Create a temp dir in specified root path.\n    2. Optionally copy input files from current directory to temp dir.\n    3. Change to temp dir.\n    4. User performs specified operations.\n    5. Optionally copy generated output files back to original directory.\n    6. Change back to original directory.\n    7. Delete temp dir.\n    ",
        "klass": "monty.tempfile.ScratchDir",
        "module": "monty"
    },
    {
        "base_classes": [
            "pymatgen.core.structure.IStructure",
            "collections.abc.MutableSequence"
        ],
        "class_docstring": "\n    Mutable version of structure.\n    ",
        "klass": "pymatgen.core.Structure",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.core.composition.Composition",
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Basic ion object. It is just a Composition object with an additional\n    variable to store charge.\n    The net charge can either be represented as Mn++, or Mn+2, or Mn[2+].\n    Note the order of the sign and magnitude in each representation.\n    ",
        "klass": "pymatgen.core.ion.Ion",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.io.abinit.netcdf.NetcdfReader"
        ],
        "class_docstring": "\n    This object reads data from a file written according to the ETSF-IO specifications.\n\n    We assume that the netcdf file contains at least the crystallographic section.\n    ",
        "klass": "pymatgen.io.abinit.ETSF_Reader",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    Atomic cluster centered around the absorbing atom.\n    ",
        "klass": "pymatgen.io.feff.inputs.Atoms",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "monty.json.MSONable"
        ],
        "class_docstring": "\n    FEFF atomic potential.\n    ",
        "klass": "pymatgen.io.feff.inputs.Potential",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "\n    FEFF control parameters.\n    ",
        "klass": "pymatgen.io.feff.inputs.Tags",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.transformations.transformation_abc.AbstractTransformation"
        ],
        "class_docstring": "\n    This transformation substitutes certain sites with certain species.\n    ",
        "klass": "pymatgen.transformations.site_transformations.InsertSitesTransformation",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.transformations.transformation_abc.AbstractTransformation"
        ],
        "class_docstring": "\n    Remove fraction of specie from a structure.\n    Requires an oxidation state decorated structure for ewald sum to be\n    computed.\n\n    Given that the solution to selecting the right removals is NP-hard, there\n    are several algorithms provided with varying degrees of accuracy and speed.\n    The options are as follows:\n\n    ALGO_FAST:\n        This is a highly optimized algorithm to quickly go through the search\n        tree. It is guaranteed to find the optimal solution, but will return\n        only a single lowest energy structure. Typically, you will want to use\n        this.\n\n    ALGO_COMPLETE:\n        The complete algo ensures that you get all symmetrically distinct\n        orderings, ranked by the estimated Ewald energy. But this can be an\n        extremely time-consuming process if the number of possible orderings is\n        very large. Use this if you really want all possible orderings. If you\n        want just the lowest energy ordering, ALGO_FAST is accurate and faster.\n\n    ALGO_BEST_FIRST:\n        This algorithm is for ordering the really large cells that defeats even\n        ALGO_FAST.  For example, if you have 48 sites of which you want to\n        remove 16 of them, the number of possible orderings is around\n        2 x 10^12. ALGO_BEST_FIRST shortcircuits the entire search tree by\n        removing the highest energy site first, then followed by the next\n        highest energy site, and so on. It is guaranteed to find a solution\n        in a reasonable time, but it is also likely to be highly inaccurate.\n\n    ALGO_ENUMERATE:\n        This algorithm uses the EnumerateStructureTransformation to perform\n        ordering. This algo returns *complete* orderings up to a single unit\n        cell size. It is more robust than the ALGO_COMPLETE, but requires\n        Gus Hart's enumlib to be installed.\n    ",
        "klass": "pymatgen.transformations.site_transformations.PartialRemoveSitesTransformation",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.transformations.transformation_abc.AbstractTransformation"
        ],
        "class_docstring": "\n    Remove certain sites in a structure.\n    ",
        "klass": "pymatgen.transformations.site_transformations.RemoveSitesTransformation",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.transformations.transformation_abc.AbstractTransformation"
        ],
        "class_docstring": "\n    This transformation substitutes certain sites with certain species.\n    ",
        "klass": "pymatgen.transformations.site_transformations.ReplaceSiteSpeciesTransformation",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "pymatgen.transformations.transformation_abc.AbstractTransformation"
        ],
        "class_docstring": "\n    This class translates a set of sites by a certain vector.\n    ",
        "klass": "pymatgen.transformations.site_transformations.TranslateSitesTransformation",
        "module": "pymatgen"
    },
    {
        "base_classes": [
            "beancount.core.amount._Amount"
        ],
        "class_docstring": "An 'Amount' represents a number of a particular unit of something.\n\n    It's essentially a typed number, with corresponding manipulation operations\n    defined on it.\n    ",
        "klass": "beancount.core.amount.Amount",
        "module": "beancount"
    },
    {
        "base_classes": [
            "tuple"
        ],
        "class_docstring": "Transaction(meta, date, flag, payee, narration, tags, links, postings)",
        "klass": "beancount.core.data.Transaction",
        "module": "beancount"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class that computes a histogram of integer values. This is used to compute\n    a length that will cover at least some decent fraction of the samples.\n    ",
        "klass": "beancount.core.distribution.Distribution",
        "module": "beancount"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A file object that will delegate writing full lines to another logging function.\n    This may be used for writing data to a logging level without having to worry about\n    lines.\n    ",
        "klass": "beancount.utils.misc_utils.LineFileProxy",
        "module": "beancount"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A proxy file for a pager that only creates a pager after a minimum number of\n    lines has been printed to it.\n    ",
        "klass": "beancount.utils.pager.ConditionalPager",
        "module": "beancount"
    },
    {
        "base_classes": [
            "_lsprof.Profiler"
        ],
        "class_docstring": "Profile(timer=None, timeunit=None, subcalls=True, builtins=True)\n\n    Builds a profiler object using the specified timer function.\n    The default timer is a fast built-in one based on real time.\n    For custom timer functions returning integers, timeunit can\n    be a float specifying a scale (i.e. how long each integer unit\n    is, in seconds).\n    ",
        "klass": "cProfile.Profile",
        "module": "cProfile"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    ",
        "klass": "threading.Condition",
        "module": "threading"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    ",
        "klass": "threading.Semaphore",
        "module": "threading"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for dvc repo lock.\n\n    Uses zc.lockfile as backend.\n    ",
        "klass": "dvc.lock.Lock",
        "module": "dvc"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for the state database.\n\n    Args:\n        repo (dvc.repo.Repo): repo instance that this state belongs to.\n        config (configobj.ConfigObj): config for the state.\n\n    Raises:\n        StateVersionTooNewError: thrown when dvc version is older than the\n            state database version.\n    ",
        "klass": "dvc.state.State",
        "module": "dvc"
    },
    {
        "base_classes": [
            "contextlib._BaseExitStack",
            "contextlib.AbstractContextManager"
        ],
        "class_docstring": "Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    ",
        "klass": "contextlib.ExitStack",
        "module": "contextlib"
    },
    {
        "base_classes": [
            "astropy.config.paths._SetTempPath"
        ],
        "class_docstring": "\n    Context manager to set a temporary path for the Astropy download cache,\n    primarily for use with testing (though there may be other applications\n    for setting a different cache directory, for example to switch to a cache\n    dedicated to large files).\n\n    If the path set by this context manager does not already exist it will be\n    created, if possible.\n\n    This may also be used as a decorator on a function to set the cache path\n    just within that function.\n\n    Parameters\n    ----------\n\n    path : str\n        The directory (which must exist) in which to find the Astropy cache\n        files, or create them if they do not already exist.  If None, this\n        restores the cache path to the user's default cache path as returned\n        by `get_cache_dir` as though this context manager were not in effect\n        (this is useful for testing).  In this case the ``delete`` argument is\n        always ignored.\n\n    delete : bool, optional\n        If True, cleans up the temporary directory after exiting the temp\n        context (default: False).\n    ",
        "klass": "astropy.config.paths.set_temp_cache",
        "module": "astropy"
    },
    {
        "base_classes": [
            "list",
            "astropy.io.fits.verify._Verify"
        ],
        "class_docstring": "\n    HDU list class.  This is the top-level FITS object.  When a FITS\n    file is opened, a `HDUList` object is returned.\n    ",
        "klass": "astropy.io.fits.HDUList",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    FITS header class.  This class exposes both a dict-like interface and a\n    list-like interface to FITS headers.\n\n    The header may be indexed by keyword and, like a dict, the associated value\n    will be returned.  When the header contains cards with duplicate keywords,\n    only the value of the first card with the given keyword will be returned.\n    It is also possible to use a 2-tuple as the index in the form (keyword,\n    n)--this returns the n-th value with that keyword, in the case where there\n    are duplicate keywords.\n\n    For example::\n\n        >>> header['NAXIS']\n        0\n        >>> header[('FOO', 1)]  # Return the value of the second FOO keyword\n        'foo'\n\n    The header may also be indexed by card number::\n\n        >>> header[0]  # Return the value of the first card in the header\n        'T'\n\n    Commentary keywords such as HISTORY and COMMENT are special cases: When\n    indexing the Header object with either 'HISTORY' or 'COMMENT' a list of all\n    the HISTORY/COMMENT values is returned::\n\n        >>> header['HISTORY']\n        This is the first history entry in this header.\n        This is the second history entry in this header.\n        ...\n\n    See the Astropy documentation for more details on working with headers.\n    ",
        "klass": "astropy.io.fits.header.Header",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    FITS header class.  This class exposes both a dict-like interface and a\n    list-like interface to FITS headers.\n\n    The header may be indexed by keyword and, like a dict, the associated value\n    will be returned.  When the header contains cards with duplicate keywords,\n    only the value of the first card with the given keyword will be returned.\n    It is also possible to use a 2-tuple as the index in the form (keyword,\n    n)--this returns the n-th value with that keyword, in the case where there\n    are duplicate keywords.\n\n    For example::\n\n        >>> header['NAXIS']\n        0\n        >>> header[('FOO', 1)]  # Return the value of the second FOO keyword\n        'foo'\n\n    The header may also be indexed by card number::\n\n        >>> header[0]  # Return the value of the first card in the header\n        'T'\n\n    Commentary keywords such as HISTORY and COMMENT are special cases: When\n    indexing the Header object with either 'HISTORY' or 'COMMENT' a list of all\n    the HISTORY/COMMENT values is returned::\n\n        >>> header['HISTORY']\n        This is the first history entry in this header.\n        This is the second history entry in this header.\n        ...\n\n    See the Astropy documentation for more details on working with headers.\n    ",
        "klass": "astropy.io.fits.Header",
        "module": "astropy"
    },
    {
        "base_classes": [
            "astropy.io.fits.hdu.image._ImageBaseHDU"
        ],
        "class_docstring": "\n    FITS primary HDU class.\n    ",
        "klass": "astropy.io.fits.PrimaryHDU",
        "module": "astropy"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "\n    A subclass of list that contains only elements of a given type or\n    types.  If an item that is not of the specified type is added to\n    the list, a `TypeError` is raised.\n    ",
        "klass": "astropy.utils.collections.HomogeneousList",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class to write well-formed and nicely indented XML.\n\n    Use like this::\n\n        w = XMLWriter(fh)\n        with w.tag('html'):\n            with w.tag('body'):\n                w.data('This is the content')\n\n    Which produces::\n\n        <html>\n         <body>\n          This is the content\n         </body>\n        </html>\n    ",
        "klass": "astropy.utils.xml.writer.XMLWriter",
        "module": "astropy"
    },
    {
        "base_classes": [
            "astropy.utils.misc.ShapedLikeNDArray"
        ],
        "class_docstring": "\n    Represent and manipulate times and dates for astronomy.\n\n    A `Time` object is initialized with one or more times in the ``val``\n    argument.  The input times in ``val`` must conform to the specified\n    ``format`` and must correspond to the specified time ``scale``.  The\n    optional ``val2`` time input should be supplied only for numeric input\n    formats (e.g. JD) where very high precision (better than 64-bit precision)\n    is required.\n\n    The allowed values for ``format`` can be listed with::\n\n      >>> list(Time.FORMATS)\n      ['jd', 'mjd', 'decimalyear', 'unix', 'cxcsec', 'gps', 'plot_date',\n       'datetime', 'iso', 'isot', 'yday', 'datetime64', 'fits', 'byear',\n       'jyear', 'byear_str', 'jyear_str']\n\n    See also: http://docs.astropy.org/en/stable/time/\n\n    Parameters\n    ----------\n    val : sequence, ndarray, number, str, bytes, or `~astropy.time.Time` object\n        Value(s) to initialize the time or times.  Bytes are decoded as ascii.\n    val2 : sequence, ndarray, or number; optional\n        Value(s) to initialize the time or times.  Only used for numerical\n        input, to help preserve precision.\n    format : str, optional\n        Format of input value(s)\n    scale : str, optional\n        Time scale of input value(s), must be one of the following:\n        ('tai', 'tcb', 'tcg', 'tdb', 'tt', 'ut1', 'utc')\n    precision : int, optional\n        Digits of precision in string representation of time\n    in_subfmt : str, optional\n        Subformat for inputting string times\n    out_subfmt : str, optional\n        Subformat for outputting string times\n    location : `~astropy.coordinates.EarthLocation` or tuple, optional\n        If given as an tuple, it should be able to initialize an\n        an EarthLocation instance, i.e., either contain 3 items with units of\n        length for geocentric coordinates, or contain a longitude, latitude,\n        and an optional height for geodetic coordinates.\n        Can be a single location, or one for each input time.\n    copy : bool, optional\n        Make a copy of the input values\n    ",
        "klass": "astropy.time.Time",
        "module": "astropy"
    },
    {
        "base_classes": [
            "astropy.time.core.Time"
        ],
        "class_docstring": "\n    Represent the time difference between two times.\n\n    A TimeDelta object is initialized with one or more times in the ``val``\n    argument.  The input times in ``val`` must conform to the specified\n    ``format``.  The optional ``val2`` time input should be supplied only for\n    numeric input formats (e.g. JD) where very high precision (better than\n    64-bit precision) is required.\n\n    The allowed values for ``format`` can be listed with::\n\n      >>> list(TimeDelta.FORMATS)\n      ['sec', 'jd', 'datetime']\n\n    Note that for time differences, the scale can be among three groups:\n    geocentric ('tai', 'tt', 'tcg'), barycentric ('tcb', 'tdb'), and rotational\n    ('ut1'). Within each of these, the scales for time differences are the\n    same. Conversion between geocentric and barycentric is possible, as there\n    is only a scale factor change, but one cannot convert to or from 'ut1', as\n    this requires knowledge of the actual times, not just their difference. For\n    a similar reason, 'utc' is not a valid scale for a time difference: a UTC\n    day is not always 86400 seconds.\n\n    See also:\n\n    - http://docs.astropy.org/en/stable/time/\n    - http://docs.astropy.org/en/stable/time/index.html#time-deltas\n\n    Parameters\n    ----------\n    val : sequence, ndarray, number, `~astropy.units.Quantity` or `~astropy.time.TimeDelta` object\n        Value(s) to initialize the time difference(s). Any quantities will\n        be converted appropriately (with care taken to avoid rounding\n        errors for regular time units).\n    val2 : sequence, ndarray, number, or `~astropy.units.Quantity`; optional\n        Additional values, as needed to preserve precision.\n    format : str, optional\n        Format of input value(s)\n    scale : str, optional\n        Time scale of input value(s), must be one of the following values:\n        ('tdb', 'tt', 'ut1', 'tcg', 'tcb', 'tai'). If not given (or\n        ``None``), the scale is arbitrary; when added or subtracted from a\n        ``Time`` instance, it will be used without conversion.\n    copy : bool, optional\n        Make a copy of the input values\n    ",
        "klass": "astropy.time.TimeDelta",
        "module": "astropy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "A `~astropy.units.Quantity` represents a number with some associated unit.\n\n    See also: http://docs.astropy.org/en/stable/units/quantity.html\n\n    Parameters\n    ----------\n    value : number, `~numpy.ndarray`, `Quantity` object (sequence), str\n        The numerical value of this quantity in the units given by unit.  If a\n        `Quantity` or sequence of them (or any other valid object with a\n        ``unit`` attribute), creates a new `Quantity` object, converting to\n        `unit` units as needed.  If a string, it is converted to a number or\n        `Quantity`, depending on whether a unit is present.\n\n    unit : `~astropy.units.UnitBase` instance, str\n        An object that represents the unit associated with the input value.\n        Must be an `~astropy.units.UnitBase` object or a string parseable by\n        the :mod:`~astropy.units` package.\n\n    dtype : ~numpy.dtype, optional\n        The dtype of the resulting Numpy array or scalar that will\n        hold the value.  If not provided, it is determined from the input,\n        except that any integer and (non-Quantity) object inputs are converted\n        to float by default.\n\n    copy : bool, optional\n        If `True` (default), then the value is copied.  Otherwise, a copy will\n        only be made if ``__array__`` returns a copy, if value is a nested\n        sequence, or if a copy is needed to satisfy an explicitly given\n        ``dtype``.  (The `False` option is intended mostly for internal use,\n        to speed up initialization where a copy is known to have been made.\n        Use with care.)\n\n    order : {'C', 'F', 'A'}, optional\n        Specify the order of the array.  As in `~numpy.array`.  This parameter\n        is ignored if the input is a `Quantity` and ``copy=False``.\n\n    subok : bool, optional\n        If `False` (default), the returned array will be forced to be a\n        `Quantity`.  Otherwise, `Quantity` subclasses will be passed through,\n        or a subclass appropriate for the unit will be used (such as\n        `~astropy.units.Dex` for ``u.dex(u.AA)``).\n\n    ndmin : int, optional\n        Specifies the minimum number of dimensions that the resulting array\n        should have.  Ones will be pre-pended to the shape as needed to meet\n        this requirement.  This parameter is ignored if the input is a\n        `Quantity` and ``copy=False``.\n\n    Raises\n    ------\n    TypeError\n        If the value provided is not a Python numeric type.\n    TypeError\n        If the unit provided is not either a :class:`~astropy.units.Unit`\n        object or a parseable string unit.\n\n    Notes\n    -----\n    Quantities can also be created by multiplying a number or array with a\n    :class:`~astropy.units.Unit`. See http://docs.astropy.org/en/latest/units/\n\n    ",
        "klass": "astropy.units.Quantity",
        "module": "astropy"
    },
    {
        "base_classes": [
            "astropy.units.core.NamedUnit"
        ],
        "class_docstring": "\n    The main unit class.\n\n    There are a number of different ways to construct a Unit, but\n    always returns a `UnitBase` instance.  If the arguments refer to\n    an already-existing unit, that existing unit instance is returned,\n    rather than a new one.\n\n    - From a string::\n\n        Unit(s, format=None, parse_strict='silent')\n\n      Construct from a string representing a (possibly compound) unit.\n\n      The optional `format` keyword argument specifies the format the\n      string is in, by default ``\"generic\"``.  For a description of\n      the available formats, see `astropy.units.format`.\n\n      The optional ``parse_strict`` keyword controls what happens when an\n      unrecognized unit string is passed in.  It may be one of the following:\n\n         - ``'raise'``: (default) raise a ValueError exception.\n\n         - ``'warn'``: emit a Warning, and return an\n           `UnrecognizedUnit` instance.\n\n         - ``'silent'``: return an `UnrecognizedUnit` instance.\n\n    - From a number::\n\n        Unit(number)\n\n      Creates a dimensionless unit.\n\n    - From a `UnitBase` instance::\n\n        Unit(unit)\n\n      Returns the given unit unchanged.\n\n    - From `None`::\n\n        Unit()\n\n      Returns the null unit.\n\n    - The last form, which creates a new `Unit` is described in detail\n      below.\n\n    See also: http://docs.astropy.org/en/stable/units/\n\n    Parameters\n    ----------\n    st : str or list of str\n        The name of the unit.  If a list, the first element is the\n        canonical (short) name, and the rest of the elements are\n        aliases.\n\n    represents : UnitBase instance\n        The unit that this named unit represents.\n\n    doc : str, optional\n        A docstring describing the unit.\n\n    format : dict, optional\n        A mapping to format-specific representations of this unit.\n        For example, for the ``Ohm`` unit, it might be nice to have it\n        displayed as ``\\Omega`` by the ``latex`` formatter.  In that\n        case, `format` argument should be set to::\n\n            {'latex': r'\\Omega'}\n\n    namespace : dictionary, optional\n        When provided, inject the unit (and all of its aliases) into\n        the given namespace.\n\n    Raises\n    ------\n    ValueError\n        If any of the given unit names are already in the registry.\n\n    ValueError\n        If any of the given unit names are not valid Python tokens.\n    ",
        "klass": "astropy.units.Unit",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A context manager (for use with the ``with`` statement) that will seed the\n    numpy random number generator (RNG) to a specific value, and then restore\n    the RNG state back to whatever it was before.\n\n    This is primarily intended for use in the astropy testing suit, but it\n    may be useful in ensuring reproducibility of Monte Carlo simulations in a\n    science context.\n\n    Parameters\n    ----------\n    seed : int\n        The value to use to seed the numpy RNG\n\n    Examples\n    --------\n    A typical use case might be::\n\n        with NumpyRNGContext(<some seed value you pick>):\n            from numpy import random\n\n            randarr = random.randn(100)\n            ... run your test using `randarr` ...\n\n        #Any code using numpy.random at this indent level will act just as it\n        #would have if it had been before the with statement - e.g. whatever\n        #the default seed is.\n\n\n    ",
        "klass": "astropy.utils.misc.NumpyRNGContext",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A context manager (for use with the ``with`` statement) that will seed the\n    numpy random number generator (RNG) to a specific value, and then restore\n    the RNG state back to whatever it was before.\n\n    This is primarily intended for use in the astropy testing suit, but it\n    may be useful in ensuring reproducibility of Monte Carlo simulations in a\n    science context.\n\n    Parameters\n    ----------\n    seed : int\n        The value to use to seed the numpy RNG\n\n    Examples\n    --------\n    A typical use case might be::\n\n        with NumpyRNGContext(<some seed value you pick>):\n            from numpy import random\n\n            randarr = random.randn(100)\n            ... run your test using `randarr` ...\n\n        #Any code using numpy.random at this indent level will act just as it\n        #would have if it had been before the with statement - e.g. whatever\n        #the default seed is.\n\n\n    ",
        "klass": "astropy.utils.NumpyRNGContext",
        "module": "astropy"
    },
    {
        "base_classes": [
            "networkx.es.graph.Graph"
        ],
        "class_docstring": "\n    Base class for directed graphs.\n\n    A DiGraph stores nodes and edges with optional data, or attributes.\n\n    DiGraphs hold directed edges.  Self loops are allowed but multiple\n    (parallel) edges are not.\n\n    Nodes can be arbitrary (hashable) Python objects with optional\n    key/value attributes. By convention `None` is not used as a node.\n\n    Edges are represented as links between nodes with optional\n    key/value attributes.\n\n    Parameters\n    ----------\n    incoming_graph_data : input graph (optional, default: None)\n        Data to initialize graph. If None (default) an empty\n        graph is created.  The data can be any format that is supported\n        by the to_networkx_graph() function, currently including edge list,\n        dict of dicts, dict of lists, NetworkX graph, NumPy matrix\n        or 2d ndarray, SciPy sparse matrix, or PyGraphviz graph.\n\n    attr : keyword arguments, optional (default= no attributes)\n        Attributes to add to graph as key=value pairs.\n\n    See Also\n    --------\n    Graph\n    MultiGraph\n    MultiDiGraph\n    OrderedDiGraph\n\n    Examples\n    --------\n    Create an empty graph structure (a \"null graph\") with no nodes and\n    no edges.\n\n    >>> G = nx.DiGraph()\n\n    G can be grown in several ways.\n\n    **Nodes:**\n\n    Add one node at a time:\n\n    >>> G.add_node(1)\n\n    Add the nodes from any container (a list, dict, set or\n    even the lines from a file or the nodes from another graph).\n\n    >>> G.add_nodes_from([2, 3])\n    >>> G.add_nodes_from(range(100, 110))\n    >>> H = nx.path_graph(10)\n    >>> G.add_nodes_from(H)\n\n    In addition to strings and integers any hashable Python object\n    (except None) can represent a node, e.g. a customized node object,\n    or even another Graph.\n\n    >>> G.add_node(H)\n\n    **Edges:**\n\n    G can also be grown by adding edges.\n\n    Add one edge,\n\n    >>> G.add_edge(1, 2)\n\n    a list of edges,\n\n    >>> G.add_edges_from([(1, 2), (1, 3)])\n\n    or a collection of edges,\n\n    >>> G.add_edges_from(H.edges)\n\n    If some edges connect nodes not yet in the graph, the nodes\n    are added automatically.  There are no errors when adding\n    nodes or edges that already exist.\n\n    **Attributes:**\n\n    Each graph, node, and edge can hold key/value attribute pairs\n    in an associated attribute dictionary (the keys must be hashable).\n    By default these are empty, but can be added or changed using\n    add_edge, add_node or direct manipulation of the attribute\n    dictionaries named graph, node and edge respectively.\n\n    >>> G = nx.DiGraph(day=\"Friday\")\n    >>> G.graph\n    {'day': 'Friday'}\n\n    Add node attributes using add_node(), add_nodes_from() or G.nodes\n\n    >>> G.add_node(1, time='5pm')\n    >>> G.add_nodes_from([3], time='2pm')\n    >>> G.nodes[1]\n    {'time': '5pm'}\n    >>> G.nodes[1]['room'] = 714\n    >>> del G.nodes[1]['room'] # remove attribute\n    >>> list(G.nodes(data=True))\n    [(1, {'time': '5pm'}), (3, {'time': '2pm'})]\n\n    Add edge attributes using add_edge(), add_edges_from(), subscript\n    notation, or G.edges.\n\n    >>> G.add_edge(1, 2, weight=4.7 )\n    >>> G.add_edges_from([(3, 4), (4, 5)], color='red')\n    >>> G.add_edges_from([(1, 2, {'color':'blue'}), (2, 3, {'weight':8})])\n    >>> G[1][2]['weight'] = 4.7\n    >>> G.edges[1, 2]['weight'] = 4\n\n    Warning: we protect the graph data structure by making `G.edges[1, 2]` a\n    read-only dict-like structure. However, you can assign to attributes\n    in e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\n    data attributes: `G.edges[1, 2]['weight'] = 4`\n    (For multigraphs: `MG.edges[u, v, key][name] = value`).\n\n    **Shortcuts:**\n\n    Many common graph features allow python syntax to speed reporting.\n\n    >>> 1 in G     # check if node in graph\n    True\n    >>> [n for n in G if n < 3]  # iterate through nodes\n    [1, 2]\n    >>> len(G)  # number of nodes in graph\n    5\n\n    Often the best way to traverse all edges of a graph is via the neighbors.\n    The neighbors are reported as an adjacency-dict `G.adj` or `G.adjacency()`\n\n    >>> for n, nbrsdict in G.adjacency():\n    ...     for nbr, eattr in nbrsdict.items():\n    ...        if 'weight' in eattr:\n    ...            # Do something useful with the edges\n    ...            pass\n\n    But the edges reporting object is often more convenient:\n\n    >>> for u, v, weight in G.edges(data='weight'):\n    ...     if weight is not None:\n    ...         # Do something useful with the edges\n    ...         pass\n\n    **Reporting:**\n\n    Simple graph information is obtained using object-attributes and methods.\n    Reporting usually provides views instead of containers to reduce memory\n    usage. The views update as the graph is updated similarly to dict-views.\n    The objects `nodes, `edges` and `adj` provide access to data attributes\n    via lookup (e.g. `nodes[n], `edges[u, v]`, `adj[u][v]`) and iteration\n    (e.g. `nodes.items()`, `nodes.data('color')`,\n    `nodes.data('color', default='blue')` and similarly for `edges`)\n    Views exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\n\n    For details on these and other miscellaneous methods, see below.\n\n    **Subclasses (Advanced):**\n\n    The Graph class uses a dict-of-dict-of-dict data structure.\n    The outer dict (node_dict) holds adjacency information keyed by node.\n    The next dict (adjlist_dict) represents the adjacency information and holds\n    edge data keyed by neighbor.  The inner dict (edge_attr_dict) represents\n    the edge data and holds edge attribute values keyed by attribute names.\n\n    Each of these three dicts can be replaced in a subclass by a user defined\n    dict-like object. In general, the dict-like features should be\n    maintained but extra features can be added. To replace one of the\n    dicts create a new graph class by changing the class(!) variable\n    holding the factory for that dict-like structure. The variable names are\n    node_dict_factory, node_attr_dict_factory, adjlist_inner_dict_factory,\n    adjlist_outer_dict_factory, edge_attr_dict_factory and graph_attr_dict_factory.\n\n    node_dict_factory : function, (default: dict)\n        Factory function to be used to create the dict containing node\n        attributes, keyed by node id.\n        It should require no arguments and return a dict-like object\n\n    node_attr_dict_factory: function, (default: dict)\n        Factory function to be used to create the node attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object\n\n    adjlist_outer_dict_factory : function, (default: dict)\n        Factory function to be used to create the outer-most dict\n        in the data structure that holds adjacency info keyed by node.\n        It should require no arguments and return a dict-like object.\n\n    adjlist_inner_dict_factory : function, optional (default: dict)\n        Factory function to be used to create the adjacency list\n        dict which holds edge data keyed by neighbor.\n        It should require no arguments and return a dict-like object\n\n    edge_attr_dict_factory : function, optional (default: dict)\n        Factory function to be used to create the edge attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    graph_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the graph attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    Typically, if your extension doesn't impact the data structure all\n    methods will inherited without issue except: `to_directed/to_undirected`.\n    By default these methods create a DiGraph/Graph class and you probably\n    want them to create your extension of a DiGraph/Graph. To facilitate\n    this we define two class variables that you can set in your subclass.\n\n    to_directed_class : callable, (default: DiGraph or MultiDiGraph)\n        Class to create a new graph structure in the `to_directed` method.\n        If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\n\n    to_undirected_class : callable, (default: Graph or MultiGraph)\n        Class to create a new graph structure in the `to_undirected` method.\n        If `None`, a NetworkX class (Graph or MultiGraph) is used.\n\n    Examples\n    --------\n\n    Create a low memory graph class that effectively disallows edge\n    attributes by using a single attribute dict for all edges.\n    This reduces the memory used, but you lose edge attributes.\n\n    >>> class ThinGraph(nx.Graph):\n    ...     all_edge_dict = {'weight': 1}\n    ...     def single_edge_dict(self):\n    ...         return self.all_edge_dict\n    ...     edge_attr_dict_factory = single_edge_dict\n    >>> G = ThinGraph()\n    >>> G.add_edge(2, 1)\n    >>> G[2][1]\n    {'weight': 1}\n    >>> G.add_edge(2, 2)\n    >>> G[2][1] is G[2][2]\n    True\n\n\n    Please see :mod:`~networkx.classes.ordered` for more examples of\n    creating graph subclasses by overwriting the base class `dict` with\n    a dictionary-like object.\n    ",
        "klass": "networkx.DiGraph",
        "module": "networkx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Base class for undirected graphs.\n\n    A Graph stores nodes and edges with optional data, or attributes.\n\n    Graphs hold undirected edges.  Self loops are allowed but multiple\n    (parallel) edges are not.\n\n    Nodes can be arbitrary (hashable) Python objects with optional\n    key/value attributes. By convention `None` is not used as a node.\n\n    Edges are represented as links between nodes with optional\n    key/value attributes.\n\n    Parameters\n    ----------\n    incoming_graph_data : input graph (optional, default: None)\n        Data to initialize graph. If None (default) an empty\n        graph is created.  The data can be any format that is supported\n        by the to_networkx_graph() function, currently including edge list,\n        dict of dicts, dict of lists, NetworkX graph, NumPy matrix\n        or 2d ndarray, SciPy sparse matrix, or PyGraphviz graph.\n\n    attr : keyword arguments, optional (default= no attributes)\n        Attributes to add to graph as key=value pairs.\n\n    See Also\n    --------\n    DiGraph\n    MultiGraph\n    MultiDiGraph\n    OrderedGraph\n\n    Examples\n    --------\n    Create an empty graph structure (a \"null graph\") with no nodes and\n    no edges.\n\n    >>> G = nx.Graph()\n\n    G can be grown in several ways.\n\n    **Nodes:**\n\n    Add one node at a time:\n\n    >>> G.add_node(1)\n\n    Add the nodes from any container (a list, dict, set or\n    even the lines from a file or the nodes from another graph).\n\n    >>> G.add_nodes_from([2, 3])\n    >>> G.add_nodes_from(range(100, 110))\n    >>> H = nx.path_graph(10)\n    >>> G.add_nodes_from(H)\n\n    In addition to strings and integers any hashable Python object\n    (except None) can represent a node, e.g. a customized node object,\n    or even another Graph.\n\n    >>> G.add_node(H)\n\n    **Edges:**\n\n    G can also be grown by adding edges.\n\n    Add one edge,\n\n    >>> G.add_edge(1, 2)\n\n    a list of edges,\n\n    >>> G.add_edges_from([(1, 2), (1, 3)])\n\n    or a collection of edges,\n\n    >>> G.add_edges_from(H.edges)\n\n    If some edges connect nodes not yet in the graph, the nodes\n    are added automatically.  There are no errors when adding\n    nodes or edges that already exist.\n\n    **Attributes:**\n\n    Each graph, node, and edge can hold key/value attribute pairs\n    in an associated attribute dictionary (the keys must be hashable).\n    By default these are empty, but can be added or changed using\n    add_edge, add_node or direct manipulation of the attribute\n    dictionaries named graph, node and edge respectively.\n\n    >>> G = nx.Graph(day=\"Friday\")\n    >>> G.graph\n    {'day': 'Friday'}\n\n    Add node attributes using add_node(), add_nodes_from() or G.nodes\n\n    >>> G.add_node(1, time='5pm')\n    >>> G.add_nodes_from([3], time='2pm')\n    >>> G.nodes[1]\n    {'time': '5pm'}\n    >>> G.nodes[1]['room'] = 714  # node must exist already to use G.nodes\n    >>> del G.nodes[1]['room']  # remove attribute\n    >>> list(G.nodes(data=True))\n    [(1, {'time': '5pm'}), (3, {'time': '2pm'})]\n\n    Add edge attributes using add_edge(), add_edges_from(), subscript\n    notation, or G.edges.\n\n    >>> G.add_edge(1, 2, weight=4.7 )\n    >>> G.add_edges_from([(3, 4), (4, 5)], color='red')\n    >>> G.add_edges_from([(1, 2, {'color': 'blue'}), (2, 3, {'weight': 8})])\n    >>> G[1][2]['weight'] = 4.7\n    >>> G.edges[1, 2]['weight'] = 4\n\n    Warning: we protect the graph data structure by making `G.edges` a\n    read-only dict-like structure. However, you can assign to attributes\n    in e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\n    data attributes: `G.edges[1, 2]['weight'] = 4`\n    (For multigraphs: `MG.edges[u, v, key][name] = value`).\n\n    **Shortcuts:**\n\n    Many common graph features allow python syntax to speed reporting.\n\n    >>> 1 in G     # check if node in graph\n    True\n    >>> [n for n in G if n < 3]  # iterate through nodes\n    [1, 2]\n    >>> len(G)  # number of nodes in graph\n    5\n\n    Often the best way to traverse all edges of a graph is via the neighbors.\n    The neighbors are reported as an adjacency-dict `G.adj` or `G.adjacency()`\n\n    >>> for n, nbrsdict in G.adjacency():\n    ...     for nbr, eattr in nbrsdict.items():\n    ...        if 'weight' in eattr:\n    ...            # Do something useful with the edges\n    ...            pass\n\n    But the edges() method is often more convenient:\n\n    >>> for u, v, weight in G.edges.data('weight'):\n    ...     if weight is not None:\n    ...         # Do something useful with the edges\n    ...         pass\n\n    **Reporting:**\n\n    Simple graph information is obtained using object-attributes and methods.\n    Reporting typically provides views instead of containers to reduce memory\n    usage. The views update as the graph is updated similarly to dict-views.\n    The objects `nodes, `edges` and `adj` provide access to data attributes\n    via lookup (e.g. `nodes[n], `edges[u, v]`, `adj[u][v]`) and iteration\n    (e.g. `nodes.items()`, `nodes.data('color')`,\n    `nodes.data('color', default='blue')` and similarly for `edges`)\n    Views exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\n\n    For details on these and other miscellaneous methods, see below.\n\n    **Subclasses (Advanced):**\n\n    The Graph class uses a dict-of-dict-of-dict data structure.\n    The outer dict (node_dict) holds adjacency information keyed by node.\n    The next dict (adjlist_dict) represents the adjacency information and holds\n    edge data keyed by neighbor.  The inner dict (edge_attr_dict) represents\n    the edge data and holds edge attribute values keyed by attribute names.\n\n    Each of these three dicts can be replaced in a subclass by a user defined\n    dict-like object. In general, the dict-like features should be\n    maintained but extra features can be added. To replace one of the\n    dicts create a new graph class by changing the class(!) variable\n    holding the factory for that dict-like structure. The variable names are\n    node_dict_factory, node_attr_dict_factory, adjlist_inner_dict_factory,\n    adjlist_outer_dict_factory, edge_attr_dict_factory and graph_attr_dict_factory.\n\n    node_dict_factory : function, (default: dict)\n        Factory function to be used to create the dict containing node\n        attributes, keyed by node id.\n        It should require no arguments and return a dict-like object\n\n    node_attr_dict_factory: function, (default: dict)\n        Factory function to be used to create the node attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object\n\n    adjlist_outer_dict_factory : function, (default: dict)\n        Factory function to be used to create the outer-most dict\n        in the data structure that holds adjacency info keyed by node.\n        It should require no arguments and return a dict-like object.\n\n    adjlist_inner_dict_factory : function, (default: dict)\n        Factory function to be used to create the adjacency list\n        dict which holds edge data keyed by neighbor.\n        It should require no arguments and return a dict-like object\n\n    edge_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the edge attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    graph_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the graph attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    Typically, if your extension doesn't impact the data structure all\n    methods will inherit without issue except: `to_directed/to_undirected`.\n    By default these methods create a DiGraph/Graph class and you probably\n    want them to create your extension of a DiGraph/Graph. To facilitate\n    this we define two class variables that you can set in your subclass.\n\n    to_directed_class : callable, (default: DiGraph or MultiDiGraph)\n        Class to create a new graph structure in the `to_directed` method.\n        If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\n\n    to_undirected_class : callable, (default: Graph or MultiGraph)\n        Class to create a new graph structure in the `to_undirected` method.\n        If `None`, a NetworkX class (Graph or MultiGraph) is used.\n\n    Examples\n    --------\n\n    Create a low memory graph class that effectively disallows edge\n    attributes by using a single attribute dict for all edges.\n    This reduces the memory used, but you lose edge attributes.\n\n    >>> class ThinGraph(nx.Graph):\n    ...     all_edge_dict = {'weight': 1}\n    ...     def single_edge_dict(self):\n    ...         return self.all_edge_dict\n    ...     edge_attr_dict_factory = single_edge_dict\n    >>> G = ThinGraph()\n    >>> G.add_edge(2, 1)\n    >>> G[2][1]\n    {'weight': 1}\n    >>> G.add_edge(2, 2)\n    >>> G[2][1] is G[2][2]\n    True\n\n    Please see :mod:`~networkx.classes.ordered` for more examples of\n    creating graph subclasses by overwriting the base class `dict` with\n    a dictionary-like object.\n    ",
        "klass": "networkx.Graph",
        "module": "networkx"
    },
    {
        "base_classes": [
            "networkx.es.multigraph.MultiGraph",
            "networkx.es.digraph.DiGraph"
        ],
        "class_docstring": "A directed graph class that can store multiedges.\n\n    Multiedges are multiple edges between two nodes.  Each edge\n    can hold optional data or attributes.\n\n    A MultiDiGraph holds directed edges.  Self loops are allowed.\n\n    Nodes can be arbitrary (hashable) Python objects with optional\n    key/value attributes. By convention `None` is not used as a node.\n\n    Edges are represented as links between nodes with optional\n    key/value attributes.\n\n    Parameters\n    ----------\n    incoming_graph_data : input graph (optional, default: None)\n        Data to initialize graph. If None (default) an empty\n        graph is created.  The data can be any format that is supported\n        by the to_networkx_graph() function, currently including edge list,\n        dict of dicts, dict of lists, NetworkX graph, NumPy matrix\n        or 2d ndarray, SciPy sparse matrix, or PyGraphviz graph.\n\n    attr : keyword arguments, optional (default= no attributes)\n        Attributes to add to graph as key=value pairs.\n\n    See Also\n    --------\n    Graph\n    DiGraph\n    MultiGraph\n    OrderedMultiDiGraph\n\n    Examples\n    --------\n    Create an empty graph structure (a \"null graph\") with no nodes and\n    no edges.\n\n    >>> G = nx.MultiDiGraph()\n\n    G can be grown in several ways.\n\n    **Nodes:**\n\n    Add one node at a time:\n\n    >>> G.add_node(1)\n\n    Add the nodes from any container (a list, dict, set or\n    even the lines from a file or the nodes from another graph).\n\n    >>> G.add_nodes_from([2, 3])\n    >>> G.add_nodes_from(range(100, 110))\n    >>> H = nx.path_graph(10)\n    >>> G.add_nodes_from(H)\n\n    In addition to strings and integers any hashable Python object\n    (except None) can represent a node, e.g. a customized node object,\n    or even another Graph.\n\n    >>> G.add_node(H)\n\n    **Edges:**\n\n    G can also be grown by adding edges.\n\n    Add one edge,\n\n    >>> key = G.add_edge(1, 2)\n\n    a list of edges,\n\n    >>> keys = G.add_edges_from([(1, 2), (1, 3)])\n\n    or a collection of edges,\n\n    >>> keys = G.add_edges_from(H.edges)\n\n    If some edges connect nodes not yet in the graph, the nodes\n    are added automatically.  If an edge already exists, an additional\n    edge is created and stored using a key to identify the edge.\n    By default the key is the lowest unused integer.\n\n    >>> keys = G.add_edges_from([(4,5,dict(route=282)), (4,5,dict(route=37))])\n    >>> G[4]\n    AdjacencyView({5: {0: {}, 1: {'route': 282}, 2: {'route': 37}}})\n\n    **Attributes:**\n\n    Each graph, node, and edge can hold key/value attribute pairs\n    in an associated attribute dictionary (the keys must be hashable).\n    By default these are empty, but can be added or changed using\n    add_edge, add_node or direct manipulation of the attribute\n    dictionaries named graph, node and edge respectively.\n\n    >>> G = nx.MultiDiGraph(day=\"Friday\")\n    >>> G.graph\n    {'day': 'Friday'}\n\n    Add node attributes using add_node(), add_nodes_from() or G.nodes\n\n    >>> G.add_node(1, time='5pm')\n    >>> G.add_nodes_from([3], time='2pm')\n    >>> G.nodes[1]\n    {'time': '5pm'}\n    >>> G.nodes[1]['room'] = 714\n    >>> del G.nodes[1]['room'] # remove attribute\n    >>> list(G.nodes(data=True))\n    [(1, {'time': '5pm'}), (3, {'time': '2pm'})]\n\n    Add edge attributes using add_edge(), add_edges_from(), subscript\n    notation, or G.edges.\n\n    >>> key = G.add_edge(1, 2, weight=4.7 )\n    >>> keys = G.add_edges_from([(3, 4), (4, 5)], color='red')\n    >>> keys = G.add_edges_from([(1,2,{'color':'blue'}), (2,3,{'weight':8})])\n    >>> G[1][2][0]['weight'] = 4.7\n    >>> G.edges[1, 2, 0]['weight'] = 4\n\n    Warning: we protect the graph data structure by making `G.edges[1, 2]` a\n    read-only dict-like structure. However, you can assign to attributes\n    in e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\n    data attributes: `G.edges[1, 2]['weight'] = 4`\n    (For multigraphs: `MG.edges[u, v, key][name] = value`).\n\n    **Shortcuts:**\n\n    Many common graph features allow python syntax to speed reporting.\n\n    >>> 1 in G     # check if node in graph\n    True\n    >>> [n for n in G if n<3]   # iterate through nodes\n    [1, 2]\n    >>> len(G)  # number of nodes in graph\n    5\n    >>> G[1] # adjacency dict-like view keyed by neighbor to edge attributes\n    AdjacencyView({2: {0: {'weight': 4}, 1: {'color': 'blue'}}})\n\n    Often the best way to traverse all edges of a graph is via the neighbors.\n    The neighbors are available as an adjacency-view `G.adj` object or via\n    the method `G.adjacency()`.\n\n    >>> for n, nbrsdict in G.adjacency():\n    ...     for nbr, keydict in nbrsdict.items():\n    ...        for key, eattr in keydict.items():\n    ...            if 'weight' in eattr:\n    ...                # Do something useful with the edges\n    ...                pass\n\n    But the edges() method is often more convenient:\n\n    >>> for u, v, keys, weight in G.edges(data='weight', keys=True):\n    ...     if weight is not None:\n    ...         # Do something useful with the edges\n    ...         pass\n\n    **Reporting:**\n\n    Simple graph information is obtained using methods and object-attributes.\n    Reporting usually provides views instead of containers to reduce memory\n    usage. The views update as the graph is updated similarly to dict-views.\n    The objects `nodes, `edges` and `adj` provide access to data attributes\n    via lookup (e.g. `nodes[n], `edges[u, v]`, `adj[u][v]`) and iteration\n    (e.g. `nodes.items()`, `nodes.data('color')`,\n    `nodes.data('color', default='blue')` and similarly for `edges`)\n    Views exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\n\n    For details on these and other miscellaneous methods, see below.\n\n    **Subclasses (Advanced):**\n\n    The MultiDiGraph class uses a dict-of-dict-of-dict-of-dict structure.\n    The outer dict (node_dict) holds adjacency information keyed by node.\n    The next dict (adjlist_dict) represents the adjacency information and holds\n    edge_key dicts keyed by neighbor. The edge_key dict holds each edge_attr\n    dict keyed by edge key. The inner dict (edge_attr_dict) represents\n    the edge data and holds edge attribute values keyed by attribute names.\n\n    Each of these four dicts in the dict-of-dict-of-dict-of-dict\n    structure can be replaced by a user defined dict-like object.\n    In general, the dict-like features should be maintained but\n    extra features can be added. To replace one of the dicts create\n    a new graph class by changing the class(!) variable holding the\n    factory for that dict-like structure. The variable names are\n    node_dict_factory, node_attr_dict_factory, adjlist_inner_dict_factory,\n    adjlist_outer_dict_factory, edge_key_dict_factory, edge_attr_dict_factory\n    and graph_attr_dict_factory.\n\n    node_dict_factory : function, (default: dict)\n        Factory function to be used to create the dict containing node\n        attributes, keyed by node id.\n        It should require no arguments and return a dict-like object\n\n    node_attr_dict_factory: function, (default: dict)\n        Factory function to be used to create the node attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object\n\n    adjlist_outer_dict_factory : function, (default: dict)\n        Factory function to be used to create the outer-most dict\n        in the data structure that holds adjacency info keyed by node.\n        It should require no arguments and return a dict-like object.\n\n    adjlist_inner_dict_factory : function, (default: dict)\n        Factory function to be used to create the adjacency list\n        dict which holds multiedge key dicts keyed by neighbor.\n        It should require no arguments and return a dict-like object.\n\n    edge_key_dict_factory : function, (default: dict)\n        Factory function to be used to create the edge key dict\n        which holds edge data keyed by edge key.\n        It should require no arguments and return a dict-like object.\n\n    edge_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the edge attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    graph_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the graph attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    Typically, if your extension doesn't impact the data structure all\n    methods will inherited without issue except: `to_directed/to_undirected`.\n    By default these methods create a DiGraph/Graph class and you probably\n    want them to create your extension of a DiGraph/Graph. To facilitate\n    this we define two class variables that you can set in your subclass.\n\n    to_directed_class : callable, (default: DiGraph or MultiDiGraph)\n        Class to create a new graph structure in the `to_directed` method.\n        If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\n\n    to_undirected_class : callable, (default: Graph or MultiGraph)\n        Class to create a new graph structure in the `to_undirected` method.\n        If `None`, a NetworkX class (Graph or MultiGraph) is used.\n\n    Examples\n    --------\n\n    Please see :mod:`~networkx.classes.ordered` for examples of\n    creating graph subclasses by overwriting the base class `dict` with\n    a dictionary-like object.\n    ",
        "klass": "networkx.MultiDiGraph",
        "module": "networkx"
    },
    {
        "base_classes": [
            "networkx.es.graph.Graph"
        ],
        "class_docstring": "\n    An undirected graph class that can store multiedges.\n\n    Multiedges are multiple edges between two nodes.  Each edge\n    can hold optional data or attributes.\n\n    A MultiGraph holds undirected edges.  Self loops are allowed.\n\n    Nodes can be arbitrary (hashable) Python objects with optional\n    key/value attributes. By convention `None` is not used as a node.\n\n    Edges are represented as links between nodes with optional\n    key/value attributes.\n\n    Parameters\n    ----------\n    incoming_graph_data : input graph (optional, default: None)\n        Data to initialize graph. If None (default) an empty\n        graph is created.  The data can be any format that is supported\n        by the to_networkx_graph() function, currently including edge list,\n        dict of dicts, dict of lists, NetworkX graph, NumPy matrix\n        or 2d ndarray, SciPy sparse matrix, or PyGraphviz graph.\n\n    attr : keyword arguments, optional (default= no attributes)\n        Attributes to add to graph as key=value pairs.\n\n    See Also\n    --------\n    Graph\n    DiGraph\n    MultiDiGraph\n    OrderedMultiGraph\n\n    Examples\n    --------\n    Create an empty graph structure (a \"null graph\") with no nodes and\n    no edges.\n\n    >>> G = nx.MultiGraph()\n\n    G can be grown in several ways.\n\n    **Nodes:**\n\n    Add one node at a time:\n\n    >>> G.add_node(1)\n\n    Add the nodes from any container (a list, dict, set or\n    even the lines from a file or the nodes from another graph).\n\n    >>> G.add_nodes_from([2, 3])\n    >>> G.add_nodes_from(range(100, 110))\n    >>> H = nx.path_graph(10)\n    >>> G.add_nodes_from(H)\n\n    In addition to strings and integers any hashable Python object\n    (except None) can represent a node, e.g. a customized node object,\n    or even another Graph.\n\n    >>> G.add_node(H)\n\n    **Edges:**\n\n    G can also be grown by adding edges.\n\n    Add one edge,\n\n    >>> key = G.add_edge(1, 2)\n\n    a list of edges,\n\n    >>> keys = G.add_edges_from([(1, 2), (1, 3)])\n\n    or a collection of edges,\n\n    >>> keys = G.add_edges_from(H.edges)\n\n    If some edges connect nodes not yet in the graph, the nodes\n    are added automatically.  If an edge already exists, an additional\n    edge is created and stored using a key to identify the edge.\n    By default the key is the lowest unused integer.\n\n    >>> keys = G.add_edges_from([(4,5,{'route':28}), (4,5,{'route':37})])\n    >>> G[4]\n    AdjacencyView({3: {0: {}}, 5: {0: {}, 1: {'route': 28}, 2: {'route': 37}}})\n\n    **Attributes:**\n\n    Each graph, node, and edge can hold key/value attribute pairs\n    in an associated attribute dictionary (the keys must be hashable).\n    By default these are empty, but can be added or changed using\n    add_edge, add_node or direct manipulation of the attribute\n    dictionaries named graph, node and edge respectively.\n\n    >>> G = nx.MultiGraph(day=\"Friday\")\n    >>> G.graph\n    {'day': 'Friday'}\n\n    Add node attributes using add_node(), add_nodes_from() or G.nodes\n\n    >>> G.add_node(1, time='5pm')\n    >>> G.add_nodes_from([3], time='2pm')\n    >>> G.nodes[1]\n    {'time': '5pm'}\n    >>> G.nodes[1]['room'] = 714\n    >>> del G.nodes[1]['room'] # remove attribute\n    >>> list(G.nodes(data=True))\n    [(1, {'time': '5pm'}), (3, {'time': '2pm'})]\n\n    Add edge attributes using add_edge(), add_edges_from(), subscript\n    notation, or G.edges.\n\n    >>> key = G.add_edge(1, 2, weight=4.7 )\n    >>> keys = G.add_edges_from([(3, 4), (4, 5)], color='red')\n    >>> keys = G.add_edges_from([(1,2,{'color':'blue'}), (2,3,{'weight':8})])\n    >>> G[1][2][0]['weight'] = 4.7\n    >>> G.edges[1, 2, 0]['weight'] = 4\n\n    Warning: we protect the graph data structure by making `G.edges[1, 2]` a\n    read-only dict-like structure. However, you can assign to attributes\n    in e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\n    data attributes: `G.edges[1, 2]['weight'] = 4`\n    (For multigraphs: `MG.edges[u, v, key][name] = value`).\n\n    **Shortcuts:**\n\n    Many common graph features allow python syntax to speed reporting.\n\n    >>> 1 in G     # check if node in graph\n    True\n    >>> [n for n in G if n<3]   # iterate through nodes\n    [1, 2]\n    >>> len(G)  # number of nodes in graph\n    5\n    >>> G[1] # adjacency dict-like view keyed by neighbor to edge attributes\n    AdjacencyView({2: {0: {'weight': 4}, 1: {'color': 'blue'}}})\n\n    Often the best way to traverse all edges of a graph is via the neighbors.\n    The neighbors are reported as an adjacency-dict `G.adj` or `G.adjacency()`.\n\n    >>> for n, nbrsdict in G.adjacency():\n    ...     for nbr, keydict in nbrsdict.items():\n    ...        for key, eattr in keydict.items():\n    ...            if 'weight' in eattr:\n    ...                # Do something useful with the edges\n    ...                pass\n\n    But the edges() method is often more convenient:\n\n    >>> for u, v, keys, weight in G.edges(data='weight', keys=True):\n    ...     if weight is not None:\n    ...         # Do something useful with the edges\n    ...         pass\n\n    **Reporting:**\n\n    Simple graph information is obtained using methods and object-attributes.\n    Reporting usually provides views instead of containers to reduce memory\n    usage. The views update as the graph is updated similarly to dict-views.\n    The objects `nodes, `edges` and `adj` provide access to data attributes\n    via lookup (e.g. `nodes[n], `edges[u, v]`, `adj[u][v]`) and iteration\n    (e.g. `nodes.items()`, `nodes.data('color')`,\n    `nodes.data('color', default='blue')` and similarly for `edges`)\n    Views exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\n\n    For details on these and other miscellaneous methods, see below.\n\n    **Subclasses (Advanced):**\n\n    The MultiGraph class uses a dict-of-dict-of-dict-of-dict data structure.\n    The outer dict (node_dict) holds adjacency information keyed by node.\n    The next dict (adjlist_dict) represents the adjacency information and holds\n    edge_key dicts keyed by neighbor. The edge_key dict holds each edge_attr\n    dict keyed by edge key. The inner dict (edge_attr_dict) represents\n    the edge data and holds edge attribute values keyed by attribute names.\n\n    Each of these four dicts in the dict-of-dict-of-dict-of-dict\n    structure can be replaced by a user defined dict-like object.\n    In general, the dict-like features should be maintained but\n    extra features can be added. To replace one of the dicts create\n    a new graph class by changing the class(!) variable holding the\n    factory for that dict-like structure. The variable names are\n    node_dict_factory, node_attr_dict_factory, adjlist_inner_dict_factory,\n    adjlist_outer_dict_factory, edge_key_dict_factory, edge_attr_dict_factory\n    and graph_attr_dict_factory.\n\n    node_dict_factory : function, (default: dict)\n        Factory function to be used to create the dict containing node\n        attributes, keyed by node id.\n        It should require no arguments and return a dict-like object\n\n    node_attr_dict_factory: function, (default: dict)\n        Factory function to be used to create the node attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object\n\n    adjlist_outer_dict_factory : function, (default: dict)\n        Factory function to be used to create the outer-most dict\n        in the data structure that holds adjacency info keyed by node.\n        It should require no arguments and return a dict-like object.\n\n    adjlist_inner_dict_factory : function, (default: dict)\n        Factory function to be used to create the adjacency list\n        dict which holds multiedge key dicts keyed by neighbor.\n        It should require no arguments and return a dict-like object.\n\n    edge_key_dict_factory : function, (default: dict)\n        Factory function to be used to create the edge key dict\n        which holds edge data keyed by edge key.\n        It should require no arguments and return a dict-like object.\n\n    edge_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the edge attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    graph_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the graph attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    Typically, if your extension doesn't impact the data structure all\n    methods will inherited without issue except: `to_directed/to_undirected`.\n    By default these methods create a DiGraph/Graph class and you probably\n    want them to create your extension of a DiGraph/Graph. To facilitate\n    this we define two class variables that you can set in your subclass.\n\n    to_directed_class : callable, (default: DiGraph or MultiDiGraph)\n        Class to create a new graph structure in the `to_directed` method.\n        If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\n\n    to_undirected_class : callable, (default: Graph or MultiGraph)\n        Class to create a new graph structure in the `to_undirected` method.\n        If `None`, a NetworkX class (Graph or MultiGraph) is used.\n\n    Examples\n    --------\n\n    Please see :mod:`~networkx.classes.ordered` for examples of\n    creating graph subclasses by overwriting the base class `dict` with\n    a dictionary-like object.\n    ",
        "klass": "networkx.MultiGraph",
        "module": "networkx"
    },
    {
        "base_classes": [
            "networkx.algorithms.isomorphism.isomorphvf2.DiGraphMatcher"
        ],
        "class_docstring": "VF2 isomorphism checker for directed graphs.\n    ",
        "klass": "networkx.algorithms.isomorphism.DiGraphMatcher",
        "module": "networkx"
    },
    {
        "base_classes": [
            "networkx.algorithms.isomorphism.isomorphvf2.GraphMatcher"
        ],
        "class_docstring": "VF2 isomorphism checker for undirected graphs.\n    ",
        "klass": "networkx.algorithms.isomorphism.GraphMatcher",
        "module": "networkx"
    },
    {
        "base_classes": [
            "networkx.es.graph.Graph"
        ],
        "class_docstring": "\n    Base class for directed graphs.\n\n    A DiGraph stores nodes and edges with optional data, or attributes.\n\n    DiGraphs hold directed edges.  Self loops are allowed but multiple\n    (parallel) edges are not.\n\n    Nodes can be arbitrary (hashable) Python objects with optional\n    key/value attributes. By convention `None` is not used as a node.\n\n    Edges are represented as links between nodes with optional\n    key/value attributes.\n\n    Parameters\n    ----------\n    incoming_graph_data : input graph (optional, default: None)\n        Data to initialize graph. If None (default) an empty\n        graph is created.  The data can be any format that is supported\n        by the to_networkx_graph() function, currently including edge list,\n        dict of dicts, dict of lists, NetworkX graph, NumPy matrix\n        or 2d ndarray, SciPy sparse matrix, or PyGraphviz graph.\n\n    attr : keyword arguments, optional (default= no attributes)\n        Attributes to add to graph as key=value pairs.\n\n    See Also\n    --------\n    Graph\n    MultiGraph\n    MultiDiGraph\n    OrderedDiGraph\n\n    Examples\n    --------\n    Create an empty graph structure (a \"null graph\") with no nodes and\n    no edges.\n\n    >>> G = nx.DiGraph()\n\n    G can be grown in several ways.\n\n    **Nodes:**\n\n    Add one node at a time:\n\n    >>> G.add_node(1)\n\n    Add the nodes from any container (a list, dict, set or\n    even the lines from a file or the nodes from another graph).\n\n    >>> G.add_nodes_from([2, 3])\n    >>> G.add_nodes_from(range(100, 110))\n    >>> H = nx.path_graph(10)\n    >>> G.add_nodes_from(H)\n\n    In addition to strings and integers any hashable Python object\n    (except None) can represent a node, e.g. a customized node object,\n    or even another Graph.\n\n    >>> G.add_node(H)\n\n    **Edges:**\n\n    G can also be grown by adding edges.\n\n    Add one edge,\n\n    >>> G.add_edge(1, 2)\n\n    a list of edges,\n\n    >>> G.add_edges_from([(1, 2), (1, 3)])\n\n    or a collection of edges,\n\n    >>> G.add_edges_from(H.edges)\n\n    If some edges connect nodes not yet in the graph, the nodes\n    are added automatically.  There are no errors when adding\n    nodes or edges that already exist.\n\n    **Attributes:**\n\n    Each graph, node, and edge can hold key/value attribute pairs\n    in an associated attribute dictionary (the keys must be hashable).\n    By default these are empty, but can be added or changed using\n    add_edge, add_node or direct manipulation of the attribute\n    dictionaries named graph, node and edge respectively.\n\n    >>> G = nx.DiGraph(day=\"Friday\")\n    >>> G.graph\n    {'day': 'Friday'}\n\n    Add node attributes using add_node(), add_nodes_from() or G.nodes\n\n    >>> G.add_node(1, time='5pm')\n    >>> G.add_nodes_from([3], time='2pm')\n    >>> G.nodes[1]\n    {'time': '5pm'}\n    >>> G.nodes[1]['room'] = 714\n    >>> del G.nodes[1]['room'] # remove attribute\n    >>> list(G.nodes(data=True))\n    [(1, {'time': '5pm'}), (3, {'time': '2pm'})]\n\n    Add edge attributes using add_edge(), add_edges_from(), subscript\n    notation, or G.edges.\n\n    >>> G.add_edge(1, 2, weight=4.7 )\n    >>> G.add_edges_from([(3, 4), (4, 5)], color='red')\n    >>> G.add_edges_from([(1, 2, {'color':'blue'}), (2, 3, {'weight':8})])\n    >>> G[1][2]['weight'] = 4.7\n    >>> G.edges[1, 2]['weight'] = 4\n\n    Warning: we protect the graph data structure by making `G.edges[1, 2]` a\n    read-only dict-like structure. However, you can assign to attributes\n    in e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\n    data attributes: `G.edges[1, 2]['weight'] = 4`\n    (For multigraphs: `MG.edges[u, v, key][name] = value`).\n\n    **Shortcuts:**\n\n    Many common graph features allow python syntax to speed reporting.\n\n    >>> 1 in G     # check if node in graph\n    True\n    >>> [n for n in G if n < 3]  # iterate through nodes\n    [1, 2]\n    >>> len(G)  # number of nodes in graph\n    5\n\n    Often the best way to traverse all edges of a graph is via the neighbors.\n    The neighbors are reported as an adjacency-dict `G.adj` or `G.adjacency()`\n\n    >>> for n, nbrsdict in G.adjacency():\n    ...     for nbr, eattr in nbrsdict.items():\n    ...        if 'weight' in eattr:\n    ...            # Do something useful with the edges\n    ...            pass\n\n    But the edges reporting object is often more convenient:\n\n    >>> for u, v, weight in G.edges(data='weight'):\n    ...     if weight is not None:\n    ...         # Do something useful with the edges\n    ...         pass\n\n    **Reporting:**\n\n    Simple graph information is obtained using object-attributes and methods.\n    Reporting usually provides views instead of containers to reduce memory\n    usage. The views update as the graph is updated similarly to dict-views.\n    The objects `nodes, `edges` and `adj` provide access to data attributes\n    via lookup (e.g. `nodes[n], `edges[u, v]`, `adj[u][v]`) and iteration\n    (e.g. `nodes.items()`, `nodes.data('color')`,\n    `nodes.data('color', default='blue')` and similarly for `edges`)\n    Views exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\n\n    For details on these and other miscellaneous methods, see below.\n\n    **Subclasses (Advanced):**\n\n    The Graph class uses a dict-of-dict-of-dict data structure.\n    The outer dict (node_dict) holds adjacency information keyed by node.\n    The next dict (adjlist_dict) represents the adjacency information and holds\n    edge data keyed by neighbor.  The inner dict (edge_attr_dict) represents\n    the edge data and holds edge attribute values keyed by attribute names.\n\n    Each of these three dicts can be replaced in a subclass by a user defined\n    dict-like object. In general, the dict-like features should be\n    maintained but extra features can be added. To replace one of the\n    dicts create a new graph class by changing the class(!) variable\n    holding the factory for that dict-like structure. The variable names are\n    node_dict_factory, node_attr_dict_factory, adjlist_inner_dict_factory,\n    adjlist_outer_dict_factory, edge_attr_dict_factory and graph_attr_dict_factory.\n\n    node_dict_factory : function, (default: dict)\n        Factory function to be used to create the dict containing node\n        attributes, keyed by node id.\n        It should require no arguments and return a dict-like object\n\n    node_attr_dict_factory: function, (default: dict)\n        Factory function to be used to create the node attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object\n\n    adjlist_outer_dict_factory : function, (default: dict)\n        Factory function to be used to create the outer-most dict\n        in the data structure that holds adjacency info keyed by node.\n        It should require no arguments and return a dict-like object.\n\n    adjlist_inner_dict_factory : function, optional (default: dict)\n        Factory function to be used to create the adjacency list\n        dict which holds edge data keyed by neighbor.\n        It should require no arguments and return a dict-like object\n\n    edge_attr_dict_factory : function, optional (default: dict)\n        Factory function to be used to create the edge attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    graph_attr_dict_factory : function, (default: dict)\n        Factory function to be used to create the graph attribute\n        dict which holds attribute values keyed by attribute name.\n        It should require no arguments and return a dict-like object.\n\n    Typically, if your extension doesn't impact the data structure all\n    methods will inherited without issue except: `to_directed/to_undirected`.\n    By default these methods create a DiGraph/Graph class and you probably\n    want them to create your extension of a DiGraph/Graph. To facilitate\n    this we define two class variables that you can set in your subclass.\n\n    to_directed_class : callable, (default: DiGraph or MultiDiGraph)\n        Class to create a new graph structure in the `to_directed` method.\n        If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\n\n    to_undirected_class : callable, (default: Graph or MultiGraph)\n        Class to create a new graph structure in the `to_undirected` method.\n        If `None`, a NetworkX class (Graph or MultiGraph) is used.\n\n    Examples\n    --------\n\n    Create a low memory graph class that effectively disallows edge\n    attributes by using a single attribute dict for all edges.\n    This reduces the memory used, but you lose edge attributes.\n\n    >>> class ThinGraph(nx.Graph):\n    ...     all_edge_dict = {'weight': 1}\n    ...     def single_edge_dict(self):\n    ...         return self.all_edge_dict\n    ...     edge_attr_dict_factory = single_edge_dict\n    >>> G = ThinGraph()\n    >>> G.add_edge(2, 1)\n    >>> G[2][1]\n    {'weight': 1}\n    >>> G.add_edge(2, 2)\n    >>> G[2][1] is G[2][2]\n    True\n\n\n    Please see :mod:`~networkx.classes.ordered` for more examples of\n    creating graph subclasses by overwriting the base class `dict` with\n    a dictionary-like object.\n    ",
        "klass": "networkx.classes.DiGraph",
        "module": "networkx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An XML element hierarchy.\n\n    This class also provides support for serialization to and from\n    standard XML.\n\n    *element* is an optional root element node,\n    *file* is an optional file handle or file name of an XML file whose\n    contents will be used to initialize the tree with.\n\n    ",
        "klass": "xml.etree.ElementTree.ElementTree",
        "module": "xml"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    Maintains the last n accessed keys\n    ",
        "klass": "synapse.lib.cache.LruDict",
        "module": "synapse"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An object that manages multiple tag globs and values for caching.\n    ",
        "klass": "synapse.lib.cache.TagGlobs",
        "module": "synapse"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Applies and verifies sequence numbers of encrypted messages coming and going\n\n    Args:\n        rx_key (bytes): TX key (used with TinFoilHat).\n        tx_key (bytes): RX key (used with TinFoilHat).\n        initial_rx_seq (int): Starting rx sequence number.\n        initial_tx_seq (int): Starting tx sequence number.\n    ",
        "klass": "synapse.lib.crypto.tinfoil.CryptSeq",
        "module": "synapse"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The Scope object assists in creating nested varible scopes.\n\n    Example:\n\n        with Scope() as scope:\n\n            scope.set('foo',10)\n\n            with scope:\n                scope.set('foo',20)\n                dostuff(scope) # 'foo' is 20...\n\n            dostuff(scope) # 'foo' is 10 again...\n\n    ",
        "klass": "synapse.lib.scope.Scope",
        "module": "synapse"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An append optimized sequence of byte blobs.\n\n    Args:\n        lenv (lmdb.Environment): The LMDB Environment.\n        name (str): The name of the sequence.\n    ",
        "klass": "synapse.lib.slabseqn.SlabSeqn",
        "module": "synapse"
    },
    {
        "base_classes": [
            "hwt.interfaces.std.BramPort_withoutClk"
        ],
        "class_docstring": "\n    BRAM port with it's own clk\n    ",
        "klass": "hwt.interfaces.std.BramPort",
        "module": "hwt"
    },
    {
        "base_classes": [
            "hwt.synthesizer.interface.Interface"
        ],
        "class_docstring": "\n    Only synchronization interface, like vld+rd signal with meaning\n    like in :class:`.Handshaked` interface\n\n    :ivar rd: when high slave is ready to receive data\n    :ivar vld: when high master is sending data to slave\n\n    transaction happens when both ready and valid are high\n\n    ",
        "klass": "hwt.interfaces.std.HandshakeSync",
        "module": "hwt"
    },
    {
        "base_classes": [
            "hwt.interfaces.std.VldSynced"
        ],
        "class_docstring": "\n    Interface data+ready+valid signal, if rd=1 slave is ready to accept data,\n    if vld=1 master is sending data,\n    if rd=1 and vld=1 then data is transfered otherwise master\n    and slave has to wait on each other\n\n    :attention: one rd/vld is set it must not go down until transaction is made\n    ",
        "klass": "hwt.interfaces.std.Handshaked",
        "module": "hwt"
    },
    {
        "base_classes": [
            "hwt.synthesizer.interface.Interface"
        ],
        "class_docstring": "\n    Interface data+valid signal, if vld=1 then data are valid and slave should\n    accept them\n    ",
        "klass": "hwt.interfaces.std.VldSynced",
        "module": "hwt"
    },
    {
        "base_classes": [
            "hwt.synthesizer.interface.Interface"
        ],
        "class_docstring": "\n    Create dynamic interface based on HStruct or HUnion description\n\n    :ivar _fieldsToInterfaces: dictionary {field from HStruct template:\n        sub interface for it}\n    :ivar _structT: HStruct instance used as template for this interface\n    :param _instantiateFieldFn: function(FieldTemplateItem instance)\n        return interface instance\n    ",
        "klass": "hwt.interfaces.structIntf.StructIntf",
        "module": "hwt"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Random() -> create a random number generator with its own internal state.",
        "klass": "_random.Random",
        "module": "_random"
    },
    {
        "base_classes": [
            "oslo_config.cfg.Opt"
        ],
        "class_docstring": "Boolean options.\n\n    Bool opts are set to True or False on the command line using --optname or\n    --nooptname respectively.\n\n    In config files, boolean values are cast with Boolean type.\n\n    :param name: the option's name\n    :param \\*\\*kwargs: arbitrary keyword arguments passed to :class:`Opt`\n    ",
        "klass": "oslo_config.cfg.BoolOpt",
        "module": "oslo_config"
    },
    {
        "base_classes": [
            "collections.abc.Mapping"
        ],
        "class_docstring": "Config options which may be set on the command line or in config files.\n\n    ConfigOpts is a configuration option manager with APIs for registering\n    option schemas, grouping options, parsing option values and retrieving\n    the values of options.\n\n    It has built-in support for :oslo.config:option:`config_file` and\n    :oslo.config:option:`config_dir` options.\n\n    ",
        "klass": "oslo_config.cfg.ConfigOpts",
        "module": "oslo_config"
    },
    {
        "base_classes": [
            "oslo_config.cfg.Opt"
        ],
        "class_docstring": "Option with String type\n\n    Option with ``type`` :class:`oslo_config.types.String`\n\n    :param name: the option's name\n    :param choices: Optional sequence of either valid values or tuples of valid\n        values with descriptions.\n    :param quotes: If True and string is enclosed with single or double\n                   quotes, will strip those quotes.\n    :param regex: Optional regular expression (string or compiled\n                  regex) that the value must match on an unanchored\n                  search.\n    :param ignore_case: If True case differences (uppercase vs. lowercase)\n                        between 'choices' or 'regex' will be ignored.\n    :param max_length: If positive integer, the value must be less than or\n                       equal to this parameter.\n    :param \\*\\*kwargs: arbitrary keyword arguments passed to :class:`Opt`\n\n    .. versionchanged:: 2.7\n       Added *quotes* parameter\n\n    .. versionchanged:: 2.7\n       Added *regex* parameter\n\n    .. versionchanged:: 2.7\n       Added *ignore_case* parameter\n\n    .. versionchanged:: 2.7\n       Added *max_length* parameter\n\n    .. versionchanged:: 5.2\n       The *choices* parameter will now accept a sequence of tuples, where each\n       tuple is of form (*choice*, *description*)\n    ",
        "klass": "oslo_config.cfg.StrOpt",
        "module": "oslo_config"
    },
    {
        "base_classes": [
            "rally.common.db.sa_types.LongText"
        ],
        "class_docstring": "Represents an immutable structure as a json-encoded string.",
        "klass": "rally.common.db.sa_types.JSONEncodedDict",
        "module": "rally"
    },
    {
        "base_classes": [
            "sqlalchemy.sql.type_api.TypeDecorator"
        ],
        "class_docstring": "Represents datetime/time timestamp object as a bigint value.\n\n    Despite the fact that timestamp objects are represented by float value in\n    python, the Float column cannot be used for storing such values, since\n    timestamps values can be bigger than the limit of Float columns at some\n    back-ends (the value will be cropped in such case). Also, using Datetime\n    type is not convenient too, since it do not accurate with microseconds.\n    ",
        "klass": "rally.common.db.sa_types.TimeStamp",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context that intercepts and logs exceptions.\n\n    Usage::\n        LOG = logging.getLogger(__name__)\n        ...\n\n        def foobar():\n            with ExceptionLogger(LOG, \"foobar warning\") as e:\n                return house_of_raising_exception()\n\n            if e.exception:\n                raise e.exception # remove if not required\n    ",
        "klass": "rally.common.logging.ExceptionLogger",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager that catches log messages.\n\n    User can make an assertion on their content or fetch them all.\n\n    Usage::\n        LOG = logging.getLogger(__name__)\n        ...\n\n        def foobar():\n            with LogCatcher(LOG) as catcher_in_rye:\n                LOG.warning(\"Running Kids\")\n\n            catcher_in_rye.assertInLogs(\"Running Kids\")\n    ",
        "klass": "rally.common.logging.LogCatcher",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a verification object.",
        "klass": "rally.common.objects.Verification",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a verifier object.",
        "klass": "rally.common.objects.Verifier",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Calculates degradation from a stream of numbers\n\n    Finds min and max values from a stream and then calculates\n    ratio between them in percentage. Works only with positive numbers.\n    ",
        "klass": "rally.common.streaming_algorithms.DegradationComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Simple incremental counter.",
        "klass": "rally.common.streaming_algorithms.IncrementComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Compute maximal value from a stream of numbers.",
        "klass": "rally.common.streaming_algorithms.MaxComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Compute mean for a stream of numbers.",
        "klass": "rally.common.streaming_algorithms.MeanComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Compute minimal value from a stream of numbers.",
        "klass": "rally.common.streaming_algorithms.MinComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.streaming_algorithms.StreamingAlgorithm"
        ],
        "class_docstring": "Compute standard deviation for a stream of numbers.",
        "klass": "rally.common.streaming_algorithms.StdDevComputation",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.context.Context"
        ],
        "class_docstring": "Dummy context.",
        "klass": "rally.plugins.common.contexts.dummy.DummyContext",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.sla.SLA"
        ],
        "class_docstring": "Failure rate minimum and maximum in percents.",
        "klass": "rally.plugins.common.sla.failure_rate.FailureRate",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.sla.SLA"
        ],
        "class_docstring": "Maximum time for one iteration in seconds.",
        "klass": "rally.plugins.common.sla.iteration_time.IterationTime",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.sla.SLA"
        ],
        "class_docstring": "Maximum average duration of one iteration in seconds.",
        "klass": "rally.plugins.common.sla.max_average_duration.MaxAverageDuration",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.sla.SLA"
        ],
        "class_docstring": "Maximum average duration of one iterations atomic actions in seconds.",
        "klass": "rally.plugins.common.sla.max_average_duration_per_atomic.MaxAverageDurationPerAtomic",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.task.sla.SLA"
        ],
        "class_docstring": "Limit the number of outliers (iterations that take too much time).\n\n    The outliers are detected automatically using the computation of the mean\n    and standard deviation (std) of the data.\n    ",
        "klass": "rally.plugins.common.sla.outliers.Outliers",
        "module": "rally"
    },
    {
        "base_classes": [
            "rally.common.utils.Timer"
        ],
        "class_docstring": "A class to measure the duration of atomic operations\n\n    This would simplify the way measure atomic operation duration\n    in certain cases. For example, if we want to get the duration\n    for each operation which runs in an iteration\n    for i in range(repetitions):\n        with atomic.ActionTimer(instance_of_action_timer, \"name_of_action\"):\n            self.clients(<client>).<operation>\n    ",
        "klass": "rally.task.atomic.ActionTimer",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create context environment and run method inside it.",
        "klass": "rally.task.context.ContextManager",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base SLA checker class.",
        "klass": "rally.task.sla.SLAChecker",
        "module": "rally"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class represents an API Version Request with convenience\n    methods for manipulation and comparison of version\n    numbers that we need to do to implement microversions.\n    ",
        "klass": "nova.api.openstack.api_version_request.APIVersionRequest",
        "module": "nova"
    },
    {
        "base_classes": [
            "nova.db.base.Base"
        ],
        "class_docstring": "API for interacting with the compute manager.",
        "klass": "nova.compute.api.API",
        "module": "nova"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager to report instance action events.",
        "klass": "nova.compute.utils.EventReporter",
        "module": "nova"
    },
    {
        "base_classes": [
            "oslo_context.context.RequestContext"
        ],
        "class_docstring": "Security context and request information.\n\n    Represents the user taking a given action within the system.\n\n    ",
        "klass": "nova.context.RequestContext",
        "module": "nova"
    },
    {
        "base_classes": [
            "Exception"
        ],
        "class_docstring": "Base Nova Exception\n\n    To correctly use this class, inherit from it and define\n    a 'msg_fmt' property. That msg_fmt will get printf'd\n    with the keyword arguments provided to the constructor.\n\n    ",
        "klass": "nova.exception.NovaException",
        "module": "nova"
    },
    {
        "base_classes": [
            "oslo_service.service.Service"
        ],
        "class_docstring": "Service object for binaries running on hosts.\n\n    A service takes a manager and enables rpc by listening to queues based\n    on topic. It also periodically runs tasks on the manager and reports\n    its state to the database services table.\n    ",
        "klass": "nova.service.Service",
        "module": "nova"
    },
    {
        "base_classes": [
            "oslo_service.service.Service"
        ],
        "class_docstring": "Provides ability to launch API from a 'paste' configuration.",
        "klass": "nova.service.WSGIService",
        "module": "nova"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Build config drives, optionally as a context manager.",
        "klass": "nova.virt.configdrive.ConfigDriveBuilder",
        "module": "nova"
    },
    {
        "base_classes": [
            "nova.virt.disk.mount.api.Mount"
        ],
        "class_docstring": "loop back support for raw images.",
        "klass": "nova.virt.disk.mount.loop.LoopMount",
        "module": "nova"
    },
    {
        "base_classes": [
            "nova.virt.disk.mount.api.Mount"
        ],
        "class_docstring": "qemu-nbd support disk images.",
        "klass": "nova.virt.disk.mount.nbd.NbdMount",
        "module": "nova"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A degenerate file-like so that an iterable could be read like a file.\n\n    As Glance client returns an iterable, but tarfile requires a file like,\n    this is the adapter between the two. This allows tarfile to access the\n    glance stream.\n    ",
        "klass": "nova.virt.xenapi.image.utils.IterableToFileAdapter",
        "module": "nova"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An FTP client class.\n\n    To create a connection, call the class using these arguments:\n            host, user, passwd, acct, timeout\n\n    The first four arguments are all strings, and have default value ''.\n    timeout must be numeric and defaults to None if not passed,\n    meaning that no timeout will be set on any ftp socket(s)\n    If a timeout is passed, then this is now the default timeout for all ftp\n    socket operations for this instance.\n\n    Then use self.connect() with optional host and port argument.\n\n    To download a file, use ftp.retrlines('RETR ' + filename),\n    or ftp.retrbinary() with slightly different arguments.\n    To upload a file, use ftp.storlines() or ftp.storbinary(),\n    which have an open file as argument (see their definitions\n    below for details).\n    The download/upload functions first issue appropriate TYPE\n    and PORT or PASV commands.\n    ",
        "klass": "ftplib.FTP",
        "module": "ftplib"
    },
    {
        "base_classes": [
            "ftplib.FTP"
        ],
        "class_docstring": "A FTP subclass which adds TLS support to FTP as described\n        in RFC-4217.\n\n        Connect as usual to port 21 implicitly securing the FTP control\n        connection before authenticating.\n\n        Securing the data connection requires user to explicitly ask\n        for it by calling prot_p() method.\n\n        Usage example:\n        >>> from ftplib import FTP_TLS\n        >>> ftps = FTP_TLS('ftp.python.org')\n        >>> ftps.login()  # login anonymously previously securing control channel\n        '230 Guest login ok, access restrictions apply.'\n        >>> ftps.prot_p()  # switch to secure data connection\n        '200 Protection level set to P'\n        >>> ftps.retrlines('LIST')  # list directory content securely\n        total 9\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 .\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 ..\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 bin\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 etc\n        d-wxrwxr-x   2 ftp      wheel        1024 Sep  5 13:43 incoming\n        drwxr-xr-x   2 root     wheel        1024 Nov 17  1993 lib\n        drwxr-xr-x   6 1094     wheel        1024 Sep 13 19:07 pub\n        drwxr-xr-x   3 root     wheel        1024 Jan  3  1994 usr\n        -rw-r--r--   1 root     root          312 Aug  1  1994 welcome.msg\n        '226 Transfer complete.'\n        >>> ftps.quit()\n        '221 Goodbye.'\n        >>>\n        ",
        "klass": "ftplib.FTP_TLS",
        "module": "ftplib"
    },
    {
        "base_classes": [
            "configobj.Section"
        ],
        "class_docstring": "An object to read, create, and write config files.",
        "klass": "configobj.ConfigObj",
        "module": "configobj"
    },
    {
        "base_classes": [
            "mailbox.Mailbox"
        ],
        "class_docstring": "A qmail-style Maildir mailbox.",
        "klass": "mailbox.Maildir",
        "module": "mailbox"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A sedes for lists of arbitrary length.\n\n    :param element_sedes: when (de-)serializing a list, this sedes will be\n                          applied to all of its elements\n    :param max_length: maximum number of allowed elements, or `None` for no limit\n    ",
        "klass": "rlp.sedes.lists.CountableList",
        "module": "rlp"
    },
    {
        "base_classes": [
            "twilio.twiml.TwiML"
        ],
        "class_docstring": " <Response> TwiML for Messages ",
        "klass": "twilio.twiml.messaging_response.MessagingResponse",
        "module": "twilio"
    },
    {
        "base_classes": [
            "nltk.stem.api.StemmerI"
        ],
        "class_docstring": "\n    A word stemmer based on the Porter stemming algorithm.\n\n        Porter, M. \"An algorithm for suffix stripping.\"\n        Program 14.3 (1980): 130-137.\n\n    See http://www.tartarus.org/~martin/PorterStemmer/ for the homepage\n    of the algorithm.\n\n    Martin Porter has endorsed several modifications to the Porter\n    algorithm since writing his original paper, and those extensions are\n    included in the implementations on his website. Additionally, others\n    have proposed further improvements to the algorithm, including NLTK\n    contributors. There are thus three modes that can be selected by\n    passing the appropriate constant to the class constructor's `mode`\n    attribute:\n\n        PorterStemmer.ORIGINAL_ALGORITHM\n        - Implementation that is faithful to the original paper.\n\n          Note that Martin Porter has deprecated this version of the\n          algorithm. Martin distributes implementations of the Porter\n          Stemmer in many languages, hosted at:\n\n            http://www.tartarus.org/~martin/PorterStemmer/\n\n          and all of these implementations include his extensions. He\n          strongly recommends against using the original, published\n          version of the algorithm; only use this mode if you clearly\n          understand why you are choosing to do so.\n\n        PorterStemmer.MARTIN_EXTENSIONS\n        - Implementation that only uses the modifications to the\n          algorithm that are included in the implementations on Martin\n          Porter's website. He has declared Porter frozen, so the\n          behaviour of those implementations should never change.\n\n        PorterStemmer.NLTK_EXTENSIONS (default)\n        - Implementation that includes further improvements devised by\n          NLTK contributors or taken from other modified implementations\n          found on the web.\n\n    For the best stemming, you should use the default NLTK_EXTENSIONS\n    version. However, if you need to get the same results as either the\n    original algorithm or one of Martin Porter's hosted versions for\n    compatibility with an existing implementation or dataset, you can use\n    one of the other modes instead.\n    ",
        "klass": "nltk.stem.porter.PorterStemmer",
        "module": "nltk"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The relativedelta type is designed to be applied to an existing datetime and\n    can replace specific components of that datetime, or represents an interval\n    of time.\n\n    It is based on the specification of the excellent work done by M.-A. Lemburg\n    in his\n    `mx.DateTime <https://www.egenix.com/products/python/mxBase/mxDateTime/>`_ extension.\n    However, notice that this type does *NOT* implement the same algorithm as\n    his work. Do *NOT* expect it to behave like mx.DateTime's counterpart.\n\n    There are two different ways to build a relativedelta instance. The\n    first one is passing it two date/datetime classes::\n\n        relativedelta(datetime1, datetime2)\n\n    The second one is passing it any number of the following keyword arguments::\n\n        relativedelta(arg1=x,arg2=y,arg3=z...)\n\n        year, month, day, hour, minute, second, microsecond:\n            Absolute information (argument is singular); adding or subtracting a\n            relativedelta with absolute information does not perform an arithmetic\n            operation, but rather REPLACES the corresponding value in the\n            original datetime with the value(s) in relativedelta.\n\n        years, months, weeks, days, hours, minutes, seconds, microseconds:\n            Relative information, may be negative (argument is plural); adding\n            or subtracting a relativedelta with relative information performs\n            the corresponding arithmetic operation on the original datetime value\n            with the information in the relativedelta.\n\n        weekday: \n            One of the weekday instances (MO, TU, etc) available in the\n            relativedelta module. These instances may receive a parameter N,\n            specifying the Nth weekday, which could be positive or negative\n            (like MO(+1) or MO(-2)). Not specifying it is the same as specifying\n            +1. You can also use an integer, where 0=MO. This argument is always\n            relative e.g. if the calculated date is already Monday, using MO(1)\n            or MO(-1) won't change the day. To effectively make it absolute, use\n            it in combination with the day argument (e.g. day=1, MO(1) for first\n            Monday of the month).\n\n        leapdays:\n            Will add given days to the date found, if year is a leap\n            year, and the date found is post 28 of february.\n\n        yearday, nlyearday:\n            Set the yearday or the non-leap year day (jump leap days).\n            These are converted to day/month/leapdays information.\n\n    There are relative and absolute forms of the keyword\n    arguments. The plural is relative, and the singular is\n    absolute. For each argument in the order below, the absolute form\n    is applied first (by setting each attribute to that value) and\n    then the relative form (by adding the value to the attribute).\n\n    The order of attributes considered when this relativedelta is\n    added to a datetime is:\n\n    1. Year\n    2. Month\n    3. Day\n    4. Hours\n    5. Minutes\n    6. Seconds\n    7. Microseconds\n\n    Finally, weekday is applied, using the rule described above.\n\n    For example\n\n    >>> from datetime import datetime\n    >>> from dateutil.relativedelta import relativedelta, MO\n    >>> dt = datetime(2018, 4, 9, 13, 37, 0)\n    >>> delta = relativedelta(hours=25, day=1, weekday=MO(1))\n    >>> dt + delta\n    datetime.datetime(2018, 4, 2, 14, 37)\n\n    First, the day is set to 1 (the first of the month), then 25 hours\n    are added, to get to the 2nd day and 14th hour, finally the\n    weekday is applied, but since the 2nd is already a Monday there is\n    no effect.\n\n    ",
        "klass": "dateutil.relativedelta.relativedelta",
        "module": "dateutil"
    },
    {
        "base_classes": [
            "apscheduler.schedulers.blocking.BlockingScheduler"
        ],
        "class_docstring": "\n    A scheduler that runs in the background using a separate thread\n    (:meth:`~apscheduler.schedulers.base.BaseScheduler.start` will return immediately).\n\n    Extra options:\n\n    ========== =============================================================================\n    ``daemon`` Set the ``daemon`` option in the background thread (defaults to ``True``, see\n               `the documentation\n               <https://docs.python.org/3.4/library/threading.html#thread-objects>`_\n               for further details)\n    ========== =============================================================================\n    ",
        "klass": "apscheduler.schedulers.background.BackgroundScheduler",
        "module": "apscheduler"
    },
    {
        "base_classes": [
            "apscheduler.schedulers.base.BaseScheduler"
        ],
        "class_docstring": "\n    A scheduler that runs in the foreground\n    (:meth:`~apscheduler.schedulers.base.BaseScheduler.start` will block).\n    ",
        "klass": "apscheduler.schedulers.blocking.BlockingScheduler",
        "module": "apscheduler"
    },
    {
        "base_classes": [
            "pymongo.common.BaseObject"
        ],
        "class_docstring": "\n    A client-side representation of a MongoDB cluster.\n\n    Instances can represent either a standalone MongoDB server, a replica\n    set, or a sharded cluster. Instances of this class are responsible for\n    maintaining up-to-date state of the cluster, and possibly cache\n    resources related to this, including background threads for monitoring,\n    and connection pools.\n    ",
        "klass": "pymongo.mongo_client.MongoClient",
        "module": "pymongo"
    },
    {
        "base_classes": [
            "email.mime.nonmultipart.MIMENonMultipart"
        ],
        "class_docstring": "Class for generating audio/* MIME documents.",
        "klass": "email.mime.audio.MIMEAudio",
        "module": "email"
    },
    {
        "base_classes": [
            "email.message.Message"
        ],
        "class_docstring": "Base class for MIME specializations.",
        "klass": "email.mime.base.MIMEBase",
        "module": "email"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Manages TCP communication to and from a Redis server",
        "klass": "redis.Connection",
        "module": "redis"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Connection and Pipeline derive from this, implementing how\n    the commands are sent and received to the Redis server\n    ",
        "klass": "redis.client.Redis",
        "module": "redis"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Connection and Pipeline derive from this, implementing how\n    the commands are sent and received to the Redis server\n    ",
        "klass": "redis.Redis",
        "module": "redis"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    ",
        "klass": "redis._compat.Queue",
        "module": "queue"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A shared, distributed Lock. Using Redis for locking allows the Lock\n    to be shared across processes and/or machines.\n\n    It's left to the user to resolve deadlock issues and make sure\n    multiple clients play nicely together.\n    ",
        "klass": "redis.lock.Lock",
        "module": "redis"
    },
    {
        "base_classes": [
            "distutils.version.Version"
        ],
        "class_docstring": "Version numbering for anal retentives and software idealists.\n    Implements the standard interface for version number classes as\n    described above.  A version number consists of two or three\n    dot-separated numeric components, with an optional \"pre-release\" tag\n    on the end.  The pre-release tag consists of the letter 'a' or 'b'\n    followed by a number.  If the numeric components of two version\n    numbers are equal, then one with a pre-release tag will always\n    be deemed earlier (lesser) than one without.\n\n    The following are valid version numbers (shown in the order that\n    would be obtained by sorting according to the supplied cmp function):\n\n        0.4       0.4.0  (these two are equivalent)\n        0.4.1\n        0.5a1\n        0.5b3\n        0.5\n        0.9.6\n        1.0\n        1.0.4a3\n        1.0.4b1\n        1.0.4\n\n    The following are examples of invalid version numbers:\n\n        1\n        2.7.2.2\n        1.3.a4\n        1.3pl1\n        1.3c4\n\n    The rationale for this version numbering system will be explained\n    in the distutils documentation.\n    ",
        "klass": "distutils.version.StrictVersion",
        "module": "distutils"
    },
    {
        "base_classes": [
            "rez.vendor.argparse._AttributeHolder",
            "rez.vendor.argparse._ActionsContainer"
        ],
        "class_docstring": "Object for parsing command line strings into Python objects.\n\n    Keyword Arguments:\n        - prog -- The name of the program (default: sys.argv[0])\n        - usage -- A usage message (default: auto-generated from arguments)\n        - description -- A description of what the program does\n        - epilog -- Text following the argument descriptions\n        - parents -- Parsers whose arguments should be copied into this one\n        - formatter_class -- HelpFormatter class for printing help messages\n        - prefix_chars -- Characters that prefix optional arguments\n        - fromfile_prefix_chars -- Characters that prefix files containing\n            additional arguments\n        - argument_default -- The default value for all arguments\n        - conflict_handler -- String indicating how to handle conflicts\n        - add_help -- Add a -h/-help option\n    ",
        "klass": "rez.vendor.argparse.ArgumentParser",
        "module": "rez"
    },
    {
        "base_classes": [
            "rez.vendor.pygraph.mixins.basegraph.basegraph",
            "rez.vendor.pygraph.mixins.common.common",
            "rez.vendor.pygraph.mixins.labeling.labeling"
        ],
        "class_docstring": "\n    Digraph class.\n    \n    Digraphs are built of nodes and directed edges.\n\n    @sort: __eq__, __init__, __ne__, add_edge, add_node, del_edge, del_node, edges, has_edge, has_node,\n    incidents, neighbors, node_order, nodes \n    ",
        "klass": "rez.vendor.pygraph.classes.digraph.digraph",
        "module": "rez"
    },
    {
        "base_classes": [
            "manila.utils.ComparableMixin"
        ],
        "class_docstring": "This class represents an API Version Request.\n\n    This class includes convenience methods for manipulation\n    and comparison of version numbers as needed to implement\n    API microversions.\n    ",
        "klass": "manila.api.openstack.api_version_request.APIVersionRequest",
        "module": "manila"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tooz coordination wrapper.\n\n    Coordination member id is created from concatenated `prefix` and\n    `agent_id` parameters.\n\n    :param str agent_id: Agent identifier\n    :param str prefix: Used to provide member identifier with a\n    meaningful prefix.\n    ",
        "klass": "manila.coordination.Coordinator",
        "module": "manila"
    },
    {
        "base_classes": [
            "tooz.locking.Lock"
        ],
        "class_docstring": "Lock with dynamic name.\n\n    :param str lock_name: Lock name.\n    :param dict lock_data: Data for lock name formatting.\n    :param coordinator: Coordinator object to use when creating lock.\n        Defaults to the global coordinator.\n\n    Using it like so::\n\n        with Lock('mylock'):\n           ...\n\n    ensures that only one process at a time will execute code in context.\n    Lock name can be formatted using Python format string syntax::\n\n        Lock('foo-{share.id}, {'share': ...,}')\n\n    Available field names are keys of lock_data.\n    ",
        "klass": "manila.coordination.Lock",
        "module": "manila"
    },
    {
        "base_classes": [
            "manila.network.linux.interface.LinuxInterfaceDriver"
        ],
        "class_docstring": "Driver for creating bridge interfaces.",
        "klass": "manila.network.linux.interface.BridgeInterfaceDriver",
        "module": "manila"
    },
    {
        "base_classes": [
            "manila.network.linux.interface.LinuxInterfaceDriver"
        ],
        "class_docstring": "Driver for creating an internal interface on an OVS bridge.",
        "klass": "manila.network.linux.interface.OVSInterfaceDriver",
        "module": "manila"
    },
    {
        "base_classes": [
            "oslo_service.service.Service"
        ],
        "class_docstring": "Service object for binaries running on hosts.\n\n    A service takes a manager and enables rpc by listening to queues based\n    on topic. It also periodically runs tasks on the manager and reports\n    it state to the database services table.\n    ",
        "klass": "manila.service.Service",
        "module": "manila"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class wraps basic building block for NetApp API request.",
        "klass": "manila.share.drivers.netapp.dataontap.client.api.NaElement",
        "module": "manila"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Encapsulates server connection logic.",
        "klass": "manila.share.drivers.netapp.dataontap.client.api.NaServer",
        "module": "manila"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager to force import to return a new module reference.\n\n    This is useful for testing module-level behaviours, such as\n    the emission of a DeprecationWarning on import.\n\n    Use like this:\n\n        with CleanImport(\"foo\"):\n            importlib.import_module(\"foo\") # new reference\n    ",
        "klass": "test.support.CleanImport",
        "module": "test"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "Class to help protect the environment variable properly.  Can be used as\n    a context manager.",
        "klass": "test.support.EnvironmentVarGuard",
        "module": "test"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Try to prevent a crash report from popping up.\n\n    On Windows, don't display the Windows Error Reporting dialog.  On UNIX,\n    disable the creation of coredump file.\n    ",
        "klass": "test.support.SuppressCrashReport",
        "module": "test"
    },
    {
        "base_classes": [
            "tkinter.Variable"
        ],
        "class_docstring": "Value holder for boolean variables.",
        "klass": "tkinter.BooleanVar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Button widget.",
        "klass": "tkinter.Button",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget",
            "tkinter.XView",
            "tkinter.YView"
        ],
        "class_docstring": "Canvas widget to display graphical elements like lines or text.",
        "klass": "tkinter.Canvas",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Checkbutton widget which is either in on- or off-state.",
        "klass": "tkinter.Checkbutton",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget",
            "tkinter.XView"
        ],
        "class_docstring": "Entry widget which allows displaying simple text.",
        "klass": "tkinter.Entry",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Frame widget which may contain other widgets and can have a 3D border.",
        "klass": "tkinter.Frame",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Variable"
        ],
        "class_docstring": "Value holder for integer variables.",
        "klass": "tkinter.IntVar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Label widget which can display text and bitmaps.",
        "klass": "tkinter.Label",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "labelframe widget.",
        "klass": "tkinter.LabelFrame",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget",
            "tkinter.XView",
            "tkinter.YView"
        ],
        "class_docstring": "Listbox widget which can display a list of strings.",
        "klass": "tkinter.Listbox",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Menu widget which allows displaying menu bars, pull-down menus and pop-up menus.",
        "klass": "tkinter.Menu",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Menubutton widget, obsolete since Tk8.0.",
        "klass": "tkinter.Menubutton",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Menubutton"
        ],
        "class_docstring": "OptionMenu which allows the user to select a value from a menu.",
        "klass": "tkinter.OptionMenu",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "panedwindow widget.",
        "klass": "tkinter.PanedWindow",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Image"
        ],
        "class_docstring": "Widget which can display images in PGM, PPM, GIF, PNG format.",
        "klass": "tkinter.PhotoImage",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Radiobutton widget which shows only one of several buttons in on-state.",
        "klass": "tkinter.Radiobutton",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Scale widget which can display a numerical scale.",
        "klass": "tkinter.Scale",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget"
        ],
        "class_docstring": "Scrollbar widget which displays a slider at a certain position.",
        "klass": "tkinter.Scrollbar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Variable"
        ],
        "class_docstring": "Value holder for strings variables.",
        "klass": "tkinter.StringVar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.Widget",
            "tkinter.XView",
            "tkinter.YView"
        ],
        "class_docstring": "Text widget which can display text in various forms.",
        "klass": "tkinter.Text",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.BaseWidget",
            "tkinter.Wm"
        ],
        "class_docstring": "Toplevel widget, e.g. for dialogs.",
        "klass": "tkinter.Toplevel",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to define value holders for e.g. buttons.\n\n    Subclasses StringVar, IntVar, DoubleVar, BooleanVar are specializations\n    that constrain the type of the value returned from get().",
        "klass": "tkinter.Variable",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "psutil.Process"
        ],
        "class_docstring": "A more convenient interface to stdlib subprocess.Popen class.\n    It starts a sub process and deals with it exactly as when using\n    subprocess.Popen class but in addition also provides all the\n    properties and methods of psutil.Process class as a unified\n    interface:\n\n      >>> import psutil\n      >>> from subprocess import PIPE\n      >>> p = psutil.Popen([\"python\", \"-c\", \"print 'hi'\"], stdout=PIPE)\n      >>> p.name()\n      'python'\n      >>> p.uids()\n      user(real=1000, effective=1000, saved=1000)\n      >>> p.username()\n      'giampaolo'\n      >>> p.communicate()\n      ('hi\n', None)\n      >>> p.terminate()\n      >>> p.wait(timeout=2)\n      0\n      >>>\n\n    For method names common to both classes such as kill(), terminate()\n    and wait(), psutil.Process implementation takes precedence.\n\n    Unlike subprocess.Popen this class pre-emptively checks whether PID\n    has been reused on send_signal(), terminate() and kill() so that\n    you don't accidentally terminate another process, fixing\n    http://bugs.python.org/issue6973.\n\n    For a complete documentation refer to:\n    http://docs.python.org/3/library/subprocess.html\n    ",
        "klass": "psutil.Popen",
        "module": "psutil"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents an OS process with the given PID.\n    If PID is omitted current process PID (os.getpid()) is used.\n    Raise NoSuchProcess if PID does not exist.\n\n    Note that most of the methods of this class do not make sure\n    the PID of the process being queried has been reused over time.\n    That means you might end up retrieving an information referring\n    to another process in case the original one this instance\n    refers to is gone in the meantime.\n\n    The only exceptions for which process identity is pre-emptively\n    checked and guaranteed are:\n\n     - parent()\n     - children()\n     - nice() (set)\n     - ionice() (set)\n     - rlimit() (set)\n     - cpu_affinity (set)\n     - suspend()\n     - resume()\n     - send_signal()\n     - terminate()\n     - kill()\n\n    To prevent this problem for all other methods you can:\n     - use is_running() before querying the process\n     - if you're continuously iterating over a set of Process\n       instances use process_iter() which pre-emptively checks\n     process identity for every yielded instance\n    ",
        "klass": "psutil.Process",
        "module": "psutil"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class allows you to send requests to a wrapped application.\n\n    The response wrapper can be a class or factory function that takes\n    three arguments: app_iter, status and headers.  The default response\n    wrapper just returns a tuple.\n\n    Example::\n\n        class ClientResponse(BaseResponse):\n            ...\n\n        client = Client(MyApplication(), response_wrapper=ClientResponse)\n\n    The use_cookies parameter indicates whether cookies should be stored and\n    sent for subsequent requests. This is True by default, but passing False\n    will disable this behaviour.\n\n    If you want to request some subdomain of your application you may set\n    `allow_subdomain_redirects` to `True` as if not no external redirects\n    are allowed.\n\n    .. versionadded:: 0.5\n       `use_cookies` is new in this version.  Older versions did not provide\n       builtin cookie support.\n\n    .. versionadded:: 0.14\n       The `mimetype` parameter was added.\n\n    .. versionadded:: 0.15\n        The ``json`` parameter.\n    ",
        "klass": "werkzeug.test.Client",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class works similar to a :class:`Local` but keeps a stack\n    of objects instead.  This is best explained with an example::\n\n        >>> ls = LocalStack()\n        >>> ls.push(42)\n        >>> ls.top\n        42\n        >>> ls.push(23)\n        >>> ls.top\n        23\n        >>> ls.pop()\n        23\n        >>> ls.top\n        42\n\n    They can be force released by using a :class:`LocalManager` or with\n    the :func:`release_local` function but the correct way is to pop the\n    item from the stack after using.  When the stack is empty it will\n    no longer be bound to the current context (and as such released).\n\n    By calling the stack without arguments it returns a proxy that resolves to\n    the topmost item on the stack.\n\n    .. versionadded:: 0.6.1\n    ",
        "klass": "werkzeug.local.LocalStack",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.datastructures.TypeConversionDict"
        ],
        "class_docstring": "A :class:`MultiDict` is a dictionary subclass customized to deal with\n    multiple values for the same key which is for example used by the parsing\n    functions in the wrappers.  This is necessary because some HTML form\n    elements pass multiple values for the same key.\n\n    :class:`MultiDict` implements all standard dictionary methods.\n    Internally, it saves all values for a key as a list, but the standard dict\n    access methods will only return the first value for a key. If you want to\n    gain access to the other values, too, you have to use the `list` methods as\n    explained below.\n\n    Basic Usage:\n\n    >>> d = MultiDict([('a', 'b'), ('a', 'c')])\n    >>> d\n    MultiDict([('a', 'b'), ('a', 'c')])\n    >>> d['a']\n    'b'\n    >>> d.getlist('a')\n    ['b', 'c']\n    >>> 'a' in d\n    True\n\n    It behaves like a normal dict thus all dict functions will only return the\n    first value when multiple values for one key are found.\n\n    From Werkzeug 0.3 onwards, the `KeyError` raised by this class is also a\n    subclass of the :exc:`~exceptions.BadRequest` HTTP exception and will\n    render a page for a ``400 BAD REQUEST`` if caught in a catch-all for HTTP\n    exceptions.\n\n    A :class:`MultiDict` can be constructed from an iterable of\n    ``(key, value)`` tuples, a dict, a :class:`MultiDict` or from Werkzeug 0.2\n    onwards some keyword parameters.\n\n    :param mapping: the initial value for the :class:`MultiDict`.  Either a\n                    regular dict, an iterable of ``(key, value)`` tuples\n                    or `None`.\n    ",
        "klass": "werkzeug.datastructures.MultiDict",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.wrappers.base_response.BaseResponse",
            "werkzeug.wrappers.etag.ETagResponseMixin",
            "werkzeug.wrappers.response.ResponseStreamMixin",
            "werkzeug.wrappers.common_descriptors.CommonResponseDescriptorsMixin",
            "werkzeug.wrappers.auth.WWWAuthenticateMixin"
        ],
        "class_docstring": "Full featured response object implementing the following mixins:\n\n    - :class:`ETagResponseMixin` for etag and cache control handling\n    - :class:`ResponseStreamMixin` to add support for the `stream` property\n    - :class:`CommonResponseDescriptorsMixin` for various HTTP descriptors\n    - :class:`WWWAuthenticateMixin` for HTTP authentication support\n    ",
        "klass": "werkzeug.Response",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A helper class that creates Atom feeds.\n\n    :param title: the title of the feed. Required.\n    :param title_type: the type attribute for the title element.  One of\n                       ``'html'``, ``'text'`` or ``'xhtml'``.\n    :param url: the url for the feed (not the url *of* the feed)\n    :param id: a globally unique id for the feed.  Must be an URI.  If\n               not present the `feed_url` is used, but one of both is\n               required.\n    :param updated: the time the feed was modified the last time.  Must\n                    be a :class:`datetime.datetime` object.  If not\n                    present the latest entry's `updated` is used.\n                    Treated as UTC if naive datetime.\n    :param feed_url: the URL to the feed.  Should be the URL that was\n                     requested.\n    :param author: the author of the feed.  Must be either a string (the\n                   name) or a dict with name (required) and uri or\n                   email (both optional).  Can be a list of (may be\n                   mixed, too) strings and dicts, too, if there are\n                   multiple authors. Required if not every entry has an\n                   author element.\n    :param icon: an icon for the feed.\n    :param logo: a logo for the feed.\n    :param rights: copyright information for the feed.\n    :param rights_type: the type attribute for the rights element.  One of\n                        ``'html'``, ``'text'`` or ``'xhtml'``.  Default is\n                        ``'text'``.\n    :param subtitle: a short description of the feed.\n    :param subtitle_type: the type attribute for the subtitle element.\n                          One of ``'text'``, ``'html'``, ``'text'``\n                          or ``'xhtml'``.  Default is ``'text'``.\n    :param links: additional links.  Must be a list of dictionaries with\n                  href (required) and rel, type, hreflang, title, length\n                  (all optional)\n    :param generator: the software that generated this feed.  This must be\n                      a tuple in the form ``(name, url, version)``.  If\n                      you don't want to specify one of them, set the item\n                      to `None`.\n    :param entries: a list with the entries for the feed. Entries can also\n                    be added later with :meth:`add`.\n\n    For more information on the elements see\n    http://www.atomenabled.org/developers/syndication/\n\n    Everywhere where a list is demanded, any iterable can be used.\n    ",
        "klass": "werkzeug.contrib.atom.AtomFeed",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.contrib.cache.BaseCache"
        ],
        "class_docstring": "A cache that stores the items on the file system.  This cache depends\n    on being the only user of the `cache_dir`.  Make absolutely sure that\n    nobody but this cache stores files there or otherwise the cache will\n    randomly delete files therein.\n\n    :param cache_dir: the directory where cache files are stored.\n    :param threshold: the maximum number of items the cache stores before\n                      it starts deleting some. A threshold value of 0\n                      indicates no threshold.\n    :param default_timeout: the default timeout that is used if no timeout is\n                            specified on :meth:`~BaseCache.set`. A timeout of\n                            0 indicates that the cache never expires.\n    :param mode: the file mode wanted for the cache files, default 0600\n    ",
        "klass": "werkzeug.contrib.cache.FileSystemCache",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.contrib.cache.BaseCache"
        ],
        "class_docstring": "Uses the Redis key-value store as a cache backend.\n\n    The first argument can be either a string denoting address of the Redis\n    server or an object resembling an instance of a redis.Redis class.\n\n    Note: Python Redis API already takes care of encoding unicode strings on\n    the fly.\n\n    .. versionadded:: 0.7\n\n    .. versionadded:: 0.8\n       `key_prefix` was added.\n\n    .. versionchanged:: 0.8\n       This cache backend now properly serializes objects.\n\n    .. versionchanged:: 0.8.3\n       This cache backend now supports password authentication.\n\n    .. versionchanged:: 0.10\n        ``**kwargs`` is now passed to the redis object.\n\n    :param host: address of the Redis server or an object which API is\n                 compatible with the official Python Redis client (redis-py).\n    :param port: port number on which Redis server listens for connections.\n    :param password: password authentication for the Redis server.\n    :param db: db (zero-based numeric index) on Redis Server to connect.\n    :param default_timeout: the default timeout that is used if no timeout is\n                            specified on :meth:`~BaseCache.set`. A timeout of\n                            0 indicates that the cache never expires.\n    :param key_prefix: A prefix that should be added to all keys.\n\n    Any additional keyword arguments will be passed to ``redis.Redis``.\n    ",
        "klass": "werkzeug.contrib.cache.RedisCache",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.contrib.cache.BaseCache"
        ],
        "class_docstring": "Simple memory cache for single process environments.  This class exists\n    mainly for the development server and is not 100% thread safe.  It tries\n    to use as many atomic operations as possible and no locks for simplicity\n    but it could happen under heavy load that keys are added multiple times.\n\n    :param threshold: the maximum number of items the cache stores before\n                      it starts deleting some.\n    :param default_timeout: the default timeout that is used if no timeout is\n                            specified on :meth:`~BaseCache.set`. A timeout of\n                            0 indicates that the cache never expires.\n    ",
        "klass": "werkzeug.contrib.cache.SimpleCache",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Instances of this object implement an interface compatible with the\n    standard Python :class:`file` object.  Streams are either read-only or\n    write-only depending on how the object is created.\n\n    If the first argument is an iterable a file like object is returned that\n    returns the contents of the iterable.  In case the iterable is empty\n    read operations will return the sentinel value.\n\n    If the first argument is a callable then the stream object will be\n    created and passed to that function.  The caller itself however will\n    not receive a stream but an iterable.  The function will be executed\n    step by step as something iterates over the returned iterable.  Each\n    call to :meth:`flush` will create an item for the iterable.  If\n    :meth:`flush` is called without any writes in-between the sentinel\n    value will be yielded.\n\n    Note for Python 3: due to the incompatible interface of bytes and\n    streams you should set the sentinel value explicitly to an empty\n    bytestring (``b''``) if you are expecting to deal with bytes as\n    otherwise the end of the stream is marked with the wrong sentinel\n    value.\n\n    .. versionadded:: 0.9\n       `sentinel` parameter was added.\n    ",
        "klass": "werkzeug.contrib.iterio.IterIO",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.contrib.sessions.SessionStore"
        ],
        "class_docstring": "Simple example session store that saves sessions on the filesystem.\n    This store works best on POSIX systems and Windows Vista / Windows\n    Server 2008 and newer.\n\n    .. versionchanged:: 0.6\n       `renew_missing` was added.  Previously this was considered `True`,\n       now the default changed to `False` and it can be explicitly\n       deactivated.\n\n    :param path: the path to the folder used for storing the sessions.\n                 If not provided the default temporary directory is used.\n    :param filename_template: a string template used to give the session\n                              a filename.  ``%s`` is replaced with the\n                              session id.\n    :param session_class: The session class to use.  Defaults to\n                          :class:`Session`.\n    :param renew_missing: set to `True` if you want the store to\n                          give the user a new sid if the session was\n                          not yet saved.\n    ",
        "klass": "werkzeug.contrib.sessions.FilesystemSessionStore",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "code(argcount, posonlyargcount, kwonlyargcount, nlocals, stacksize,\n      flags, codestring, constants, names, varnames, filename, name,\n      firstlineno, lnotab[, freevars[, cellvars]])\n\nCreate a code object.  Not for the faint of heart.",
        "klass": "code",
        "module": "code"
    },
    {
        "base_classes": [
            "werkzeug.exceptions.HTTPException"
        ],
        "class_docstring": "*403* `Forbidden`\n\n    Raise if the user doesn't have the permission for the requested resource\n    but was authenticated.\n    ",
        "klass": "werkzeug.exceptions.Forbidden",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.exceptions.HTTPException"
        ],
        "class_docstring": "*404* `Not Found`\n\n    Raise if a resource does not exist and never existed.\n    ",
        "klass": "werkzeug.exceptions.NotFound",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.exceptions.HTTPException"
        ],
        "class_docstring": "*401* ``Unauthorized``\n\n    Raise if the user is not authorized to access a resource.\n\n    The ``www_authenticate`` argument should be used to set the\n    ``WWW-Authenticate`` header. This is used for HTTP basic auth and\n    other schemes. Use :class:`~werkzeug.datastructures.WWWAuthenticate`\n    to create correctly formatted values. Strictly speaking a 401\n    response is invalid if it doesn't provide at least one value for\n    this header, although real clients typically don't care.\n\n    :param description: Override the default message used for the body\n        of the response.\n    :param www-authenticate: A single value, or list of values, for the\n        WWW-Authenticate header.\n\n    .. versionchanged:: 0.15.3\n        If the ``www_authenticate`` argument is not set, the\n        ``WWW-Authenticate`` header is not set.\n\n    .. versionchanged:: 0.15.3\n        The ``response`` argument was restored.\n\n    .. versionchanged:: 0.15.1\n        ``description`` was moved back as the first argument, restoring\n         its previous position.\n\n    .. versionchanged:: 0.15.0\n        ``www_authenticate`` was added as the first argument, ahead of\n        ``description``.\n    ",
        "klass": "werkzeug.exceptions.Unauthorized",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The map class stores all the URL rules and some configuration\n    parameters.  Some of the configuration values are only stored on the\n    `Map` instance since those affect all rules, others are just defaults\n    and can be overridden for each rule.  Note that you have to specify all\n    arguments besides the `rules` as keyword arguments!\n\n    :param rules: sequence of url rules for this map.\n    :param default_subdomain: The default subdomain for rules without a\n                              subdomain defined.\n    :param charset: charset of the url. defaults to ``\"utf-8\"``\n    :param strict_slashes: Take care of trailing slashes.\n    :param redirect_defaults: This will redirect to the default rule if it\n                              wasn't visited that way. This helps creating\n                              unique URLs.\n    :param converters: A dict of converters that adds additional converters\n                       to the list of converters. If you redefine one\n                       converter this will override the original one.\n    :param sort_parameters: If set to `True` the url parameters are sorted.\n                            See `url_encode` for more details.\n    :param sort_key: The sort key function for `url_encode`.\n    :param encoding_errors: the error method to use for decoding\n    :param host_matching: if set to `True` it enables the host matching\n                          feature and disables the subdomain one.  If\n                          enabled the `host` parameter to rules is used\n                          instead of the `subdomain` one.\n\n    .. versionadded:: 0.5\n        `sort_parameters` and `sort_key` was added.\n\n    .. versionadded:: 0.7\n        `encoding_errors` and `host_matching` was added.\n    ",
        "klass": "werkzeug.routing.Map",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Very basic request object.  This does not implement advanced stuff like\n    entity tag parsing or cache controls.  The request object is created with\n    the WSGI environment as first argument and will add itself to the WSGI\n    environment as ``'werkzeug.request'`` unless it's created with\n    `populate_request` set to False.\n\n    There are a couple of mixins available that add additional functionality\n    to the request object, there is also a class called `Request` which\n    subclasses `BaseRequest` and all the important mixins.\n\n    It's a good idea to create a custom subclass of the :class:`BaseRequest`\n    and add missing functionality either via mixins or direct implementation.\n    Here an example for such subclasses::\n\n        from werkzeug.wrappers import BaseRequest, ETagRequestMixin\n\n        class Request(BaseRequest, ETagRequestMixin):\n            pass\n\n    Request objects are **read only**.  As of 0.5 modifications are not\n    allowed in any place.  Unlike the lower level parsing functions the\n    request object will use immutable objects everywhere possible.\n\n    Per default the request object will assume all the text data is `utf-8`\n    encoded.  Please refer to :doc:`the unicode chapter </unicode>` for more\n    details about customizing the behavior.\n\n    Per default the request object will be added to the WSGI\n    environment as `werkzeug.request` to support the debugging system.\n    If you don't want that, set `populate_request` to `False`.\n\n    If `shallow` is `True` the environment is initialized as shallow\n    object around the environ.  Every operation that would modify the\n    environ in any way (such as consuming form data) raises an exception\n    unless the `shallow` attribute is explicitly set to `False`.  This\n    is useful for middlewares where you don't want to consume the form\n    data by accident.  A shallow request is not populated to the WSGI\n    environment.\n\n    .. versionchanged:: 0.5\n       read-only mode was enforced by using immutables classes for all\n       data.\n    ",
        "klass": "werkzeug.wrappers.BaseRequest",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base response class.  The most important fact about a response object\n    is that it's a regular WSGI application.  It's initialized with a couple\n    of response parameters (headers, body, status code etc.) and will start a\n    valid WSGI response when called with the environ and start response\n    callable.\n\n    Because it's a WSGI application itself processing usually ends before the\n    actual response is sent to the server.  This helps debugging systems\n    because they can catch all the exceptions before responses are started.\n\n    Here a small example WSGI application that takes advantage of the\n    response objects::\n\n        from werkzeug.wrappers import BaseResponse as Response\n\n        def index():\n            return Response('Index page')\n\n        def application(environ, start_response):\n            path = environ.get('PATH_INFO') or '/'\n            if path == '/':\n                response = index()\n            else:\n                response = Response('Not Found', status=404)\n            return response(environ, start_response)\n\n    Like :class:`BaseRequest` which object is lacking a lot of functionality\n    implemented in mixins.  This gives you a better control about the actual\n    API of your response objects, so you can create subclasses and add custom\n    functionality.  A full featured response object is available as\n    :class:`Response` which implements a couple of useful mixins.\n\n    To enforce a new type of already existing responses you can use the\n    :meth:`force_type` method.  This is useful if you're working with different\n    subclasses of response objects and you want to post process them with a\n    known interface.\n\n    Per default the response object will assume all the text data is `utf-8`\n    encoded.  Please refer to :doc:`the unicode chapter </unicode>` for more\n    details about customizing the behavior.\n\n    Response can be any kind of iterable or string.  If it's a string it's\n    considered being an iterable with one item which is the string passed.\n    Headers can be a list of tuples or a\n    :class:`~werkzeug.datastructures.Headers` object.\n\n    Special note for `mimetype` and `content_type`:  For most mime types\n    `mimetype` and `content_type` work the same, the difference affects\n    only 'text' mimetypes.  If the mimetype passed with `mimetype` is a\n    mimetype starting with `text/`, the charset parameter of the response\n    object is appended to it.  In contrast the `content_type` parameter is\n    always added as header unmodified.\n\n    .. versionchanged:: 0.5\n       the `direct_passthrough` parameter was added.\n\n    :param response: a string or response iterable.\n    :param status: a string with a status or an integer with the status code.\n    :param headers: a list of headers or a\n                    :class:`~werkzeug.datastructures.Headers` object.\n    :param mimetype: the mimetype for the response.  See notice above.\n    :param content_type: the content type for the response.  See notice above.\n    :param direct_passthrough: if set to `True` :meth:`iter_encoded` is not\n                               called before iteration which makes it\n                               possible to pass special iterators through\n                               unchanged (see :func:`wrap_file` for more\n                               details.)\n    ",
        "klass": "werkzeug.wrappers.BaseResponse",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.wrappers.base_request.BaseRequest",
            "werkzeug.wrappers.accept.AcceptMixin",
            "werkzeug.wrappers.etag.ETagRequestMixin",
            "werkzeug.wrappers.user_agent.UserAgentMixin",
            "werkzeug.wrappers.auth.AuthorizationMixin",
            "werkzeug.wrappers.common_descriptors.CommonRequestDescriptorsMixin"
        ],
        "class_docstring": "Full featured request object implementing the following mixins:\n\n    - :class:`AcceptMixin` for accept header parsing\n    - :class:`ETagRequestMixin` for etag and cache control handling\n    - :class:`UserAgentMixin` for user agent introspection\n    - :class:`AuthorizationMixin` for http auth handling\n    - :class:`CommonRequestDescriptorsMixin` for common headers\n    ",
        "klass": "werkzeug.wrappers.Request",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "werkzeug.wrappers.base_response.BaseResponse",
            "werkzeug.wrappers.etag.ETagResponseMixin",
            "werkzeug.wrappers.response.ResponseStreamMixin",
            "werkzeug.wrappers.common_descriptors.CommonResponseDescriptorsMixin",
            "werkzeug.wrappers.auth.WWWAuthenticateMixin"
        ],
        "class_docstring": "Full featured response object implementing the following mixins:\n\n    - :class:`ETagResponseMixin` for etag and cache control handling\n    - :class:`ResponseStreamMixin` to add support for the `stream` property\n    - :class:`CommonResponseDescriptorsMixin` for various HTTP descriptors\n    - :class:`WWWAuthenticateMixin` for HTTP authentication support\n    ",
        "klass": "werkzeug.wrappers.Response",
        "module": "werkzeug"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An object containing validation information for an xblock instance.\n\n    An instance of this class can be used as a boolean to determine if the xblock has validation issues,\n    where `True` signifies that the xblock passes validation.\n    ",
        "klass": "xblock.validation.Validation",
        "module": "xblock"
    },
    {
        "base_classes": [
            "web_fragments.fragment.Fragment"
        ],
        "class_docstring": "\n    A wrapper around web_fragments.fragment.Fragment that provides\n    backwards compatibility for the old location.\n\n    Deprecated.\n    ",
        "klass": "xblock.fragment.Fragment",
        "module": "xblock"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A fragment of a web page to be included on another page.\n\n    A fragment consists of HTML for the body of the page, and a series of\n    resources needed by the body. Resources are specified with a MIME type\n    (such as \"application/javascript\" or \"text/css\") that determines how they\n    are inserted into the page.  The resource is provided either as literal\n    text, or as a URL.  Text will be included on the page, wrapped\n    appropriately for the MIME type.  URLs will be used as-is on the page.\n\n    Resources are only inserted into the page once, even if many Fragments\n    in the page ask for them.  Determining duplicates is done by simple text\n    matching.\n    ",
        "klass": "web_fragments.fragment.Fragment",
        "module": "web_fragments"
    },
    {
        "base_classes": [
            "yattag.simpledoc.SimpleDoc"
        ],
        "class_docstring": "\n    The Doc class extends the SimpleDoc class with form rendering capabilities. \n    Pass default values or errors as dictionnaries to the Doc constructor, and \n    use the `input`, `textarea`, `select`, `option` methods\n    to append form elements to the document.\n    ",
        "klass": "yattag.Doc",
        "module": "yattag"
    },
    {
        "base_classes": [
            "django.http.response.HttpResponseBase"
        ],
        "class_docstring": "\n    An HTTP response class with a string as content.\n\n    This content that can be read, appended to or replaced.\n    ",
        "klass": "django.http.response.HttpResponse",
        "module": "django"
    },
    {
        "base_classes": [
            "Exception"
        ],
        "class_docstring": "An error while validating data.",
        "klass": "django.core.exceptions.ValidationError",
        "module": "django"
    },
    {
        "base_classes": [
            "pylint.config.OptionsManagerMixIn",
            "pylint.message.message_handler_mix_in.MessagesHandlerMixIn",
            "pylint.reporters.reports_handler_mix_in.ReportsHandlerMixIn",
            "pylint.checkers.base_checker.BaseTokenChecker"
        ],
        "class_docstring": "lint Python modules using external checkers.\n\n    This is the main checker controlling the other ones and the reports\n    generation. It is itself both a raw checker and an astroid checker in order\n    to:\n    * handle message activation / deactivation at the module level\n    * handle some basic but necessary stats'data (number of classes, methods...)\n\n    IDE plugin developers: you may have to call\n    `astroid.builder.MANAGER.astroid_cache.clear()` across runs if you want\n    to ensure the latest code version is actually checked.\n    ",
        "klass": "pylint.lint.PyLinter",
        "module": "pylint"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Wraps a WSGI application in a more convenient interface for\n    testing. It uses extended version of :class:`webob.BaseRequest`\n    and :class:`webob.Response`.\n\n    :param app:\n        May be an WSGI application or Paste Deploy app,\n        like ``'config:filename.ini#test'``.\n\n        .. versionadded:: 2.0\n\n        It can also be an actual full URL to an http server and webtest\n        will proxy requests with `WSGIProxy2\n        <https://pypi.org/project/WSGIProxy2/>`_.\n    :type app:\n        WSGI application\n    :param extra_environ:\n        A dictionary of values that should go\n        into the environment for each request. These can provide a\n        communication channel with the application.\n    :type extra_environ:\n        dict\n    :param relative_to:\n        A directory used for file\n        uploads are calculated relative to this.  Also ``config:``\n        URIs that aren't absolute.\n    :type relative_to:\n        string\n    :param cookiejar:\n        :class:`cookielib.CookieJar` alike API that keeps cookies\n        across requets.\n    :type cookiejar:\n        CookieJar instance\n\n    .. attribute:: cookies\n\n        A convenient shortcut for a dict of all cookies in\n        ``cookiejar``.\n\n    :param parser_features:\n        Passed to BeautifulSoup when parsing responses.\n    :type parser_features:\n        string or list\n    :param json_encoder:\n        Passed to json.dumps when encoding json\n    :type json_encoder:\n        A subclass of json.JSONEncoder\n    :param lint:\n        If True (default) then check that the application is WSGI compliant\n    :type lint:\n        A boolean\n    ",
        "klass": "webtest.TestApp",
        "module": "webtest"
    },
    {
        "base_classes": [
            "chainercv.experimental.links.model.fcis.fcis.FCIS"
        ],
        "class_docstring": "FCIS based on ResNet101.\n\n    When you specify the path of a pre-trained chainer model serialized as\n    a :obj:`.npz` file in the constructor, this chain model automatically\n    initializes all the parameters with it.\n    When a string in prespecified set is provided, a pretrained model is\n    loaded from weights distributed on the Internet.\n    The list of pretrained models supported are as follows:\n\n    * :obj:`sbd`: Loads weights trained with the trainval split of Semantic     Boundaries Dataset.\n\n    For descriptions on the interface of this model, please refer to\n    :class:`~chainercv.experimental.links.model.fcis.FCIS`.\n\n    :class:`~chainercv.experimental.links.model.fcis.FCISResNet101`\n    supports finer control on random initializations of weights by arguments\n    :obj:`resnet_initialW`, :obj:`rpn_initialW` and :obj:`head_initialW`.\n    It accepts a callable that takes an array and edits its values.\n    If :obj:`None` is passed as an initializer, the default initializer is\n    used.\n\n    Args:\n        n_fg_class (int): The number of classes excluding the background.\n        pretrained_model (str): The destination of the pre-trained\n            chainer model serialized as a :obj:`.npz` file.\n            If this is one of the strings described\n            above, it automatically loads weights stored under a directory\n            :obj:`$CHAINER_DATASET_ROOT/pfnet/chainercv/models/`,\n            where :obj:`$CHAINER_DATASET_ROOT` is set as\n            :obj:`$HOME/.chainer/dataset` unless you specify another value\n            by modifying the environment variable.\n        min_size (int): A preprocessing paramter for :meth:`prepare`.\n        max_size (int): A preprocessing paramter for :meth:`prepare`.\n        roi_size (int): Height and width of the feature maps after\n            Position Sensitive RoI pooling.\n        group_size (int): Group height and width for Position Sensitive\n            ROI pooling.\n        ratios (list of floats): This is ratios of width to height of\n            the anchors.\n        anchor_scales (list of numbers): This is areas of anchors.\n            Those areas will be the product of the square of an element in\n            :obj:`anchor_scales` and the original area of the reference\n            window.\n        loc_normalize_mean (tuple of four floats): Mean values of\n            localization estimates.\n        loc_normalize_std (tupler of four floats): Standard deviation\n            of localization estimates.\n        iter2 (bool): if the value is set :obj:`True`, Position Sensitive\n            ROI pooling is executed twice. In the second time, Position\n            Sensitive ROI pooling uses improved ROIs by the localization\n            parameters calculated in the first time.\n        resnet_initialW (callable): Initializer for the layers corresponding to\n            the ResNet101 layers.\n        rpn_initialW (callable): Initializer for Region Proposal Network\n            layers.\n        head_initialW (callable): Initializer for the head layers.\n        proposal_creator_params (dict): Key valued paramters for\n            :class:`~chainercv.links.model.faster_rcnn.ProposalCreator`.\n\n    ",
        "klass": "chainercv.experimental.links.FCISResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.link.Chain"
        ],
        "class_docstring": "Calculate losses for FCIS and report them.\n\n    This is used to train FCIS in the joint training scheme [#FCISCVPR]_.\n\n    The losses include:\n\n    * :obj:`rpn_loc_loss`: The localization loss for         Region Proposal Network (RPN).\n    * :obj:`rpn_cls_loss`: The classification loss for RPN.\n    * :obj:`roi_loc_loss`: The localization loss for the head module.\n    * :obj:`roi_cls_loss`: The classification loss for the head module.\n    * :obj:`roi_mask_loss`: The mask loss for the head module.\n\n    .. [#FCISCVPR] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei.     Fully Convolutional Instance-aware Semantic Segmentation. CVPR 2017.\n\n    Args:\n        fcis (~chainercv.experimental.links.model.fcis.FCIS):\n            A FCIS model for training.\n        rpn_sigma (float): Sigma parameter for the localization loss\n            of Region Proposal Network (RPN). The default value is 3,\n            which is the value used in [#FCISCVPR]_.\n        roi_sigma (float): Sigma paramter for the localization loss of\n            the head. The default value is 1, which is the value used\n            in [#FCISCVPR]_.\n        anchor_target_creator: An instantiation of\n            :class:`~chainercv.links.model.faster_rcnn.AnchorTargetCreator`.\n        proposal_target_creator: An instantiation of\n            :class:`~chainercv.experimental.links.model.fcis.ProposalTargetCreator`.\n\n    ",
        "klass": "chainercv.experimental.links.FCISTrainChain",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.experimental.links.model.pspnet.pspnet.PSPNet"
        ],
        "class_docstring": "PSPNet with Dilated ResNet101 as the feature extractor.\n\n    .. seealso::\n        :class:`chainercv.experimental.links.model.pspnet.PSPNet`\n\n    ",
        "klass": "chainercv.experimental.links.PSPNetResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.yolo.yolo_v2.YOLOv2Base"
        ],
        "class_docstring": "YOLOv2 tiny.\n\n    This is a model of YOLOv2 tiny a.k.a. Tiny YOLO.\n    This model uses :class:`~chainercv.links.model.yolo.DarknetExtractor` as\n    its feature extractor.\n\n    Args:\n        n_fg_class (int): The number of classes excluding the background.\n        pretrained_model (string): The weight file to be loaded.\n            This can take :obj:`'voc0712'`, `filepath` or :obj:`None`.\n            The default value is :obj:`None`.\n\n            * :obj:`'voc0712'`: Load weights trained on trainval split of                 PASCAL VOC 2007 and 2012.                 The weight file is downloaded and cached automatically.                 :obj:`n_fg_class` must be :obj:`20` or :obj:`None`.                 These weights were converted from the darknet model                 provided by `the original implementation                 <https://pjreddie.com/darknet/yolov2/>`_.                 The conversion code is                 `chainercv/examples/yolo/darknet2npz.py`.\n            * `filepath`: A path of npz file. In this case, :obj:`n_fg_class`                 must be specified properly.\n            * :obj:`None`: Do not load weights.\n\n    ",
        "klass": "chainercv.experimental.links.YOLOv2Tiny",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.training.extensions.evaluator.Evaluator"
        ],
        "class_docstring": "An extension that evaluates a detection model by MS COCO metric.\n\n    This extension iterates over an iterator and evaluates the prediction\n    results.\n    The results consist of average precisions (APs) and average\n    recalls (ARs) as well as the mean of each (mean average precision and mean\n    average recall).\n    This extension reports the following values with keys.\n    Please note that if\n    :obj:`label_names` is not specified, only the mAPs and  mARs are reported.\n\n    The underlying dataset of the iterator is assumed to return\n    :obj:`img, bbox, label` or :obj:`img, bbox, label, area, crowded`.\n\n    .. csv-table::\n        :header: key, description\n\n        ap/iou=0.50:0.95/area=all/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_\n        ap/iou=0.50/area=all/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_\n        ap/iou=0.75/area=all/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_\n        ap/iou=0.50:0.95/area=small/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_ [#coco_det_ext_5]_\n        ap/iou=0.50:0.95/area=medium/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_ [#coco_det_ext_5]_\n        ap/iou=0.50:0.95/area=large/max_dets=100/<label_names[l]>,             [#coco_det_ext_1]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=all/max_dets=1/<label_names[l]>,             [#coco_det_ext_2]_\n        ar/iou=0.50/area=all/max_dets=10/<label_names[l]>,             [#coco_det_ext_2]_\n        ar/iou=0.75/area=all/max_dets=100/<label_names[l]>,             [#coco_det_ext_2]_\n        ar/iou=0.50:0.95/area=small/max_dets=100/<label_names[l]>,             [#coco_det_ext_2]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=medium/max_dets=100/<label_names[l]>,             [#coco_det_ext_2]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=large/max_dets=100/<label_names[l]>,             [#coco_det_ext_2]_ [#coco_det_ext_5]_\n        map/iou=0.50:0.95/area=all/max_dets=100,             [#coco_det_ext_3]_\n        map/iou=0.50/area=all/max_dets=100,             [#coco_det_ext_3]_\n        map/iou=0.75/area=all/max_dets=100,             [#coco_det_ext_3]_\n        map/iou=0.50:0.95/area=small/max_dets=100,             [#coco_det_ext_3]_ [#coco_det_ext_5]_\n        map/iou=0.50:0.95/area=medium/max_dets=100,             [#coco_det_ext_3]_ [#coco_det_ext_5]_\n        map/iou=0.50:0.95/area=large/max_dets=100,             [#coco_det_ext_3]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=all/max_dets=1,             [#coco_det_ext_4]_\n        ar/iou=0.50/area=all/max_dets=10,             [#coco_det_ext_4]_\n        ar/iou=0.75/area=all/max_dets=100,             [#coco_det_ext_4]_\n        ar/iou=0.50:0.95/area=small/max_dets=100,             [#coco_det_ext_4]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=medium/max_dets=100,             [#coco_det_ext_4]_ [#coco_det_ext_5]_\n        ar/iou=0.50:0.95/area=large/max_dets=100,             [#coco_det_ext_4]_ [#coco_det_ext_5]_\n\n    .. [#coco_det_ext_1] Average precision for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         If class :math:`l` does not exist in either :obj:`pred_labels` or         :obj:`gt_labels`, the corresponding value is set to :obj:`numpy.nan`.\n    .. [#coco_det_ext_2] Average recall for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         If class :math:`l` does not exist in either :obj:`pred_labels` or         :obj:`gt_labels`, the corresponding value is set to :obj:`numpy.nan`.\n    .. [#coco_det_ext_3] The average of average precisions over classes.\n    .. [#coco_det_ext_4] The average of average recalls over classes.\n    .. [#coco_det_ext_5] Skip if :obj:`gt_areas` is :obj:`None`.\n\n    Args:\n        iterator (chainer.Iterator): An iterator. Each sample should be\n            following tuple :obj:`img, bbox, label, area, crowded`.\n        target (chainer.Link): A detection link. This link must have\n            :meth:`predict` method that takes a list of images and returns\n            :obj:`bboxes`, :obj:`labels` and :obj:`scores`.\n        label_names (iterable of strings): An iterable of names of classes.\n            If this value is specified, average precision and average\n            recalls for each class are reported.\n        comm (~chainermn.communicators.CommunicatorBase):\n            A ChainerMN communicator.\n            If it is specified, this extension scatters the iterator of\n            root worker and gathers the results to the root worker.\n\n    ",
        "klass": "chainercv.extensions.DetectionCOCOEvaluator",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.training.extensions.evaluator.Evaluator"
        ],
        "class_docstring": "An extension that evaluates a detection model by PASCAL VOC metric.\n\n    This extension iterates over an iterator and evaluates the prediction\n    results by average precisions (APs) and mean of them\n    (mean Average Precision, mAP).\n    This extension reports the following values with keys.\n    Please note that :obj:`'ap/<label_names[l]>'` is reported only if\n    :obj:`label_names` is specified.\n\n    * :obj:`'map'`: Mean of average precisions (mAP).\n    * :obj:`'ap/<label_names[l]>'`: Average precision for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         For example, this evaluator reports :obj:`'ap/aeroplane'`,         :obj:`'ap/bicycle'`, etc. if :obj:`label_names` is         :obj:`~chainercv.datasets.voc_bbox_label_names`.         If there is no bounding box assigned to class :obj:`label_names[l]`         in either ground truth or prediction, it reports :obj:`numpy.nan` as         its average precision.         In this case, mAP is computed without this class.\n\n    Args:\n        iterator (chainer.Iterator): An iterator. Each sample should be\n            following tuple :obj:`img, bbox, label` or\n            :obj:`img, bbox, label, difficult`.\n            :obj:`img` is an image, :obj:`bbox` is coordinates of bounding\n            boxes, :obj:`label` is labels of the bounding boxes and\n            :obj:`difficult` is whether the bounding boxes are difficult or\n            not. If :obj:`difficult` is returned, difficult ground truth\n            will be ignored from evaluation.\n        target (chainer.Link): A detection link. This link must have\n            :meth:`predict` method that takes a list of images and returns\n            :obj:`bboxes`, :obj:`labels` and :obj:`scores`.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n        label_names (iterable of strings): An iterable of names of classes.\n            If this value is specified, average precision for each class is\n            also reported with the key :obj:`'ap/<label_names[l]>'`.\n        comm (~chainermn.communicators.CommunicatorBase):\n            A ChainerMN communicator.\n            If it is specified, this extension scatters the iterator of\n            root worker and gathers the results to the root worker.\n\n    ",
        "klass": "chainercv.extensions.DetectionVOCEvaluator",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.training.extensions.evaluator.Evaluator"
        ],
        "class_docstring": "An extension that evaluates a instance segmentation model by MS COCO metric.\n\n    This extension iterates over an iterator and evaluates the prediction\n    results.\n    The results consist of average precisions (APs) and average\n    recalls (ARs) as well as the mean of each (mean average precision and mean\n    average recall).\n    This extension reports the following values with keys.\n    Please note that if\n    :obj:`label_names` is not specified, only the mAPs and  mARs are reported.\n\n    The underlying dataset of the iterator is assumed to return\n    :obj:`img, mask, label` or :obj:`img, mask, label, area, crowded`.\n\n    .. csv-table::\n        :header: key, description\n\n        ap/iou=0.50:0.95/area=all/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_\n        ap/iou=0.50/area=all/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_\n        ap/iou=0.75/area=all/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_\n        ap/iou=0.50:0.95/area=small/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_ [#coco_ins_ext_5]_\n        ap/iou=0.50:0.95/area=medium/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_ [#coco_ins_ext_5]_\n        ap/iou=0.50:0.95/area=large/max_dets=100/<label_names[l]>,             [#coco_ins_ext_1]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=all/max_dets=1/<label_names[l]>,             [#coco_ins_ext_2]_\n        ar/iou=0.50/area=all/max_dets=10/<label_names[l]>,             [#coco_ins_ext_2]_\n        ar/iou=0.75/area=all/max_dets=100/<label_names[l]>,             [#coco_ins_ext_2]_\n        ar/iou=0.50:0.95/area=small/max_dets=100/<label_names[l]>,             [#coco_ins_ext_2]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=medium/max_dets=100/<label_names[l]>,             [#coco_ins_ext_2]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=large/max_dets=100/<label_names[l]>,             [#coco_ins_ext_2]_ [#coco_ins_ext_5]_\n        map/iou=0.50:0.95/area=all/max_dets=100,             [#coco_ins_ext_3]_\n        map/iou=0.50/area=all/max_dets=100,             [#coco_ins_ext_3]_\n        map/iou=0.75/area=all/max_dets=100,             [#coco_ins_ext_3]_\n        map/iou=0.50:0.95/area=small/max_dets=100,             [#coco_ins_ext_3]_ [#coco_ins_ext_5]_\n        map/iou=0.50:0.95/area=medium/max_dets=100,             [#coco_ins_ext_3]_ [#coco_ins_ext_5]_\n        map/iou=0.50:0.95/area=large/max_dets=100,             [#coco_ins_ext_3]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=all/max_dets=1,             [#coco_ins_ext_4]_\n        ar/iou=0.50/area=all/max_dets=10,             [#coco_ins_ext_4]_\n        ar/iou=0.75/area=all/max_dets=100,             [#coco_ins_ext_4]_\n        ar/iou=0.50:0.95/area=small/max_dets=100,             [#coco_ins_ext_4]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=medium/max_dets=100,             [#coco_ins_ext_4]_ [#coco_ins_ext_5]_\n        ar/iou=0.50:0.95/area=large/max_dets=100,             [#coco_ins_ext_4]_ [#coco_ins_ext_5]_\n\n    .. [#coco_ins_ext_1] Average precision for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         If class :math:`l` does not exist in either :obj:`pred_labels` or         :obj:`gt_labels`, the corresponding value is set to :obj:`numpy.nan`.\n    .. [#coco_ins_ext_2] Average recall for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         If class :math:`l` does not exist in either :obj:`pred_labels` or         :obj:`gt_labels`, the corresponding value is set to :obj:`numpy.nan`.\n    .. [#coco_ins_ext_3] The average of average precisions over classes.\n    .. [#coco_ins_ext_4] The average of average recalls over classes.\n    .. [#coco_ins_ext_5] Skip if :obj:`gt_areas` is :obj:`None`.\n\n    Args:\n        iterator (chainer.Iterator): An iterator. Each sample should be\n            following tuple :obj:`img, mask, label, area, crowded`.\n        target (chainer.Link): A detection link. This link must have\n            :meth:`predict` method that takes a list of images and returns\n            :obj:`masks`, :obj:`labels` and :obj:`scores`.\n        label_names (iterable of strings): An iterable of names of classes.\n            If this value is specified, average precision and average\n            recalls for each class are reported.\n        comm (~chainermn.communicators.CommunicatorBase):\n            A ChainerMN communicator.\n            If it is specified, this extension scatters the iterator of\n            root worker and gathers the results to the root worker.\n\n    ",
        "klass": "chainercv.extensions.InstanceSegmentationCOCOEvaluator",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.training.extensions.evaluator.Evaluator"
        ],
        "class_docstring": "An evaluation extension of instance-segmentation by PASCAL VOC metric.\n\n    This extension iterates over an iterator and evaluates the prediction\n    results by average precisions (APs) and mean of them\n    (mean Average Precision, mAP).\n    This extension reports the following values with keys.\n    Please note that :obj:`'ap/<label_names[l]>'` is reported only if\n    :obj:`label_names` is specified.\n\n    * :obj:`'map'`: Mean of average precisions (mAP).\n    * :obj:`'ap/<label_names[l]>'`: Average precision for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         For example, this evaluator reports :obj:`'ap/aeroplane'`,         :obj:`'ap/bicycle'`, etc. if :obj:`label_names` is         :obj:`~chainercv.datasets.sbd_instance_segmentation_label_names`.         If there is no bounding box assigned to class :obj:`label_names[l]`         in either ground truth or prediction, it reports :obj:`numpy.nan` as         its average precision.         In this case, mAP is computed without this class.\n\n    Args:\n        iterator (chainer.Iterator): An iterator. Each sample should be\n            following tuple :obj:`img, bbox, label` or\n            :obj:`img, bbox, label, difficult`.\n            :obj:`img` is an image, :obj:`bbox` is coordinates of bounding\n            boxes, :obj:`label` is labels of the bounding boxes and\n            :obj:`difficult` is whether the bounding boxes are difficult or\n            not. If :obj:`difficult` is returned, difficult ground truth\n            will be ignored from evaluation.\n        target (chainer.Link): An instance-segmentation link. This link must\n            have :meth:`predict` method that takes a list of images and returns\n            :obj:`bboxes`, :obj:`labels` and :obj:`scores`.\n        iou_thresh (float): Intersection over Union (IoU) threshold for\n            calulating average precision. The default value is 0.5.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n        label_names (iterable of strings): An iterable of names of classes.\n            If this value is specified, average precision for each class is\n            also reported with the key :obj:`'ap/<label_names[l]>'`.\n        comm (~chainermn.communicators.CommunicatorBase):\n            A ChainerMN communicator.\n            If it is specified, this extension scatters the iterator of\n            root worker and gathers the results to the root worker.\n\n    ",
        "klass": "chainercv.extensions.InstanceSegmentationVOCEvaluator",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.training.extensions.evaluator.Evaluator"
        ],
        "class_docstring": "An extension that evaluates a semantic segmentation model.\n\n    This extension iterates over an iterator and evaluates the prediction\n    results of the model by common evaluation metrics for semantic\n    segmentation.\n    This extension reports values with keys below.\n    Please note that :obj:`'iou/<label_names[l]>'` and\n    :obj:`'class_accuracy/<label_names[l]>'` are reported only if\n    :obj:`label_names` is specified.\n\n    * :obj:`'miou'`: Mean of IoUs (mIoU).\n    * :obj:`'iou/<label_names[l]>'`: IoU for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.         For example, if :obj:`label_names` is         :obj:`~chainercv.datasets.camvid_label_names`,         this evaluator reports :obj:`'iou/Sky'`,         :obj:`'ap/Building'`, etc.\n    * :obj:`'mean_class_accuracy'`: Mean of class accuracies.\n    * :obj:`'class_accuracy/<label_names[l]>'`: Class accuracy for class         :obj:`label_names[l]`, where :math:`l` is the index of the class.\n    * :obj:`'pixel_accuracy'`: Pixel accuracy.\n\n    If there is no label assigned to class :obj:`label_names[l]`\n    in the ground truth, values corresponding to keys\n    :obj:`'iou/<label_names[l]>'` and :obj:`'class_accuracy/<label_names[l]>'`\n    are :obj:`numpy.nan`.\n    In that case, the means of them are calculated by excluding them from\n    calculation.\n\n    For details on the evaluation metrics, please see the documentation\n    for :func:`chainercv.evaluations.eval_semantic_segmentation`.\n\n    .. seealso::\n        :func:`chainercv.evaluations.eval_semantic_segmentation`.\n\n    Args:\n        iterator (chainer.Iterator): An iterator. Each sample should be\n            following tuple :obj:`img, label`.\n            :obj:`img` is an image, :obj:`label` is pixel-wise label.\n        target (chainer.Link): A semantic segmentation link. This link should\n            have :meth:`predict` method that takes a list of images and\n            returns :obj:`labels`.\n        label_names (iterable of strings): An iterable of names of classes.\n            If this value is specified, IoU and class accuracy for each class\n            are also reported with the keys\n            :obj:`'iou/<label_names[l]>'` and\n            :obj:`'class_accuracy/<label_names[l]>'`.\n        comm (~chainermn.communicators.CommunicatorBase):\n            A ChainerMN communicator.\n            If it is specified, this extension scatters the iterator of\n            root worker and gathers the results to the root worker.\n\n    ",
        "klass": "chainercv.extensions.SemanticSegmentationEvaluator",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.fpn.faster_rcnn_fpn_resnet.FasterRCNNFPNResNet"
        ],
        "class_docstring": "Faster R-CNN with ResNet-101 and FPN.\n\n    Please refer to :class:`~chainercv.links.model.fpn.FasterRCNNFPNResNet`.\n\n    ",
        "klass": "chainercv.links.FasterRCNNFPNResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.fpn.faster_rcnn_fpn_resnet.FasterRCNNFPNResNet"
        ],
        "class_docstring": "Faster R-CNN with ResNet-50 and FPN.\n\n    Please refer to :class:`~chainercv.links.model.fpn.FasterRCNNFPNResNet`.\n\n    ",
        "klass": "chainercv.links.FasterRCNNFPNResNet50",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.faster_rcnn.faster_rcnn.FasterRCNN"
        ],
        "class_docstring": "Faster R-CNN based on VGG-16.\n\n    When you specify the path of a pre-trained chainer model serialized as\n    a :obj:`.npz` file in the constructor, this chain model automatically\n    initializes all the parameters with it.\n    When a string in prespecified set is provided, a pretrained model is\n    loaded from weights distributed on the Internet.\n    The list of pretrained models supported are as follows:\n\n    * :obj:`voc07`: Loads weights trained with the trainval split of         PASCAL VOC2007 Detection Dataset.\n    * :obj:`imagenet`: Loads weights trained with ImageNet Classfication         task for the feature extractor and the head modules.         Weights that do not have a corresponding layer in VGG-16         will be randomly initialized.\n\n    For descriptions on the interface of this model, please refer to\n    :class:`~chainercv.links.model.faster_rcnn.FasterRCNN`.\n\n    :class:`~chainercv.links.model.faster_rcnn.FasterRCNNVGG16`\n    supports finer control on random initializations of weights by arguments\n    :obj:`vgg_initialW`, :obj:`rpn_initialW`, :obj:`loc_initialW` and\n    :obj:`score_initialW`.\n    It accepts a callable that takes an array and edits its values.\n    If :obj:`None` is passed as an initializer, the default initializer is\n    used.\n\n    Args:\n        n_fg_class (int): The number of classes excluding the background.\n        pretrained_model (string): The destination of the pre-trained\n            chainer model serialized as a :obj:`.npz` file.\n            If this is one of the strings described\n            above, it automatically loads weights stored under a directory\n            :obj:`$CHAINER_DATASET_ROOT/pfnet/chainercv/models/`,\n            where :obj:`$CHAINER_DATASET_ROOT` is set as\n            :obj:`$HOME/.chainer/dataset` unless you specify another value\n            by modifying the environment variable.\n        min_size (int): A preprocessing paramter for :meth:`prepare`.\n        max_size (int): A preprocessing paramter for :meth:`prepare`.\n        ratios (list of floats): This is ratios of width to height of\n            the anchors.\n        anchor_scales (list of numbers): This is areas of anchors.\n            Those areas will be the product of the square of an element in\n            :obj:`anchor_scales` and the original area of the reference\n            window.\n        vgg_initialW (callable): Initializer for the layers corresponding to\n            the VGG-16 layers.\n        rpn_initialW (callable): Initializer for Region Proposal Network\n            layers.\n        loc_initialW (callable): Initializer for the localization head.\n        score_initialW (callable): Initializer for the score head.\n        proposal_creator_params (dict): Key valued paramters for\n            :class:`~chainercv.links.model.faster_rcnn.ProposalCreator`.\n\n    ",
        "klass": "chainercv.links.FasterRCNNVGG16",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.fpn.faster_rcnn_fpn_resnet.MaskRCNNFPNResNet"
        ],
        "class_docstring": "Mask R-CNN with ResNet-101 and FPN.\n\n    Please refer to :class:`~chainercv.links.model.fpn.FasterRCNNFPNResNet`.\n\n    ",
        "klass": "chainercv.links.MaskRCNNFPNResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.fpn.faster_rcnn_fpn_resnet.MaskRCNNFPNResNet"
        ],
        "class_docstring": "Mask R-CNN with ResNet-50 and FPN.\n\n    Please refer to :class:`~chainercv.links.model.fpn.FasterRCNNFPNResNet`.\n\n    ",
        "klass": "chainercv.links.MaskRCNNFPNResNet50",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.link.Chain"
        ],
        "class_docstring": "A pixel-wise classifier.\n\n    It computes the loss based on a given input/label pair for\n    semantic segmentation.\n\n    Args:\n        predictor (~chainer.Link): Predictor network.\n        ignore_label (int): A class id that is going to be ignored in\n            evaluation. The default value is -1.\n        class_weight (array): An array\n            that contains constant weights that will be multiplied with the\n            loss values along with the channel dimension. This will be\n            used in :func:`chainer.functions.softmax_cross_entropy`.\n\n    ",
        "klass": "chainercv.links.PixelwiseSoftmaxClassifier",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.resnet.resnet.ResNet"
        ],
        "class_docstring": "ResNet-101 Network.\n\n    Please consult the documentation for :class:`ResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.resnet.ResNet`\n\n    ",
        "klass": "chainercv.links.ResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.resnet.resnet.ResNet"
        ],
        "class_docstring": "ResNet-152 Network.\n\n    Please consult the documentation for :class:`ResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.resnet.ResNet`\n\n    ",
        "klass": "chainercv.links.ResNet152",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.resnet.resnet.ResNet"
        ],
        "class_docstring": "ResNet-50 Network.\n\n    Please consult the documentation for :class:`ResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.resnet.ResNet`\n\n    ",
        "klass": "chainercv.links.ResNet50",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.senet.se_resnext.SEResNeXt"
        ],
        "class_docstring": "SE-ResNeXt-101 Network\n\n    Please consult the documentation for :class:`SEResNeXt`.\n\n    .. seealso::\n        :class:`chainercv.links.model.senet.SEResNeXt`\n\n    ",
        "klass": "chainercv.links.SEResNeXt101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.senet.se_resnext.SEResNeXt"
        ],
        "class_docstring": "SE-ResNeXt-50 Network\n\n    Please consult the documentation for :class:`SEResNeXt`.\n\n    .. seealso::\n        :class:`chainercv.links.model.senet.SEResNeXt`\n\n    ",
        "klass": "chainercv.links.SEResNeXt50",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.senet.se_resnet.SEResNet"
        ],
        "class_docstring": "SE-ResNet-101 Network.\n\n    Please consult the documentation for :class:`SEResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.senet.SEResNet`\n\n    ",
        "klass": "chainercv.links.SEResNet101",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.senet.se_resnet.SEResNet"
        ],
        "class_docstring": "SE-ResNet-152 Network.\n\n    Please consult the documentation for :class:`SEResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.senet.SEResNet`\n\n    ",
        "klass": "chainercv.links.SEResNet152",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.senet.se_resnet.SEResNet"
        ],
        "class_docstring": "SE-ResNet-50 Network.\n\n    Please consult the documentation for :class:`SEResNet`.\n\n    .. seealso::\n        :class:`chainercv.links.model.senet.SEResNet`\n\n    ",
        "klass": "chainercv.links.SEResNet50",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.ssd.ssd.SSD"
        ],
        "class_docstring": "Single Shot Multibox Detector with 300x300 inputs.\n\n    This is a model of Single Shot Multibox Detector [#]_.\n    This model uses :class:`~chainercv.links.model.ssd.VGG16Extractor300` as\n    its feature extractor.\n\n    .. [#] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,\n       Scott Reed, Cheng-Yang Fu, Alexander C. Berg.\n       SSD: Single Shot MultiBox Detector. ECCV 2016.\n\n    Args:\n       n_fg_class (int): The number of classes excluding the background.\n       pretrained_model (string): The weight file to be loaded.\n           This can take :obj:`'voc0712'`, `filepath` or :obj:`None`.\n           The default value is :obj:`None`.\n\n            * :obj:`'voc0712'`: Load weights trained on trainval split of                 PASCAL VOC 2007 and 2012.                 The weight file is downloaded and cached automatically.                 :obj:`n_fg_class` must be :obj:`20` or :obj:`None`.                 These weights were converted from the Caffe model provided by                 `the original implementation                 <https://github.com/weiliu89/caffe/tree/ssd>`_.                 The conversion code is `chainercv/examples/ssd/caffe2npz.py`.\n            * :obj:`'imagenet'`: Load weights of VGG-16 trained on ImageNet.                 The weight file is downloaded and cached automatically.                 This option initializes weights partially and the rests are                 initialized randomly. In this case, :obj:`n_fg_class`                 can be set to any number.\n            * `filepath`: A path of npz file. In this case, :obj:`n_fg_class`                 must be specified properly.\n            * :obj:`None`: Do not load weights.\n\n    ",
        "klass": "chainercv.links.SSD300",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.ssd.ssd.SSD"
        ],
        "class_docstring": "Single Shot Multibox Detector with 512x512 inputs.\n\n    This is a model of Single Shot Multibox Detector [#]_.\n    This model uses :class:`~chainercv.links.model.ssd.VGG16Extractor512` as\n    its feature extractor.\n\n    .. [#] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,\n       Scott Reed, Cheng-Yang Fu, Alexander C. Berg.\n       SSD: Single Shot MultiBox Detector. ECCV 2016.\n\n    Args:\n       n_fg_class (int): The number of classes excluding the background.\n       pretrained_model (string): The weight file to be loaded.\n           This can take :obj:`'voc0712'`, `filepath` or :obj:`None`.\n           The default value is :obj:`None`.\n\n            * :obj:`'voc0712'`: Load weights trained on trainval split of                 PASCAL VOC 2007 and 2012.                 The weight file is downloaded and cached automatically.                 :obj:`n_fg_class` must be :obj:`20` or :obj:`None`.                 These weights were converted from the Caffe model provided by                 `the original implementation                 <https://github.com/weiliu89/caffe/tree/ssd>`_.                 The conversion code is `chainercv/examples/ssd/caffe2npz.py`.\n            * :obj:`'imagenet'`: Load weights of VGG-16 trained on ImageNet.                 The weight file is downloaded and cached automatically.                 This option initializes weights partially and the rests are                 initialized randomly. In this case, :obj:`n_fg_class`                 can be set to any number.\n            * `filepath`: A path of npz file. In this case, :obj:`n_fg_class`                 must be specified properly.\n            * :obj:`None`: Do not load weights.\n\n    ",
        "klass": "chainercv.links.SSD512",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainer.link.Chain"
        ],
        "class_docstring": "SegNet Basic for semantic segmentation.\n\n    This is a SegNet [#]_ model for semantic segmenation. This is based on\n    SegNetBasic model that is found here_.\n\n    When you specify the path of a pretrained chainer model serialized as\n    a :obj:`.npz` file in the constructor, this chain model automatically\n    initializes all the parameters with it.\n    When a string in prespecified set is provided, a pretrained model is\n    loaded from weights distributed on the Internet.\n    The list of pretrained models supported are as follows:\n\n    * :obj:`camvid`: Loads weights trained with the train split of         CamVid dataset.\n\n    .. [#] Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla \"SegNet: A     Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\"     PAMI, 2017\n\n    .. _here: http://github.com/alexgkendall/SegNet-Tutorial\n\n    Args:\n        n_class (int): The number of classes. If :obj:`None`, it can\n            be infered if :obj:`pretrained_model` is given.\n        pretrained_model (string): The destination of the pretrained\n            chainer model serialized as a :obj:`.npz` file.\n            If this is one of the strings described\n            above, it automatically loads weights stored under a directory\n            :obj:`$CHAINER_DATASET_ROOT/pfnet/chainercv/models/`,\n            where :obj:`$CHAINER_DATASET_ROOT` is set as\n            :obj:`$HOME/.chainer/dataset` unless you specify another value\n            by modifying the environment variable.\n        initialW (callable): Initializer for convolution layers.\n\n    ",
        "klass": "chainercv.links.SegNetBasic",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.yolo.yolo_v2.YOLOv2Base"
        ],
        "class_docstring": "YOLOv2.\n\n    This is a model of YOLOv2 [#]_.\n    This model uses :class:`~chainercv.links.model.yolo.Darknet19Extractor` as\n    its feature extractor.\n\n    .. [#] Joseph Redmon, Ali Farhadi.\n       YOLO9000: Better, Faster, Stronger. CVPR 2017.\n\n    Args:\n        n_fg_class (int): The number of classes excluding the background.\n        pretrained_model (string): The weight file to be loaded.\n            This can take :obj:`'voc0712'`, `filepath` or :obj:`None`.\n            The default value is :obj:`None`.\n\n            * :obj:`'voc0712'`: Load weights trained on trainval split of                 PASCAL VOC 2007 and 2012.                 The weight file is downloaded and cached automatically.                 :obj:`n_fg_class` must be :obj:`20` or :obj:`None`.                 These weights were converted from the darknet model                 provided by `the original implementation                 <https://pjreddie.com/darknet/yolov2/>`_.                 The conversion code is                 `chainercv/examples/yolo/darknet2npz.py`.\n            * `filepath`: A path of npz file. In this case, :obj:`n_fg_class`                 must be specified properly.\n            * :obj:`None`: Do not load weights.\n\n    ",
        "klass": "chainercv.links.YOLOv2",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "chainercv.links.model.yolo.yolo_base.YOLOBase"
        ],
        "class_docstring": "YOLOv3.\n\n    This is a model of YOLOv3 [#]_.\n    This model uses :class:`~chainercv.links.model.yolo.Darknet53Extractor` as\n    its feature extractor.\n\n    .. [#] Joseph Redmon, Ali Farhadi.\n       YOLOv3: An Incremental Improvement. arXiv 2018.\n\n    Args:\n       n_fg_class (int): The number of classes excluding the background.\n       pretrained_model (string): The weight file to be loaded.\n           This can take :obj:`'voc0712'`, `filepath` or :obj:`None`.\n           The default value is :obj:`None`.\n\n            * :obj:`'voc0712'`: Load weights trained on trainval split of                 PASCAL VOC 2007 and 2012.                 The weight file is downloaded and cached automatically.                 :obj:`n_fg_class` must be :obj:`20` or :obj:`None`.                 These weights were converted from the darknet model.                 The conversion code is                 `chainercv/examples/yolo/darknet2npz.py`.\n            * `filepath`: A path of npz file. In this case, :obj:`n_fg_class`                 must be specified properly.\n            * :obj:`None`: Do not load weights.\n\n    ",
        "klass": "chainercv.links.YOLOv3",
        "module": "chainercv"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "ThreadGroup object.\n\n    It is advised to use TreadGroup as a context manager instead\n    of instantiating and calling _wait() manually. The __exit__()\n    guaranties to exit only after all child threads are done, even if\n    spawning code have thrown an exception\n    ",
        "klass": "sahara.context.ThreadGroup",
        "module": "sahara"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A list of files built by on exploring the filesystem and filtered by\n    applying various patterns to what we find there.\n\n    Instance attributes:\n      dir\n        directory from which files will be taken -- only used if\n        'allfiles' not supplied to constructor\n      files\n        list of filenames currently being built/filtered/manipulated\n      allfiles\n        complete list of files under consideration (ie. without any\n        filtering applied)\n    ",
        "klass": "distutils.filelist.FileList",
        "module": "distutils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a compiled template.\n\n    :class:`.Template` includes a reference to the original\n    template source (via the :attr:`.source` attribute)\n    as well as the source code of the\n    generated Python module (i.e. the :attr:`.code` attribute),\n    as well as a reference to an actual Python module.\n\n    :class:`.Template` is constructed using either a literal string\n    representing the template text, or a filename representing a filesystem\n    path to a source file.\n\n    :param text: textual template source.  This argument is mutually\n     exclusive versus the ``filename`` parameter.\n\n    :param filename: filename of the source template.  This argument is\n     mutually exclusive versus the ``text`` parameter.\n\n    :param buffer_filters: string list of filters to be applied\n     to the output of ``%def``\\ s which are buffered, cached, or otherwise\n     filtered, after all filters\n     defined with the ``%def`` itself have been applied. Allows the\n     creation of default expression filters that let the output\n     of return-valued ``%def``\\ s \"opt out\" of that filtering via\n     passing special attributes or objects.\n\n    :param bytestring_passthrough: When ``True``, and ``output_encoding`` is\n     set to ``None``, and :meth:`.Template.render` is used to render,\n     the `StringIO` or `cStringIO` buffer will be used instead of the\n     default \"fast\" buffer.   This allows raw bytestrings in the\n     output stream, such as in expressions, to pass straight\n     through to the buffer.  This flag is forced\n     to ``True`` if ``disable_unicode`` is also configured.\n\n     .. versionadded:: 0.4\n        Added to provide the same behavior as that of the previous series.\n\n    :param cache_args: Dictionary of cache configuration arguments that\n     will be passed to the :class:`.CacheImpl`.   See :ref:`caching_toplevel`.\n\n    :param cache_dir:\n\n     .. deprecated:: 0.6\n        Use the ``'dir'`` argument in the ``cache_args`` dictionary.\n        See :ref:`caching_toplevel`.\n\n    :param cache_enabled: Boolean flag which enables caching of this\n     template.  See :ref:`caching_toplevel`.\n\n    :param cache_impl: String name of a :class:`.CacheImpl` caching\n     implementation to use.   Defaults to ``'beaker'``.\n\n    :param cache_type:\n\n     .. deprecated:: 0.6\n        Use the ``'type'`` argument in the ``cache_args`` dictionary.\n        See :ref:`caching_toplevel`.\n\n    :param cache_url:\n\n     .. deprecated:: 0.6\n        Use the ``'url'`` argument in the ``cache_args`` dictionary.\n        See :ref:`caching_toplevel`.\n\n    :param default_filters: List of string filter names that will\n     be applied to all expressions.  See :ref:`filtering_default_filters`.\n\n    :param disable_unicode: Disables all awareness of Python Unicode\n     objects.  See :ref:`unicode_disabled`.\n\n    :param enable_loop: When ``True``, enable the ``loop`` context variable.\n     This can be set to ``False`` to support templates that may\n     be making usage of the name \"``loop``\".   Individual templates can\n     re-enable the \"loop\" context by placing the directive\n     ``enable_loop=\"True\"`` inside the ``<%page>`` tag -- see\n     :ref:`migrating_loop`.\n\n    :param encoding_errors: Error parameter passed to ``encode()`` when\n     string encoding is performed. See :ref:`usage_unicode`.\n\n    :param error_handler: Python callable which is called whenever\n     compile or runtime exceptions occur. The callable is passed\n     the current context as well as the exception. If the\n     callable returns ``True``, the exception is considered to\n     be handled, else it is re-raised after the function\n     completes. Is used to provide custom error-rendering\n     functions.\n\n     .. seealso::\n\n        :paramref:`.Template.include_error_handler` - include-specific\n        error handler function\n\n    :param format_exceptions: if ``True``, exceptions which occur during\n     the render phase of this template will be caught and\n     formatted into an HTML error page, which then becomes the\n     rendered result of the :meth:`.render` call. Otherwise,\n     runtime exceptions are propagated outwards.\n\n    :param imports: String list of Python statements, typically individual\n     \"import\" lines, which will be placed into the module level\n     preamble of all generated Python modules. See the example\n     in :ref:`filtering_default_filters`.\n\n    :param future_imports: String list of names to import from `__future__`.\n     These will be concatenated into a comma-separated string and inserted\n     into the beginning of the template, e.g. ``futures_imports=['FOO',\n     'BAR']`` results in ``from __future__ import FOO, BAR``.  If you're\n     interested in using features like the new division operator, you must\n     use future_imports to convey that to the renderer, as otherwise the\n     import will not appear as the first executed statement in the generated\n     code and will therefore not have the desired effect.\n\n    :param include_error_handler: An error handler that runs when this template\n     is included within another one via the ``<%include>`` tag, and raises an\n     error.  Compare to the :paramref:`.Template.error_handler` option.\n\n     .. versionadded:: 1.0.6\n\n     .. seealso::\n\n        :paramref:`.Template.error_handler` - top-level error handler function\n\n    :param input_encoding: Encoding of the template's source code.  Can\n     be used in lieu of the coding comment. See\n     :ref:`usage_unicode` as well as :ref:`unicode_toplevel` for\n     details on source encoding.\n\n    :param lookup: a :class:`.TemplateLookup` instance that will be used\n     for all file lookups via the ``<%namespace>``,\n     ``<%include>``, and ``<%inherit>`` tags. See\n     :ref:`usage_templatelookup`.\n\n    :param module_directory: Filesystem location where generated\n     Python module files will be placed.\n\n    :param module_filename: Overrides the filename of the generated\n     Python module file. For advanced usage only.\n\n    :param module_writer: A callable which overrides how the Python\n     module is written entirely.  The callable is passed the\n     encoded source content of the module and the destination\n     path to be written to.   The default behavior of module writing\n     uses a tempfile in conjunction with a file move in order\n     to make the operation atomic.   So a user-defined module\n     writing function that mimics the default behavior would be:\n\n     .. sourcecode:: python\n\n         import tempfile\n         import os\n         import shutil\n\n         def module_writer(source, outputpath):\n             (dest, name) = \\\\\n                 tempfile.mkstemp(\n                     dir=os.path.dirname(outputpath)\n                 )\n\n             os.write(dest, source)\n             os.close(dest)\n             shutil.move(name, outputpath)\n\n         from mako.template import Template\n         mytemplate = Template(\n                         filename=\"index.html\",\n                         module_directory=\"/path/to/modules\",\n                         module_writer=module_writer\n                     )\n\n     The function is provided for unusual configurations where\n     certain platform-specific permissions or other special\n     steps are needed.\n\n    :param output_encoding: The encoding to use when :meth:`.render`\n     is called.\n     See :ref:`usage_unicode` as well as :ref:`unicode_toplevel`.\n\n    :param preprocessor: Python callable which will be passed\n     the full template source before it is parsed. The return\n     result of the callable will be used as the template source\n     code.\n\n    :param lexer_cls: A :class:`.Lexer` class used to parse\n     the template.   The :class:`.Lexer` class is used by\n     default.\n\n     .. versionadded:: 0.7.4\n\n    :param strict_undefined: Replaces the automatic usage of\n     ``UNDEFINED`` for any undeclared variables not located in\n     the :class:`.Context` with an immediate raise of\n     ``NameError``. The advantage is immediate reporting of\n     missing variables which include the name.\n\n     .. versionadded:: 0.3.6\n\n    :param uri: string URI or other identifier for this template.\n     If not provided, the ``uri`` is generated from the filesystem\n     path, or from the in-memory identity of a non-file-based\n     template. The primary usage of the ``uri`` is to provide a key\n     within :class:`.TemplateLookup`, as well as to generate the\n     file path of the generated Python module file, if\n     ``module_directory`` is specified.\n\n    ",
        "klass": "mako.template.Template",
        "module": "mako"
    },
    {
        "base_classes": [
            "mako.lookup.TemplateCollection"
        ],
        "class_docstring": "Represent a collection of templates that locates template source files\n    from the local filesystem.\n\n    The primary argument is the ``directories`` argument, the list of\n    directories to search:\n\n    .. sourcecode:: python\n\n        lookup = TemplateLookup([\"/path/to/templates\"])\n        some_template = lookup.get_template(\"/index.html\")\n\n    The :class:`.TemplateLookup` can also be given :class:`.Template` objects\n    programatically using :meth:`.put_string` or :meth:`.put_template`:\n\n    .. sourcecode:: python\n\n        lookup = TemplateLookup()\n        lookup.put_string(\"base.html\", '''\n            <html><body>${self.next()}</body></html>\n        ''')\n        lookup.put_string(\"hello.html\", '''\n            <%include file='base.html'/>\n\n            Hello, world !\n        ''')\n\n\n    :param directories: A list of directory names which will be\n     searched for a particular template URI. The URI is appended\n     to each directory and the filesystem checked.\n\n    :param collection_size: Approximate size of the collection used\n     to store templates. If left at its default of ``-1``, the size\n     is unbounded, and a plain Python dictionary is used to\n     relate URI strings to :class:`.Template` instances.\n     Otherwise, a least-recently-used cache object is used which\n     will maintain the size of the collection approximately to\n     the number given.\n\n    :param filesystem_checks: When at its default value of ``True``,\n     each call to :meth:`.TemplateLookup.get_template()` will\n     compare the filesystem last modified time to the time in\n     which an existing :class:`.Template` object was created.\n     This allows the :class:`.TemplateLookup` to regenerate a\n     new :class:`.Template` whenever the original source has\n     been updated. Set this to ``False`` for a very minor\n     performance increase.\n\n    :param modulename_callable: A callable which, when present,\n     is passed the path of the source file as well as the\n     requested URI, and then returns the full path of the\n     generated Python module file. This is used to inject\n     alternate schemes for Python module location. If left at\n     its default of ``None``, the built in system of generation\n     based on ``module_directory`` plus ``uri`` is used.\n\n    All other keyword parameters available for\n    :class:`.Template` are mirrored here. When new\n    :class:`.Template` objects are created, the keywords\n    established with this :class:`.TemplateLookup` are passed on\n    to each new :class:`.Template`.\n\n    ",
        "klass": "mako.lookup.TemplateLookup",
        "module": "mako"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "a very rudimentary buffer that is faster than StringIO,\n    but doesn't crash on unicode data like cStringIO.",
        "klass": "mako.util.FastEncodingBuffer",
        "module": "mako"
    },
    {
        "base_classes": [
            "contextlib._RedirectStream"
        ],
        "class_docstring": "Context manager for temporarily redirecting stderr to another file.",
        "klass": "contextlib.redirect_stderr",
        "module": "contextlib"
    },
    {
        "base_classes": [
            "contextlib._RedirectStream"
        ],
        "class_docstring": "Context manager for temporarily redirecting stdout to another file.\n\n        # How to send help() to stderr\n        with redirect_stdout(sys.stderr):\n            help(dir)\n\n        # How to write help() to a file\n        with open('help.txt', 'w') as f:\n            with redirect_stdout(f):\n                help(pow)\n    ",
        "klass": "contextlib.redirect_stdout",
        "module": "contextlib"
    },
    {
        "base_classes": [
            "oauthlib.oauth2.rfc6749.clients.base.Client"
        ],
        "class_docstring": "A public client utilizing the client credentials grant workflow.\n\n    The client can request an access token using only its client\n    credentials (or other supported means of authentication) when the\n    client is requesting access to the protected resources under its\n    control, or those of another resource owner which has been previously\n    arranged with the authorization server (the method of which is beyond\n    the scope of this specification).\n\n    The client credentials grant type MUST only be used by confidential\n    clients.\n\n    Since the client authentication is used as the authorization grant,\n    no additional authorization request is needed.\n    ",
        "klass": "oauthlib.oauth2.BackendApplicationClient",
        "module": "oauthlib"
    },
    {
        "base_classes": [
            "oauthlib.oauth2.rfc6749.clients.base.Client"
        ],
        "class_docstring": "A public client using the resource owner password and username directly.\n\n    The resource owner password credentials grant type is suitable in\n    cases where the resource owner has a trust relationship with the\n    client, such as the device operating system or a highly privileged\n    application.  The authorization server should take special care when\n    enabling this grant type, and only allow it when other flows are not\n    viable.\n\n    The grant type is suitable for clients capable of obtaining the\n    resource owner's credentials (username and password, typically using\n    an interactive form).  It is also used to migrate existing clients\n    using direct authentication schemes such as HTTP Basic or Digest\n    authentication to OAuth by converting the stored credentials to an\n    access token.\n\n    The method through which the client obtains the resource owner\n    credentials is beyond the scope of this specification.  The client\n    MUST discard the credentials once an access token has been obtained.\n    ",
        "klass": "oauthlib.oauth2.LegacyApplicationClient",
        "module": "oauthlib"
    },
    {
        "base_classes": [
            "oauthlib.oauth2.rfc6749.clients.base.Client"
        ],
        "class_docstring": "A public client utilizing the implicit code grant workflow.\n\n    A user-agent-based application is a public client in which the\n    client code is downloaded from a web server and executes within a\n    user-agent (e.g. web browser) on the device used by the resource\n    owner.  Protocol data and credentials are easily accessible (and\n    often visible) to the resource owner.  Since such applications\n    reside within the user-agent, they can make seamless use of the\n    user-agent capabilities when requesting authorization.\n\n    The implicit grant type is used to obtain access tokens (it does not\n    support the issuance of refresh tokens) and is optimized for public\n    clients known to operate a particular redirection URI.  These clients\n    are typically implemented in a browser using a scripting language\n    such as JavaScript.\n\n    As a redirection-based flow, the client must be capable of\n    interacting with the resource owner's user-agent (typically a web\n    browser) and capable of receiving incoming requests (via redirection)\n    from the authorization server.\n\n    Unlike the authorization code grant type in which the client makes\n    separate requests for authorization and access token, the client\n    receives the access token as the result of the authorization request.\n\n    The implicit grant type does not include client authentication, and\n    relies on the presence of the resource owner and the registration of\n    the redirection URI.  Because the access token is encoded into the\n    redirection URI, it may be exposed to the resource owner and other\n    applications residing on the same device.\n    ",
        "klass": "oauthlib.oauth2.MobileApplicationClient",
        "module": "oauthlib"
    },
    {
        "base_classes": [
            "oauthlib.oauth2.rfc6749.clients.base.Client"
        ],
        "class_docstring": "A public client utilizing the JWT bearer grant.\n\n    JWT bearer tokes can be used to request an access token when a client\n    wishes to utilize an existing trust relationship, expressed through the\n    semantics of (and digital signature or keyed message digest calculated\n    over) the JWT, without a direct user approval step at the authorization\n    server.\n\n    This grant type does not involve an authorization step. It may be\n    used by both public and confidential clients.\n    ",
        "klass": "oauthlib.oauth2.ServiceApplicationClient",
        "module": "oauthlib"
    },
    {
        "base_classes": [
            "oauthlib.oauth2.rfc6749.clients.base.Client"
        ],
        "class_docstring": "A client utilizing the authorization code grant workflow.\n\n    A web application is a confidential client running on a web\n    server.  Resource owners access the client via an HTML user\n    interface rendered in a user-agent on the device used by the\n    resource owner.  The client credentials as well as any access\n    token issued to the client are stored on the web server and are\n    not exposed to or accessible by the resource owner.\n\n    The authorization code grant type is used to obtain both access\n    tokens and refresh tokens and is optimized for confidential clients.\n    As a redirection-based flow, the client must be capable of\n    interacting with the resource owner's user-agent (typically a web\n    browser) and capable of receiving incoming requests (via redirection)\n    from the authorization server.\n    ",
        "klass": "oauthlib.oauth2.WebApplicationClient",
        "module": "oauthlib"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    After rule 001 checks for the \"after\" keyword in signal assignments in clock processes.\n    ",
        "klass": "vsg.rules.after.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    After rule 002 ensures the alignment of the after keyword in clock processes\n    ",
        "klass": "vsg.rules.after.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Attribute rule 001 checks the indent of attribute statements.\n    ",
        "klass": "vsg.rules.attribute.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Attribute rule 002 checks the **attribute** keyword has proper case.\n    ",
        "klass": "vsg.rules.attribute.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Attribute rule 003 checks for a single space after the **attribute** keyword.\n    ",
        "klass": "vsg.rules.attribute.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Case rule 001 checks for the proper indentation at the beginning of the line.",
        "klass": "vsg.rules.case.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Case rule 002 checks for a single space after the \"case\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_rule.single_space_before_rule"
        ],
        "class_docstring": "\n    Case rule 003 checks for a single space before the \"is\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Case rule 004 checks for a single space after the \"when\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Case rule 005 checks for a single space before the \"=>\" keyword.",
        "klass": "vsg.rules.case.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Case rule 006 checks for a single space between the \"end\" and \"case\" keywords.\n    ",
        "klass": "vsg.rules.case.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Case rule 007 ensures a blank line exists before the \"case\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Case rule 008 ensures a blank line exists below the \"case\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Case rule 009 ensures a blank line exists above the \"end case\" keywords.\n    ",
        "klass": "vsg.rules.case.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Case rule 010 ensures a blank line exists below the \"end case\" keywords.\n    ",
        "klass": "vsg.rules.case.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Case rule 011 ensures the alignment of multiline \"when\" statements.\n    ",
        "klass": "vsg.rules.case.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Case rule 012 ensures code does not exist after the => operator.\n    ",
        "klass": "vsg.rules.case.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Case rule 013 verifies the indent of the \"Null\" keyword.\n    ",
        "klass": "vsg.rules.case.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 014 checks the case keyword has proper case.\n    ",
        "klass": "vsg.rules.case.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 015 checks the is keyword has proper case.\n    ",
        "klass": "vsg.rules.case.rule_015",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 016 checks the when keyword has proper case.\n    ",
        "klass": "vsg.rules.case.rule_016",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 017 checks the end keyword has proper case.\n    ",
        "klass": "vsg.rules.case.rule_017",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Case rule 019 checks for labels before the case case keyword.\n    ",
        "klass": "vsg.rules.case.rule_019",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Case rule 020 checks for labels after the \"end case\" keywords.\n    ",
        "klass": "vsg.rules.case.rule_020",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "Comment rule 003 ensures the alignment of \"--\" keywords between process \"begin\" and \"end process\" keywords.",
        "klass": "vsg.rules.comment.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Comment rule 004 ensures there is at least one space before comments on a line with code.\n    ",
        "klass": "vsg.rules.comment.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Comment rule 005 checks consecutive comment lines above a \"when\" keyword\n    in a case statement are aligned with the \"when\" keyword.\n    ",
        "klass": "vsg.rules.comment.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Comment rule 006 ensures the alignment of \"--\" keywords between the process sensitivity list and the process \"begin\" keywords.\n    ",
        "klass": "vsg.rules.comment.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Comment rule 008 checks consecutive comment lines above an \"elsif\" keyword\n    in an if statement are aligned with the \"elsif\" keyword.\n    ",
        "klass": "vsg.rules.comment.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Comment rule 009 checks consecutive comment lines above an \"else\" keyword\n    in an if statement are aligned with the \"else\" keyword.\n    ",
        "klass": "vsg.rules.comment.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Case rule 010 checks for the proper alignment of comments.\n    ",
        "klass": "vsg.rules.comment.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Concurrent rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.concurrent.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Concurrent rule 002 checks there is a single space after the assignment.\n    ",
        "klass": "vsg.rules.concurrent.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Concurrent rule 003 checks the alignment of multiline concurrent assignments.\n    ",
        "klass": "vsg.rules.concurrent.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Concurrent rule 004 checks there is at least a single space before the assignment.\n    ",
        "klass": "vsg.rules.concurrent.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Concurrent rule 005 checks for labels on concurrent assignments.\n    ",
        "klass": "vsg.rules.concurrent.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.concurrent.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Concurrent rule 006 ensures the alignment of the \"<=\" keyword over multiple lines.\n    ",
        "klass": "vsg.rules.concurrent.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Concurrent rule 007 checks for code after the \"else\" keyword.\n    ",
        "klass": "vsg.rules.concurrent.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Entity rule 001 checks for spaces before the entity keyword.\n    ",
        "klass": "vsg.rules.entity.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Entity rule 002 checks for a single space after the entity keyword.\n    ",
        "klass": "vsg.rules.entity.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 003 checks for a blank line above the entity keyword.\n    ",
        "klass": "vsg.rules.entity.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 004 checks the entity keyword has proper case.\n    ",
        "klass": "vsg.rules.entity.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 005 checks the \"is\" keyword is on the same line as the\n    entity keyword.\n    ",
        "klass": "vsg.rules.entity.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 006 checks the is keyword has proper case.\n    ",
        "klass": "vsg.rules.entity.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_rule.single_space_before_rule"
        ],
        "class_docstring": "\n    Entity rule 007 checks for a single space before the \"is\" keyword.\n    ",
        "klass": "vsg.rules.entity.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 008 checks the entity name has proper case in the entity declaration line.\n    ",
        "klass": "vsg.rules.entity.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Entity rule 009 checks for spaces before the \"end\" keyword.\n    ",
        "klass": "vsg.rules.entity.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 010 checks the \"end\" keyword has proper case.\n    ",
        "klass": "vsg.rules.entity.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Entity rule 011 checks for a single space after the \"end\" keyword\n    ",
        "klass": "vsg.rules.entity.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 012 checks entity name is uppercase in \"end\" keyword line.\n    ",
        "klass": "vsg.rules.entity.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Entity rule 013 checks for a single space after the \"entity\" keyword in the closing of the entity.\n    ",
        "klass": "vsg.rules.entity.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 014 checks the entity keyword has proper case.\n    ",
        "klass": "vsg.rules.entity.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 015 checks the \"end\" keyword, \"entity\" keyword, and entity name are on the same line.\n    ",
        "klass": "vsg.rules.entity.rule_015",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.remove_blank_lines_above_rule.remove_blank_lines_above_rule"
        ],
        "class_docstring": "\n    Entity rule 016 checks for a blank line above the \"end entity\" keywords.\n    ",
        "klass": "vsg.rules.entity.rule_016",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 017 ensures the alignment of the : operator for every port in the entity.\n    ",
        "klass": "vsg.rules.entity.rule_017",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Entity rule 018 ensures the alignment of comments in the entity.\n    ",
        "klass": "vsg.rules.entity.rule_018",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    File rule 001 checks the indent of file statements.\n    ",
        "klass": "vsg.rules.file_statement.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    File rule 002 checks the **file** keyword has proper case.\n    ",
        "klass": "vsg.rules.file_statement.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.multiple_spaces_after_rule.multiple_spaces_after_rule"
        ],
        "class_docstring": "\n    File rule 003 checks for a single space after the **attribute** keyword.\n    ",
        "klass": "vsg.rules.file_statement.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    For loop rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.for_loop.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    For loop rule 002 checks for the proper indentation of the \"end loop\" keywords.\n    ",
        "klass": "vsg.rules.for_loop.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    For Loop rule 003 checks the label has proper case.\n    ",
        "klass": "vsg.rules.for_loop.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Generate rule 004 checks for a single space between the label and :.",
        "klass": "vsg.rules.for_loop.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Generate rule 005 checks for a single space between the label and :.",
        "klass": "vsg.rules.for_loop.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Function rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.function.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Function rule 002 checks there is a single space between the function keyword and the function name.\n    ",
        "klass": "vsg.rules.function.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Function rule 003 checks there is a single space between the function name and the (.\n    ",
        "klass": "vsg.rules.function.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Function rule 004 checks the \"begin\" keyword has proper case.\n    ",
        "klass": "vsg.rules.function.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Function rule 005 checks the function keyword has proper case.\n    ",
        "klass": "vsg.rules.function.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Function rule 006 enforces a blank line above the \"function\" keyword.\n    ",
        "klass": "vsg.rules.function.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Function rule 007 enforces a blank line below the \"function\" keyword.\n    ",
        "klass": "vsg.rules.function.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Function rule 008 checks the indent of function parameters when they are on multiple lines.\n    ",
        "klass": "vsg.rules.function.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Function rule 009 checks for a function parameter on the same line as the **function** keyword without a **begin** keyword.\n    ",
        "klass": "vsg.rules.function.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.remove_blank_lines_above_rule.remove_blank_lines_above_rule"
        ],
        "class_docstring": "\n    Generic rule 001 checks for a blank line above the \"generic\" keyword.\n    ",
        "klass": "vsg.rules.generic.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Generic rule 002 checks indentation of the \"generic\" keyword.\n    ",
        "klass": "vsg.rules.generic.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Generic rule 003 checks spacing between \"generic\" keyword and the open parenthesis.\n    ",
        "klass": "vsg.rules.generic.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Generic rule 004 checks indentation of generics.\n    ",
        "klass": "vsg.rules.generic.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "Generic rule 005 checks for a single space after the colon in a generic declaration.",
        "klass": "vsg.rules.generic.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Generic rule 006 checks for a single space after the default assignment in a generic declaration.\n    ",
        "klass": "vsg.rules.generic.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Generic rule 007 checks generic names are uppercase.\n    ",
        "klass": "vsg.rules.generic.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Generic rule 008 checks the indentation of closing parenthesis for generic maps.\n    ",
        "klass": "vsg.rules.generic.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Generic rule 009 checks the \"generic\" keyword has proper case.\n    ",
        "klass": "vsg.rules.generic.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Generic rule 010 checks the closing parenthesis for generics are on a line by itself.",
        "klass": "vsg.rules.generic.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Generic rule 012 ensures the alignment of the : operator for every\n    generic in the entity.\n    ",
        "klass": "vsg.rules.generic.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Generic rule 013 checks for a generic keyword on the same line as a generic declaration.",
        "klass": "vsg.rules.generic.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Generic rule 014 checks for at least a single space before the :.\n    ",
        "klass": "vsg.rules.generic.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Generic rule 015 ensures the alignment of the := operator for every\n    generic in the entity.\n    ",
        "klass": "vsg.rules.generic.rule_015",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Generic rule 016 checks for multiple generic terms on a single line.\n    ",
        "klass": "vsg.rules.generic.rule_016",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.remove_blank_lines_above_rule.remove_blank_lines_above_rule"
        ],
        "class_docstring": "\n    Generic rule 019 checks for blank lines above the closing parenthesis.\n    ",
        "klass": "vsg.rules.generic.rule_019",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    If rule 001 checks the indent of the \"if\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 002 checks the if boolean expression is enclosed in ()'s.\n    ",
        "klass": "vsg.rules.if_statement.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    If rule 003 checks there is a single space between the if keyword and the (.\n    ",
        "klass": "vsg.rules.if_statement.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_rule.single_space_before_rule"
        ],
        "class_docstring": "\n    If rule 004 checks there is a single space between the ) and \"then\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    If rule 005 checks there is a single space between the \"elsif\" keyword and the (.\n    ",
        "klass": "vsg.rules.if_statement.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 006 checks for an empty line after the then keyword.",
        "klass": "vsg.rules.if_statement.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.remove_blank_lines_above_rule.remove_blank_lines_above_rule"
        ],
        "class_docstring": "\n    If rule 007 checks for an empty line before the \"elsif\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 008 checks for an empty line before the \"end if\" keyword.",
        "klass": "vsg.rules.if_statement.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 009 checks the alignment of multiline boolean expressions.\n    ",
        "klass": "vsg.rules.if_statement.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 010 checks for an empty line before the \"else\" keyword.",
        "klass": "vsg.rules.if_statement.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 011 checks for an empty line after the \"else\" keyword.",
        "klass": "vsg.rules.if_statement.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    If rule 012 checks the indent of the \"if\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    If rule 013 checks the indent of the \"else\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    If rule 014 checks the indent of the \"end if\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 015 checks there is a single space between the \"end\" and \"if\" keywords.",
        "klass": "vsg.rules.if_statement.rule_015",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 020 checks the \"end if\" keyword is on it's own line.",
        "klass": "vsg.rules.if_statement.rule_020",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 021 checks the \"else\" keyword is on it's own line.\n    ",
        "klass": "vsg.rules.if_statement.rule_021",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "If rule 022 checks for code after the \"else\" keyword.",
        "klass": "vsg.rules.if_statement.rule_022",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 024 checks for code after the \"then\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_024",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    If rule 025 checks the **if** keyword has proper case.\n    ",
        "klass": "vsg.rules.if_statement.rule_025",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    If rule 026 checks the **elsif** keyword has proper case.\n    ",
        "klass": "vsg.rules.if_statement.rule_026",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    If rule 027 checks the **else** keyword has proper case.\n    ",
        "klass": "vsg.rules.if_statement.rule_027",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 028 checks the **end if** keywords are lowercase.\n    ",
        "klass": "vsg.rules.if_statement.rule_028",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 029 checks the **then** keyword is lowercase.\n    ",
        "klass": "vsg.rules.if_statement.rule_029",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 030 checks for a blank line after the \"end if\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_030",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    If rule 031 checks for a blank line before the \"end if\" keyword.\n    ",
        "klass": "vsg.rules.if_statement.rule_031",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 001 checks for proper indent of instantiations.\n    ",
        "klass": "vsg.rules.instantiation.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Instantiation rule 002 checks for a single space after the :\n    ",
        "klass": "vsg.rules.instantiation.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Instantiation rule 003 checks for a single space before the :\n    ",
        "klass": "vsg.rules.instantiation.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Instantiation rule 004 checks for a blank line above the instantiation\n    declaration.\n    ",
        "klass": "vsg.rules.instantiation.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 005 checks the instantiation declaration and\n    \"port map\" keywords are not on the same line.\n    ",
        "klass": "vsg.rules.instantiation.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 006 checks the \"port map\" keywords are lower case.\n    ",
        "klass": "vsg.rules.instantiation.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 007 checks the closing ) for the port map is on it's own line.\n    ",
        "klass": "vsg.rules.instantiation.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Instantiation rule 008 checks the instance name has proper case in the instantiation declaration line.\n    ",
        "klass": "vsg.rules.instantiation.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 009 checks the entity name is uppercase in the instantiation declaration line.\n    ",
        "klass": "vsg.rules.instantiation.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 010 ensures the alignment of the => operator for every port in the instantiation.\n    ",
        "klass": "vsg.rules.instantiation.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Instantiation rule 011 checks the port name has proper case.\n    ",
        "klass": "vsg.rules.instantiation.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 012 checks the instantiation declaration and\n    \"generic map\" keywords are not on the same line.\n    ",
        "klass": "vsg.rules.instantiation.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 013 checks the \"generic map\" keywords are lower case.\n    ",
        "klass": "vsg.rules.instantiation.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 014 checks the closing ) for the generic map is on it's own line.\n    ",
        "klass": "vsg.rules.instantiation.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Instantiation rule 015 ensures the alignment of the => operator for every generic in the instantiation.\n    ",
        "klass": "vsg.rules.instantiation.rule_015",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 016 checks the generic name is uppercase.\n    ",
        "klass": "vsg.rules.instantiation.rule_016",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 016 checks for generic map keyword and generic assignment on the same line.\n    ",
        "klass": "vsg.rules.instantiation.rule_017",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 018 checks for a single space between map and (\n    ",
        "klass": "vsg.rules.instantiation.rule_018",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Instantiation rule 019 checks for a blank line below the end of the port\n    declaration.\n    ",
        "klass": "vsg.rules.instantiation.rule_019",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 020 checks for a port assignment on the same line as the port map keywords.\n    ",
        "klass": "vsg.rules.instantiation.rule_020",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 021 checks multiple port assignments on the same line.\n    ",
        "klass": "vsg.rules.instantiation.rule_021",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 022 checks for a single space after the => operator in port maps.\n    ",
        "klass": "vsg.rules.instantiation.rule_022",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 023 checks for comments after port and generic assignments.\n    ",
        "klass": "vsg.rules.instantiation.rule_023",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.search_for_and_replace_keyword_rule.search_for_and_replace_keyword_rule"
        ],
        "class_docstring": "\n    Instantiation rule 025 checks the **map** keyword is on the same line as the (.\n    ",
        "klass": "vsg.rules.instantiation.rule_025",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.search_for_and_replace_keyword_rule.search_for_and_replace_keyword_rule"
        ],
        "class_docstring": "\n    Instantiation rule 026 checks the **port map** keyword is on the same line as the (.\n    ",
        "klass": "vsg.rules.instantiation.rule_026",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 027 checks the **entity** keyword is lowercase in direct instantiations.\n    ",
        "klass": "vsg.rules.instantiation.rule_027",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 028 checks the entity name is uppercase in direct instantiations.\n    ",
        "klass": "vsg.rules.instantiation.rule_028",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Entity rule 029 ensures the alignment of comments in an instantiation.\n    ",
        "klass": "vsg.rules.instantiation.rule_029",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 030 checks for a single space after the => operator in generic maps.\n    ",
        "klass": "vsg.rules.instantiation.rule_030",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 031 checks the component keyword is lowercase if it is used.\n    ",
        "klass": "vsg.rules.instantiation.rule_031",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 032 checks for a single space after the component keyword if used.\n    ",
        "klass": "vsg.rules.instantiation.rule_032",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Instantiation rule 033 checks for the component keyword.\n    ",
        "klass": "vsg.rules.instantiation.rule_033",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Library rule 001 checks for spaces at the beginning of the line.\n    ",
        "klass": "vsg.rules.library.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Library rule 002 checks for a single space after the library keyword.\n    ",
        "klass": "vsg.rules.library.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Library rule 003 checks for a blank line above the library keyword.\n    ",
        "klass": "vsg.rules.library.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Library rule 004 checks the library keyword has proper case.\n    ",
        "klass": "vsg.rules.library.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Library rule 005 checks the use keyword has proper case.\n    ",
        "klass": "vsg.rules.library.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Library rule 006 checks for a single space after the use keyword.\n    ",
        "klass": "vsg.rules.library.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.remove_blank_lines_above_rule.remove_blank_lines_above_rule"
        ],
        "class_docstring": "\n    Library rule 007 checks for a blank line above the \"use\" keyword.\n    ",
        "klass": "vsg.rules.library.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Library rule 008 checks indentation of the use keyword.\n    ",
        "klass": "vsg.rules.library.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Package rule 001 checks for spaces at the beginning of the line.\n    ",
        "klass": "vsg.rules.package.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 002 checks for a single space between \"package\" and \"is\" keywords.\n    ",
        "klass": "vsg.rules.package.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Package rule 003 checks for a blank line above the package keyword.\n    ",
        "klass": "vsg.rules.package.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Package rule 004 checks the \"package\" keyword has proper case.\n    ",
        "klass": "vsg.rules.package.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 005 checks if the \"is\" keyword is on the same line as the \"package\" keyword.\n    ",
        "klass": "vsg.rules.package.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 006 checks for the \"end\" and \"package\" keyword are lower case.\n    ",
        "klass": "vsg.rules.package.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 007 checks for the \"package\" keyword on the end package declaration.\n    ",
        "klass": "vsg.rules.package.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.package.uppercase_package_name_rule.uppercase_package_name_rule"
        ],
        "class_docstring": "\n    Package rule 008 checks the package name is upper case on the closing \"end package\" line.\n    ",
        "klass": "vsg.rules.package.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 009 checks for a single space between the \"end\" and \"package\" keywords and component name.\n    ",
        "klass": "vsg.rules.package.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.package.uppercase_package_name_rule.uppercase_package_name_rule"
        ],
        "class_docstring": "\n    Package rule 010 checks the package name is upper case in the package declaration.\n    ",
        "klass": "vsg.rules.package.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Package rule 011 checks for a blank line below the package keyword.\n    ",
        "klass": "vsg.rules.package.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Package rule 012 checks for a blank line above the \"end package\" keywords.\n    ",
        "klass": "vsg.rules.package.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Package rule 013 checks the \"is\" keyword is lower case.\n    ",
        "klass": "vsg.rules.package.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Procedure rule 001 checks the indent of procedures.\n    ",
        "klass": "vsg.rules.procedure.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Procedure rule 005 checks the indent of lines in the procedure declarative region.\n    ",
        "klass": "vsg.rules.procedure.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Procedure rule 001 checks the indent of procedures.\n    ",
        "klass": "vsg.rules.procedure.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    General rule 007 checks capitalization consistency of procedure names.\n    ",
        "klass": "vsg.rules.procedure.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Sequential rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.sequential.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Sequential rule 002 checks for a single space after the \"<=\" keyword.\n    ",
        "klass": "vsg.rules.sequential.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Sequential rule 003 checks for a single space before the \"<=\" keyword.\n    ",
        "klass": "vsg.rules.sequential.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Sequential rule 004 ensures the alignment of multiline sequential\n    statements.\n    ",
        "klass": "vsg.rules.sequential.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Sequential rule 005 ensures the alignment of the \"<=\" keyword over\n    multiple lines.\n    ",
        "klass": "vsg.rules.sequential.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Sequential rule 006 checks for commented out lines within a multiline sequential statement.\n    ",
        "klass": "vsg.rules.sequential.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Signal rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.signal.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Signal rule 002 checks the \"signal\" keyword has proper case.\n    ",
        "klass": "vsg.rules.signal.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.multiple_spaces_after_rule.multiple_spaces_after_rule"
        ],
        "class_docstring": "\n    Signal rule 003 checks there is a single space after the \"signal\" keyword.\n    ",
        "klass": "vsg.rules.signal.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Signal rule 004 checks the signal name is lowercase.\n    ",
        "klass": "vsg.rules.signal.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Signal rule 005 checks there is a single space after the colon.\n    ",
        "klass": "vsg.rules.signal.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Signal rule 006 checks there is at least a single space before the colon.\n    ",
        "klass": "vsg.rules.signal.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Signal rule 010 checks the signal type has proper case if it is a VHDL keyword.\n    ",
        "klass": "vsg.rules.signal.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Signal rule 011 checks the signal type has proper case.\n    ",
        "klass": "vsg.rules.signal.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Signal rule 012 checks the second signal in a signal declaration are aligned within the architecture declarative region.\n    ",
        "klass": "vsg.rules.signal.rule_012",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Signal rule 013 checks the colons are in the same column for all signals.\n    ",
        "klass": "vsg.rules.signal.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    General rule 014 checks capitalization consistency of signal names.\n    ",
        "klass": "vsg.rules.signal.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Type rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.type_definition.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Type rule 002 checks the \"type\" keyword has proper case.\n    ",
        "klass": "vsg.rules.type_definition.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.multiple_spaces_after_rule.multiple_spaces_after_rule"
        ],
        "class_docstring": "\n    Type rule 003 checks the spaces after the \"type\" keyword.\n    ",
        "klass": "vsg.rules.type_definition.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Type rule 004 checks the type name has proper case.\n    ",
        "klass": "vsg.rules.type_definition.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Type rule 005 checks for the proper indentation of multiline types.\n    ",
        "klass": "vsg.rules.type_definition.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_rule.single_space_before_rule"
        ],
        "class_docstring": "\n    Type rule 006 checks for a single space before the \"is\" keyword.\n    ",
        "klass": "vsg.rules.type_definition.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Type rule 007 checks for a single space after the \"is\" keyword.\n    ",
        "klass": "vsg.rules.type_definition.rule_007",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Type rule 008 checks the closing parenthesis of a multi-line type declaration is on it's own line.\n    ",
        "klass": "vsg.rules.type_definition.rule_008",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Type rule 009 checks for enumerated types after the open parenthesis on a multi-line type declaration.\n    ",
        "klass": "vsg.rules.type_definition.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_above_rule.line_above_rule"
        ],
        "class_docstring": "\n    Type 010 checks for a blank line above the \"type\" declaration.\n    ",
        "klass": "vsg.rules.type_definition.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.line_below_rule.line_below_rule"
        ],
        "class_docstring": "\n    Type 011 checks for a blank line below the \"type\" declaration.\n    ",
        "klass": "vsg.rules.type_definition.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Entity rule 013 checks the \"is\" keyword has proper case in type definitions.\n    ",
        "klass": "vsg.rules.type_definition.rule_013",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    General rule 014 checks capitalization consistency of type names.\n    ",
        "klass": "vsg.rules.type_definition.rule_014",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Signal rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.variable.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Signal rule 002 checks the \"variable\" keyword has proper case.\n    ",
        "klass": "vsg.rules.variable.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_rule.single_space_after_rule"
        ],
        "class_docstring": "\n    Signal rule 003 checks there is a single space after the \"variable\" keyword.\n    ",
        "klass": "vsg.rules.variable.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.case_rule.case_rule"
        ],
        "class_docstring": "\n    Variable rule 004 checks the variable identifiers have proper case.\n    ",
        "klass": "vsg.rules.variable.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Signal rule 005 checks there is a single space after the colon.\n    ",
        "klass": "vsg.rules.variable.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_before_character_rule.single_space_before_character_rule"
        ],
        "class_docstring": "\n    Signal rule 006 checks there is at least a single space before the colon.\n    ",
        "klass": "vsg.rules.variable.rule_006",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.keyword_alignment_rule.keyword_alignment_rule"
        ],
        "class_docstring": "\n    Signal rule 009 checks the colons are in the same column for all variables.\n    ",
        "klass": "vsg.rules.variable.rule_009",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Signal rule 010 checks the variable type is lowercase.\n    ",
        "klass": "vsg.rules.variable.rule_010",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    General rule 011 checks capitalization consistency of variable names.\n    ",
        "klass": "vsg.rules.variable.rule_011",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Variable assignment rule 001 checks for the proper indentation at the beginning of the line.\n    ",
        "klass": "vsg.rules.variable_assignment.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.single_space_after_character_rule.single_space_after_character_rule"
        ],
        "class_docstring": "\n    Variable assignment rule 002 checks for a single space after the \":=\" keyword.\n    ",
        "klass": "vsg.rules.variable_assignment.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "Variable assignment rule 003 checks for a single space before the \":=\" keyword.",
        "klass": "vsg.rules.variable_assignment.rule_003",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Variable assignment rule 004 ensures the alignment of multiline variable_assignment statements.\n    ",
        "klass": "vsg.rules.variable_assignment.rule_004",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    Variable assignment rule 005 ensures the alignment of the \":=\" keyword\n    over multiple lines.\n    ",
        "klass": "vsg.rules.variable_assignment.rule_005",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Wait rule 001 checks for the proper indentation of the \"wait\" keyword.\n    ",
        "klass": "vsg.rules.wait.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rule.rule"
        ],
        "class_docstring": "\n    With rule 001 checks the when and else keywords are on the same line\n    ",
        "klass": "vsg.rules.when.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Checks the indent level of the \"while\" keyword.\n    ",
        "klass": "vsg.rules.while_loop.rule_001",
        "module": "vsg"
    },
    {
        "base_classes": [
            "vsg.rules.indent_rule.indent_rule"
        ],
        "class_docstring": "\n    Checks the indent level of the \"end loop\" keywords.\n    ",
        "klass": "vsg.rules.while_loop.rule_002",
        "module": "vsg"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a WSGI response.\n\n    If no arguments are passed, creates a :class:`~Response` that uses a\n    variety of defaults. The defaults may be changed by sub-classing the\n    :class:`~Response`. See the :ref:`sub-classing notes\n    <response_subclassing_notes>`.\n\n    :cvar ~Response.body: If ``body`` is a ``text_type``, then it will be\n        encoded using either ``charset`` when provided or ``default_encoding``\n        when ``charset`` is not provided if the ``content_type`` allows for a\n        ``charset``. This argument is mutually  exclusive with ``app_iter``.\n\n    :vartype ~Response.body: bytes or text_type\n\n    :cvar ~Response.status: Either an :class:`int` or a string that is\n        an integer followed by the status text. If it is an integer, it will be\n        converted to a proper status that also includes the status text.  Any\n        existing status text will be kept. Non-standard values are allowed.\n\n    :vartype ~Response.status: int or str\n\n    :cvar ~Response.headerlist: A list of HTTP headers for the response.\n\n    :vartype ~Response.headerlist: list\n\n    :cvar ~Response.app_iter: An iterator that is used as the body of the\n        response. Should conform to the WSGI requirements and should provide\n        bytes. This argument is mutually exclusive with ``body``.\n\n    :vartype ~Response.app_iter: iterable\n\n    :cvar ~Response.content_type: Sets the ``Content-Type`` header. If no\n        ``content_type`` is provided, and there is no ``headerlist``, the\n        ``default_content_type`` will be automatically set. If ``headerlist``\n        is provided then this value is ignored.\n\n    :vartype ~Response.content_type: str or None\n\n    :cvar conditional_response: Used to change the behavior of the\n        :class:`~Response` to check the original request for conditional\n        response headers. See :meth:`~Response.conditional_response_app` for\n        more information.\n\n    :vartype conditional_response: bool\n\n    :cvar ~Response.charset: Adds a ``charset`` ``Content-Type`` parameter. If\n        no ``charset`` is provided and the ``Content-Type`` is text, then the\n        ``default_charset`` will automatically be added.  Currently the only\n        ``Content-Type``'s that allow for a ``charset`` are defined to be\n        ``text/*``, ``application/xml``, and ``*/*+xml``. Any other\n        ``Content-Type``'s will not have a ``charset`` added. If a\n        ``headerlist`` is provided this value is ignored.\n\n    :vartype ~Response.charset: str or None\n\n    All other response attributes may be set on the response by providing them\n    as keyword arguments. A :exc:`TypeError` will be raised for any unexpected\n    keywords.\n\n    .. _response_subclassing_notes:\n\n    **Sub-classing notes:**\n\n    * The ``default_content_type`` is used as the default for the\n      ``Content-Type`` header that is returned on the response. It is\n      ``text/html``.\n\n    * The ``default_charset`` is used as the default character set to return on\n      the ``Content-Type`` header, if the ``Content-Type`` allows for a\n      ``charset`` parameter. Currently the only ``Content-Type``'s that allow\n      for a ``charset`` are defined to be: ``text/*``, ``application/xml``, and\n      ``*/*+xml``. Any other ``Content-Type``'s will not have a ``charset``\n      added.\n\n    * The ``unicode_errors`` is set to ``strict``, and access on a\n      :attr:`~Response.text` will raise an error if it fails to decode the\n      :attr:`~Response.body`.\n\n    * ``default_conditional_response`` is set to ``False``. This flag may be\n      set to ``True`` so that all ``Response`` objects will attempt to check\n      the original request for conditional response headers. See\n      :meth:`~Response.conditional_response_app` for more information.\n\n    * ``default_body_encoding`` is set to 'UTF-8' by default. It exists to\n      allow users to get/set the ``Response`` object using ``.text``, even if\n      no ``charset`` has been set for the ``Content-Type``.\n    ",
        "klass": "webob.Response",
        "module": "webob"
    },
    {
        "base_classes": [
            "webob.exc.HTTPClientError"
        ],
        "class_docstring": "\n    subclass of :class:`~HTTPClientError`\n\n    This indicates that the server understood the request, but is\n    refusing to fulfill it.\n\n    code: 403, title: Forbidden\n    ",
        "klass": "webob.exc.HTTPForbidden",
        "module": "webob"
    },
    {
        "base_classes": [
            "webob.exc.HTTPClientError"
        ],
        "class_docstring": "\n    subclass of :class:`~HTTPClientError`\n\n    This indicates that the server did not find anything matching the\n    Request-URI.\n\n    code: 404, title: Not Found\n    ",
        "klass": "webob.exc.HTTPNotFound",
        "module": "webob"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n        An ordered dictionary that can have multiple values for each key.\n        Adds the methods getall, getone, mixed and extend and add to the normal\n        dictionary interface.\n    ",
        "klass": "webob.multidict.MultiDict",
        "module": "webob"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Frame widget is a container, used to group other widgets\n    together.",
        "klass": "tkinter.ttk.Frame",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget",
            "tkinter.Scrollbar"
        ],
        "class_docstring": "Ttk Scrollbar controls the viewport of a scrollable widget.",
        "klass": "tkinter.ttk.Scrollbar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a named font.\n\n    Constructor options are:\n\n    font -- font specifier (name, system font, or (family, size, style)-tuple)\n    name -- name to use for this font configuration (defaults to a unique name)\n    exists -- does a named font by this name already exist?\n       Creates a new named font if False, points to the existing font if True.\n       Raises _tkinter.TclError if the assertion is false.\n\n       the following are ignored if font is specified:\n\n    family -- font 'family', e.g. Courier, Times, Helvetica\n    size -- font size in points\n    weight -- font thickness: NORMAL, BOLD\n    slant -- font slant: ROMAN, ITALIC\n    underline -- font underlining: false (0), true (1)\n    overstrike -- font strikeout: false (0), true (1)\n\n    ",
        "klass": "tkinter.font.Font",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Button widget, displays a textual label and/or image, and\n    evaluates a command when pressed.",
        "klass": "tkinter.ttk.Button",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Checkbutton widget which is either in on- or off-state.",
        "klass": "tkinter.ttk.Checkbutton",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget",
            "tkinter.Entry"
        ],
        "class_docstring": "Ttk Entry widget displays a one-line text string and allows that\n    string to be edited by the user.",
        "klass": "tkinter.ttk.Entry",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Label widget displays a textual label and/or image.",
        "klass": "tkinter.ttk.Label",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Notebook widget manages a collection of windows and displays\n    a single one at a time. Each child window is associated with a tab,\n    which the user may select to change the currently-displayed window.",
        "klass": "tkinter.ttk.Notebook",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Support for redirecting arbitrary widget subcommands.\n\n    Some Tk operations don't normally pass through tkinter.  For example, if a\n    character is inserted into a Text widget by pressing a key, a default Tk\n    binding to the widget's 'insert' operation is activated, and the Tk library\n    processes the insert without calling back into tkinter.\n\n    Although a binding to <Key> could be made via tkinter, what we really want\n    to do is to hook the Tk 'insert' operation itself.  For one thing, we want\n    a text.insert call in idle code to have the same effect as a key press.\n\n    When a widget is instantiated, a Tcl command is created whose name is the\n    same as the pathname widget._w.  This command is used to invoke the various\n    widget operations, e.g. insert (for a Text widget). We are going to hook\n    this command and provide a facility ('register') to intercept the widget\n    operation.  We will also intercept method calls on the tkinter class\n    instance that represents the tk widget.\n\n    In IDLE, WidgetRedirector is used in Percolator to intercept Text\n    commands.  The function being registered provides access to the top\n    of a Percolator chain.  At the bottom of the chain is a call to the\n    original Tk widget operation.\n    ",
        "klass": "idlelib.redirector.WidgetRedirector",
        "module": "idlelib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A blockingly bounded semaphore.\n\n    Optionally initialize with a resource *count*, then :meth:`acquire` and\n    :meth:`release` resources as needed. Attempting to :meth:`acquire` when\n    *count* is zero suspends the calling greenthread until count becomes nonzero\n    again.  Attempting to :meth:`release` after *count* has reached *limit*\n    suspends the calling greenthread until *count* becomes less than *limit*\n    again.\n\n    This has the same API as :class:`threading.Semaphore`, though its\n    semantics and behavior differ subtly due to the upper limit on calls\n    to :meth:`release`.  It is **not** compatible with\n    :class:`threading.BoundedSemaphore` because it blocks when reaching *limit*\n    instead of raising a ValueError.\n\n    It is a context manager, and thus can be used in a with block::\n\n      sem = CappedSemaphore(2)\n      with sem:\n        do_some_stuff()\n    ",
        "klass": "eventlet.CappedSemaphore",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An abstraction where an arbitrary number of coroutines\n    can wait for one event from another.\n\n    Events are similar to a Queue that can only hold one item, but differ\n    in two important ways:\n\n    1. calling :meth:`send` never unschedules the current greenthread\n    2. :meth:`send` can only be called once; create a new event to send again.\n\n    They are good for communicating results between coroutines, and\n    are the basis for how\n    :meth:`GreenThread.wait() <eventlet.greenthread.GreenThread.wait>`\n    is implemented.\n\n    >>> from eventlet import event\n    >>> import eventlet\n    >>> evt = event.Event()\n    >>> def baz(b):\n    ...     evt.send(b + 1)\n    ...\n    >>> _ = eventlet.spawn_n(baz, 3)\n    >>> evt.wait()\n    4\n    ",
        "klass": "eventlet.event.Event",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An abstraction where an arbitrary number of coroutines\n    can wait for one event from another.\n\n    Events are similar to a Queue that can only hold one item, but differ\n    in two important ways:\n\n    1. calling :meth:`send` never unschedules the current greenthread\n    2. :meth:`send` can only be called once; create a new event to send again.\n\n    They are good for communicating results between coroutines, and\n    are the basis for how\n    :meth:`GreenThread.wait() <eventlet.greenthread.GreenThread.wait>`\n    is implemented.\n\n    >>> from eventlet import event\n    >>> import eventlet\n    >>> evt = event.Event()\n    >>> def baz(b):\n    ...     evt.send(b + 1)\n    ...\n    >>> _ = eventlet.spawn_n(baz, 3)\n    >>> evt.wait()\n    4\n    ",
        "klass": "eventlet.Event",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The GreenPool class is a pool of green threads.\n    ",
        "klass": "eventlet.greenpool.GreenPool",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The GreenPool class is a pool of green threads.\n    ",
        "klass": "eventlet.GreenPool",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "eventlet.queue.LightQueue"
        ],
        "class_docstring": "Create a queue object with a given maximum size.\n\n    If *maxsize* is less than zero or ``None``, the queue size is infinite.\n\n    ``Queue(0)`` is a channel, that is, its :meth:`put` method always blocks\n    until the item is delivered. (This is unlike the standard\n    :class:`Stdlib_Queue`, where 0 means infinite size).\n\n    In all other respects, this Queue class resembles the standard library,\n    :class:`Stdlib_Queue`.\n    ",
        "klass": "eventlet.Queue",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "Raises *exception* in the current greenthread after *timeout* seconds.\n\n    When *exception* is omitted or ``None``, the :class:`Timeout` instance\n    itself is raised. If *seconds* is None, the timer is not scheduled, and is\n    only useful if you're planning to raise it directly.\n\n    Timeout objects are context managers, and so can be used in with statements.\n    When used in a with statement, if *exception* is ``False``, the timeout is\n    still raised, but the context manager suppresses it, so the code outside the\n    with-block won't see it.\n    ",
        "klass": "eventlet.Timeout",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "code.InteractiveInterpreter"
        ],
        "class_docstring": "Closely emulate the behavior of the interactive Python interpreter.\n\n    This class builds on InteractiveInterpreter and adds prompting\n    using the familiar sys.ps1 and sys.ps2, and input buffering.\n\n    ",
        "klass": "code.InteractiveConsole",
        "module": "code"
    },
    {
        "base_classes": [
            "socketserver.BaseServer"
        ],
        "class_docstring": "Base class for various socket-based server classes.\n\n    Defaults to synchronous IP stream (i.e., TCP).\n\n    Methods for the caller:\n\n    - __init__(server_address, RequestHandlerClass, bind_and_activate=True)\n    - serve_forever(poll_interval=0.5)\n    - shutdown()\n    - handle_request()  # if you don't use serve_forever()\n    - fileno() -> int   # for selector\n\n    Methods that may be overridden:\n\n    - server_bind()\n    - server_activate()\n    - get_request() -> request, client_address\n    - handle_timeout()\n    - verify_request(request, client_address)\n    - process_request(request, client_address)\n    - shutdown_request(request)\n    - close_request(request)\n    - handle_error()\n\n    Methods for derived classes:\n\n    - finish_request(request, client_address)\n\n    Class variables that may be overridden by derived classes or\n    instances:\n\n    - timeout\n    - address_family\n    - socket_type\n    - request_queue_size (only for stream sockets)\n    - allow_reuse_address\n\n    Instance variables:\n\n    - server_address\n    - RequestHandlerClass\n    - socket\n\n    ",
        "klass": "socketserver.TCPServer",
        "module": "socketserver"
    },
    {
        "base_classes": [
            "socketserver.TCPServer"
        ],
        "class_docstring": "UDP server class.",
        "klass": "socketserver.UDPServer",
        "module": "socketserver"
    },
    {
        "base_classes": [
            "selectors._PollLikeSelector"
        ],
        "class_docstring": "Poll-based selector.",
        "klass": "selectors.PollSelector",
        "module": "selectors"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "socket(family=AF_INET, type=SOCK_STREAM, proto=0) -> socket object\nsocket(family=-1, type=-1, proto=-1, fileno=None) -> socket object\n\nOpen a socket of the given type.  The family argument specifies the\naddress family; it defaults to AF_INET.  The type argument specifies\nwhether this is a stream (SOCK_STREAM, this is the default)\nor datagram (SOCK_DGRAM) socket.  The protocol argument defaults to 0,\nspecifying the default protocol.  Keyword arguments are accepted.\nThe socket is created as non-inheritable.\n\nWhen a fileno is passed in, family, type and proto are auto-detected,\nunless they are explicitly set.\n\nA socket object represents one endpoint of a network connection.\n\nMethods of socket objects (keyword arguments not allowed):\n\n_accept() -- accept connection, returning new socket fd and client address\nbind(addr) -- bind the socket to a local address\nclose() -- close the socket\nconnect(addr) -- connect the socket to a remote address\nconnect_ex(addr) -- connect, return an error code instead of an exception\ndup() -- return a new socket fd duplicated from fileno()\nfileno() -- return underlying file descriptor\ngetpeername() -- return remote address [*]\ngetsockname() -- return local address\ngetsockopt(level, optname[, buflen]) -- get socket options\ngettimeout() -- return timeout or None\nlisten([n]) -- start listening for incoming connections\nrecv(buflen[, flags]) -- receive data\nrecv_into(buffer[, nbytes[, flags]]) -- receive data (into a buffer)\nrecvfrom(buflen[, flags]) -- receive data and sender's address\nrecvfrom_into(buffer[, nbytes, [, flags])\n  -- receive data and sender's address (into a buffer)\nsendall(data[, flags]) -- send all data\nsend(data[, flags]) -- send data, may not send all of it\nsendto(data[, flags], addr) -- send data to a given address\nsetblocking(0 | 1) -- set or clear the blocking I/O flag\ngetblocking() -- return True if socket is blocking, False if non-blocking\nsetsockopt(level, optname, value[, optlen]) -- set socket options\nsettimeout(None | float) -- set or clear the timeout\nshutdown(how) -- shut down traffic in one or both directions\nif_nameindex() -- return all network interface indices and names\nif_nametoindex(name) -- return the corresponding interface index\nif_indextoname(index) -- return the corresponding interface name\n\n [*] not available on all platforms!",
        "klass": "eventlet.green.socket.socket",
        "module": "_socket"
    },
    {
        "base_classes": [
            "subprocess.Popen"
        ],
        "class_docstring": "eventlet-friendly version of subprocess.Popen",
        "klass": "eventlet.green.subprocess.Popen",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This is a variant of Queue that behaves mostly like the standard\n    :class:`Stdlib_Queue`.  It differs by not supporting the\n    :meth:`task_done <Stdlib_Queue.task_done>` or\n    :meth:`join <Stdlib_Queue.join>` methods, and is a little faster for\n    not having that overhead.\n    ",
        "klass": "eventlet.queue.LightQueue",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "greenlet(run=None, parent=None) -> greenlet\n\nCreates a new greenlet object (without running it).\n\n - *run* -- The callable to invoke.\n - *parent* -- The parent greenlet. The default is the current greenlet.",
        "klass": "greenlet.greenlet",
        "module": "greenlet"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "greenlet(run=None, parent=None) -> greenlet\n\nCreates a new greenlet object (without running it).\n\n - *run* -- The callable to invoke.\n - *parent* -- The parent greenlet. The default is the current greenlet.",
        "klass": "eventlet.support.greenlets.greenlet",
        "module": "greenlet"
    },
    {
        "base_classes": [
            "invoke.context.Context"
        ],
        "class_docstring": "\n    A connection to an SSH daemon, with methods for commands and file transfer.\n\n    **Basics**\n\n    This class inherits from Invoke's `~invoke.context.Context`, as it is a\n    context within which commands, tasks etc can operate. It also encapsulates\n    a Paramiko `~paramiko.client.SSHClient` instance, performing useful high\n    level operations with that `~paramiko.client.SSHClient` and\n    `~paramiko.channel.Channel` instances generated from it.\n\n    .. _connect_kwargs:\n\n    .. note::\n        Many SSH specific options -- such as specifying private keys and\n        passphrases, timeouts, disabling SSH agents, etc -- are handled\n        directly by Paramiko and should be specified via the\n        :ref:`connect_kwargs argument <connect_kwargs-arg>` of the constructor.\n\n    **Lifecycle**\n\n    `.Connection` has a basic \"`create <__init__>`, `connect/open <open>`, `do\n    work <run>`, `disconnect/close <close>`\" lifecycle:\n\n    - `Instantiation <__init__>` imprints the object with its connection\n      parameters (but does **not** actually initiate the network connection).\n\n        - An alternate constructor exists for users :ref:`upgrading piecemeal\n          from Fabric 1 <from-v1>`: `from_v1`\n\n    - Methods like `run`, `get` etc automatically trigger a call to\n      `open` if the connection is not active; users may of course call `open`\n      manually if desired.\n    - Connections do not always need to be explicitly closed; much of the\n      time, Paramiko's garbage collection hooks or Python's own shutdown\n      sequence will take care of things. **However**, should you encounter edge\n      cases (for example, sessions hanging on exit) it's helpful to explicitly\n      close connections when you're done with them.\n\n      This can be accomplished by manually calling `close`, or by using the\n      object as a contextmanager::\n\n        with Connection('host') as c:\n            c.run('command')\n            c.put('file')\n\n    .. note::\n        This class rebinds `invoke.context.Context.run` to `.local` so both\n        remote and local command execution can coexist.\n\n    **Configuration**\n\n    Most `.Connection` parameters honor :doc:`Invoke-style configuration\n    </concepts/configuration>` as well as any applicable :ref:`SSH config file\n    directives <connection-ssh-config>`. For example, to end up with a\n    connection to ``admin@myhost``, one could:\n\n    - Use any built-in config mechanism, such as ``/etc/fabric.yml``,\n      ``~/.fabric.json``, collection-driven configuration, env vars, etc,\n      stating ``user: admin`` (or ``{\"user\": \"admin\"}``, depending on config\n      format.) Then ``Connection('myhost')`` would implicitly have a ``user``\n      of ``admin``.\n    - Use an SSH config file containing ``User admin`` within any applicable\n      ``Host`` header (``Host myhost``, ``Host *``, etc.) Again,\n      ``Connection('myhost')`` will default to an ``admin`` user.\n    - Leverage host-parameter shorthand (described in `.Config.__init__`), i.e.\n      ``Connection('admin@myhost')``.\n    - Give the parameter directly: ``Connection('myhost', user='admin')``.\n\n    The same applies to agent forwarding, gateways, and so forth.\n\n    .. versionadded:: 2.0\n    ",
        "klass": "fabric.Connection",
        "module": "fabric"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "\n    A collection of `.Connection` objects whose API operates on its contents.\n\n    .. warning::\n        **This is a partially abstract class**; you need to use one of its\n        concrete subclasses (such as `.SerialGroup` or `.ThreadingGroup`) or\n        you'll get ``NotImplementedError`` on most of the methods.\n\n    Most methods in this class mirror those of `.Connection`, taking the same\n    arguments; however their return values and exception-raising behavior\n    differs:\n\n    - Return values are dict-like objects (`.GroupResult`) mapping\n      `.Connection` objects to the return value for the respective connections:\n      `.Group.run` returns a map of `.Connection` to `.runners.Result`,\n      `.Group.get` returns a map of `.Connection` to `.transfer.Result`, etc.\n    - If any connections encountered exceptions, a `.GroupException` is raised,\n      which is a thin wrapper around what would otherwise have been the\n      `.GroupResult` returned; within that wrapped `.GroupResult`, the\n      excepting connections map to the exception that was raised, in place of a\n      ``Result`` (as no ``Result`` was obtained.) Any non-excepting connections\n      will have a ``Result`` value, as normal.\n\n    For example, when no exceptions occur, a session might look like this::\n\n        >>> group = SerialGroup('host1', 'host2')\n        >>> group.run(\"this is fine\")\n        {\n            <Connection host='host1'>: <Result cmd='this is fine' exited=0>,\n            <Connection host='host2'>: <Result cmd='this is fine' exited=0>,\n        }\n\n    With exceptions (anywhere from 1 to \"all of them\"), it looks like so; note\n    the different exception classes, e.g. `~invoke.exceptions.UnexpectedExit`\n    for a completed session whose command exited poorly, versus\n    `socket.gaierror` for a host that had DNS problems::\n\n        >>> group = SerialGroup('host1', 'host2', 'notahost')\n        >>> group.run(\"will it blend?\")\n        {\n            <Connection host='host1'>: <Result cmd='will it blend?' exited=0>,\n            <Connection host='host2'>: <UnexpectedExit: cmd='...' exited=1>,\n            <Connection host='notahost'>: gaierror(...),\n        }\n\n    As with `.Connection`, `.Group` objects may be used as context managers,\n    which will automatically `.close` the object on block exit.\n\n    .. versionadded:: 2.0\n    .. versionchanged:: 2.4\n        Added context manager behavior.\n    ",
        "klass": "fabric.Group",
        "module": "fabric"
    },
    {
        "base_classes": [
            "fabric.group.Group"
        ],
        "class_docstring": "\n    Subclass of `.Group` which uses threading to execute concurrently.\n\n    .. versionadded:: 2.0\n    ",
        "klass": "fabric.ThreadingGroup",
        "module": "fabric"
    },
    {
        "base_classes": [
            "paramiko.util.ClosingContextManager"
        ],
        "class_docstring": "\n    A high-level representation of a session with an SSH server.  This class\n    wraps `.Transport`, `.Channel`, and `.SFTPClient` to take care of most\n    aspects of authenticating and opening channels.  A typical use case is::\n\n        client = SSHClient()\n        client.load_system_host_keys()\n        client.connect('ssh.example.com')\n        stdin, stdout, stderr = client.exec_command('ls -l')\n\n    You may pass in explicit overrides for authentication and server host key\n    checking.  The default mechanism is to try to use local key files or an\n    SSH agent (if one is running).\n\n    Instances of this class may be used as context managers.\n\n    .. versionadded:: 1.6\n    ",
        "klass": "paramiko.client.SSHClient",
        "module": "paramiko"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    `.Connection`-wrapping class responsible for managing file upload/download.\n\n    .. versionadded:: 2.0\n    ",
        "klass": "fabric.transfer.Transfer",
        "module": "fabric"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "\n    Thread handler making it easier for parent to handle thread exceptions.\n\n    Based in part on Fabric 1's ThreadHandler. See also Fabric GH issue #204.\n\n    When used directly, can be used in place of a regular ``threading.Thread``.\n    If subclassed, the subclass must do one of:\n\n    - supply ``target`` to ``__init__``\n    - define ``_run()`` instead of ``run()``\n\n    This is because this thread's entire point is to wrap behavior around the\n    thread's execution; subclasses could not redefine ``run()`` without\n    breaking that functionality.\n\n    .. versionadded:: 1.0\n    ",
        "klass": "invoke.util.ExceptionHandlingThread",
        "module": "invoke"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QIODevice"
        ],
        "class_docstring": "QBuffer(parent: QObject = None)\nQBuffer(QByteArray, parent: QObject = None)",
        "klass": "PyQt5.QtCore.QBuffer",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QByteArray()\nQByteArray(int, str)\nQByteArray(Union[QByteArray, bytes, bytearray])",
        "klass": "PyQt5.QtCore.QByteArray",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QCoreApplication(List[str])",
        "klass": "PyQt5.QtCore.QCoreApplication",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QDir(QDir)\nQDir(path: str = '')\nQDir(str, str, sort: QDir.SortFlags = QDir.Name|QDir.IgnoreCase, filters: QDir.Filters = QDir.AllEntries)",
        "klass": "PyQt5.QtCore.QDir",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QFileDevice"
        ],
        "class_docstring": "QFile()\nQFile(str)\nQFile(QObject)\nQFile(str, QObject)",
        "klass": "PyQt5.QtCore.QFile",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QFileDevice"
        ],
        "class_docstring": "QFile()\nQFile(str)\nQFile(QObject)\nQFile(str, QObject)",
        "klass": "PyQt5.Qt.QFile",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QFileInfo()\nQFileInfo(str)\nQFileInfo(QFile)\nQFileInfo(QDir, str)\nQFileInfo(QFileInfo)",
        "klass": "PyQt5.QtCore.QFileInfo",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QLocale()\nQLocale(str)\nQLocale(QLocale.Language, country: QLocale.Country = QLocale.AnyCountry)\nQLocale(QLocale)\nQLocale(QLocale.Language, QLocale.Script, QLocale.Country)",
        "klass": "PyQt5.QtCore.QLocale",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QModelIndex()\nQModelIndex(QModelIndex)\nQModelIndex(QPersistentModelIndex)",
        "klass": "PyQt5.QtCore.QModelIndex",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtNetwork.QAbstractNetworkCache"
        ],
        "class_docstring": "QNetworkDiskCache(parent: QObject = None)",
        "klass": "PyQt5.QtNetwork.QNetworkDiskCache",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.wrapper"
        ],
        "class_docstring": "QObject(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QObject",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QPoint()\nQPoint(int, int)\nQPoint(QPoint)",
        "klass": "PyQt5.QtCore.QPoint",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QPointF()\nQPointF(float, float)\nQPointF(QPoint)\nQPointF(QPointF)",
        "klass": "PyQt5.QtCore.QPointF",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QIODevice"
        ],
        "class_docstring": "QProcess(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QProcess",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QRect()\nQRect(int, int, int, int)\nQRect(QPoint, QPoint)\nQRect(QPoint, QSize)\nQRect(QRect)",
        "klass": "PyQt5.QtCore.QRect",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QRegExp()\nQRegExp(str, cs: Qt.CaseSensitivity = Qt.CaseSensitive, syntax: QRegExp.PatternSyntax = QRegExp.RegExp)\nQRegExp(QRegExp)",
        "klass": "PyQt5.QtCore.QRegExp",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QSettings(str, application: str = '', parent: QObject = None)\nQSettings(QSettings.Scope, str, application: str = '', parent: QObject = None)\nQSettings(QSettings.Format, QSettings.Scope, str, application: str = '', parent: QObject = None)\nQSettings(str, QSettings.Format, parent: QObject = None)\nQSettings(QSettings.Scope, parent: QObject = None)\nQSettings(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QSettings",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QSignalMapper(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QSignalMapper",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QAbstractProxyModel"
        ],
        "class_docstring": "QSortFilterProxyModel(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QSortFilterProxyModel",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QAbstractProxyModel"
        ],
        "class_docstring": "QSortFilterProxyModel(parent: QObject = None)",
        "klass": "PyQt5.Qt.QSortFilterProxyModel",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QTcpServer(parent: QObject = None)",
        "klass": "PyQt5.QtNetwork.QTcpServer",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtNetwork.QAbstractSocket"
        ],
        "class_docstring": "QTcpSocket(parent: QObject = None)",
        "klass": "PyQt5.QtNetwork.QTcpSocket",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QFile"
        ],
        "class_docstring": "QTemporaryFile()\nQTemporaryFile(str)\nQTemporaryFile(QObject)\nQTemporaryFile(str, QObject)",
        "klass": "PyQt5.QtCore.QTemporaryFile",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QTextStream()\nQTextStream(QIODevice)\nQTextStream(QByteArray, mode: Union[QIODevice.OpenMode, QIODevice.OpenModeFlag] = QIODevice.ReadWrite)",
        "klass": "PyQt5.QtCore.QTextStream",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QThread(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QThread",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QTimer(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QTimer",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QTimer(parent: QObject = None)",
        "klass": "PyQt5.Qt.QTimer",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "PyQt5.QtCore.QObject"
        ],
        "class_docstring": "QTranslator(parent: QObject = None)",
        "klass": "PyQt5.QtCore.QTranslator",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QUrl()\nQUrl(str, mode: QUrl.ParsingMode = QUrl.TolerantMode)\nQUrl(QUrl)",
        "klass": "PyQt5.QtCore.QUrl",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "sip.simplewrapper"
        ],
        "class_docstring": "QUrl()\nQUrl(str, mode: QUrl.ParsingMode = QUrl.TolerantMode)\nQUrl(QUrl)",
        "klass": "PyQt5.Qt.QUrl",
        "module": "PyQt5"
    },
    {
        "base_classes": [
            "chainer.link.Link",
            "collections.abc.MutableSequence"
        ],
        "class_docstring": "Composable link with list-like interface.\n\n    This is another example of compositional link. Unlike :class:`Chain`, this\n    class can be used like a list of child links. Each child link is indexed by\n    a non-negative integer, and it maintains the current number of registered\n    child links. The :meth:`add_link` method inserts a new link at the end of\n    the list. It is useful to write a chain with arbitrary number of child\n    links, e.g. an arbitrarily deep multi-layer perceptron.\n\n    This class inherits the methods `index`, `count`, `append`, `reverse`,\n    `extend`, `pop`, `remove` from `collections.abc.MutableSequence` and\n    can be accessed and assigned by index or slice.\n\n    Args:\n        links: Initial child links.\n\n    ",
        "klass": "chainer.ChainList",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.device_resident.DeviceResident"
        ],
        "class_docstring": "Building block of model definitions.\n\n    Link is a building block of neural network models that support various\n    features like handling parameters, defining network fragments,\n    serialization, etc.\n\n    Link is the primitive structure for the model definitions. It supports\n    management of parameter variables and *persistent values* that should be\n    incorporated to serialization.\n\n    Parameter is an instance of :class:`~chainer.Parameter` registered to a\n    link. A :class:`~chainer.Parameter` object can be registered as a\n    parameter of the link by assigning it to an attribute within *an\n    initialization scope*, which is a code surrounded by a\n    :meth:`init_scope` context manager using the ``with`` statement.\n\n    Persistent values are arrays, scalars, or any other serializable values\n    registered via :meth:`register_persistent` or :meth:`add_persistent`.\n\n    .. note::\n       Whereas arbitrary serializable objects can be registered as persistent\n       values, it is strongly recommended that you just register values that\n       should be treated as results of learning. A typical example of\n       persistent values is ones computed during training and required for\n       testing, e.g. running statistics for batch normalization.\n\n    Parameters and persistent values are referred by their names. They can be\n    accessed as attributes of the links. Link class itself manages the lists\n    of names of parameters and persistent values to distinguish parameters and\n    persistent values from other attributes.\n\n    Link can be composed into more complex models. This composition feature is\n    supported by child classes like :class:`Chain` and :class:`ChainList`. One\n    can create a chain by combining one or more links. See the documents for\n    these classes for details.\n\n    As noted above, Link supports the serialization protocol of the\n    :class:`~chainer.Serializer` class. **Note that only parameters and\n    persistent values are saved and loaded.** Other attributes are considered\n    as a part of user program (i.e. a part of network definition). In order to\n    construct a link from saved file, other attributes must be identically\n    reconstructed by user codes.\n\n    .. admonition:: Example\n\n       This is a simple example of custom link definition. Chainer itself also\n       provides many links defined under the :mod:`~chainer.links` module. They\n       might serve as examples, too.\n\n       Consider we want to define a simple primitive link that implements a\n       fully-connected layer based on the :func:`~functions.linear` function.\n       Note that this function takes input units, a weight variable, and a bias\n       variable as arguments. Then, the fully-connected layer can be defined as\n       follows::\n\n          import chainer\n          import chainer.functions as F\n          from chainer import initializers\n          import numpy as np\n\n          class LinearLayer(chainer.Link):\n\n              def __init__(self, n_in, n_out):\n                  super(LinearLayer, self).__init__()\n                  with self.init_scope():\n                      self.W = chainer.Parameter(\n                          initializers.Normal(), (n_out, n_in))\n                      self.b = chainer.Parameter(\n                          initializers.Zero(), (n_out,))\n\n              def forward(self, x):\n                  return F.linear(x, self.W, self.b)\n\n       This example shows that a user can define arbitrary parameters and use\n       them in any methods. Links typically implement the ``forward``\n       operator, although they can also provide other methods to implement the\n       forward propagation.\n\n    Args:\n        params:\n            Names, shapes, and optional dtypes of initial parameters.\n            The keywords are used as the parameter names and the corresponding\n            values consist either of the shape or a tuple of shape and a dtype\n            ``(shape, dtype)``.\n            If only the shape is supplied, the default dtype will be used.\n\n    Attributes:\n        name (str): Name of this link, given by the parent chain (if exists).\n\n    ",
        "klass": "chainer.link.Link",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.device_resident.DeviceResident"
        ],
        "class_docstring": "Building block of model definitions.\n\n    Link is a building block of neural network models that support various\n    features like handling parameters, defining network fragments,\n    serialization, etc.\n\n    Link is the primitive structure for the model definitions. It supports\n    management of parameter variables and *persistent values* that should be\n    incorporated to serialization.\n\n    Parameter is an instance of :class:`~chainer.Parameter` registered to a\n    link. A :class:`~chainer.Parameter` object can be registered as a\n    parameter of the link by assigning it to an attribute within *an\n    initialization scope*, which is a code surrounded by a\n    :meth:`init_scope` context manager using the ``with`` statement.\n\n    Persistent values are arrays, scalars, or any other serializable values\n    registered via :meth:`register_persistent` or :meth:`add_persistent`.\n\n    .. note::\n       Whereas arbitrary serializable objects can be registered as persistent\n       values, it is strongly recommended that you just register values that\n       should be treated as results of learning. A typical example of\n       persistent values is ones computed during training and required for\n       testing, e.g. running statistics for batch normalization.\n\n    Parameters and persistent values are referred by their names. They can be\n    accessed as attributes of the links. Link class itself manages the lists\n    of names of parameters and persistent values to distinguish parameters and\n    persistent values from other attributes.\n\n    Link can be composed into more complex models. This composition feature is\n    supported by child classes like :class:`Chain` and :class:`ChainList`. One\n    can create a chain by combining one or more links. See the documents for\n    these classes for details.\n\n    As noted above, Link supports the serialization protocol of the\n    :class:`~chainer.Serializer` class. **Note that only parameters and\n    persistent values are saved and loaded.** Other attributes are considered\n    as a part of user program (i.e. a part of network definition). In order to\n    construct a link from saved file, other attributes must be identically\n    reconstructed by user codes.\n\n    .. admonition:: Example\n\n       This is a simple example of custom link definition. Chainer itself also\n       provides many links defined under the :mod:`~chainer.links` module. They\n       might serve as examples, too.\n\n       Consider we want to define a simple primitive link that implements a\n       fully-connected layer based on the :func:`~functions.linear` function.\n       Note that this function takes input units, a weight variable, and a bias\n       variable as arguments. Then, the fully-connected layer can be defined as\n       follows::\n\n          import chainer\n          import chainer.functions as F\n          from chainer import initializers\n          import numpy as np\n\n          class LinearLayer(chainer.Link):\n\n              def __init__(self, n_in, n_out):\n                  super(LinearLayer, self).__init__()\n                  with self.init_scope():\n                      self.W = chainer.Parameter(\n                          initializers.Normal(), (n_out, n_in))\n                      self.b = chainer.Parameter(\n                          initializers.Zero(), (n_out,))\n\n              def forward(self, x):\n                  return F.linear(x, self.W, self.b)\n\n       This example shows that a user can define arbitrary parameters and use\n       them in any methods. Links typically implement the ``forward``\n       operator, although they can also provide other methods to implement the\n       forward propagation.\n\n    Args:\n        params:\n            Names, shapes, and optional dtypes of initial parameters.\n            The keywords are used as the parameter names and the corresponding\n            values consist either of the shape or a tuple of shape and a dtype\n            ``(shape, dtype)``.\n            If only the shape is supplied, the default dtype will be used.\n\n    Attributes:\n        name (str): Name of this link, given by the parent chain (if exists).\n\n    ",
        "klass": "chainer.Link",
        "module": "chainer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Object to which observed values are reported.\n\n    Reporter is used to collect values that users want to watch. The reporter\n    object holds a mapping from value names to the actually observed values.\n    We call this mapping `observations`.\n\n    When a value is passed to the reporter, an object called `observer` can be\n    optionally attached. In this case, the name of the observer is added as the\n    prefix of the value name. The observer name should be registered\n    beforehand.\n\n    See the following example::\n\n       >>> from chainer import Reporter, report, report_scope\n       >>>\n       >>> reporter = Reporter()\n       >>> observer = object()  # it can be an arbitrary (reference) object\n       >>> reporter.add_observer('my_observer', observer)\n       >>> observation = {}\n       >>> with reporter.scope(observation):\n       ...     reporter.report({'x': 1}, observer)\n       ...\n       >>> observation\n       {'my_observer/x': 1}\n\n    There are also a global API to add values::\n\n       >>> observation = {}\n       >>> with report_scope(observation):\n       ...     report({'x': 1}, observer)\n       ...\n       >>> observation\n       {'my_observer/x': 1}\n\n    The most important application of Reporter is to report observed values\n    from each link or chain in the training and validation procedures.\n    :class:`~chainer.training.Trainer` and some extensions prepare their own\n    Reporter object with the hierarchy of the target link registered as\n    observers. We can use :func:`report` function inside any links and chains\n    to report the observed values (e.g., training loss, accuracy, activation\n    statistics, etc.).\n\n    Attributes:\n        observation: Dictionary of observed values.\n\n    ",
        "klass": "chainer.reporter.Reporter",
        "module": "chainer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Object to which observed values are reported.\n\n    Reporter is used to collect values that users want to watch. The reporter\n    object holds a mapping from value names to the actually observed values.\n    We call this mapping `observations`.\n\n    When a value is passed to the reporter, an object called `observer` can be\n    optionally attached. In this case, the name of the observer is added as the\n    prefix of the value name. The observer name should be registered\n    beforehand.\n\n    See the following example::\n\n       >>> from chainer import Reporter, report, report_scope\n       >>>\n       >>> reporter = Reporter()\n       >>> observer = object()  # it can be an arbitrary (reference) object\n       >>> reporter.add_observer('my_observer', observer)\n       >>> observation = {}\n       >>> with reporter.scope(observation):\n       ...     reporter.report({'x': 1}, observer)\n       ...\n       >>> observation\n       {'my_observer/x': 1}\n\n    There are also a global API to add values::\n\n       >>> observation = {}\n       >>> with report_scope(observation):\n       ...     report({'x': 1}, observer)\n       ...\n       >>> observation\n       {'my_observer/x': 1}\n\n    The most important application of Reporter is to report observed values\n    from each link or chain in the training and validation procedures.\n    :class:`~chainer.training.Trainer` and some extensions prepare their own\n    Reporter object with the hierarchy of the target link registered as\n    observers. We can use :func:`report` function inside any links and chains\n    to report the observed values (e.g., training loss, accuracy, activation\n    statistics, etc.).\n\n    Attributes:\n        observation: Dictionary of observed values.\n\n    ",
        "klass": "chainer.Reporter",
        "module": "chainer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "__init__(data=None, *, name=None, grad=None, requires_grad=True)\n\n    Array with a structure to keep track of computation.\n\n    Every variable holds a data array of type either :class:`numpy.ndarray` or\n    :class:`cupy.ndarray`.\n\n    A variable object holds a data array and a\n    :class:`~chainer.variable.VariableNode` object of\n    a computational graph. If the variable is constructed by the user, the node\n    is *root* and does not hold any parent. If the variable is constructed by a\n    :class:`~chainer.FunctionNode` object (i.e., by calling functions under\n    ``chainer.functions`` or user-defined functions), or by using operators\n    (see the list below), the node holds a reference to its parent called\n    :attr:`creator_node`.\n    This reference is used in backpropagation to backtrack the graph.\n\n    Users can disable (resp. enable) this chaining behavior by calling\n    :func:`~chainer.no_backprop_mode` (resp.\n    :func:`~chainer.force_backprop_mode`).\n    In the former context, a variable never creates a computational graph,\n    whereas in the latter context, it is forced to create.\n\n    .. note::\n\n        The following operators are defined for variable(s).\n\n        * Indexing: ``a[slices]`` (:meth:`__getitem__`)\n        * Addition: ``a + b`` (:meth:`__add__`, :meth:`__radd__`)\n        * Subtraction: ``a - b`` (:meth:`__sub__`, :meth:`__rsub__`)\n        * Multiplication: ``a * b`` (:meth:`__mul__`, :meth:`__rmul__`)\n        * Division: ``a / b`` (:meth:`__div__`, :meth:`__rdiv__`,                                :meth:`__truediv__`, :meth:`__rtruediv__`)\n        * Floor Division: ``a // b`` (:meth:`__floordiv__`,                                       :meth:`__rfloordiv__`)\n        * Exponentiation: ``a ** b`` (:meth:`__pow__`, :meth:`__rpow__`)\n        * Matrix Multiplication: ``a @ b`` (:meth:`__matmul__`,                                             :meth:`__rmatmul__`)\n        * Negation (Arithmetic): ``- a`` (:meth:`__neg__`)\n        * Absolute value: ``abs(a)`` (:meth:`__abs__`)\n\n    Args:\n        data (:ref:`ndarray`): Initial data array.\n        name (str): Name of the variable.\n        grad (:ref:`ndarray`): Initial gradient array.\n        requires_grad (bool): Boolean indicating whether ``grad`` will be set\n            in backward calculation.\n\n    ",
        "klass": "chainer.Variable",
        "module": "chainer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Writer class that makes PickleDataset.\n\n    To make :class:`PickleDataset`, a user needs to prepare data using\n    :class:`PickleDatasetWriter`.\n\n    Args:\n        writer: File like object that supports ``write`` and ``tell`` methods.\n        protocol (int): Valid protocol for :mod:`pickle`.\n\n    .. seealso: chainer.datasets.PickleDataset\n\n    ",
        "klass": "chainer.datasets.PickleDatasetWriter",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.dataset.iterator.Iterator"
        ],
        "class_docstring": "Dataset iterator that loads examples in parallel.\n\n    This is an implementation of :class:`~chainer.dataset.Iterator` that loads\n    examples with worker processes. It uses the standard :mod:`multiprocessing`\n    module to parallelize the loading. The dataset is sent to the worker\n    processes in the standard way using pickle.\n\n    Note that this iterator effectively prefetches the examples for the next\n    batch asynchronously after the current batch is returned.\n\n    This iterator saves ``-1`` instead of ``None`` in snapshots since some\n    serializers do not support ``None``.\n\n    .. note::\n\n            When you are using OpenCV somewhere in your code and the\n            ``MultiprocessIterator`` is used in the training code, the\n            training loop may get stuck at some point. In such situation,\n            there are several workarounds to prevent the process got stuck.\n\n            1. Set the environment variable as follows: ``OMP_NUM_THREADS=1``\n            2. Add ``cv2.setNumThreads(0)`` right after ``import cv2`` in your\n               training script.\n            3. Use :class:`~chainer.iterators.MultithreadIterator` instead of\n               ``MultiprocessIterator``.\n\n    Args:\n        dataset (~chainer.dataset.Dataset): Dataset to iterate.\n        batch_size (int): Number of examples within each batch.\n        repeat (bool): If ``True``, it infinitely loops over the dataset.\n            Otherwise, it stops iteration at the end of the first epoch.\n        shuffle (bool): If ``True``, the order of examples is shuffled at the\n            beginning of each epoch. Otherwise, examples are extracted in the\n            order of indexes. If ``None`` and no ``order_sampler`` is given,\n            the behavior is the same as the case with ``shuffle=True``.\n        n_processes (int): Number of worker processes. The number of CPUs is\n            used by default.\n        n_prefetch (int): Number of prefetch batches.\n        shared_mem (int): The size of using shared memory per data.\n            If ``None``, size is adjusted automatically.\n        dataset_timeout (float): :class:`MultiprocessIterator.TimeoutWarning`\n            will be issued after this time in seconds elapsed in each dataset\n            realization. ``None`` to disable the warning. You can turn this\n            warning into an error by using :func:`warnings.simplefilter`::\n\n                warnings.simplefilter(\n                    'error',\n                    chainer.iterators.MultiprocessIterator.TimeoutWarning)\n\n        order_sampler (callable): A callable that generates the order\n            of the indices to sample in the next epoch when a epoch finishes.\n            This function should take two arguments: the current order\n            and the current position of the iterator.\n            This should return the next order. The size of the order\n            should remain constant.\n            This option cannot be used when ``shuffle`` is not ``None``.\n        maxtasksperchild (int): Number of tasks a worker of prefetch process\n            can complete before it will exit and be replaced with a fresh\n            worker process, to enable unused resources to be freed. If\n            ``None``, worker processes will live as long as the pool.\n\n    ",
        "klass": "chainer.iterators.MultiprocessIterator",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.dataset.iterator.Iterator"
        ],
        "class_docstring": "Dataset iterator that serially reads the examples.\n\n    This is a simple implementation of :class:`~chainer.dataset.Iterator`\n    that just visits each example in either the order of indexes or a shuffled\n    order.\n\n    To avoid unintentional performance degradation, the ``shuffle`` option is\n    set to ``True`` by default. For validation, it is better to set it to\n    ``False`` when the underlying dataset supports fast slicing. If the\n    order of examples has an important meaning and the updater depends on the\n    original order, this option should be set to ``False``.\n\n    This iterator saves ``-1`` instead of ``None`` in snapshots since some\n    serializers do not support ``None``.\n\n    Args:\n        dataset: Dataset to iterate.\n        batch_size (int): Number of examples within each batch.\n        repeat (bool): If ``True``, it infinitely loops over the dataset.\n            Otherwise, it stops iteration at the end of the first epoch.\n        shuffle (bool): If ``True``, the order of examples is shuffled at the\n            beginning of each epoch. Otherwise, examples are extracted in the\n            order of indexes. If ``None`` and no ``order_sampler`` is given,\n            the behavior is the same as the case with ``shuffle=True``.\n        order_sampler (callable): A callable that generates the order\n            of the indices to sample in the next epoch when a epoch finishes.\n            This function should take two arguments: the current order\n            and the current position of the iterator.\n            This should return the next order. The size of the order\n            should remain constant.\n            This option cannot be used when ``shuffle`` is not ``None``.\n\n    ",
        "klass": "chainer.iterators.SerialIterator",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.link_hook.LinkHook"
        ],
        "class_docstring": "Spectral Normalization link hook implementation.\n\n    This hook normalizes a weight using max singular value and this value\n    is computed via power iteration method. Currently, this hook is supposed to\n    be added to :class:`chainer.links.Linear`, :class:`chainer.links.EmbedID`,\n    :class:`chainer.links.Convolution2D`, :class:`chainer.links.ConvolutionND`,\n    :class:`chainer.links.Deconvolution2D`,\n    and :class:`chainer.links.DeconvolutionND`. However, you can use this to\n    other links like RNNs by specifying ``weight_name``.\n    It is highly recommended to add this hook before optimizer setup because\n    this hook add a scaling parameter ``gamma`` if ``use_gamma`` is True.\n    Otherwise, the registered ``gamma`` will not be updated.\n\n    .. math::\n\n       \\bar{\\mathbf{W}} &=& \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n       \\text{, where} \\ \\sigma(\\mathbf{W}) &:=&\n        \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0}\n       \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n        = \\max_{\\|\\mathbf{h}\\|_2 \\le 1} \\|\\mathbf{W}\\mathbf{h}\\|_2\n\n    See: T. Miyato et. al., `Spectral Normalization for Generative Adversarial\n    Networks <https://arxiv.org/abs/1802.05957>`_\n\n    Args:\n        n_power_iteration (int): Number of power iteration.\n            The default value is 1.\n        eps (float): Numerical stability in norm calculation.\n            The default value is 1e-6 for the compatibility with\n            mixed precision training. The value used in the author's\n            implementation is 1e-12.\n        use_gamma (bool): If ``True``, weight scaling parameter gamma which is\n            initialized by initial weight's max singular value is introduced.\n        factor (float, None): Scaling parameter to divide maximum singular\n            value.  The default value is 1.0.\n        weight_name (str): Link's weight name to apply this hook. The default\n            value is ``'W'``.\n        name (str or None): Name of this hook. The default value is\n            ``'SpectralNormalization'``.\n\n    Attributes:\n        vector_name (str): Name of the approximate first left singular vector\n            registered in the target link.\n            the target link.\n        axis (int): Axis of weight represents the number of output\n            feature maps or output units (``out_channels`` and\n            ``out_size``, respectively).\n\n    .. admonition:: Example\n\n        There are almost the same but 2 ways to apply spectral normalization\n        (SN) hook to links.\n\n        1. Initialize link and SN separately. This makes it easy to handle\n        buffer and parameter of links registered by SN hook.\n\n            >>> l = L.Convolution2D(3, 5, 3)\n            >>> hook = chainer.link_hooks.SpectralNormalization()\n            >>> _ = l.add_hook(hook)\n            >>> # Check the shape of the first left singular vector.\n            >>> getattr(l, hook.vector_name).shape\n            (5,)\n            >>> # Delete SN hook from this link.\n            >>> l.delete_hook(hook.name)\n\n        2. Initialize both link and SN hook at one time. This makes it easy to\n        define your original :class:`~chainer.Chain`.\n\n            >>> # SN hook handles lazy initialization!\n            >>> layer = L.Convolution2D(\n            ...     5, 3, stride=1, pad=1).add_hook(\n            ...         chainer.link_hooks.SpectralNormalization())\n    ",
        "klass": "chainer.link_hooks.spectral_normalization.SpectralNormalization",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.link_hook.LinkHook"
        ],
        "class_docstring": "Link hook for measuring elapsed time of :meth:`Link.forward() <chainer.Link.forward>`.\n\n    Example:\n        Code example::\n\n            from chainer.link_hooks import TimerHook\n            hook = TimerHook()\n            with hook:\n                trainer.run()\n            hook.print_report()\n\n        Output example::\n\n              LinkName  ElapsedTime  Occurrence\n                Linear     41.42sec        2100\n                   MLP     42.09sec         700\n            Classifier     42.39sec         700\n\n        where *LinkName* is the name of link that calls the hook,\n        and *ElapsedTime* is the elapsed time the link consumed,\n        and *Occurrence* is the number of calls.\n    Warning:\n        Call graph of links are hierarchical. That means reported elapsed times\n        may be overlapping with each other and the sum may exceed the total\n        time.\n    Attributes:\n        call_history: List of measurement results. It consists of pairs of\n            the name of the link that calls this hook and the elapsed time\n            the :meth:`forward` method of link consumes.\n    ",
        "klass": "chainer.link_hooks.TimerHook",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.link_hook.LinkHook"
        ],
        "class_docstring": "Weight Standardization (WS) link hook implementation.\n\n    This hook standardizes a weight by *weight statistics*.\n\n    This link hook implements a WS which computes the mean and variance along\n    axis \"output channels\", then normalizes by these statistics.\n    WS improves training by reducing the Lipschitz constants of the loss and\n    the gradients like batch normalization (BN) but without relying on large\n    batch sizes during training. Specifically, the performance of WS with group\n    normalization (GN) trained with small-batch is able to match or outperforms\n    that of BN trained with large-batch.\n    WS is originally proposed for 2D convolution layers followed by mainly GN\n    and sometimes BN.\n    Note that this hook is able to handle layers such as N-dimensional\n    convolutional, linear and embedding layers but there is no guarantee that\n    this hook helps training.\n\n    See: Siyuan Qiao et. al., `Weight Standardization\n    <https://arxiv.org/abs/1903.10520>`_\n\n    Args:\n        eps (float): Numerical stability in standard deviation calculation.\n            The default value is 1e-5.\n        weight_name (str): Link's weight name to appky this hook. The default\n            value is ``'W'``.\n        name (str or None): Name of this hook. The default value is\n            ``'WeightStandardization'``.\n    ",
        "klass": "chainer.link_hooks.weight_standardization.WeightStandardization",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.link.Chain"
        ],
        "class_docstring": "A simple classifier model.\n\n    This is an example of chain that wraps another chain. It computes the\n    loss and accuracy based on a given input/label pair.\n\n    Args:\n        predictor (~chainer.Link): Predictor network.\n        lossfun (callable):\n            Loss function.\n            You can specify one of loss functions from\n            :doc:`built-in loss functions </reference/functions>`, or\n            your own loss function (see the example below).\n            It should not be an\n            :doc:`loss functions with parameters </reference/links>`\n            (i.e., :class:`~chainer.Link` instance).\n            The function must accept two argument (an output from predictor\n            and its ground truth labels), and return a loss.\n            Returned value must be a Variable derived from the input Variable\n            to perform backpropagation on the variable.\n        accfun (callable):\n            Function that computes accuracy.\n            You can specify one of evaluation functions from\n            :doc:`built-in evaluation functions </reference/functions>`, or\n            your own evaluation function.\n            The signature of the function is the same as ``lossfun``.\n        label_key (int or str): Key to specify label variable from arguments.\n            When it is ``int``, a variable in positional arguments is used.\n            And when it is ``str``, a variable in keyword arguments is used.\n\n    Attributes:\n        predictor (~chainer.Link): Predictor network.\n        lossfun (callable):\n            Loss function.\n            See the description in the arguments for details.\n        accfun (callable):\n            Function that computes accuracy.\n            See the description in the arguments for details.\n        y (~chainer.Variable): Prediction for the last minibatch.\n        loss (~chainer.Variable): Loss value for the last minibatch.\n        accuracy (~chainer.Variable): Accuracy for the last minibatch.\n        compute_accuracy (bool): If ``True``, compute accuracy on the forward\n            computation. The default value is ``True``.\n\n    .. note::\n        This link uses :func:`chainer.softmax_cross_entropy` with\n        default arguments as a loss function (specified by ``lossfun``),\n        if users do not explicitly change it. In particular, the loss function\n        does not support double backpropagation.\n        If you need second or higher order differentiation, you need to turn\n        it on with ``enable_double_backprop=True``:\n\n          >>> import chainer.functions as F\n          >>> import chainer.links as L\n          >>>\n          >>> def lossfun(x, t):\n          ...     return F.softmax_cross_entropy(\n          ...         x, t, enable_double_backprop=True)\n          >>>\n          >>> predictor = L.Linear(10)\n          >>> model = L.Classifier(predictor, lossfun=lossfun)\n\n    ",
        "klass": "chainer.links.Classifier",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.link.Chain"
        ],
        "class_docstring": "Caffe emulator based on the model file of Caffe.\n\n    Given a protocol buffers file of a Caffe model, this class loads and\n    emulates it on :class:`~chainer.Variable` objects. It supports the official\n    reference models provided by BVLC.\n\n    .. note::\n\n       CaffeFunction ignores the following layers:\n\n       - Layers that CaffeFunction does not support (including data layers)\n       - Layers that have no top blobs\n       - Layers whose bottom blobs are incomplete (i.e., some or all of them\n         are not given nor computed)\n\n    .. warning::\n\n       It does not support full compatibility against Caffe. Some layers and\n       configurations are not implemented in Chainer yet, though the reference\n       models provided by the BVLC team are supported except data layers.\n\n    .. admonition:: Example\n\n       Consider we want to extract the (unnormalized) log class probability\n       of given images using BVLC reference CaffeNet. The model can be\n       downloaded from:\n\n       http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel\n\n       We want to compute the ``fc8`` blob from the ``data`` blob. It is simply\n       written as follows::\n\n          # Load the model\n          func = CaffeFunction('path/to/bvlc_reference_caffenet.caffemodel')\n\n          # Minibatch of size 10\n          x_data = numpy.ndarray((10, 3, 227, 227), dtype=numpy.float32)\n          ...  # (Fill the minibatch here)\n\n          # Forward the pre-trained net\n          x = Variable(x_data)\n          y, = func(inputs={'data': x}, outputs=['fc8'])\n\n       The result ``y`` contains the Variable corresponding to the ``fc8``\n       blob. The computational graph is memorized as a usual forward\n       computation in Chainer, so we can run backprop through this pre-trained\n       net.\n\n    Args:\n        model_path (str): Path to the binary-proto model file of Caffe.\n\n    Attributes:\n        forwards (dict): A mapping from layer names to corresponding functions.\n\n    ",
        "klass": "chainer.links.caffe.CaffeFunction",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "Zeiler's ADADELTA.\n\n    See: http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf\n\n    Args:\n        rho (float): Exponential decay rate of the first and second order\n            moments.\n        eps (float): Small value for the numerical stability.\n\n    ",
        "klass": "chainer.optimizers.AdaDelta",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "AdaGrad optimizer.\n\n    See: http://jmlr.org/papers/v12/duchi11a.html\n\n    Args:\n        lr (float): Learning rate.\n        eps (float): Small value for the numerical stability.\n\n    ",
        "klass": "chainer.optimizers.AdaGrad",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "Adam optimizer.\n\n    See: `Adam: A Method for Stochastic Optimization\n    <https://arxiv.org/abs/1412.6980v8>`_\n\n    Modified for proper weight decay (also called\n    :class:`~chainer.optimizers.AdamW`).\n    AdamW introduces the additional parameters ``eta``\n    and ``weight_decay_rate``, which can be used to properly scale the\n    learning rate, and decouple the weight decay rate from ``alpha``,\n    as shown in the below paper.\n\n    Note that with the default values ``eta = 1`` and\n    ``weight_decay_rate = 0``, this implementation is identical to\n    the standard Adam method.\n\n    See: `Fixing Weight Decay Regularization in Adam\n    <https://openreview.net/forum?id=rk6qdGgCZ>`_\n\n    A flag ``amsgrad`` to use the :class:`~chainer.optimizers.AMSGrad`\n    variant of Adam from the paper:\n    `On the Convergence of Adam and Beyond\n    <https://openreview.net/forum?id=ryQu7f-RZ>`_\n\n    A flag ``adabound`` to use the :class:`~chainer.optimizers.AdaBound`\n    variant of Adam from the paper:\n    `Adaptive Gradient Methods with Dynamic Bound of Learning Rate\n    <https://openreview.net/forum?id=Bkg3g2R9FX>`_\n\n    If both ``amsgrad`` and ``adabound`` are ``True``, the optimizer is\n    equivalent to :class:`~chainer.optimizers.AMSBound` proposed in the\n    AdaBound paper.\n\n    Args:\n        alpha (float): Coefficient of learning rate.\n        beta1 (float): Exponential decay rate of the first order moment.\n        beta2 (float): Exponential decay rate of the second order moment.\n        eps (float): Small value for the numerical stability.\n        eta (float): Schedule multiplier, can be used for warm restarts.\n        weight_decay_rate (float): Weight decay rate.\n        amsgrad (bool): Whether to use AMSGrad variant of Adam.\n        adabound (bool): Whether to use the AdaBound variant of Adam.\n        final_lr (float): Final (SGD) learning rate in AdaBound.\n        gamma (float): Convergence speed of the bound functions in AdaBound.\n\n    ",
        "klass": "chainer.optimizers.Adam",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "Momentum SGD optimizer.\n\n    Args:\n        lr (float): Learning rate.\n        momentum (float): Exponential decay rate of the first order moment.\n\n    ",
        "klass": "chainer.optimizers.MomentumSGD",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "RMSprop optimizer.\n\n    See: T. Tieleman and G. Hinton (2012). Lecture 6.5 - rmsprop, COURSERA:\n    Neural Networks for Machine Learning.\n\n    Args:\n        lr (float): Learning rate.\n        alpha (float): Exponential decay rate of the second order moment.\n        eps (float): Small value for the numerical stability.\n        eps_inside_sqrt (bool): When ``True``, gradient will be divided by\n            :math:`\\sqrt{ms + eps}` where ``ms`` is the mean square. When\n            ``False`` (default), gradient will be divided by\n            :math:`\\sqrt{ms} + eps` instead.\n            This option may be convenient for users porting code from other\n            frameworks;\n            see `#4754 <https://github.com/chainer/chainer/issues/4754>`__ for\n            details.\n\n    ",
        "klass": "chainer.optimizers.RMSprop",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "Alex Graves's RMSprop.\n\n    See: https://arxiv.org/abs/1308.0850\n\n    Args:\n        lr (float): Learning rate.\n        alpha (float): Exponential decay rate of the first and second order\n            moments of the raw gradient.\n        momentum (float): Exponential decay rate of the first order moment of\n            the adjusted gradient.\n        eps (float): Small value for the numerical stability.\n\n    ",
        "klass": "chainer.optimizers.RMSpropGraves",
        "module": "chainer"
    },
    {
        "base_classes": [
            "chainer.optimizer.GradientMethod"
        ],
        "class_docstring": "Vanilla Stochastic Gradient Descent.\n\n    Args:\n        lr (float): Learning rate.\n\n    ",
        "klass": "chainer.optimizers.SGD",
        "module": "chainer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The standard training loop in Chainer.\n\n    Trainer is an implementation of a training loop. Users can invoke the\n    training by calling the :meth:`run` method.\n\n    Each iteration of the training loop proceeds as follows.\n\n    - Update of the parameters. It includes the mini-batch loading, forward\n      and backward computations, and an execution of the update formula.\n      These are all done by the update object held by the trainer.\n    - Invocation of trainer extensions in the descending order of their\n      priorities. A trigger object is attached to each extension, and it\n      decides at each iteration whether the extension should be executed.\n      Trigger objects are callable objects that take the trainer object as the\n      argument and return a boolean value indicating whether the extension\n      should be called or not.\n\n    Extensions are callable objects that take the trainer object as the\n    argument. There are three ways to define custom extensions: inheriting the\n    :class:`Extension` class, decorating functions by :func:`make_extension`,\n    and defining any callable including lambda functions. See\n    :class:`Extension` for more details on custom extensions and how to\n    configure them.\n\n    Users can register extensions to the trainer by calling the :meth:`extend`\n    method, where some configurations can be added.\n\n    - Trigger object, which is also explained above. In most cases,\n      :class:`IntervalTrigger` is used, in which case users can simply specify\n      a tuple of the interval length and its unit, like\n      ``(1000, 'iteration')`` or ``(1, 'epoch')``.\n    - The order of execution of extensions is determined by their priorities.\n      Extensions of higher priorities are invoked earlier. There are three\n      standard values for the priorities:\n\n      - ``PRIORITY_WRITER``. This is the priority for extensions that write\n        some records to the :attr:`observation` dictionary. It includes cases\n        that the extension directly adds values to the observation dictionary,\n        or the extension uses the :func:`chainer.report` function to report\n        values to the observation dictionary.\n      - ``PRIORITY_EDITOR``. This is the priority for extensions that edit the\n        :attr:`observation` dictionary based on already reported values.\n      - ``PRIORITY_READER``. This is the priority for extensions that only read\n        records from the :attr:`observation` dictionary. This is also suitable\n        for extensions that do not use the :attr:`observation` dictionary at\n        all.\n\n    The current state of the trainer object and objects handled by the trainer\n    can be serialized through the standard serialization protocol of Chainer.\n    It enables us to easily suspend and resume the training loop.\n\n    .. code-block:: python\n\n        >>> serializers.save_npz('my.trainer', trainer)  # To suspend and save\n        >>> serializers.load_npz('my.trainer', trainer)  # To load and resume\n\n    The :meth:`~chainer.training.extensions.snapshot` method makes regular\n    snapshots of the :class:`~chainer.training.Trainer` object during training.\n\n    .. note::\n       The serialization does not recover everything of the training loop. It\n       only recovers the states which change over the training (e.g.\n       parameters, optimizer states, the batch iterator state, extension\n       states, etc.). You must initialize the objects correctly before\n       deserializing the states.\n\n       On the other hand, it means that users can change the settings on\n       deserialization. For example, the exit condition can be changed on the\n       deserialization, so users can train the model for some iterations,\n       suspend it, and then resume it with larger number of total iterations.\n\n    During the training, it also creates a :class:`~chainer.Reporter` object to\n    store observed values on each update. For each iteration, it creates a\n    fresh observation dictionary and stores it in the :attr:`observation`\n    attribute.\n\n    Links of the target model of each optimizer are registered to the reporter\n    object as observers, where the name of each observer is constructed as the\n    format ``<optimizer name><link name>``. The link name is given by the\n    :meth:`chainer.Link.namedlink` method, which represents the path to each\n    link in the hierarchy. Other observers can be registered by accessing the\n    reporter object via the :attr:`reporter` attribute.\n\n    The default trainer is `plain`, i.e., it does not contain any extensions.\n\n    Args:\n        updater (~chainer.training.Updater): Updater object. It defines how to\n            update the models.\n        stop_trigger: Trigger that determines when to stop the training loop.\n            If it is not callable, it is passed to :class:`IntervalTrigger`.\n        out: Output directory.\n        extensions: Extensions registered to the trainer.\n\n    Attributes:\n        updater: The updater object for this trainer.\n        stop_trigger: Trigger that determines when to stop the training loop.\n            The training loop stops at the iteration on which this trigger\n            returns ``True``.\n        observation: Observation of values made at the last update. See the\n            :class:`Reporter` class for details.\n        out: Output directory.\n        reporter: Reporter object to report observed values.\n\n    ",
        "klass": "chainer.training.Trainer",
        "module": "chainer"
    },
    {
        "base_classes": [
            "urwid.widget.delegate_to_widget_mixin.locals.DelegateToWidgetMixin",
            "urwid.decoration.WidgetDecoration"
        ],
        "class_docstring": "\n    AttrMap is a decoration that maps one set of attributes to another.\n    This object will pass all function calls and variable references to the\n    wrapped widget.\n    ",
        "klass": "urwid.AttrMap",
        "module": "urwid"
    },
    {
        "base_classes": [
            "urwid.widget.Widget",
            "urwid.container.WidgetContainerMixin",
            "urwid.container.WidgetContainerListContentsMixin"
        ],
        "class_docstring": "\n    Widgets arranged horizontally in columns from left to right\n    ",
        "klass": "urwid.Columns",
        "module": "urwid"
    },
    {
        "base_classes": [
            "urwid.widget.Text"
        ],
        "class_docstring": "\n    Text editing widget implements cursor movement, text insertion and\n    deletion.  A caption may prefix the editing area.  Uses text class\n    for text layout.\n\n    Users of this class may listen for ``\"change\"`` or ``\"postchange\"``\n    events.  See :func:``connect_signal``.\n\n    * ``\"change\"`` is sent just before the value of edit_text changes.\n      It receives the new text as an argument.  Note that ``\"change\"`` cannot\n      change the text in question as edit_text changes the text afterwards.\n    * ``\"postchange\"`` is sent after the value of edit_text changes.\n      It receives the old value of the text as an argument and thus is\n      appropriate for changing the text.  It is possible for a ``\"postchange\"``\n      event handler to get into a loop of changing the text and then being\n      called when the event is re-emitted.  It is up to the event\n      handler to guard against this case (for instance, by not changing the\n      text if it is signaled for for text that it has already changed once).\n    ",
        "klass": "urwid.Edit",
        "module": "urwid"
    },
    {
        "base_classes": [
            "urwid.widget.Widget",
            "urwid.container.WidgetContainerMixin"
        ],
        "class_docstring": "\n    Frame widget is a box widget with optional header and footer\n    flow widgets placed above and below the box widget.\n\n    .. note:: The main difference between a Frame and a :class:`Pile` widget\n        defined as: `Pile([('pack', header), body, ('pack', footer)])` is that\n        the Frame will not automatically change focus up and down in response to\n        keystrokes.\n    ",
        "klass": "urwid.Frame",
        "module": "urwid"
    },
    {
        "base_classes": [
            "urwid.widget.Widget",
            "urwid.container.WidgetContainerMixin"
        ],
        "class_docstring": "\n    a horizontally stacked list of widgets\n    ",
        "klass": "urwid.ListBox",
        "module": "urwid"
    },
    {
        "base_classes": [
            "urwid.widget.Widget"
        ],
        "class_docstring": "\n    a horizontally resizeable text widget\n    ",
        "klass": "urwid.Text",
        "module": "urwid"
    },
    {
        "base_classes": [
            "buildbot.util.ComparableMixin"
        ],
        "class_docstring": "\n    I represent a set of properties that can be interpolated into various\n    strings in buildsteps.\n\n    @ivar properties: dictionary mapping property values to tuples\n        (value, source), where source is a string identifying the source\n        of the property.\n\n    Objects of this class can be read like a dictionary -- in this case,\n    only the property value is returned.\n\n    As a special case, a property value of None is returned as an empty\n    string when used as a mapping.\n    ",
        "klass": "buildbot.process.properties.Properties",
        "module": "buildbot"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "\n    A basic abstraction for an error that has occurred.\n\n    This is necessary because Python's built-in error mechanisms are\n    inconvenient for asynchronous communication.\n\n    The C{stack} and C{frame} attributes contain frames.  Each frame is a tuple\n    of (funcName, fileName, lineNumber, localsItems, globalsItems), where\n    localsItems and globalsItems are the contents of\n    C{locals().items()}/C{globals().items()} for that frame, or an empty tuple\n    if those details were not captured.\n\n    @ivar value: The exception instance responsible for this failure.\n    @ivar type: The exception's class.\n    @ivar stack: list of frames, innermost last, excluding C{Failure.__init__}.\n    @ivar frames: list of frames, innermost first.\n    ",
        "klass": "twisted.python.failure.Failure",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Call a function repeatedly.\n\n    If C{f} returns a deferred, rescheduling will not take place until the\n    deferred has fired. The result value is ignored.\n\n    @ivar f: The function to call.\n    @ivar a: A tuple of arguments to pass the function.\n    @ivar kw: A dictionary of keyword arguments to pass to the function.\n    @ivar clock: A provider of\n        L{twisted.internet.interfaces.IReactorTime}.  The default is\n        L{twisted.internet.reactor}. Feel free to set this to\n        something else, but it probably ought to be set *before*\n        calling L{start}.\n\n    @type running: C{bool}\n    @ivar running: A flag which is C{True} while C{f} is scheduled to be called\n        (or is currently being called). It is set to C{True} when L{start} is\n        called and set to C{False} when L{stop} is called or if C{f} raises an\n        exception. In either case, it will be C{False} by the time the\n        C{Deferred} returned by L{start} fires its callback or errback.\n\n    @type _realLastTime: C{float}\n    @ivar _realLastTime: When counting skips, the time at which the skip\n        counter was last invoked.\n\n    @type _runAtStart: C{bool}\n    @ivar _runAtStart: A flag indicating whether the 'now' argument was passed\n        to L{LoopingCall.start}.\n    ",
        "klass": "twisted.internet.task.LoopingCall",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Basic message object.\n\n    A message object is defined as something that has a bunch of RFC 2822\n    headers and a payload.  It may optionally have an envelope header\n    (a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\n    multipart or a message/rfc822), then the payload is a list of Message\n    objects, otherwise it is a string.\n\n    Message objects implement part of the `mapping' interface, which assumes\n    there is exactly one occurrence of the header per message.  Some headers\n    do in fact appear multiple times (e.g. Received) and for those headers,\n    you must use the explicit API to set or get all the headers.  Not all of\n    the mapping methods are implemented.\n    ",
        "klass": "email.message.Message",
        "module": "email"
    },
    {
        "base_classes": [
            "buildbot.process.buildstep.BuildStep"
        ],
        "class_docstring": "\n    Run a shell command locally - on the buildmaster.  The shell command\n    COMMAND is specified just as for a RemoteShellCommand.  Note that extra\n    logfiles are not supported.\n    ",
        "klass": "buildbot.steps.master.MasterShellCommand",
        "module": "buildbot"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This is a callback which will be put off until later.\n\n    Why do we want this? Well, in cases where a function in a threaded\n    program would block until it gets a result, for Twisted it should\n    not block. Instead, it should return a L{Deferred}.\n\n    This can be implemented for protocols that run over the network by\n    writing an asynchronous protocol for L{twisted.internet}. For methods\n    that come from outside packages that are not under our control, we use\n    threads (see for example L{twisted.enterprise.adbapi}).\n\n    For more information about Deferreds, see doc/core/howto/defer.html or\n    U{http://twistedmatrix.com/documents/current/core/howto/defer.html}\n\n    When creating a Deferred, you may provide a canceller function, which\n    will be called by d.cancel() to let you do any clean-up necessary if the\n    user decides not to wait for the deferred to complete.\n\n    @ivar called: A flag which is C{False} until either C{callback} or\n        C{errback} is called and afterwards always C{True}.\n    @type called: L{bool}\n\n    @ivar paused: A counter of how many unmatched C{pause} calls have been made\n        on this instance.\n    @type paused: L{int}\n\n    @ivar _suppressAlreadyCalled: A flag used by the cancellation mechanism\n        which is C{True} if the Deferred has no canceller and has been\n        cancelled, C{False} otherwise.  If C{True}, it can be expected that\n        C{callback} or C{errback} will eventually be called and the result\n        should be silently discarded.\n    @type _suppressAlreadyCalled: L{bool}\n\n    @ivar _runningCallbacks: A flag which is C{True} while this instance is\n        executing its callback chain, used to stop recursive execution of\n        L{_runCallbacks}\n    @type _runningCallbacks: L{bool}\n\n    @ivar _chainedTo: If this L{Deferred} is waiting for the result of another\n        L{Deferred}, this is a reference to the other Deferred.  Otherwise,\n        L{None}.\n    ",
        "klass": "twisted.internet.defer.Deferred",
        "module": "twisted"
    },
    {
        "base_classes": [
            "twisted.web.client._AgentBase"
        ],
        "class_docstring": "\n    L{Agent} is a very basic HTTP client.  It supports I{HTTP} and I{HTTPS}\n    scheme URIs.\n\n    @ivar _pool: An L{HTTPConnectionPool} instance.\n\n    @ivar _endpointFactory: The L{IAgentEndpointFactory} which will\n        be used to create endpoints for outgoing connections.\n\n    @since: 9.0\n    ",
        "klass": "twisted.web.client.Agent",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Works similarly to python 3.2+ TemporaryDirectory except the\n        also sets the permissions of the created directory and\n\n        Note, that Windows ignores the permissions.\n    ",
        "klass": "buildbot.util.private_tempdir.PrivateTemporaryDirectory",
        "module": "buildbot"
    },
    {
        "base_classes": [
            "twisted.python.logfile.BaseLogFile"
        ],
        "class_docstring": "\n    A log file that can be rotated.\n\n    A rotateLength of None disables automatic log rotation.\n    ",
        "klass": "twisted.python.logfile.LogFile",
        "module": "twisted"
    },
    {
        "base_classes": [
            "autobahn.twisted.websocket.WebSocketAdapterFactory",
            "autobahn.websocket.protocol.WebSocketServerFactory",
            "twisted.internet.protocol.ServerFactory"
        ],
        "class_docstring": "\n    Base class for Twisted-based WebSocket server factories.\n\n    Implements :class:`autobahn.websocket.interfaces.IWebSocketServerChannelFactory`\n    ",
        "klass": "autobahn.twisted.websocket.WebSocketServerFactory",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "email.mime.nonmultipart.MIMENonMultipart"
        ],
        "class_docstring": "Class representing message/* MIME documents.",
        "klass": "email.mime.message.MIMEMessage",
        "module": "email"
    },
    {
        "base_classes": [
            "mailman.email.message.Message"
        ],
        "class_docstring": "Class for internally crafted messages.",
        "klass": "mailman.email.message.UserNotification",
        "module": "mailman"
    },
    {
        "base_classes": [
            "mailman.bin.master.Loop"
        ],
        "class_docstring": "A testable master loop watcher.",
        "klass": "mailman.testing.helpers.TestableMaster",
        "module": "mailman"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A validator of parameter input.",
        "klass": "mailman.rest.validator.Validator",
        "module": "mailman"
    },
    {
        "base_classes": [
            "mailbox.MMDF"
        ],
        "class_docstring": "A mailbox that interoperates with the 'with' statement.",
        "klass": "mailman.utilities.mailbox.Mailbox",
        "module": "mailman"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A decorator/context manager for temporarily setting configurations.",
        "klass": "mailman.testing.helpers.configuration",
        "module": "mailman"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The TarFile Class provides an interface to tar archives.\n    ",
        "klass": "tarfile.TarFile",
        "module": "tarfile"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An API Version Request object.",
        "klass": "senlin.api.common.version_request.APIVersionRequest",
        "module": "senlin"
    },
    {
        "base_classes": [
            "oslo_context.context.RequestContext"
        ],
        "class_docstring": "Stores information about the security context.\n\n    The context encapsulates information related to the user accessing the\n    system, as well as additional request information.\n    ",
        "klass": "senlin.common.context.RequestContext",
        "module": "senlin"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An action can be performed on a cluster or a node of a cluster.",
        "klass": "senlin.engine.actions.base.Action",
        "module": "senlin"
    },
    {
        "base_classes": [
            "senlin.engine.actions.base.Action"
        ],
        "class_docstring": "An action that can be performed on a cluster.",
        "klass": "senlin.engine.actions.cluster_action.ClusterAction",
        "module": "senlin"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Object representing a binding between a cluster and a policy.\n\n    This object also records the runtime data of a policy, if any.\n    ",
        "klass": "senlin.engine.cluster_policy.ClusterPolicy",
        "module": "senlin"
    },
    {
        "base_classes": [
            "oslo_service.service.Service"
        ],
        "class_docstring": "RPC server for dispatching actions.\n\n    Receive notification from engine services and schedule actions.\n    ",
        "klass": "senlin.engine.dispatcher.Dispatcher",
        "module": "senlin"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Thread group manager.",
        "klass": "senlin.engine.scheduler.ThreadGroupManager",
        "module": "senlin"
    },
    {
        "base_classes": [
            "senlin.objects.base.SenlinObject",
            "oslo_versionedobjects.base.VersionedObjectDictCompat"
        ],
        "class_docstring": "Senlin cluster-policy binding object.",
        "klass": "senlin.objects.cluster_policy.ClusterPolicy",
        "module": "senlin"
    },
    {
        "base_classes": [
            "logging.Handler"
        ],
        "class_docstring": "\n    A handler class which sends formatted logging records to a syslog\n    server. Based on Sam Rushing's syslog module:\n    http://www.nightmare.com/squirl/python-ext/misc/syslog.py\n    Contributed by Nicolas Untz (after which minor refactoring changes\n    have been made).\n    ",
        "klass": "logging.handlers.SysLogHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "logging.handlers.BaseRotatingHandler"
        ],
        "class_docstring": "\n    Handler for logging to a file, rotating the log file at certain timed\n    intervals.\n\n    If backupCount is > 0, when rollover is done, no more than backupCount\n    files are kept - the oldest ones are deleted.\n    ",
        "klass": "logging.handlers.TimedRotatingFileHandler",
        "module": "logging"
    },
    {
        "base_classes": [
            "Exception"
        ],
        "class_docstring": "Django is somehow improperly configured",
        "klass": "django.core.exceptions.ImproperlyConfigured",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a lazy database lookup for a set of objects.\n    ",
        "klass": "django.db.models.query.QuerySet",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Elasticsearch low-level client. Provides a straightforward mapping from\n    Python to ES REST endpoints.\n\n    The instance has attributes ``cat``, ``cluster``, ``indices``, ``ingest``,\n    ``nodes``, ``snapshot`` and ``tasks`` that provide access to instances of\n    :class:`~elasticsearch.client.CatClient`,\n    :class:`~elasticsearch.client.ClusterClient`,\n    :class:`~elasticsearch.client.IndicesClient`,\n    :class:`~elasticsearch.client.IngestClient`,\n    :class:`~elasticsearch.client.NodesClient`,\n    :class:`~elasticsearch.client.SnapshotClient` and\n    :class:`~elasticsearch.client.TasksClient` respectively. This is the\n    preferred (and only supported) way to get access to those classes and their\n    methods.\n\n    You can specify your own connection class which should be used by providing\n    the ``connection_class`` parameter::\n\n        # create connection to localhost using the ThriftConnection\n        es = Elasticsearch(connection_class=ThriftConnection)\n\n    If you want to turn on :ref:`sniffing` you have several options (described\n    in :class:`~elasticsearch.Transport`)::\n\n        # create connection that will automatically inspect the cluster to get\n        # the list of active nodes. Start with nodes running on 'esnode1' and\n        # 'esnode2'\n        es = Elasticsearch(\n            ['esnode1', 'esnode2'],\n            # sniff before doing anything\n            sniff_on_start=True,\n            # refresh nodes after a node fails to respond\n            sniff_on_connection_fail=True,\n            # and also every 60 seconds\n            sniffer_timeout=60\n        )\n\n    Different hosts can have different parameters, use a dictionary per node to\n    specify those::\n\n        # connect to localhost directly and another node using SSL on port 443\n        # and an url_prefix. Note that ``port`` needs to be an int.\n        es = Elasticsearch([\n            {'host': 'localhost'},\n            {'host': 'othernode', 'port': 443, 'url_prefix': 'es', 'use_ssl': True},\n        ])\n\n    If using SSL, there are several parameters that control how we deal with\n    certificates (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # make sure we verify SSL certificates\n            verify_certs=True,\n            # provide a path to CA certs on disk\n            ca_certs='/path/to/CA_certs'\n        )\n\n    If using SSL, but don't verify the certs, a warning message is showed\n    optionally (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # no verify SSL certificates\n            verify_certs=False,\n            # don't show warnings about ssl certs verification\n            ssl_show_warn=False\n        )\n\n    SSL client authentication is supported\n    (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # make sure we verify SSL certificates\n            verify_certs=True,\n            # provide a path to CA certs on disk\n            ca_certs='/path/to/CA_certs',\n            # PEM formatted SSL client certificate\n            client_cert='/path/to/clientcert.pem',\n            # PEM formatted SSL client key\n            client_key='/path/to/clientkey.pem'\n        )\n\n    Alternatively you can use RFC-1738 formatted URLs, as long as they are not\n    in conflict with other options::\n\n        es = Elasticsearch(\n            [\n                'http://user:secret@localhost:9200/',\n                'https://user:secret@other_host:443/production'\n            ],\n            verify_certs=True\n        )\n\n    By default, `JSONSerializer\n    <https://github.com/elastic/elasticsearch-py/blob/master/elasticsearch/serializer.py#L24>`_\n    is used to encode all outgoing requests.\n    However, you can implement your own custom serializer::\n\n        from elasticsearch.serializer import JSONSerializer\n\n        class SetEncoder(JSONSerializer):\n            def default(self, obj):\n                if isinstance(obj, set):\n                    return list(obj)\n                if isinstance(obj, Something):\n                    return 'CustomSomethingRepresentation'\n                return JSONSerializer.default(self, obj)\n\n        es = Elasticsearch(serializer=SetEncoder())\n\n    ",
        "klass": "elasticsearch.client.Elasticsearch",
        "module": "elasticsearch"
    },
    {
        "base_classes": [
            "setuptools.command.py36compat.sdist_add_defaults",
            "distutils.command.sdist.sdist"
        ],
        "class_docstring": "Smart sdist that finds anything supported by revision control",
        "klass": "setuptools.command.sdist.sdist",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "pyparsing.TokenConverter"
        ],
        "class_docstring": "Converter to concatenate all matching tokens to a single string.\n    By default, the matching patterns must also be contiguous in the\n    input string; this can be disabled by specifying\n    ``'adjacent=False'`` in the constructor.\n\n    Example::\n\n        real = Word(nums) + '.' + Word(nums)\n        print(real.parseString('3.1416')) # -> ['3', '.', '1416']\n        # will also erroneously match the following\n        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']\n\n        real = Combine(Word(nums) + '.' + Word(nums))\n        print(real.parseString('3.1416')) # -> ['3.1416']\n        # no match when there are internal spaces\n        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)\n    ",
        "klass": "pyparsing.Combine",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.ParseElementEnhance"
        ],
        "class_docstring": "Forward declaration of an expression to be defined later -\n    used for recursive grammars, such as algebraic infix notation.\n    When the expression is known, it is assigned to the ``Forward``\n    variable using the '<<' operator.\n\n    Note: take care when assigning to ``Forward`` not to overlook\n    precedence of operators.\n\n    Specifically, '|' has a lower precedence than '<<', so that::\n\n        fwdExpr << a | b | c\n\n    will actually be evaluated as::\n\n        (fwdExpr << a) | b | c\n\n    thereby leaving b and c out as parseable alternatives.  It is recommended that you\n    explicitly group the values inserted into the ``Forward``::\n\n        fwdExpr << (a | b | c)\n\n    Converting to use the '<<=' operator instead will avoid this problem.\n\n    See :class:`ParseResults.pprint` for an example of a recursive\n    parser created using ``Forward``.\n    ",
        "klass": "pyparsing.Forward",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "flexget.utils.lazy_dict.LazyDict"
        ],
        "class_docstring": "\n    Represents one item in task. Must have `url` and *title* fields.\n\n    Stores automatically *original_url* key, which is necessary because\n    plugins (eg. urlrewriters) may change *url* into something else\n    and otherwise that information would be lost.\n\n    Entry will also transparently convert all ascii strings into unicode\n    and raises :class:`EntryUnicodeError` if conversion fails on any value\n    being set. Such failures are caught by :class:`~flexget.task.Task`\n    and trigger :meth:`~flexget.task.Task.abort`.\n    ",
        "klass": "flexget.entry.Entry",
        "module": "flexget"
    },
    {
        "base_classes": [
            "rpyc.utils.server.Server"
        ],
        "class_docstring": "\n    A server that spawns a thread for each connection. Works on any platform\n    that supports threads.\n\n    Parameters: see :class:`Server`\n    ",
        "klass": "rpyc.utils.server.ThreadedServer",
        "module": "rpyc"
    },
    {
        "base_classes": [
            "argparse.ArgumentParser"
        ],
        "class_docstring": "\n    Mimics the default :class:`argparse.ArgumentParser` class, with a few distinctions, mostly to ease subparser usage:\n\n    - If `add_subparsers` is called with the `nested_namespaces` kwarg, all subcommand options will be stored in a\n      nested namespace based on the command name for the subparser\n    - Adds the `add_subparser` method. After `add_subparsers` has been called, the `add_subparser` method can be used\n      instead of the `add_parser` method of the object returned by the `add_subparsers` call.\n    - `add_subparser` takes takes the `parent_defaults` argument, which will set/change the defaults for the parent\n      parser when that subparser is selected.\n    - The `get_subparser` method will get the :class:`ArgumentParser` instance for an existing subparser on this parser\n    - For any arguments defined both in this parser and one of its subparsers, the selected subparser default will\n      override the main one.\n    - Adds the `set_post_defaults` method. This works like the normal argparse `set_defaults` method, but all actions\n      and subparsers will be run before any of these defaults are set.\n    - Command shortening: If the command for a subparser is abbreviated unambiguously, it will still be accepted.\n    - The add_argument `nargs` keyword argument supports a range of arguments, e.g. `\"2-4\"\n    - If the `raise_errors` keyword argument to `parse_args` is True, a `ParserError` will be raised instead of sys.exit\n    - If the `file` argument is given to `parse_args`, output will be printed there instead of sys.stdout or stderr\n    ",
        "klass": "flexget.options.ArgumentParser",
        "module": "flexget"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " rTorrent API client ",
        "klass": "flexget.plugins.clients.rtorrent.RTorrent",
        "module": "flexget"
    },
    {
        "base_classes": [
            "fontTools.misc.loggingTools.LogMixin",
            "fontTools.designspaceLib.AsDictMixin"
        ],
        "class_docstring": " Read, write data from the designspace file",
        "klass": "fontTools.designspaceLib.DesignSpaceDocument",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "fontTools.pens.boundsPen.ControlBoundsPen"
        ],
        "class_docstring": "Pen to calculate the bounds of a shape. It calculates the\n\tcorrect bounds even when the shape contains curves that don't\n\thave points on their extremes. This is somewhat slower to compute\n\tthan the \"control bounds\".\n\n\tWhen the shape has been drawn, the bounds are available as the\n\t'bounds' attribute of the pen object. It's a 4-tuple:\n\t\t(xMin, yMin, xMax, yMax)\n\t",
        "klass": "fontTools.pens.boundsPen.BoundsPen",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "fontTools.pens.basePen.BasePen"
        ],
        "class_docstring": "Pen to calculate the \"control bounds\" of a shape. This is the\n\tbounding box of all control points, so may be larger than the\n\tactual bounding box if there are curves that don't have points\n\ton their extremes.\n\n\tWhen the shape has been drawn, the bounds are available as the\n\t'bounds' attribute of the pen object. It's a 4-tuple:\n\t\t(xMin, yMin, xMax, yMax).\n\n\tIf 'ignoreSinglePoints' is True, single points are ignored.\n\t",
        "klass": "fontTools.pens.boundsPen.ControlBoundsPen",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "fontTools.pens.basePen.BasePen"
        ],
        "class_docstring": "This pen implements \"point inside\" testing: to test whether\n\ta given point lies inside the shape (black) or outside (white).\n\tInstances of this class can be recycled, as long as the\n\tsetTestPoint() method is used to set the new point to test.\n\n\tTypical usage:\n\n\t\tpen = PointInsidePen(glyphSet, (100, 200))\n\t\toutline.draw(pen)\n\t\tisInside = pen.getResult()\n\n\tBoth the even-odd algorithm and the non-zero-winding-rule\n\talgorithm are implemented. The latter is the default, specify\n\tTrue for the evenOdd argument of __init__ or setTestPoint\n\tto use the even-odd algorithm.\n\t",
        "klass": "fontTools.pens.pointInsidePen.PointInsidePen",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "fontTools.pens.basePen.LoggingPen"
        ],
        "class_docstring": "Pen used for drawing to a TrueType glyph.\n\n    If `handleOverflowingTransforms` is True, the components' transform values\n    are checked that they don't overflow the limits of a F2Dot14 number:\n    -2.0 <= v < +2.0. If any transform value exceeds these, the composite\n    glyph is decomposed.\n    An exception to this rule is done for values that are very close to +2.0\n    (both for consistency with the -2.0 case, and for the relative frequency\n    these occur in real fonts). When almost +2.0 values occur (and all other\n    values are within the range -2.0 <= x <= +2.0), they are clamped to the\n    maximum positive value that can still be encoded as an F2Dot14: i.e.\n    1.99993896484375.\n    If False, no check is done and all components are translated unmodified\n    into the glyf table, followed by an inevitable `struct.error` once an\n    attempt is made to compile them.\n    ",
        "klass": "fontTools.pens.ttGlyphPen.TTGlyphPen",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Type 1 font class.\n\n\tUses a minimal interpeter that supports just about enough PS to parse\n\tType 1 fonts.\n\t",
        "klass": "fontTools.t1Lib.T1Font",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The main font object. It manages file input and output, and offers\n\ta convenient way of accessing tables.\n\tTables will be only decompiled when necessary, ie. when they're actually\n\taccessed. This means that simple operations can be extremely fast.\n\t",
        "klass": "fontTools.ttLib.TTFont",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The main font object. It manages file input and output, and offers\n\ta convenient way of accessing tables.\n\tTables will be only decompiled when necessary, ie. when they're actually\n\taccessed. This means that simple operations can be extremely fast.\n\t",
        "klass": "fontTools.ttx.TTFont",
        "module": "fontTools"
    },
    {
        "base_classes": [
            "unittest.case.TestCase",
            "case.case.CaseMixin"
        ],
        "class_docstring": "Test Case\n\n    Subclass of :class:`unittest.TestCase` adding convenience\n    methods.\n\n    **setup / teardown**\n\n    New :meth:`setup` and :meth:`teardown` methods can be defined\n    in addition to the core :meth:`setUp` + :meth:`tearDown`\n    methods.\n\n    Note: If you redefine the core :meth:`setUp` + :meth:`tearDown`\n          methods you must make sure ``super`` is called.\n          ``super`` is not necessary for the lowercase versions.\n\n    **Python 2.6 compatibility**\n\n    This class also implements :meth:`assertWarns`, :meth:`assertWarnsRegex`,\n    :meth:`assertDictContainsSubset`, and :meth:`assertItemsEqual`\n    which are not available in the original Python 2.6 unittest\n    implementation.\n\n    ",
        "klass": "case.Case",
        "module": "case"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class is used to control the SQLAlchemy integration to one\n    or more Flask applications.  Depending on how you initialize the\n    object it is usable right away or will attach as needed to a\n    Flask application.\n\n    There are two usage modes which work very similarly.  One is binding\n    the instance to a very specific Flask application::\n\n        app = Flask(__name__)\n        db = SQLAlchemy(app)\n\n    The second possibility is to create the object once and configure the\n    application later to support it::\n\n        db = SQLAlchemy()\n\n        def create_app():\n            app = Flask(__name__)\n            db.init_app(app)\n            return app\n\n    The difference between the two is that in the first case methods like\n    :meth:`create_all` and :meth:`drop_all` will work all the time but in\n    the second case a :meth:`flask.Flask.app_context` has to exist.\n\n    By default Flask-SQLAlchemy will apply some backend-specific settings\n    to improve your experience with them.\n\n    As of SQLAlchemy 0.6 SQLAlchemy\n    will probe the library for native unicode support.  If it detects\n    unicode it will let the library handle that, otherwise do that itself.\n    Sometimes this detection can fail in which case you might want to set\n    ``use_native_unicode`` (or the ``SQLALCHEMY_NATIVE_UNICODE`` configuration\n    key) to ``False``.  Note that the configuration key overrides the\n    value you pass to the constructor.  Direct support for ``use_native_unicode``\n    and SQLALCHEMY_NATIVE_UNICODE are deprecated as of v2.4 and will be removed\n    in v3.0.  ``engine_options`` and ``SQLALCHEMY_ENGINE_OPTIONS`` may be used\n    instead.\n\n    This class also provides access to all the SQLAlchemy functions and classes\n    from the :mod:`sqlalchemy` and :mod:`sqlalchemy.orm` modules.  So you can\n    declare models like this::\n\n        class User(db.Model):\n            username = db.Column(db.String(80), unique=True)\n            pw_hash = db.Column(db.String(80))\n\n    You can still use :mod:`sqlalchemy` and :mod:`sqlalchemy.orm` directly, but\n    note that Flask-SQLAlchemy customizations are available only through an\n    instance of this :class:`SQLAlchemy` class.  Query classes default to\n    :class:`BaseQuery` for `db.Query`, `db.Model.query_class`, and the default\n    query_class for `db.relationship` and `db.backref`.  If you use these\n    interfaces through :mod:`sqlalchemy` and :mod:`sqlalchemy.orm` directly,\n    the default query class will be that of :mod:`sqlalchemy`.\n\n    .. admonition:: Check types carefully\n\n       Don't perform type or `isinstance` checks against `db.Table`, which\n       emulates `Table` behavior but is not a class. `db.Table` exposes the\n       `Table` interface, but is a function which allows omission of metadata.\n\n    The ``session_options`` parameter, if provided, is a dict of parameters\n    to be passed to the session constructor.  See :class:`~sqlalchemy.orm.session.Session`\n    for the standard options.\n\n    The ``engine_options`` parameter, if provided, is a dict of parameters\n    to be passed to create engine.  See :func:`~sqlalchemy.create_engine`\n    for the standard options.  The values given here will be merged with and\n    override anything set in the ``'SQLALCHEMY_ENGINE_OPTIONS'`` config\n    variable or othewise set by this library.\n\n    .. versionadded:: 0.10\n       The `session_options` parameter was added.\n\n    .. versionadded:: 0.16\n       `scopefunc` is now accepted on `session_options`. It allows specifying\n        a custom function which will define the SQLAlchemy session's scoping.\n\n    .. versionadded:: 2.1\n       The `metadata` parameter was added. This allows for setting custom\n       naming conventions among other, non-trivial things.\n\n       The `query_class` parameter was added, to allow customisation\n       of the query class, in place of the default of :class:`BaseQuery`.\n\n       The `model_class` parameter was added, which allows a custom model\n       class to be used in place of :class:`Model`.\n\n    .. versionchanged:: 2.1\n       Utilise the same query class across `session`, `Model.query` and `Query`.\n\n    .. versionadded:: 2.4\n       The `engine_options` parameter was added.\n\n    .. versionchanged:: 2.4\n       The `use_native_unicode` parameter was deprecated.\n    ",
        "klass": "flask_sqlalchemy.SQLAlchemy",
        "module": "flask_sqlalchemy"
    },
    {
        "base_classes": [
            "twilio.jwt.Jwt"
        ],
        "class_docstring": "A JWT included on requests so that Twilio can verify request authenticity",
        "klass": "twilio.jwt.validation.ClientValidationJwt",
        "module": "twilio"
    },
    {
        "base_classes": [
            "twilio.jwt.Jwt"
        ],
        "class_docstring": "A token to control permissions with Twilio Client",
        "klass": "twilio.jwt.client.ClientCapabilityToken",
        "module": "twilio"
    },
    {
        "base_classes": [
            "twilio.twiml.TwiML"
        ],
        "class_docstring": " <Dial> TwiML Verb ",
        "klass": "twilio.twiml.voice_response.Dial",
        "module": "twilio"
    },
    {
        "base_classes": [
            "twilio.twiml.TwiML"
        ],
        "class_docstring": " <Response> TwiML for Voice ",
        "klass": "twilio.twiml.voice_response.VoiceResponse",
        "module": "twilio"
    },
    {
        "base_classes": [
            "fs.base.FS"
        ],
        "class_docstring": "Create an OSFS.\n\n    Arguments:\n        root_path (str or ~os.PathLike): An OS path or path-like object to\n            the location on your HD you wish to manage.\n        create (bool): Set to `True` to create the root directory if it\n            does not already exist, otherwise the directory should exist\n            prior to creating the ``OSFS`` instance (defaults to `False`).\n        create_mode (int): The permissions that will be used to create\n            the directory if ``create`` is `True` and the path doesn't\n            exist, defaults to ``0o777``.\n        expand_vars(bool): If `True` (the default) environment variables of\n            the form $name or ${name} will be expanded.\n\n    Raises:\n        `fs.errors.CreateFailed`: If ``root_path`` does not\n            exist, or could not be created.\n\n    Examples:\n        >>> current_directory_fs = OSFS('.')\n        >>> home_fs = OSFS('~/')\n        >>> windows_system32_fs = OSFS('c://system32')\n\n    ",
        "klass": "fs.osfs.OSFS",
        "module": "fs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A :class:`~Globber` object bound to a filesystem.\n\n    An instance of this object is available on every Filesystem object\n    as ``.glob``.\n\n    Arguments:\n        fs (FS): A filesystem object.\n\n    ",
        "klass": "fs.glob.BoundGlobber",
        "module": "fs"
    },
    {
        "base_classes": [
            "fs.base.FS"
        ],
        "class_docstring": "A FTP (File Transport Protocol) Filesystem.\n\n    Arguments:\n        host (str): A FTP host, e.g. ``'ftp.mirror.nl'``.\n        user (str): A username (default is ``'anonymous'``).\n        passwd (str): Password for the server, or `None` for anon.\n        acct (str): FTP account.\n        timeout (int): Timeout for contacting server (in seconds,\n            defaults to 10).\n        port (int): FTP port number (default 21).\n        proxy (str, optional): An FTP proxy, or ``None`` (default)\n            for no proxy.\n\n    ",
        "klass": "fs.ftpfs.FTPFS",
        "module": "fs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Container for :ref:`info`.\n\n    Resource information is returned by the following methods:\n\n         * `~fs.base.FS.getinfo`\n         * `~fs.base.FS.scandir`\n         * `~fs.base.FS.filterdir`\n\n    Arguments:\n        raw_info (dict): A dict containing resource info.\n        to_datetime (callable): A callable that converts an\n            epoch time to a datetime object. The default uses\n            :func:`~fs.time.epoch_to_datetime`.\n\n    ",
        "klass": "fs.info.Info",
        "module": "fs"
    },
    {
        "base_classes": [
            "fs.base.FS"
        ],
        "class_docstring": "A filesystem that stored in memory.\n\n    Memory filesystems are useful for caches, temporary data stores,\n    unit testing, etc. Since all the data is in memory, they are very\n    fast, but non-permanent. The `MemoryFS` constructor takes no\n    arguments.\n\n    Example:\n        >>> mem_fs = MemoryFS()\n\n    Or via an FS URL:\n        >>> import fs\n        >>> mem_fs = fs.open_fs('mem://')\n\n    ",
        "klass": "fs.memoryfs.MemoryFS",
        "module": "fs"
    },
    {
        "base_classes": [
            "fs.base.FS"
        ],
        "class_docstring": "A virtual filesystem that maps directories on to other file-systems.\n\n    Arguments:\n        auto_close (bool): If `True` (the default), the child\n            filesystems will be closed when `MountFS` is closed.\n\n    ",
        "klass": "fs.mountfs.MountFS",
        "module": "fs"
    },
    {
        "base_classes": [
            "fs.osfs.OSFS"
        ],
        "class_docstring": "A temporary filesystem on the OS.\n\n    Arguments:\n        identifier (str): A string to distinguish the directory within\n            the OS temp location, used as part of the directory name.\n        temp_dir (str, optional): An OS path to your temp directory\n            (leave as `None` to auto-detect)\n        auto_clean (bool): If `True` (the default), the directory\n            contents will be wiped on close.\n        ignore_clean_errors (bool): If `True` (the default), any errors\n            in the clean process will be suppressed. If `False`, they\n            will be raised.\n\n    ",
        "klass": "fs.tempfs.TempFS",
        "module": "fs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Convert Markdown to HTML.",
        "klass": "markdown.Markdown",
        "module": "markdown"
    },
    {
        "base_classes": [
            "persistent.Persistent"
        ],
        "class_docstring": "A BLOB supports efficient handling of large data within ZODB.",
        "klass": "ZODB.blob.Blob",
        "module": "ZODB"
    },
    {
        "base_classes": [
            "mutagen.id3.ID3Tags",
            "mutagen.Metadata"
        ],
        "class_docstring": "ID3(filething=None)\n\n    A file with an ID3v2 tag.\n\n    If any arguments are given, the :meth:`load` is called with them. If no\n    arguments are given then an empty `ID3` object is created.\n\n    ::\n\n        ID3(\"foo.mp3\")\n        # same as\n        t = ID3()\n        t.load(\"foo.mp3\")\n\n    Arguments:\n        filething (filething): or `None`\n\n    Attributes:\n        version (tuple[int]): ID3 tag version as a tuple\n        unknown_frames (list[bytes]): raw frame data of any unknown frames\n            found\n        size (int): the total size of the ID3 tag, including the header\n    ",
        "klass": "mutagen.id3.ID3",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.FileType"
        ],
        "class_docstring": "APEv2File(filething)\n\n    Arguments:\n        filething (filething)\n\n    Attributes:\n        tags (`APEv2`)\n    ",
        "klass": "mutagen.apev2.APEv2File",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen._util.DictMixin",
            "mutagen.Metadata"
        ],
        "class_docstring": "EasyID3(filething=None)\n\n    A file with an ID3 tag.\n\n    Like Vorbis comments, EasyID3 keys are case-insensitive ASCII\n    strings. Only a subset of ID3 frames are supported by default. Use\n    EasyID3.RegisterKey and its wrappers to support more.\n\n    You can also set the GetFallback, SetFallback, and DeleteFallback\n    to generic key getter/setter/deleter functions, which are called\n    if no specific handler is registered for a key. Additionally,\n    ListFallback can be used to supply an arbitrary list of extra\n    keys. These can be set on EasyID3 or on individual instances after\n    creation.\n\n    To use an EasyID3 class with mutagen.mp3.MP3::\n\n        from mutagen.mp3 import EasyMP3 as MP3\n        MP3(filename)\n\n    Because many of the attributes are constructed on the fly, things\n    like the following will not work::\n\n        ezid3[\"performer\"].append(\"Joe\")\n\n    Instead, you must do::\n\n        values = ezid3[\"performer\"]\n        values.append(\"Joe\")\n        ezid3[\"performer\"] = values\n\n    ",
        "klass": "mutagen.easyid3.EasyID3",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.FileType"
        ],
        "class_docstring": "MP4(filething)\n\n    An MPEG-4 audio file, probably containing AAC.\n\n    If more than one track is present in the file, the first is used.\n    Only audio ('soun') tracks will be read.\n\n    Arguments:\n        filething (filething)\n\n    Attributes:\n        info (`MP4Info`)\n        tags (`MP4Tags`)\n    ",
        "klass": "mutagen.mp4.MP4",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.FileType"
        ],
        "class_docstring": "FLAC(filething)\n\n    A FLAC audio file.\n\n    Args:\n        filething (filething)\n\n    Attributes:\n        cuesheet (`CueSheet`): if any or `None`\n        seektable (`SeekTable`): if any or `None`\n        pictures (list[Picture]): list of embedded pictures\n        info (`StreamInfo`)\n        tags (`mutagen._vorbis.VCommentDict`)\n    ",
        "klass": "mutagen.flac.FLAC",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.id3._specs.Spec"
        ],
        "class_docstring": "A fixed size ASCII only payload.",
        "klass": "mutagen.id3._specs.StringSpec",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.mp3.MP3"
        ],
        "class_docstring": "EasyMP3(filething)\n\n    Like MP3, but uses EasyID3 for tags.\n\n    Arguments:\n        filething (filething)\n\n    Attributes:\n        info (`MPEGInfo`)\n        tags (`mutagen.easyid3.EasyID3`)\n    ",
        "klass": "mutagen.mp3.EasyMP3",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.id3.ID3FileType"
        ],
        "class_docstring": "MP3(filething)\n\n    An MPEG audio (usually MPEG-1 Layer 3) file.\n\n    Arguments:\n        filething (filething)\n\n    Attributes:\n        info (`MPEGInfo`)\n        tags (`mutagen.id3.ID3`)\n    ",
        "klass": "mutagen.mp3.MP3",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.ogg.OggFileType"
        ],
        "class_docstring": "OggOpus(filething)\n\n    An Ogg Opus file.\n\n    Arguments:\n        filething (filething)\n\n    Attributes:\n        info (`OggOpusInfo`)\n        tags (`mutagen._vorbis.VCommentDict`)\n\n    ",
        "klass": "mutagen.oggopus.OggOpus",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "mutagen.ogg.OggFileType"
        ],
        "class_docstring": "OggVorbis(filething)\n\n    Arguments:\n        filething (filething)\n\n    An Ogg Vorbis file.\n\n    Attributes:\n        info (`OggVorbisInfo`)\n        tags (`mutagen._vorbis.VCommentDict`)\n    ",
        "klass": "mutagen.oggvorbis.OggVorbis",
        "module": "mutagen"
    },
    {
        "base_classes": [
            "hachoir.field.bit_field.RawBits"
        ],
        "class_docstring": "\n    Positive integer with a size in bits\n\n    @see: L{Bit}\n    @see: L{RawBits}\n    ",
        "klass": "hachoir.field.Bits",
        "module": "hachoir"
    },
    {
        "base_classes": [
            "hachoir.field.byte_field.RawBytes"
        ],
        "class_docstring": "\n    Byte vector: can be used for magic number or GUID/UUID for example.\n\n    @see: L{RawBytes}\n    ",
        "klass": "hachoir.field.Bytes",
        "module": "hachoir"
    },
    {
        "base_classes": [
            "ansible.cli.CLI"
        ],
        "class_docstring": " is an extra-simple tool/framework/API for doing 'remote things'.\n        this command allows you to define and run a single task 'playbook' against a set of hosts\n    ",
        "klass": "ansible.cli.adhoc.AdHocCLI",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.playbook.base.Base",
            "ansible.playbook.taggable.Taggable",
            "ansible.playbook.collectionsearch.CollectionSearch"
        ],
        "class_docstring": "\n    A play is a language feature that represents a list of roles and/or\n    task/handler blocks to execute on a given set of hosts.\n\n    Usage:\n\n       Play.load(datastructure) -> Play\n       Play.something(...)\n    ",
        "klass": "ansible.playbook.play.Play",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.cli.CLI"
        ],
        "class_docstring": " the tool to run *Ansible playbooks*, which are a configuration and multinode deployment system.\n        See the project home page (https://docs.ansible.com) for more information. ",
        "klass": "ansible.cli.playbook.PlaybookCLI",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.cli.CLI"
        ],
        "class_docstring": " can encrypt any structured data file used by Ansible.\n    This can include *group_vars/* or *host_vars/* inventory variables,\n    variables loaded by *include_vars* or *vars_files*, or variable files\n    passed on the ansible-playbook command line with *-e @file.yml* or *-e @file.json*.\n    Role variables and defaults are also included!\n\n    Because Ansible tasks, handlers, and other objects are data, these can also be encrypted with vault.\n    If you'd like to not expose what variables you are using, you can keep an individual task file entirely encrypted.\n    ",
        "klass": "ansible.cli.vault.VaultCLI",
        "module": "ansible"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An ansible module class for AWS modules\n\n    AnsibleAWSModule provides an a class for building modules which\n    connect to Amazon Web Services.  The interface is currently more\n    restricted than the basic module class with the aim that later the\n    basic module class can be reduced.  If you find that any key\n    feature is missing please contact the author/Ansible AWS team\n    (available on #ansible-aws on IRC) to request the additional\n    features needed.\n    ",
        "klass": "ansible.module_utils.aws.core.AnsibleAWSModule",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.parsing.yaml.objects.AnsibleBaseYAMLObject",
            "dict"
        ],
        "class_docstring": " sub class for dictionaries ",
        "klass": "ansible.parsing.yaml.objects.AnsibleMapping",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.playbook.task.Task"
        ],
        "class_docstring": "\n    A task include is derived from a regular task to handle the special\n    circumstances related to the `- include: ...` task.\n    ",
        "klass": "ansible.playbook.task_include.TaskInclude",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.plugins.connection.NetworkConnectionBase"
        ],
        "class_docstring": "NetConf connections",
        "klass": "ansible.plugins.connection.netconf.Connection",
        "module": "ansible"
    },
    {
        "base_classes": [
            "ansible.plugins.connection.NetworkConnectionBase"
        ],
        "class_docstring": " CLI (shell) SSH connections on Paramiko ",
        "klass": "ansible.plugins.connection.network_cli.Connection",
        "module": "ansible"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Cosimulation class. ",
        "klass": "myhdl._Cosimulation.Cosimulation",
        "module": "myhdl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Dynamically converts between various forms of passed in input data.\n\n    Exactly one of ``arg``, ``data`` or ``file`` must be not-``None``.\n\n    :param arg: Dynamic argument. If a bytestring, will be interpreted as raw\n                bytes. A unicode string will be interpreted as text and any\n                object that has a ``read()`` method as a file-like.\n                Any instance of ``Data`` will be passed through and rendered\n                unusable.\n    :param encoding: The data's encoding. Will be used for every conversion\n                     from bytestrings to text (unicode) if necessary and the\n                     other way around.\n    :param data: Buffer argument. If unicode string, will be interpreted as\n                 text, otherwise as bytestring.\n    :param file: File argument. Any object with a ``read()`` method will be\n                 treated as file-like. Everything else is considered a\n                 filename.",
        "klass": "data.Data",
        "module": "data"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A mutable list of groups of operations to apply to some qubits.\n\n    Methods returning information about the circuit:\n        next_moment_operating_on\n        prev_moment_operating_on\n        next_moments_operating_on\n        operation_at\n        all_qubits\n        all_operations\n        findall_operations\n        findall_operations_between\n        findall_operations_until_blocked\n        findall_operations_with_gate_type\n        reachable_frontier_from\n        has_measurements\n        are_all_matches_terminal\n        are_all_measurements_terminal\n        unitary\n        final_wavefunction\n        to_text_diagram\n        to_text_diagram_drawer\n\n    Methods for mutation:\n        insert\n        append\n        insert_into_range\n        clear_operations_touching\n        batch_insert\n        batch_remove\n        batch_insert_into\n        insert_at_frontier\n\n    Circuits can also be iterated over,\n        for moment in circuit:\n            ...\n    and sliced,\n        circuit[1:3] is a new Circuit made up of two moments, the first being\n            circuit[1] and the second being circuit[2];\n    and concatenated,\n        circuit1 + circuit2 is a new Circuit made up of the moments in circuit1\n            followed by the moments in circuit2;\n    and multiplied by an integer,\n        circuit * k is a new Circuit made up of the moments in circuit repeated\n            k times.\n    and mutated,\n        circuit[1:7] = [Moment(...)]\n    ",
        "klass": "cirq.Circuit",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Attempts to convert non-native gates into IonGates.\n    ",
        "klass": "cirq.ion.convert_to_ion_gates.ConvertToIonGates",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.sim.simulator.SimulatesSamples",
            "cirq.sim.simulator.SimulatesIntermediateState"
        ],
        "class_docstring": "A simulator for density matrices and noisy quantum circuits.\n\n    This simulator can be applied on circuits that are made up of operations\n    that have:\n        * a `_channel_` method\n        * a `_mixture_` method for a probabilistic combination of unitary gates.\n        * a `_unitary_` method\n        * a `_has_unitary_` and `_apply_unitary_` method.\n        * measurements\n        * a `_decompose_` that eventually yields one of the above\n    That is, the circuit must have elements that follow on of the protocols:\n        * `cirq.SupportsChannel`\n        * `cirq.SupportsMixture`\n        * `cirq.SupportsConsistentApplyUnitary`\n        * `cirq.SupportsUnitary`\n        * `cirq.SupportsDecompose`\n    or is a measurement.\n\n    This simulator supports three types of simulation.\n\n    Run simulations which mimic running on actual quantum hardware. These\n    simulations do not give access to the density matrix (like actual hardware).\n    There are two variations of run methods, one which takes in a single\n    (optional) way to resolve parameterized circuits, and a second which\n    takes in a list or sweep of parameter resolver:\n\n        run(circuit, param_resolver, repetitions)\n\n        run_sweep(circuit, params, repetitions)\n\n    These methods return `TrialResult`s which contain both the measurement\n    results, but also the parameters used for the parameterized\n    circuit operations. The initial state of a run is always the all 0s state\n    in the computational basis.\n\n    By contrast the simulate methods of the simulator give access to the density\n    matrix of the simulation at the end of the simulation of the circuit.\n    Note that if the circuit contains measurements then the density matrix\n    is that result for those particular measurement results. For example\n    if there is one measurement, then the simulation may result in the\n    measurement result for this measurement, and the density matrix will\n    be that conditional on that result. It will not be the density matrix formed\n    by summing over the different measurements and their probabilities.\n    The simulate methods take in two parameters that the run methods do not: a\n    qubit order and an initial state. The qubit order is necessary because an\n    ordering must be chosen for the kronecker product (see\n    `DensityMatrixTrialResult` for details of this ordering). The initial\n    state can be either the full density matrix, the full wave function (for\n    pure states), or an integer which represents the initial state of being\n    in a computational basis state for the binary representation of that\n    integer. Similar to run methods, there are two simulate methods that run\n    for single simulations or for sweeps across different parameters:\n\n        simulate(circuit, param_resolver, qubit_order, initial_state)\n\n        simulate_sweep(circuit, params, qubit_order, initial_state)\n\n    The simulate methods in contrast to the run methods do not perform\n    repetitions. The result of these simulations is a\n    `DensityMatrixTrialResult` which contains, in addition to measurement\n    results and information about the parameters that were used in the\n    simulation, access to the density matrix via the `density_matrix` method.\n\n    If one wishes to perform simulations that have access to the\n    density matrix as one steps through running the circuit there is a generator\n    which can be iterated over and each step is an object that gives access\n    to the density matrix.  This stepping through a `Circuit` is done on a\n    `Moment` by `Moment` manner.\n\n        simulate_moment_steps(circuit, param_resolver, qubit_order,\n                              initial_state)\n\n    One can iterate over the moments via\n\n        for step_result in simulate_moments(circuit):\n           # do something with the density matrix via\n           # step_result.density_matrix()\n    ",
        "klass": "cirq.DensityMatrixSimulator",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.sim.simulator.SimulatesSamples",
            "cirq.sim.wave_function_simulator.SimulatesIntermediateWaveFunction"
        ],
        "class_docstring": "A sparse matrix wave function simulator that uses numpy.\n\n    This simulator can be applied on circuits that are made up of operations\n    that have a `_unitary_` method, or `_has_unitary_` and\n    `_apply_unitary_`, `_mixture_` methods, are measurements, or support a\n    `_decompose_` method that returns operations satisfying these same\n    conditions. That is to say, the operations should follow the\n    `cirq.SupportsConsistentApplyUnitary` protocol, the `cirq.SupportsUnitary`\n    protocol, the `cirq.SupportsMixture` protocol, or the\n    `cirq.CompositeOperation` protocol. It is also permitted for the circuit\n    to contain measurements which are operations that support\n    `cirq.SupportsChannel` and `cirq.SupportsMeasurementKey`\n\n    This simulator supports three types of simulation.\n\n    Run simulations which mimic running on actual quantum hardware. These\n    simulations do not give access to the wave function (like actual hardware).\n    There are two variations of run methods, one which takes in a single\n    (optional) way to resolve parameterized circuits, and a second which\n    takes in a list or sweep of parameter resolver:\n\n        run(circuit, param_resolver, repetitions)\n\n        run_sweep(circuit, params, repetitions)\n\n    The simulation performs optimizations if the number of repetitions is\n    greater than one and all measurements in the circuit are terminal (at the\n    end of the circuit). These methods return `TrialResult`s which contain both\n    the measurement results, but also the parameters used for the parameterized\n    circuit operations. The initial state of a run is always the all 0s state\n    in the computational basis.\n\n    By contrast the simulate methods of the simulator give access to the\n    wave function of the simulation at the end of the simulation of the circuit.\n    These methods take in two parameters that the run methods do not: a\n    qubit order and an initial state. The qubit order is necessary because an\n    ordering must be chosen for the kronecker product (see\n    `SparseSimulationTrialResult` for details of this ordering). The initial\n    state can be either the full wave function, or an integer which represents\n    the initial state of being in a computational basis state for the binary\n    representation of that integer. Similar to run methods, there are two\n    simulate methods that run for single runs or for sweeps across different\n    parameters:\n\n        simulate(circuit, param_resolver, qubit_order, initial_state)\n\n        simulate_sweep(circuit, params, qubit_order, initial_state)\n\n    The simulate methods in contrast to the run methods do not perform\n    repetitions. The result of these simulations is a\n    `SparseSimulationTrialResult` which contains, in addition to measurement\n    results and information about the parameters that were used in the\n    simulation,access to the state via the `state` method and `StateVectorMixin`\n    methods.\n\n    If one wishes to perform simulations that have access to the\n    wave function as one steps through running the circuit there is a generator\n    which can be iterated over and each step is an object that gives access\n    to the wave function.  This stepping through a `Circuit` is done on a\n    `Moment` by `Moment` manner.\n\n        simulate_moment_steps(circuit, param_resolver, qubit_order,\n                              initial_state)\n\n    One can iterate over the moments via\n\n        for step_result in simulate_moments(circuit):\n           # do something with the wave function via step_result.state\n\n    Note also that simulations can be stochastic, i.e. return different results\n    for different runs.  The first version of this occurs for measurements,\n    where the results of the measurement are recorded.  This can also\n    occur when the circuit has mixtures of unitaries.\n\n    See `Simulator` for the definitions of the supported methods.\n    ",
        "klass": "cirq.sim.sparse_simulator.Simulator",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.sim.simulator.SimulatesSamples",
            "cirq.sim.wave_function_simulator.SimulatesIntermediateWaveFunction"
        ],
        "class_docstring": "A sparse matrix wave function simulator that uses numpy.\n\n    This simulator can be applied on circuits that are made up of operations\n    that have a `_unitary_` method, or `_has_unitary_` and\n    `_apply_unitary_`, `_mixture_` methods, are measurements, or support a\n    `_decompose_` method that returns operations satisfying these same\n    conditions. That is to say, the operations should follow the\n    `cirq.SupportsConsistentApplyUnitary` protocol, the `cirq.SupportsUnitary`\n    protocol, the `cirq.SupportsMixture` protocol, or the\n    `cirq.CompositeOperation` protocol. It is also permitted for the circuit\n    to contain measurements which are operations that support\n    `cirq.SupportsChannel` and `cirq.SupportsMeasurementKey`\n\n    This simulator supports three types of simulation.\n\n    Run simulations which mimic running on actual quantum hardware. These\n    simulations do not give access to the wave function (like actual hardware).\n    There are two variations of run methods, one which takes in a single\n    (optional) way to resolve parameterized circuits, and a second which\n    takes in a list or sweep of parameter resolver:\n\n        run(circuit, param_resolver, repetitions)\n\n        run_sweep(circuit, params, repetitions)\n\n    The simulation performs optimizations if the number of repetitions is\n    greater than one and all measurements in the circuit are terminal (at the\n    end of the circuit). These methods return `TrialResult`s which contain both\n    the measurement results, but also the parameters used for the parameterized\n    circuit operations. The initial state of a run is always the all 0s state\n    in the computational basis.\n\n    By contrast the simulate methods of the simulator give access to the\n    wave function of the simulation at the end of the simulation of the circuit.\n    These methods take in two parameters that the run methods do not: a\n    qubit order and an initial state. The qubit order is necessary because an\n    ordering must be chosen for the kronecker product (see\n    `SparseSimulationTrialResult` for details of this ordering). The initial\n    state can be either the full wave function, or an integer which represents\n    the initial state of being in a computational basis state for the binary\n    representation of that integer. Similar to run methods, there are two\n    simulate methods that run for single runs or for sweeps across different\n    parameters:\n\n        simulate(circuit, param_resolver, qubit_order, initial_state)\n\n        simulate_sweep(circuit, params, qubit_order, initial_state)\n\n    The simulate methods in contrast to the run methods do not perform\n    repetitions. The result of these simulations is a\n    `SparseSimulationTrialResult` which contains, in addition to measurement\n    results and information about the parameters that were used in the\n    simulation,access to the state via the `state` method and `StateVectorMixin`\n    methods.\n\n    If one wishes to perform simulations that have access to the\n    wave function as one steps through running the circuit there is a generator\n    which can be iterated over and each step is an object that gives access\n    to the wave function.  This stepping through a `Circuit` is done on a\n    `Moment` by `Moment` manner.\n\n        simulate_moment_steps(circuit, param_resolver, qubit_order,\n                              initial_state)\n\n    One can iterate over the moments via\n\n        for step_result in simulate_moments(circuit):\n           # do something with the wave function via step_result.state\n\n    Note also that simulations can be stochastic, i.e. return different results\n    for different runs.  The first version of this occurs for measurements,\n    where the results of the measurement are recorded.  This can also\n    occur when the circuit has mixtures of unitaries.\n\n    See `Simulator` for the definitions of the supported methods.\n    ",
        "klass": "cirq.Simulator",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.ops.raw_types.Gate"
        ],
        "class_docstring": "A gate that must be applied to exactly one qubit.",
        "klass": "cirq.SingleQubitGate",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.ops.gate_features.TwoQubitGate"
        ],
        "class_docstring": "A 2-qubit gate defined only by its matrix.\n\n    More general than specialized classes like `CZPowGate`, but more expensive\n    and more float-error sensitive to work with (due to using\n    eigendecompositions).\n    ",
        "klass": "cirq.TwoQubitMatrixGate",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.ops.eigen_gate.EigenGate",
            "cirq.ops.gate_features.SingleQubitGate"
        ],
        "class_docstring": "A gate that rotates around the X axis of the Bloch sphere.\n\n    The unitary matrix of ``XPowGate(exponent=t)`` is:\n\n        [[g\u00b7c, -i\u00b7g\u00b7s],\n         [-i\u00b7g\u00b7s, g\u00b7c]]\n\n    where:\n\n        c = cos(\u03c0\u00b7t/2)\n        s = sin(\u03c0\u00b7t/2)\n        g = exp(i\u00b7\u03c0\u00b7t/2).\n\n    Note in particular that this gate has a global phase factor of\n    e^{i\u00b7\u03c0\u00b7t/2} vs the traditionally defined rotation matrices\n    about the Pauli X axis. See `cirq.Rx` for rotations without the global\n    phase. The global phase factor can be adjusted by using the `global_shift`\n    parameter when initializing.\n\n    `cirq.X`, the Pauli X gate, is an instance of this gate at exponent=1.\n    ",
        "klass": "cirq.ops.common_gates.XPowGate",
        "module": "cirq"
    },
    {
        "base_classes": [
            "cirq.ops.eigen_gate.EigenGate",
            "cirq.ops.gate_features.SingleQubitGate"
        ],
        "class_docstring": "A gate that rotates around the Y axis of the Bloch sphere.\n\n    The unitary matrix of ``YPowGate(exponent=t)`` is:\n\n        [[g\u00b7c, -g\u00b7s],\n         [g\u00b7s, g\u00b7c]]\n\n    where:\n\n        c = cos(\u03c0\u00b7t/2)\n        s = sin(\u03c0\u00b7t/2)\n        g = exp(i\u00b7\u03c0\u00b7t/2).\n\n    Note in particular that this gate has a global phase factor of\n    e^{i\u00b7\u03c0\u00b7t/2} vs the traditionally defined rotation matrices\n    about the Pauli Y axis. See `cirq.Ry` for rotations without the global\n    phase. The global phase factor can be adjusted by using the `global_shift`\n    parameter when initializing.\n\n    `cirq.Y`, the Pauli Y gate, is an instance of this gate at exponent=1.\n    ",
        "klass": "cirq.ops.common_gates.YPowGate",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A mutable list of groups of operations to apply to some qubits.\n\n    Methods returning information about the circuit:\n        next_moment_operating_on\n        prev_moment_operating_on\n        next_moments_operating_on\n        operation_at\n        all_qubits\n        all_operations\n        findall_operations\n        findall_operations_between\n        findall_operations_until_blocked\n        findall_operations_with_gate_type\n        reachable_frontier_from\n        has_measurements\n        are_all_matches_terminal\n        are_all_measurements_terminal\n        unitary\n        final_wavefunction\n        to_text_diagram\n        to_text_diagram_drawer\n\n    Methods for mutation:\n        insert\n        append\n        insert_into_range\n        clear_operations_touching\n        batch_insert\n        batch_remove\n        batch_insert_into\n        insert_at_frontier\n\n    Circuits can also be iterated over,\n        for moment in circuit:\n            ...\n    and sliced,\n        circuit[1:3] is a new Circuit made up of two moments, the first being\n            circuit[1] and the second being circuit[2];\n    and concatenated,\n        circuit1 + circuit2 is a new Circuit made up of the moments in circuit1\n            followed by the moments in circuit2;\n    and multiplied by an integer,\n        circuit * k is a new Circuit made up of the moments in circuit repeated\n            k times.\n    and mutated,\n        circuit[1:7] = [Moment(...)]\n    ",
        "klass": "cirq.circuits.Circuit",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A utility class for creating simple text diagrams.\n    ",
        "klass": "cirq.circuits.TextDiagramDrawer",
        "module": "cirq"
    },
    {
        "base_classes": [
            "typing.Generic"
        ],
        "class_docstring": "A priority queue for when priorities are integers over a small range.\n\n    Items are dequeued in ascending priority order. Items with the same priority\n    are dequeued in FIFO order.\n\n    Works by having an explicit list for each priority (from the current min\n    priority to the current max priority). Enqueued items are placed into the\n    list corresponding to their bucket (after adding more buckets if necessary).\n    Dequeued items come from the lowest list containing items, and result in\n    empty buckets at the bottom end of the range being removed.\n\n    Let P be the length of the priority range, and N be the number of items that\n    are enqueued and dequeued. If the priority of items being enqueued is never\n    smaller than the priority of previously dequeued items (the \"monotonic use\n    case\"), then the worst case runtime complexity is O(N+P). In more general\n    use the worst case runtime complexity is O(N*P).\n    ",
        "klass": "cirq.circuits._bucket_priority_queue.BucketPriorityQueue",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tests equality against user-provided disjoint equivalence groups.",
        "klass": "cirq.testing.equals_tester.EqualsTester",
        "module": "cirq"
    },
    {
        "base_classes": [
            "matplotlib.axes._axes.Axes"
        ],
        "class_docstring": "\n    3D axes object.\n    ",
        "klass": "mpl_toolkits.mplot3d.axes3d.Axes3D",
        "module": "mpl_toolkits"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Attempts to convert non-native gates into IonGates.\n    ",
        "klass": "cirq.ion.ConvertToIonGates",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tests equality against user-provided disjoint equivalence groups.",
        "klass": "cirq.testing.EqualsTester",
        "module": "cirq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n\n    ",
        "klass": "bokeh._testing.util.filesystem.TmpDir",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n\n    ",
        "klass": "bokeh._testing.util.filesystem.WorkingDir",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " The basic unit of serialization for Bokeh.\n\n    Document instances collect Bokeh models (e.g. plots, layouts, widgets,\n    etc.) so that they may be reflected into the BokehJS client runtime.\n    Because models may refer to other models (e.g., a plot *has* a list of\n    renderers), it is not generally useful or meaningful to convert individual\n    models to JSON. Accordingly,  the ``Document`` is thus the smallest unit\n    of serialization for Bokeh.\n\n    ",
        "klass": "bokeh.document.document.Document",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "tornado.util.Configurable"
        ],
        "class_docstring": "A level-triggered I/O loop.\n\n    We use ``epoll`` (Linux) or ``kqueue`` (BSD and Mac OS X) if they\n    are available, or else we fall back on select(). If you are\n    implementing a system that needs to handle thousands of\n    simultaneous connections, you should use a system that supports\n    either ``epoll`` or ``kqueue``.\n\n    Example usage for a simple TCP server:\n\n    .. testcode::\n\n        import errno\n        import functools\n        import tornado.ioloop\n        import socket\n\n        def connection_ready(sock, fd, events):\n            while True:\n                try:\n                    connection, address = sock.accept()\n                except socket.error as e:\n                    if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):\n                        raise\n                    return\n                connection.setblocking(0)\n                handle_connection(connection, address)\n\n        if __name__ == '__main__':\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            sock.setblocking(0)\n            sock.bind((\"\", port))\n            sock.listen(128)\n\n            io_loop = tornado.ioloop.IOLoop.current()\n            callback = functools.partial(connection_ready, sock)\n            io_loop.add_handler(sock.fileno(), callback, io_loop.READ)\n            io_loop.start()\n\n    .. testoutput::\n       :hide:\n\n    By default, a newly-constructed `IOLoop` becomes the thread's current\n    `IOLoop`, unless there already is a current `IOLoop`. This behavior\n    can be controlled with the ``make_current`` argument to the `IOLoop`\n    constructor: if ``make_current=True``, the new `IOLoop` will always\n    try to become current and it raises an error if there is already a\n    current instance. If ``make_current=False``, the new `IOLoop` will\n    not try to become current.\n\n    .. versionchanged:: 4.2\n       Added the ``make_current`` keyword argument to the `IOLoop`\n       constructor.\n    ",
        "klass": "tornado.ioloop.IOLoop",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Control validation of bokeh properties\n\n    This can be used as a context manager, or as a normal callable\n\n    Args:\n        value (bool) : Whether validation should occur or not\n\n    Example:\n        .. code-block:: python\n\n            with validate(False):  # do no validate while within this block\n                pass\n\n            validate(False)  # don't validate ever\n\n    See Also:\n        :func:`~bokeh.core.property.bases.validation_on`: check the state of validation\n\n        :func:`~bokeh.core.properties.without_property_validation`: function decorator\n\n    ",
        "klass": "bokeh.core.property.validation.validate",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "A string that is ready to be safely inserted into an HTML or XML\n    document, either because it was escaped or because it was marked\n    safe.\n\n    Passing an object to the constructor converts it to text and wraps\n    it to mark it safe without escaping. To escape the text, use the\n    :meth:`escape` class method instead.\n\n    >>> Markup('Hello, <em>World</em>!')\n    Markup('Hello, <em>World</em>!')\n    >>> Markup(42)\n    Markup('42')\n    >>> Markup.escape('Hello, <em>World</em>!')\n    Markup('Hello &lt;em&gt;World&lt;/em&gt;!')\n\n    This implements the ``__html__()`` interface that some frameworks\n    use. Passing an object that implements ``__html__()`` will wrap the\n    output of that method, marking it safe.\n\n    >>> class Foo:\n    ...     def __html__(self):\n    ...         return '<a href=\"/foo\">foo</a>'\n    ...\n    >>> Markup(Foo())\n    Markup('<a href=\"/foo\">foo</a>')\n\n    This is a subclass of the text type (``str`` in Python 3,\n    ``unicode`` in Python 2). It has the same methods as that type, but\n    all methods escape their arguments and return a ``Markup`` instance.\n\n    >>> Markup('<em>%s</em>') % 'foo & bar'\n    Markup('<em>foo &amp; bar</em>')\n    >>> Markup('<em>Hello</em> ') + '<foo>'\n    Markup('<em>Hello</em> &lt;foo&gt;')\n    ",
        "klass": "markupsafe.Markup",
        "module": "markupsafe"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " The basic unit of serialization for Bokeh.\n\n    Document instances collect Bokeh models (e.g. plots, layouts, widgets,\n    etc.) so that they may be reflected into the BokehJS client runtime.\n    Because models may refer to other models (e.g., a plot *has* a list of\n    renderers), it is not generally useful or meaningful to convert individual\n    models to JSON. Accordingly,  the ``Document`` is thus the smallest unit\n    of serialization for Bokeh.\n\n    ",
        "klass": "bokeh.document.Document",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Wrap a Document object so that only methods that can safely be used\n    from unlocked callbacks or threads are exposed. Attempts to otherwise\n    access or change the Document results in an exception.\n\n    ",
        "klass": "bokeh.document.locking.UnlockedDocumentProxy",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.buttons.AbstractButton"
        ],
        "class_docstring": " A click button.\n\n    ",
        "klass": "bokeh.models.Button",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.sources.ColumnarDataSource"
        ],
        "class_docstring": " Maps names of columns to sequences or arrays.\n\n    The ``ColumnDataSource`` is a fundamental data structure of Bokeh. Most\n    plots, data tables, etc. will be driven by a ``ColumnDataSource``.\n\n    If the ``ColumnDataSource`` initializer is called with a single argument that\n    can be any of the following:\n\n    * A Python ``dict`` that maps string names to sequences of values, e.g.\n      lists, arrays, etc.\n\n      .. code-block:: python\n\n          data = {'x': [1,2,3,4], 'y': np.ndarray([10.0, 20.0, 30.0, 40.0])}\n\n          source = ColumnDataSource(data)\n\n    .. note::\n        ``ColumnDataSource`` only creates a shallow copy of ``data``. Use e.g.\n        ``ColumnDataSource(copy.deepcopy(data))`` if initializing from another\n        ``ColumnDataSource.data`` object that you want to keep independent.\n\n    * A Pandas ``DataFrame`` object\n\n      .. code-block:: python\n\n          source = ColumnDataSource(df)\n\n      In this case the CDS will have columns corresponding to the columns of\n      the ``DataFrame``. If the ``DataFrame`` columns have multiple levels,\n      they will be flattened using an underscore (e.g. level_0_col_level_1_col).\n      The index of the ``DataFrame`` will be flattened to an ``Index`` of tuples\n      if it's a ``MultiIndex``, and then reset using ``reset_index``. The result\n      will be a column with the same name if the index was named, or\n      level_0_name_level_1_name if it was a named ``MultiIndex``. If the\n      ``Index`` did not have a name or the ``MultiIndex`` name could not be\n      flattened/determined, the ``reset_index`` function will name the index column\n      ``index``, or ``level_0`` if the name ``index`` is not available.\n\n    * A Pandas ``GroupBy`` object\n\n      .. code-block:: python\n\n          group = df.groupby(('colA', 'ColB'))\n\n      In this case the CDS will have columns corresponding to the result of\n      calling ``group.describe()``. The ``describe`` method generates columns\n      for statistical measures such as ``mean`` and ``count`` for all the\n      non-grouped original columns. The CDS columns are formed by joining\n      original column names with the computed measure. For example, if a\n      ``DataFrame`` has columns ``'year'`` and ``'mpg'``. Then passing\n      ``df.groupby('year')`` to a CDS will result in columns such as\n      ``'mpg_mean'``\n\n      If the ``GroupBy.describe`` result has a named index column, then\n      CDS will also have a column with this name. However, if the index name\n      (or any subname of a ``MultiIndex``) is ``None``, then the CDS will have\n      a column generically named ``index`` for the index.\n\n      Note this capability to adapt ``GroupBy`` objects may only work with\n      Pandas ``>=0.20.0``.\n\n    .. note::\n        There is an implicit assumption that all the columns in a given\n        ``ColumnDataSource`` all have the same length at all times. For this\n        reason, it is usually preferable to update the ``.data`` property\n        of a data source \"all at once\".\n\n    ",
        "klass": "bokeh.models.ColumnDataSource",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.map_plots.MapPlot"
        ],
        "class_docstring": " A Bokeh Plot with a `Google Map`_ displayed underneath.\n\n    Data placed on this plot should be specified in decimal lat/lon coordinates\n    e.g. ``(37.123, -122.404)``. It will be automatically converted into the\n    web mercator projection to display properly over google maps tiles.\n\n    Note that Google Maps exert explicit control over aspect ratios at all\n    times, which imposes some limitations on ``GMapPlot``:\n\n    * Only ``Range1d`` ranges are supported. Attempting to use other range\n      types will result in an error.\n\n    * Usage of ``BoxZoomTool`` is incompatible with ``GMapPlot``. Adding a\n      ``BoxZoomTool`` will have no effect.\n\n    .. _Google Map: https://www.google.com/maps/\n\n    ",
        "klass": "bokeh.models.GMapPlot",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.layouts.LayoutDOM"
        ],
        "class_docstring": " Model representing a plot, containing glyphs, guides, annotations.\n\n    ",
        "klass": "bokeh.models.Plot",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.groups.ButtonGroup"
        ],
        "class_docstring": " A group of radio boxes rendered as toggle buttons.\n\n    ",
        "klass": "bokeh.models.RadioButtonGroup",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.inputs.InputWidget"
        ],
        "class_docstring": " Single-select widget.\n\n    ",
        "klass": "bokeh.models.Select",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.sliders.AbstractSlider"
        ],
        "class_docstring": " Slider-based number selection widget. ",
        "klass": "bokeh.models.Slider",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.inputs.InputWidget"
        ],
        "class_docstring": " Single-line input widget.\n\n    ",
        "klass": "bokeh.models.TextInput",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.inputs.InputWidget"
        ],
        "class_docstring": " Multi-select widget.\n\n    ",
        "klass": "bokeh.models.widgets.MultiSelect",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "bokeh.models.widgets.inputs.InputWidget"
        ],
        "class_docstring": " Single-select widget.\n\n    ",
        "klass": "bokeh.models.widgets.Select",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for HTTP request handlers.\n\n    Subclasses must define at least one of the methods defined in the\n    \"Entry points\" section below.\n    ",
        "klass": "tornado.web.RequestHandler",
        "module": "tornado"
    },
    {
        "base_classes": [
            "tornado.routing.ReversibleRouter"
        ],
        "class_docstring": "A collection of request handlers that make up a web application.\n\n    Instances of this class are callable and can be passed directly to\n    HTTPServer to serve the application::\n\n        application = web.Application([\n            (r\"/\", MainPageHandler),\n        ])\n        http_server = httpserver.HTTPServer(application)\n        http_server.listen(8080)\n        ioloop.IOLoop.current().start()\n\n    The constructor for this class takes in a list of `~.routing.Rule`\n    objects or tuples of values corresponding to the arguments of\n    `~.routing.Rule` constructor: ``(matcher, target, [target_kwargs], [name])``,\n    the values in square brackets being optional. The default matcher is\n    `~.routing.PathMatches`, so ``(regexp, target)`` tuples can also be used\n    instead of ``(PathMatches(regexp), target)``.\n\n    A common routing target is a `RequestHandler` subclass, but you can also\n    use lists of rules as a target, which create a nested routing configuration::\n\n        application = web.Application([\n            (HostMatches(\"example.com\"), [\n                (r\"/\", MainPageHandler),\n                (r\"/feed\", FeedHandler),\n            ]),\n        ])\n\n    In addition to this you can use nested `~.routing.Router` instances,\n    `~.httputil.HTTPMessageDelegate` subclasses and callables as routing targets\n    (see `~.routing` module docs for more information).\n\n    When we receive requests, we iterate over the list in order and\n    instantiate an instance of the first request class whose regexp\n    matches the request path. The request class can be specified as\n    either a class object or a (fully-qualified) name.\n\n    A dictionary may be passed as the third element (``target_kwargs``)\n    of the tuple, which will be used as keyword arguments to the handler's\n    constructor and `~RequestHandler.initialize` method. This pattern\n    is used for the `StaticFileHandler` in this example (note that a\n    `StaticFileHandler` can be installed automatically with the\n    static_path setting described below)::\n\n        application = web.Application([\n            (r\"/static/(.*)\", web.StaticFileHandler, {\"path\": \"/var/www\"}),\n        ])\n\n    We support virtual hosts with the `add_handlers` method, which takes in\n    a host regular expression as the first argument::\n\n        application.add_handlers(r\"www\\.myhost\\.com\", [\n            (r\"/article/([0-9]+)\", ArticleHandler),\n        ])\n\n    If there's no match for the current request's host, then ``default_host``\n    parameter value is matched against host regular expressions.\n\n\n    .. warning::\n\n       Applications that do not use TLS may be vulnerable to :ref:`DNS\n       rebinding <dnsrebinding>` attacks. This attack is especially\n       relevant to applications that only listen on ``127.0.0.1`` or\n       other private networks. Appropriate host patterns must be used\n       (instead of the default of ``r'.*'``) to prevent this risk. The\n       ``default_host`` argument must not be used in applications that\n       may be vulnerable to DNS rebinding.\n\n    You can serve static files by sending the ``static_path`` setting\n    as a keyword argument. We will serve those files from the\n    ``/static/`` URI (this is configurable with the\n    ``static_url_prefix`` setting), and we will serve ``/favicon.ico``\n    and ``/robots.txt`` from the same directory.  A custom subclass of\n    `StaticFileHandler` can be specified with the\n    ``static_handler_class`` setting.\n\n    .. versionchanged:: 4.5\n       Integration with the new `tornado.routing` module.\n\n    ",
        "klass": "tornado.web.Application",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " A mixin class to provide an interface for registering and\n    triggering event callbacks on the Python side.\n\n    ",
        "klass": "bokeh.util.callback_manager.EventCallbackManager",
        "module": "bokeh"
    },
    {
        "base_classes": [
            "flavio.es.NamedInstanceClass"
        ],
        "class_docstring": "An Observable is something that can be measured experimentally and\n    predicted theoretically.",
        "klass": "flavio.Observable",
        "module": "flavio"
    },
    {
        "base_classes": [
            "flavio.es.NamedInstanceClass"
        ],
        "class_docstring": "An Observable is something that can be measured experimentally and\n    predicted theoretically.",
        "klass": "flavio.classes.Observable",
        "module": "flavio"
    },
    {
        "base_classes": [
            "oslo_context.context.RequestContext"
        ],
        "class_docstring": "Extends security contexts from the OpenStack common library.",
        "klass": "magnum.common.context.RequestContext",
        "module": "magnum"
    },
    {
        "base_classes": [
            "aiohttp.abc.AbstractResolver"
        ],
        "class_docstring": "Use the `aiodns` package to make asynchronous DNS lookups",
        "klass": "aiohttp.resolver.AsyncResolver",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "First-class interface for making HTTP requests.",
        "klass": "aiohttp.client.ClientSession",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "First-class interface for making HTTP requests.",
        "klass": "aiohttp.ClientSession",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "aiohttp.abc.AbstractResolver"
        ],
        "class_docstring": "Use Executor for synchronous getaddrinfo() calls, which defaults to\n    concurrent.futures.ThreadPoolExecutor.\n    ",
        "klass": "aiohttp.resolver.ThreadedResolver",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "aiohttp.payload.Payload"
        ],
        "class_docstring": "Multipart body writer.",
        "klass": "aiohttp.MultipartWriter",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "aiohttp.base_protocol.BaseProtocol",
            "aiohttp.streams.DataQueue"
        ],
        "class_docstring": "Helper class to adapt between Protocol and StreamReader.",
        "klass": "aiohttp.client_proto.ResponseHandler",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "aiohttp.web_runner.BaseRunner"
        ],
        "class_docstring": "Web Application runner",
        "klass": "aiohttp.web_runner.AppRunner",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A test client implementation.\n\n    To write functional tests for aiohttp based servers.\n\n    ",
        "klass": "aiohttp.test_utils.TestClient",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "aiohttp.web_runner.BaseRunner"
        ],
        "class_docstring": "Web Application runner",
        "klass": "aiohttp.web.AppRunner",
        "module": "aiohttp"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Converts a token stream to text.\n\n    Options accepted:\n\n    ``style``\n        The style to use, can be a string or a Style subclass\n        (default: \"default\"). Not used by e.g. the\n        TerminalFormatter.\n    ``full``\n        Tells the formatter to output a \"full\" document, i.e.\n        a complete self-contained document. This doesn't have\n        any effect for some formatters (default: false).\n    ``title``\n        If ``full`` is true, the title that should be used to\n        caption the document (default: '').\n    ``encoding``\n        If given, must be an encoding name. This will be used to\n        convert the Unicode token strings to byte strings in the\n        output. If it is \"\" or None, Unicode strings will be written\n        to the output file, which most file-like objects do not\n        support (default: None).\n    ``outencoding``\n        Overrides ``encoding`` if given.\n    ",
        "klass": "pygments.formatter.Formatter",
        "module": "pygments"
    },
    {
        "base_classes": [
            "_io._TextIOBase"
        ],
        "class_docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.",
        "klass": "pygments.util.StringIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class represents an API Version Request.\n\n    This class provides convenience methods for manipulation\n    and comparison of version numbers that we need to do to\n    implement microversions.\n\n    :param version_string: String representation of APIVersionRequest.\n            Correct format is 'X.Y', where 'X' and 'Y' are int values.\n            None value should be used to create Null APIVersionRequest,\n            which is equal to 0.0\n    ",
        "klass": "tempest.lib.common.api_version_request.APIVersionRequest",
        "module": "tempest"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " The `Graph` class represents the graph data storage space within\n    a Neo4j graph database. Connection details are provided using URIs\n    and/or individual settings.\n\n    Supported URI schemes are:\n\n    - ``http``\n    - ``https``\n    - ``bolt``\n\n    The full set of supported `settings` are:\n\n    ===================  ========================================================  ==============  =========================\n    Keyword              Description                                               Type            Default\n    ===================  ========================================================  ==============  =========================\n    ``auth``             A 2-tuple of (user, password)                             tuple           ``('neo4j', 'password')``\n    ``host``             Database server host name                                 str             ``'localhost'``\n    ``password``         Password to use for authentication                        str             ``'password'``\n    ``port``             Database server port                                      int             ``7687``\n    ``scheme``           Use a specific URI scheme                                 str             ``'bolt'``\n    ``secure``           Use a secure connection (TLS)                             bool            ``False``\n    ``user``             User to authenticate as                                   str             ``'neo4j'``\n    ``user_agent``       User agent to send for all connections                    str             `(depends on URI scheme)`\n    ``max_connections``  The maximum number of simultaneous connections permitted  int             40\n    ===================  ========================================================  ==============  =========================\n\n    Each setting can be provided as a keyword argument or as part of\n    an ``http:``, ``https:`` or ``bolt:`` URI. Therefore, the examples\n    below are equivalent::\n\n        >>> from py2neo import Graph\n        >>> graph_1 = Graph()\n        >>> graph_2 = Graph(host=\"localhost\")\n        >>> graph_3 = Graph(\"bolt://localhost:7687\")\n\n    Once obtained, the `Graph` instance provides direct or indirect\n    access to most of the functionality available within py2neo.\n\n    Note that py2neo does not support routing with a Neo4j causal cluster\n    (bolt+routing). For this functionality, please use the official Neo4j\n    Driver for Python.\n    ",
        "klass": "py2neo.Graph",
        "module": "py2neo"
    },
    {
        "base_classes": [
            "py2neo.data.Entity"
        ],
        "class_docstring": " A node is a fundamental unit of data storage within a property\n    graph that may optionally be connected, via relationships, to\n    other nodes.\n\n    All positional arguments passed to the constructor are interpreted\n    as labels and all keyword arguments as properties::\n\n        >>> from py2neo import Node\n        >>> a = Node(\"Person\", name=\"Alice\")\n\n    ",
        "klass": "py2neo.Node",
        "module": "py2neo"
    },
    {
        "base_classes": [
            "py2neo.data.Entity"
        ],
        "class_docstring": " A relationship represents a typed connection between a pair of nodes.\n\n    The positional arguments passed to the constructor identify the nodes to\n    relate and the type of the relationship. Keyword arguments describe the\n    properties of the relationship::\n\n        >>> from py2neo import Node, Relationship\n        >>> a = Node(\"Person\", name=\"Alice\")\n        >>> b = Node(\"Person\", name=\"Bob\")\n        >>> a_knows_b = Relationship(a, \"KNOWS\", b, since=1999)\n\n    This class may be extended to allow relationship types names to be\n    derived from the class name. For example::\n\n        >>> WORKS_WITH = Relationship.type(\"WORKS_WITH\")\n        >>> a_works_with_b = WORKS_WITH(a, b)\n        >>> a_works_with_b\n        (Alice)-[:WORKS_WITH {}]->(Bob)\n\n    ",
        "klass": "py2neo.Relationship",
        "module": "py2neo"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " A Neo4j auth file, generally located at data/dbms/auth.\n    ",
        "klass": "py2neo.admin.install.AuthFile",
        "module": "py2neo"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for reading and writing KNX/IP packets.",
        "klass": "xknx.XKNX",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for binary sensor.",
        "klass": "xknx.devices.BinarySensor",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for virtual date/time device.",
        "klass": "xknx.devices.DateTime",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for devices.",
        "klass": "xknx.devices.Device",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a sensor.",
        "klass": "xknx.devices.ExposeSensor",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a fan.",
        "klass": "xknx.devices.Fan",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a light.",
        "klass": "xknx.devices.Light",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a notification.",
        "klass": "xknx.devices.Notification",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for managing remote knx value.",
        "klass": "xknx.devices.RemoteValue",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX 6.010 (DPT_Value_1_Count).",
        "klass": "xknx.devices.RemoteValue1Count",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 232.600 (DPT_Color_RGB).",
        "klass": "xknx.devices.RemoteValueColorRGB",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 251.600 (DPT_Color_RGBW).",
        "klass": "xknx.devices.RemoteValueColorRGBW",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 7.001.",
        "klass": "xknx.devices.RemoteValueDpt2ByteUnsigned",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 5.010.",
        "klass": "xknx.devices.RemoteValueDptValue1Ucount",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 17.001 (DPT_Scene_Number).",
        "klass": "xknx.devices.RemoteValueSceneNumber",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 1.007 / DPT_Step.",
        "klass": "xknx.devices.RemoteValueStep",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 1.001 / DPT_Switch.",
        "klass": "xknx.devices.RemoteValueSwitch",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.remote_value.RemoteValue"
        ],
        "class_docstring": "Abstraction for remote value of KNX DPT 1.008 / DPT_UpDown.",
        "klass": "xknx.devices.RemoteValueUpDown",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a scene.",
        "klass": "xknx.devices.Scene",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a sensor.",
        "klass": "xknx.devices.Sensor",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.devices.device.Device"
        ],
        "class_docstring": "Class for managing a switch.",
        "klass": "xknx.devices.Switch",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for handling KNX/IP tunnels.",
        "klass": "xknx.io.Tunnel",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for filtering Addresses according to patterns.",
        "klass": "xknx.knx.AddressFilter",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 2 Octet Floating Point Numbers.\n\n    DPT 9.***\n    ",
        "klass": "xknx.knx.DPT2ByteFloat",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 2 Byte signed values.\n\n    DPT 8.***\n    ",
        "klass": "xknx.knx.DPT2ByteSigned",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 4 Octet Floating Point Numbers, with a maximum usable range as specified in IEEE 754.\n\n    The largest positive finite float literal is 3.40282347e+38f.\n    The smallest positive finite non-zero literal of type float is 1.40239846e-45f.\n    The negative minimum finite float literal is -3.40282347e+38f.\n    No value range are defined for DPTs 14.000-079.\n\n    DPT 14.***\n    ",
        "klass": "xknx.knx.DPT4ByteFloat",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt_4byte_int.DPT4ByteUnsigned"
        ],
        "class_docstring": "\n    Abstraction for KNX 4 Byte \"32-bit signed\".\n\n    DPT 13.***\n    ",
        "klass": "xknx.knx.DPT4ByteSigned",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 4 Byte \"32-bit unsigned\".\n\n    DPT 12.***\n    ",
        "klass": "xknx.knx.DPT4ByteUnsigned",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt_scaling.DPTScaling"
        ],
        "class_docstring": "\n    Abstraction for KNX 1 Octet Angle.\n\n    DPT 5.003\n    ",
        "klass": "xknx.knx.DPTAngle",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "Abstraction for KNX 3 octet date (DPT 11.001).",
        "klass": "xknx.knx.DPTDate",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "Abstraction for KNX 8 octet datetime (DPT 19.001).",
        "klass": "xknx.knx.DPTDateTime",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 1 Octet Percent.\n\n    DPT 5.001\n    ",
        "klass": "xknx.knx.DPTScaling",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt_1byte_uint.DPTValue1Ucount"
        ],
        "class_docstring": "\n    Abstraction for KNX 1 Octet Scene Number.\n\n    DPT 17.001\n    ",
        "klass": "xknx.knx.DPTSceneNumber",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 14 Octet String.\n\n    DPT 16.000\n    ",
        "klass": "xknx.knx.DPTString",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt_2byte_float.DPT2ByteFloat"
        ],
        "class_docstring": "DPT 9.001 DPT_Value_Temp.",
        "klass": "xknx.knx.DPTTemperature",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 3 Octet Time.\n\n    DPT 10.001\n    ",
        "klass": "xknx.knx.DPTTime",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt_2byte_uint.DPT2ByteUnsigned"
        ],
        "class_docstring": "DPT 7.012 Abstraction for KNX 2 Byte DPTUElCurrentmA.",
        "klass": "xknx.knx.DPTUElCurrentmA",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knx.dpt.DPTBase"
        ],
        "class_docstring": "\n    Abstraction for KNX 1 Octet.\n\n    DPT 5.010\n    ",
        "klass": "xknx.knx.DPTValue1Ucount",
        "module": "xknx"
    },
    {
        "base_classes": [
            "xknx.knxip.dib.DIB"
        ],
        "class_docstring": "\n    Module for serialization and deserialization of KNX DIB Generic.\n\n    Fallback for not implemented DIBTypeCodes.\n    ",
        "klass": "xknx.knxip.DIBGeneric",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for Module for Serialization and Deserialization.",
        "klass": "xknx.knxip.HPAI",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for KNX/IP Frames.",
        "klass": "xknx.knxip.KNXIPFrame",
        "module": "xknx"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for serialization and deserialization of KNX/IP Header.",
        "klass": "xknx.knxip.KNXIPHeader",
        "module": "xknx"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    An ordered dictionary that can have multiple values for each key.\n    Adds the methods getall, getone, mixed, and add to the normal\n    dictionary interface.\n    ",
        "klass": "paste.util.multidict.MultiDict",
        "module": "paste"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    A MultiDict wrapper that decodes returned values to unicode on the\n    fly. Decoding is not applied to assigned values.\n\n    The key/value contents are assumed to be ``str``/``strs`` or\n    ``str``/``FieldStorages`` (as is returned by the ``paste.request.parse_``\n    functions).\n\n    Can optionally also decode keys when the ``decode_keys`` argument is\n    True.\n\n    ``FieldStorage`` instances are cloned, and the clone's ``filename``\n    variable is decoded. Its ``name`` variable is decoded when ``decode_keys``\n    is enabled.\n\n    ",
        "klass": "paste.util.multidict.UnicodeMultiDict",
        "module": "paste"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "WSGI Request API Object\n\n    This object represents a WSGI request with a more friendly interface.\n    This does not expose every detail of the WSGI environment, and attempts\n    to express nothing beyond what is available in the environment\n    dictionary.\n\n    The only state maintained in this object is the desired ``charset``,\n    its associated ``errors`` handler, and the ``decode_param_names``\n    option.\n\n    The incoming parameter values will be automatically coerced to unicode\n    objects of the ``charset`` encoding when ``charset`` is set. The\n    incoming parameter names are not decoded to unicode unless the\n    ``decode_param_names`` option is enabled.\n\n    When unicode is expected, ``charset`` will overridden by the the\n    value of the ``Content-Type`` header's charset parameter if one was\n    specified by the client.\n\n    The class variable ``defaults`` specifies default values for\n    ``charset``, ``errors``, and ``langauge``. These can be overridden for the\n    current request via the registry.\n\n    The ``language`` default value is considered the fallback during i18n\n    translations to ensure in odd cases that mixed languages don't occur should\n    the ``language`` file contain the string but not another language in the\n    accepted languages list. The ``language`` value only applies when getting\n    a list of accepted languages from the HTTP Accept header.\n\n    This behavior is duplicated from Aquarium, and may seem strange but is\n    very useful. Normally, everything in the code is in \"en-us\".  However,\n    the \"en-us\" translation catalog is usually empty.  If the user requests\n    ``[\"en-us\", \"zh-cn\"]`` and a translation isn't found for a string in\n    \"en-us\", you don't want gettext to fallback to \"zh-cn\".  You want it to\n    just use the string itself.  Hence, if a string isn't found in the\n    ``language`` catalog, the string in the source code will be used.\n\n    *All* other state is kept in the environment dictionary; this is\n    essential for interoperability.\n\n    You are free to subclass this object.\n\n    ",
        "klass": "paste.wsgiwrappers.WSGIRequest",
        "module": "paste"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A basic HTTP response with content, headers, and out-bound cookies\n\n    The class variable ``defaults`` specifies default values for\n    ``content_type``, ``charset`` and ``errors``. These can be overridden\n    for the current request via the registry.\n\n    ",
        "klass": "paste.wsgiwrappers.WSGIResponse",
        "module": "paste"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon_threadsafe().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.",
        "klass": "tornado.concurrent.Future",
        "module": "_asyncio"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon_threadsafe().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.",
        "klass": "tornado.gen.Future",
        "module": "_asyncio"
    },
    {
        "base_classes": [
            "tornado.util.Configurable"
        ],
        "class_docstring": "An non-blocking HTTP client.\n\n    Example usage::\n\n        async def f():\n            http_client = AsyncHTTPClient()\n            try:\n                response = await http_client.fetch(\"http://www.google.com\")\n            except Exception as e:\n                print(\"Error: %s\" % e)\n            else:\n                print(response.body)\n\n    The constructor for this class is magic in several respects: It\n    actually creates an instance of an implementation-specific\n    subclass, and instances are reused as a kind of pseudo-singleton\n    (one per `.IOLoop`). The keyword argument ``force_instance=True``\n    can be used to suppress this singleton behavior. Unless\n    ``force_instance=True`` is used, no arguments should be passed to\n    the `AsyncHTTPClient` constructor. The implementation subclass as\n    well as arguments to its constructor can be set with the static\n    method `configure()`\n\n    All `AsyncHTTPClient` implementations support a ``defaults``\n    keyword argument, which can be used to set default values for\n    `HTTPRequest` attributes.  For example::\n\n        AsyncHTTPClient.configure(\n            None, defaults=dict(user_agent=\"MyUserAgent\"))\n        # or with force_instance:\n        client = AsyncHTTPClient(force_instance=True,\n            defaults=dict(user_agent=\"MyUserAgent\"))\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n\n    ",
        "klass": "tornado.httpclient.AsyncHTTPClient",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A blocking HTTP client.\n\n    This interface is provided to make it easier to share code between\n    synchronous and asynchronous applications. Applications that are\n    running an `.IOLoop` must use `AsyncHTTPClient` instead.\n\n    Typical usage looks like this::\n\n        http_client = httpclient.HTTPClient()\n        try:\n            response = http_client.fetch(\"http://www.google.com/\")\n            print(response.body)\n        except httpclient.HTTPError as e:\n            # HTTPError is raised for non-200 responses; the response\n            # can be found in e.response.\n            print(\"Error: \" + str(e))\n        except Exception as e:\n            # Other errors are possible, such as IOError.\n            print(\"Error: \" + str(e))\n        http_client.close()\n\n    .. versionchanged:: 5.0\n\n       Due to limitations in `asyncio`, it is no longer possible to\n       use the synchronous ``HTTPClient`` while an `.IOLoop` is running.\n       Use `AsyncHTTPClient` instead.\n\n    ",
        "klass": "tornado.httpclient.HTTPClient",
        "module": "tornado"
    },
    {
        "base_classes": [
            "tornado.tcpserver.TCPServer",
            "tornado.util.Configurable",
            "tornado.httputil.HTTPServerConnectionDelegate"
        ],
        "class_docstring": "A non-blocking, single-threaded HTTP server.\n\n    A server is defined by a subclass of `.HTTPServerConnectionDelegate`,\n    or, for backwards compatibility, a callback that takes an\n    `.HTTPServerRequest` as an argument. The delegate is usually a\n    `tornado.web.Application`.\n\n    `HTTPServer` supports keep-alive connections by default\n    (automatically for HTTP/1.1, or for HTTP/1.0 when the client\n    requests ``Connection: keep-alive``).\n\n    If ``xheaders`` is ``True``, we support the\n    ``X-Real-Ip``/``X-Forwarded-For`` and\n    ``X-Scheme``/``X-Forwarded-Proto`` headers, which override the\n    remote IP and URI scheme/protocol for all requests.  These headers\n    are useful when running Tornado behind a reverse proxy or load\n    balancer.  The ``protocol`` argument can also be set to ``https``\n    if Tornado is run behind an SSL-decoding proxy that does not set one of\n    the supported ``xheaders``.\n\n    By default, when parsing the ``X-Forwarded-For`` header, Tornado will\n    select the last (i.e., the closest) address on the list of hosts as the\n    remote host IP address.  To select the next server in the chain, a list of\n    trusted downstream hosts may be passed as the ``trusted_downstream``\n    argument.  These hosts will be skipped when parsing the ``X-Forwarded-For``\n    header.\n\n    To make this server serve SSL traffic, send the ``ssl_options`` keyword\n    argument with an `ssl.SSLContext` object. For compatibility with older\n    versions of Python ``ssl_options`` may also be a dictionary of keyword\n    arguments for the `ssl.wrap_socket` method.::\n\n       ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n       ssl_ctx.load_cert_chain(os.path.join(data_dir, \"mydomain.crt\"),\n                               os.path.join(data_dir, \"mydomain.key\"))\n       HTTPServer(application, ssl_options=ssl_ctx)\n\n    `HTTPServer` initialization follows one of three patterns (the\n    initialization methods are defined on `tornado.tcpserver.TCPServer`):\n\n    1. `~tornado.tcpserver.TCPServer.listen`: simple single-process::\n\n            server = HTTPServer(app)\n            server.listen(8888)\n            IOLoop.current().start()\n\n       In many cases, `tornado.web.Application.listen` can be used to avoid\n       the need to explicitly create the `HTTPServer`.\n\n    2. `~tornado.tcpserver.TCPServer.bind`/`~tornado.tcpserver.TCPServer.start`:\n       simple multi-process::\n\n            server = HTTPServer(app)\n            server.bind(8888)\n            server.start(0)  # Forks multiple sub-processes\n            IOLoop.current().start()\n\n       When using this interface, an `.IOLoop` must *not* be passed\n       to the `HTTPServer` constructor.  `~.TCPServer.start` will always start\n       the server on the default singleton `.IOLoop`.\n\n    3. `~tornado.tcpserver.TCPServer.add_sockets`: advanced multi-process::\n\n            sockets = tornado.netutil.bind_sockets(8888)\n            tornado.process.fork_processes(0)\n            server = HTTPServer(app)\n            server.add_sockets(sockets)\n            IOLoop.current().start()\n\n       The `~.TCPServer.add_sockets` interface is more complicated,\n       but it can be used with `tornado.process.fork_processes` to\n       give you more flexibility in when the fork happens.\n       `~.TCPServer.add_sockets` can also be used in single-process\n       servers if you want to create your listening sockets in some\n       way other than `tornado.netutil.bind_sockets`.\n\n    .. versionchanged:: 4.0\n       Added ``decompress_request``, ``chunk_size``, ``max_header_size``,\n       ``idle_connection_timeout``, ``body_timeout``, ``max_body_size``\n       arguments.  Added support for `.HTTPServerConnectionDelegate`\n       instances as ``request_callback``.\n\n    .. versionchanged:: 4.1\n       `.HTTPServerConnectionDelegate.start_request` is now called with\n       two arguments ``(server_conn, request_conn)`` (in accordance with the\n       documentation) instead of one ``(request_conn)``.\n\n    .. versionchanged:: 4.2\n       `HTTPServer` is now a subclass of `tornado.util.Configurable`.\n\n    .. versionchanged:: 4.5\n       Added the ``trusted_downstream`` argument.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument has been removed.\n    ",
        "klass": "tornado.httpserver.HTTPServer",
        "module": "tornado"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "A dictionary that maintains ``Http-Header-Case`` for all keys.\n\n    Supports multiple values per key via a pair of new methods,\n    `add()` and `get_list()`.  The regular dictionary interface\n    returns a single value per key, with multiple values joined by a\n    comma.\n\n    >>> h = HTTPHeaders({\"content-type\": \"text/html\"})\n    >>> list(h.keys())\n    ['Content-Type']\n    >>> h[\"Content-Type\"]\n    'text/html'\n\n    >>> h.add(\"Set-Cookie\", \"A=B\")\n    >>> h.add(\"Set-Cookie\", \"C=D\")\n    >>> h[\"set-cookie\"]\n    'A=B,C=D'\n    >>> h.get_list(\"set-cookie\")\n    ['A=B', 'C=D']\n\n    >>> for (k,v) in sorted(h.get_all()):\n    ...    print('%s: %s' % (k,v))\n    ...\n    Content-Type: text/html\n    Set-Cookie: A=B\n    Set-Cookie: C=D\n    ",
        "klass": "tornado.httputil.HTTPHeaders",
        "module": "tornado"
    },
    {
        "base_classes": [
            "tornado.iostream.BaseIOStream"
        ],
        "class_docstring": "Socket-based `IOStream` implementation.\n\n    This class supports the read and write methods from `BaseIOStream`\n    plus a `connect` method.\n\n    The ``socket`` parameter may either be connected or unconnected.\n    For server operations the socket is the result of calling\n    `socket.accept <socket.socket.accept>`.  For client operations the\n    socket is created with `socket.socket`, and may either be\n    connected before passing it to the `IOStream` or connected with\n    `IOStream.connect`.\n\n    A very simple (and broken) HTTP client using this class:\n\n    .. testcode::\n\n        import tornado.ioloop\n        import tornado.iostream\n        import socket\n\n        async def main():\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n            stream = tornado.iostream.IOStream(s)\n            await stream.connect((\"friendfeed.com\", 80))\n            await stream.write(b\"GET / HTTP/1.0\\r\\nHost: friendfeed.com\\r\\n\\r\\n\")\n            header_data = await stream.read_until(b\"\\r\\n\\r\\n\")\n            headers = {}\n            for line in header_data.split(b\"\\r\\n\"):\n                parts = line.split(b\":\")\n                if len(parts) == 2:\n                    headers[parts[0].strip()] = parts[1].strip()\n            body_data = await stream.read_bytes(int(headers[b\"Content-Length\"]))\n            print(body_data)\n            stream.close()\n\n        if __name__ == '__main__':\n            tornado.ioloop.IOLoop.current().run_sync(main)\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\n            stream = tornado.iostream.IOStream(s)\n            stream.connect((\"friendfeed.com\", 80), send_request)\n            tornado.ioloop.IOLoop.current().start()\n\n    .. testoutput::\n       :hide:\n\n    ",
        "klass": "tornado.iostream.IOStream",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A lock for coroutines.\n\n    A Lock begins unlocked, and `acquire` locks it immediately. While it is\n    locked, a coroutine that yields `acquire` waits until another coroutine\n    calls `release`.\n\n    Releasing an unlocked lock raises `RuntimeError`.\n\n    A Lock can be used as an async context manager with the ``async\n    with`` statement:\n\n    >>> from tornado import locks\n    >>> lock = locks.Lock()\n    >>>\n    >>> async def f():\n    ...    async with lock:\n    ...        # Do something holding the lock.\n    ...        pass\n    ...\n    ...    # Now the lock is released.\n\n    For compatibility with older versions of Python, the `.acquire`\n    method asynchronously returns a regular context manager:\n\n    >>> async def f2():\n    ...    with (yield lock.acquire()):\n    ...        # Do something holding the lock.\n    ...        pass\n    ...\n    ...    # Now the lock is released.\n\n    .. versionchanged:: 4.3\n       Added ``async with`` support in Python 3.5.\n\n    ",
        "klass": "tornado.locks.Lock",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A collection of options, a dictionary with object-like access.\n\n    Normally accessed via static functions in the `tornado.options` module,\n    which reference a global instance.\n    ",
        "klass": "tornado.options.OptionParser",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Wraps ``subprocess.Popen`` with IOStream support.\n\n    The constructor is the same as ``subprocess.Popen`` with the following\n    additions:\n\n    * ``stdin``, ``stdout``, and ``stderr`` may have the value\n      ``tornado.process.Subprocess.STREAM``, which will make the corresponding\n      attribute of the resulting Subprocess a `.PipeIOStream`. If this option\n      is used, the caller is responsible for closing the streams when done\n      with them.\n\n    The ``Subprocess.STREAM`` option and the ``set_exit_callback`` and\n    ``wait_for_exit`` methods do not work on Windows. There is\n    therefore no reason to use this class instead of\n    ``subprocess.Popen`` on that platform.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n\n    ",
        "klass": "tornado.process.Subprocess",
        "module": "tornado"
    },
    {
        "base_classes": [
            "typing.Generic"
        ],
        "class_docstring": "Coordinate producer and consumer coroutines.\n\n    If maxsize is 0 (the default) the queue size is unbounded.\n\n    .. testcode::\n\n        from tornado import gen\n        from tornado.ioloop import IOLoop\n        from tornado.queues import Queue\n\n        q = Queue(maxsize=2)\n\n        async def consumer():\n            async for item in q:\n                try:\n                    print('Doing work on %s' % item)\n                    await gen.sleep(0.01)\n                finally:\n                    q.task_done()\n\n        async def producer():\n            for item in range(5):\n                await q.put(item)\n                print('Put %s' % item)\n\n        async def main():\n            # Start consumer without waiting (since it never finishes).\n            IOLoop.current().spawn_callback(consumer)\n            await producer()     # Wait for producer to put all tasks.\n            await q.join()       # Wait for consumer to finish all tasks.\n            print('Done')\n\n        IOLoop.current().run_sync(main)\n\n    .. testoutput::\n\n        Put 0\n        Put 1\n        Doing work on 0\n        Put 2\n        Doing work on 1\n        Put 3\n        Doing work on 2\n        Put 4\n        Doing work on 3\n        Doing work on 4\n        Done\n\n\n    In versions of Python without native coroutines (before 3.5),\n    ``consumer()`` could be written as::\n\n        @gen.coroutine\n        def consumer():\n            while True:\n                item = yield q.get()\n                try:\n                    print('Doing work on %s' % item)\n                    yield gen.sleep(0.01)\n                finally:\n                    q.task_done()\n\n    .. versionchanged:: 4.3\n       Added ``async for`` support in Python 3.5.\n\n    ",
        "klass": "tornado.queues.Queue",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A compiled template.\n\n    We compile into Python from the given template_string. You can generate\n    the template from variables with generate().\n    ",
        "klass": "tornado.template.Template",
        "module": "tornado"
    },
    {
        "base_classes": [
            "logging.Filter"
        ],
        "class_docstring": "Context manager to capture and suppress expected log output.\n\n    Useful to make tests of error conditions less noisy, while still\n    leaving unexpected log entries visible.  *Not thread safe.*\n\n    The attribute ``logged_stack`` is set to ``True`` if any exception\n    stack trace was logged.\n\n    Usage::\n\n        with ExpectLog('tornado.application', \"Uncaught exception\"):\n            error_response = self.fetch(\"/some_page\")\n\n    .. versionchanged:: 4.3\n       Added the ``logged_stack`` attribute.\n    ",
        "klass": "tornado.testing.ExpectLog",
        "module": "tornado"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Topology stores the topological information about a system.\n\n    The structure of a Topology object is similar to that of a PDB file.\n    It consists of a set of Chains (often but not always corresponding to\n    polymer chains).  Each Chain contains a set of Residues, and each Residue\n    contains a set of Atoms.  In addition, the Topology stores a list of which\n    atom pairs are bonded to each other.\n\n    Atom and residue names should follow the PDB 3.0 nomenclature for all\n    molecules for which one exists.\n\n    Attributes\n    ----------\n    chains : generator\n        Iterator over all Chains in the Topology.\n    residues : generator\n        Iterator over all Residues in the Chain.\n    atoms : generator\n        Iterator over all Atoms in the Chain.\n    bonds : generator\n        Iterator over all Bonds in the Topology\n\n    Examples\n    --------\n    >>> topology = md.load('example.pdb').topology\n    >>> print(topology)\n    <mdtraj.Topology with 1 chains, 3 residues, 22 atoms, 21 bonds at 0x105a98e90>\n    >>> table, bonds = topology.to_dataframe()\n    >>> print(table.head())\n       serial name element  resSeq resName  chainID\n    0       0   H1       H       1     CYS        0\n    1       1  CH3       C       1     CYS        0\n    2       2   H2       H       1     CYS        0\n    3       3   H3       H       1     CYS        0\n    4       4    C       C       1     CYS        0\n    >>> # rename residue \"CYS\" to \"CYSS\"\n    >>> table[table['residue'] == 'CYS']['residue'] = 'CYSS'\n    >>> print(table.head())\n       serial name element  resSeq resName   chainID\n    0       0   H1       H       1     CYSS        0\n    1       1  CH3       C       1     CYSS        0\n    2       2   H2       H       1     CYSS        0\n    3       3   H3       H       1     CYSS        0\n    4       4    C       C       1     CYSS        0\n    >>> t2 = md.Topology.from_dataframe(table, bonds)\n    ",
        "klass": "mdtraj.core.topology.Topology",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Topology stores the topological information about a system.\n\n    The structure of a Topology object is similar to that of a PDB file.\n    It consists of a set of Chains (often but not always corresponding to\n    polymer chains).  Each Chain contains a set of Residues, and each Residue\n    contains a set of Atoms.  In addition, the Topology stores a list of which\n    atom pairs are bonded to each other.\n\n    Atom and residue names should follow the PDB 3.0 nomenclature for all\n    molecules for which one exists.\n\n    Attributes\n    ----------\n    chains : generator\n        Iterator over all Chains in the Topology.\n    residues : generator\n        Iterator over all Residues in the Chain.\n    atoms : generator\n        Iterator over all Atoms in the Chain.\n    bonds : generator\n        Iterator over all Bonds in the Topology\n\n    Examples\n    --------\n    >>> topology = md.load('example.pdb').topology\n    >>> print(topology)\n    <mdtraj.Topology with 1 chains, 3 residues, 22 atoms, 21 bonds at 0x105a98e90>\n    >>> table, bonds = topology.to_dataframe()\n    >>> print(table.head())\n       serial name element  resSeq resName  chainID\n    0       0   H1       H       1     CYS        0\n    1       1  CH3       C       1     CYS        0\n    2       2   H2       H       1     CYS        0\n    3       3   H3       H       1     CYS        0\n    4       4    C       C       1     CYS        0\n    >>> # rename residue \"CYS\" to \"CYSS\"\n    >>> table[table['residue'] == 'CYS']['residue'] = 'CYSS'\n    >>> print(table.head())\n       serial name element  resSeq resName   chainID\n    0       0   H1       H       1     CYSS        0\n    1       1  CH3       C       1     CYSS        0\n    2       2   H2       H       1     CYSS        0\n    3       3   H3       H       1     CYSS        0\n    4       4    C       C       1     CYSS        0\n    >>> t2 = md.Topology.from_dataframe(table, bonds)\n    ",
        "klass": "mdtraj.Topology",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Container object for a molecular dynamics trajectory\n\n    A Trajectory represents a collection of one or more molecular structures,\n    generally (but not necessarily) from a molecular dynamics trajectory. The\n    Trajectory stores a number of fields describing the system through time,\n    including the cartesian coordinates of each atoms (``xyz``), the topology\n    of the molecular system (``topology``), and information about the\n    unitcell if appropriate (``unitcell_vectors``, ``unitcell_length``,\n    ``unitcell_angles``).\n\n    A Trajectory should generally be constructed by loading a file from disk.\n    Trajectories can be loaded from (and saved to) the PDB, XTC, TRR, DCD,\n    binpos, NetCDF or MDTraj HDF5 formats.\n\n    Trajectory supports fancy indexing, so you can extract one or more frames\n    from a Trajectory as a separate trajectory. For example, to form a\n    trajectory with every other frame, you can slice with ``traj[::2]``.\n\n    Trajectory uses the nanometer, degree & picosecond unit system.\n\n    Examples\n    --------\n    >>> # loading a trajectory\n    >>> import mdtraj as md\n    >>> md.load('trajectory.xtc', top='native.pdb')\n    <mdtraj.Trajectory with 1000 frames, 22 atoms at 0x1058a73d0>\n\n    >>> # slicing a trajectory\n    >>> t = md.load('trajectory.h5')\n    >>> print(t)\n    <mdtraj.Trajectory with 100 frames, 22 atoms>\n    >>> print(t[::2])\n    <mdtraj.Trajectory with 50 frames, 22 atoms>\n\n    >>> # calculating the average distance between two atoms\n    >>> import mdtraj as md\n    >>> import numpy as np\n    >>> t = md.load('trajectory.h5')\n    >>> np.mean(np.sqrt(np.sum((t.xyz[:, 0, :] - t.xyz[:, 21, :])**2, axis=1)))\n\n    See Also\n    --------\n    mdtraj.load : High-level function that loads files and returns an ``md.Trajectory``\n\n    Attributes\n    ----------\n    n_frames : int\n    n_atoms : int\n    n_residues : int\n    time : np.ndarray, shape=(n_frames,)\n    timestep : float\n    topology : md.Topology\n    top : md.Topology\n    xyz : np.ndarray, shape=(n_frames, n_atoms, 3)\n    unitcell_vectors : {np.ndarray, shape=(n_frames, 3, 3), None}\n    unitcell_lengths : {np.ndarray, shape=(n_frames, 3), None}\n    unitcell_angles : {np.ndarray, shape=(n_frames, 3), None}\n    ",
        "klass": "mdtraj.Trajectory",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "DCDTrajectoryFile(filename, mode='r', force_overwrite=True)\n\n    Interface for reading and writing to a CHARMM/NAMD DCD file.\n    This is a file-like object, that both reading or writing depending\n    on the `mode` flag. It implements the context manager protocol,\n    so you can also use it with the python 'with' statement.\n\n    The conventional units in the DCD file are angstroms and degrees. The format\n    supports saving coordinates and unit cell parameters (lengths and angles)\n\n    Parameters\n    ----------\n    filename : string\n        Path to the file to open\n    mode : {'r', 'w'}\n        Mode in which to open the file. 'r' is for reading, and 'w' is for writing.\n    force_overwrite : bool\n        In mode='w', how do you want to behave if a file by the name of `filename`\n        already exists? if `force_overwrite=True`, it will be overwritten.\n\n    Examples\n    --------\n    >>> # read a single frame, and then read the remaining frames\n    >>> f = DCDTrajectoryFile('mytrajectory.dcd', 'r')\n    >>> f.read(n_frames=1)  # read a single frame from the file\n    >>> xyz.read()            # read all of the remaining frames\n    >>> f.close()\n\n    >>> # read all of the data with automatic closing of the file\n    >>> with DCDTrajectoryFile('mytrajectory.dcd', 'r') as f:\n    >>>    xyz, cell_lengths, cell_angles = f.read()\n\n    >>> # write some xyz coordinates to a new file\n    >>> with DCDTrajectoryFile('mytrajectory2.dcd', 'w') as f:\n    >>>     f.write(np.random.randn(10,3,3))\n\n    >>> # write frames one at a time\n    >>> with DCDTrajectoryFile('mytrajectory2.dcd', 'w') as f:\n    >>>     n_frames, n_atoms = 5, 10\n    >>>     for i in range(n_frames):\n    >>>         f.write(np.random.randn(n_atoms, 3))\n\n    See Also\n    --------\n    mdtraj.load_dcd : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.core.trajectory.DCDTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "TRRTrajectoryFile(filename, mode='r', force_overwrite=True, **kwargs)\n\n    Interface for reading and writing to a GROMACS TRR file.\n    This is a file-like objec that supports both reading and writing.\n    It also supports the context manager protocol, so you can use it\n    with the python 'with' statement.\n\n    The conventional units in the TRR file are nanometers and picoseconds.\n    The format only supports saving coordinates, the time, the md step,\n    and the unit cell parametrs (box vectors)\n\n    Parameters\n    ----------\n    filename : str\n        The filename to open. A path to a file on disk.\n    mode : {'r', 'w'}\n        The mode in which to open the file, either 'r' for read or 'w' for write.\n    force_overwrite : bool\n        If opened in write mode, and a file by the name of `filename` already\n        exists on disk, should we overwrite it?\n\n    Other Parameters\n    ----------------\n    min_chunk_size : int, default=100\n        In read mode, we need to allocate a buffer in which to store the data\n        without knowing how many frames are in the file. This parameter is the\n        minimum size of the buffer to allocate.\n    chunk_size_multiplier, int, default=1.5\n        In read mode, we need to allocate a buffer in which to store the data\n        without knowing how many frames are in the file. We can *guess* this\n        information based on the size of the file on disk, but it's not perfect.\n        This parameter inflates the guess by a multiplicative factor.\n\n    Examples\n    --------\n    >>> # load up the data from a trr\n    >>> with TRRTrajectoryFile('traj.trr') as f:\n    >>>    xyz, time, step, box, lambdas = f.read()\n\n    See Also\n    --------\n    mdtraj.load_trr : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.core.trajectory.TRRTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "XTCTrajectoryFile(filenamee, mode='r', force_overwrite=True, **kwargs)\n\n    Interface for reading and writing to a GROMACS XTC file.\n    This is a file-like objec that supports both reading and writing.\n    It also supports the context manager ptorocol, so you can use it\n    with the python 'with' statement.\n\n    The conventional units in the XTC file are nanometers and picoseconds.\n    The format only supports saving coordinates, the time, the md step,\n    and the unit cell parametrs (box vectors)\n\n    Parameters\n    ----------\n    filename : str\n        The filename to open. A path to a file on disk.\n    mode : {'r', 'w'}\n        The mode in which to open the file, either 'r' for read or 'w' for write.\n    force_overwrite : bool\n        If opened in write mode, and a file by the name of `filename` already exists on disk, should we overwrite it?\n\n    Other Parameters\n    ----------------\n    min_chunk_size : int, default=100\n        In read mode, we need to allocate a buffer in which to store the data\n        without knowing how many frames are in the file. This parameter is the\n        minimum size of the buffer to allocate.\n    chunk_size_multiplier : int, default=1.5\n        In read mode, we need to allocate a buffer in which to store the data without knowing how many frames are in\n        the file. We can *guess* this information based on the size of the file on disk, but it's not perfect. This\n        parameter inflates the guess by a multiplicative factor.\n\n    Examples\n    --------\n    >>> # read the data from from an XTC file\n    >>> with XTCTrajectoryFile('traj.xtc') as f:\n    >>>    xyz, time, step, box = f.read()\n\n    >>> # write some random coordinates to an XTC file\n    >>> with XTCTrajectoryFile('output.xtc', 'w') as f:\n    >>>     f.write(np.random.randn(10,1,3))\n\n    See Also\n    --------\n    mdtraj.load_xtc : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.core.trajectory.XTCTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Interface for reading and writing AMBER NetCDF files. This is a file-like\n    object, that supports both reading and writing depending on the `mode` flag.\n    It implements the context manager protocol, so you can also use it with the\n    python 'with' statement.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to open\n    mode : {'r', 'w'}, default='r'\n        The mode in which to open the file. Valid options are 'r' or 'w' for\n        'read' or 'write'\n    force_overwrite : bool, default=False\n        In write mode, if a file named `filename` already exists, clobber it and\n        overwrite it\n    ",
        "klass": "mdtraj.formats.AmberNetCDFRestartFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Interface for reading and writing AMBER ASCII restart files. This is a\n    file-like object, that supports both reading and writing depending on the\n    `mode` flag.  It implements the context manager protocol, so you can also\n    use it with the python 'with' statement.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to open\n    mode : {'r', 'w'}, default='r'\n        The mode in which to open the file. Valid options are 'r' or 'w' for\n        'read' or 'write'\n    force_overwrite : bool, default=False\n        In write mode, if a file named `filename` already exists, clobber it and\n        overwrite it\n\n    See Also\n    --------\n    md.AmberNetCDFRestartFile : Low level interface to AMBER NetCDF-format restart files\n    ",
        "klass": "mdtraj.formats.AmberRestartFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "DCDTrajectoryFile(filename, mode='r', force_overwrite=True)\n\n    Interface for reading and writing to a CHARMM/NAMD DCD file.\n    This is a file-like object, that both reading or writing depending\n    on the `mode` flag. It implements the context manager protocol,\n    so you can also use it with the python 'with' statement.\n\n    The conventional units in the DCD file are angstroms and degrees. The format\n    supports saving coordinates and unit cell parameters (lengths and angles)\n\n    Parameters\n    ----------\n    filename : string\n        Path to the file to open\n    mode : {'r', 'w'}\n        Mode in which to open the file. 'r' is for reading, and 'w' is for writing.\n    force_overwrite : bool\n        In mode='w', how do you want to behave if a file by the name of `filename`\n        already exists? if `force_overwrite=True`, it will be overwritten.\n\n    Examples\n    --------\n    >>> # read a single frame, and then read the remaining frames\n    >>> f = DCDTrajectoryFile('mytrajectory.dcd', 'r')\n    >>> f.read(n_frames=1)  # read a single frame from the file\n    >>> xyz.read()            # read all of the remaining frames\n    >>> f.close()\n\n    >>> # read all of the data with automatic closing of the file\n    >>> with DCDTrajectoryFile('mytrajectory.dcd', 'r') as f:\n    >>>    xyz, cell_lengths, cell_angles = f.read()\n\n    >>> # write some xyz coordinates to a new file\n    >>> with DCDTrajectoryFile('mytrajectory2.dcd', 'w') as f:\n    >>>     f.write(np.random.randn(10,3,3))\n\n    >>> # write frames one at a time\n    >>> with DCDTrajectoryFile('mytrajectory2.dcd', 'w') as f:\n    >>>     n_frames, n_atoms = 5, 10\n    >>>     for i in range(n_frames):\n    >>>         f.write(np.random.randn(n_atoms, 3))\n\n    See Also\n    --------\n    mdtraj.load_dcd : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.formats.DCDTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "DTRTrajectoryFile(filename, mode='r', force_overwrite=True)\n\n    Interface for reading and writing to a DESMOND dtr file.\n    This is a file-like object, that both reading or writing depending\n    on the `mode` flag. It implements the context manager protocol,\n    so you can also use it with the python 'with' statement.\n\n    The conventional units in the dtr file are angstroms and degrees. The format\n    supports saving coordinates and unit cell parameters (lengths and angles)\n\n    Parameters\n    ----------\n    filename : string\n        Path to the file to open\n    mode : {'r', 'w'}\n        Mode in which to open the file. 'r' is for reading, and 'w' is for writing.\n    force_overwrite : bool\n        In mode='w', how do you want to behave if a file by the name of `filename`\n        already exists? if `force_overwrite=True`, it will be overwritten.\n\n    Examples\n    --------\n    >>> # read a single frame, and then read the remaining frames\n    >>> f = DTRTrajectoryFile('mytrajectory.dtr', 'r')\n    >>> f.read(n_frames=1)  # read a single frame from the file\n    >>> f.read()            # read all of the remaining frames\n    >>> f.close()\n\n    >>> # read all of the data with automatic closing of the file\n    >>> with DTRTrajectoryFile('mytrajectory.dtr') as f:\n    >>>    xyz, cell_lengths, cell_angles = f.read()\n\n    >>> # write some xyz coordinates to a new file\n    >>> with DTRTrajectoryFile('mytrajectory2.dtr. 'w') as f:\n    >>>     f.write(np.random.randn(10,3,3))\n\n    >>> # write frames one at a time\n    >>> with DTRTrajectoryFile('mytrajectory2.dtr. 'w') as f:\n    >>>     n_frames, n_atoms = 5, 10\n    >>>     for i in range(n_frames):\n    >>>         f.write(np.random.randn(n_atoms, 3))\n\n    See Also\n    --------\n    mdtraj.load_dtr : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.formats.DTRTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Interface for reading and writing to a MDTraj HDF5 molecular\n    dynamics trajectory file, whose format is described\n    `here <https://github.com/rmcgibbo/mdtraj/issues/36>`_.\n\n    This is a file-like object, that both reading or writing depending\n    on the `mode` flag. It implements the context manager protocol,\n    so you can also use it with the python 'with' statement.\n\n    The format is extremely flexible and high performance. It can hold a wide\n    variety of information about a trajectory, including fields like the\n    temperature and energies. Because it's built on the fantastic HDF5 library,\n    it's easily extensible too.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file to open\n    mode :  {'r, 'w'}\n        Mode in which to open the file. 'r' is for reading and 'w' is for\n        writing\n    force_overwrite : bool\n        In mode='w', how do you want to behave if a file by the name of `filename`\n        already exists? if `force_overwrite=True`, it will be overwritten.\n    compression : {'zlib', None}\n        Apply compression to the file? This will save space, and does not\n        cost too many cpu cycles, so it's recommended.\n\n    Attributes\n    ----------\n    root\n    title\n    application\n    topology\n    randomState\n    forcefield\n    reference\n    constraints\n\n    See Also\n    --------\n    mdtraj.load_hdf5 : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.formats.HDF5TrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Interface for reading and writing to AMBER NetCDF files. This is a\n    file-like object, that supports both reading or writing depending\n    on the `mode` flag. It implements the context manager protocol,\n    so you can also use it with the python 'with' statement.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to open\n    mode : {'r', 'w'}, default='r'\n        The mode in which to open the file. Valid options are 'r' and 'w' for\n        'read' and 'write', respectively.\n    force_overwrite : bool, default=False\n        In write mode, if a file named `filename` already exists, clobber\n        it and overwrite it.\n    ",
        "klass": "mdtraj.formats.NetCDFTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "TNGTrajectoryFile(filename, mode='r', force_overwrite=True, **kwargs)\n\n    Interface for reading and writing to a Gromacs TNG file.\n    This is a file-like object that supports both reading and writing.\n    It also supports the context manager protocol, so you can use it\n    with the python 'with' statement.\n\n    This class supports both reading and writing of coordinates, time,\n    and unit cell parameters.  It also supports reading (but not writing)\n    of topology information.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to open. A path to a file on disk.\n    mode : {'r', 'w'}\n        The mode in which to open the file, either 'r' for read or 'w' for write.\n    force_overwrite : bool\n        If opened in write mode, and a file by the name of `filename` already exists on disk, should we overwrite it?\n\n    Examples\n    --------\n    >>> # read the data from from an TNG file\n    >>> with TNGTrajectoryFile('traj.xtc') as f:\n    >>>    xyz, time, box = f.read()\n    >>>    top = f.topology\n\n    >>> # write some random coordinates to an TNG file\n    >>> with TNGTrajectoryFile('output.xtc', 'w') as f:\n    >>>     f.write(np.random.randn(10,1,3))\n\n    See Also\n    --------\n    mdtraj.load_tng : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.formats.TNGTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "XTCTrajectoryFile(filenamee, mode='r', force_overwrite=True, **kwargs)\n\n    Interface for reading and writing to a GROMACS XTC file.\n    This is a file-like objec that supports both reading and writing.\n    It also supports the context manager ptorocol, so you can use it\n    with the python 'with' statement.\n\n    The conventional units in the XTC file are nanometers and picoseconds.\n    The format only supports saving coordinates, the time, the md step,\n    and the unit cell parametrs (box vectors)\n\n    Parameters\n    ----------\n    filename : str\n        The filename to open. A path to a file on disk.\n    mode : {'r', 'w'}\n        The mode in which to open the file, either 'r' for read or 'w' for write.\n    force_overwrite : bool\n        If opened in write mode, and a file by the name of `filename` already exists on disk, should we overwrite it?\n\n    Other Parameters\n    ----------------\n    min_chunk_size : int, default=100\n        In read mode, we need to allocate a buffer in which to store the data\n        without knowing how many frames are in the file. This parameter is the\n        minimum size of the buffer to allocate.\n    chunk_size_multiplier : int, default=1.5\n        In read mode, we need to allocate a buffer in which to store the data without knowing how many frames are in\n        the file. We can *guess* this information based on the size of the file on disk, but it's not perfect. This\n        parameter inflates the guess by a multiplicative factor.\n\n    Examples\n    --------\n    >>> # read the data from from an XTC file\n    >>> with XTCTrajectoryFile('traj.xtc') as f:\n    >>>    xyz, time, step, box = f.read()\n\n    >>> # write some random coordinates to an XTC file\n    >>> with XTCTrajectoryFile('output.xtc', 'w') as f:\n    >>>     f.write(np.random.randn(10,1,3))\n\n    See Also\n    --------\n    mdtraj.load_xtc : High-level wrapper that returns a ``md.Trajectory``\n    ",
        "klass": "mdtraj.formats.XTCTrajectoryFile",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A timing context manager\n\n    Examples\n    --------\n    >>> long_function = lambda : None\n    >>> with timing('long_function'):\n    ...     long_function()\n    long_function: 0.000 seconds\n    ",
        "klass": "mdtraj.utils.timing",
        "module": "mdtraj"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "glance.db.sqlalchemy.models_metadef.GlanceMetadefBase"
        ],
        "class_docstring": "Represents a metadata-schema namespace-property in the datastore.",
        "klass": "glance.db.sqlalchemy.models_metadef.MetadefNamespaceResourceType",
        "module": "glance"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "glance.db.sqlalchemy.models_metadef.GlanceMetadefBase"
        ],
        "class_docstring": "Represents a metadata-schema object in the datastore.",
        "klass": "glance.db.sqlalchemy.models_metadef.MetadefObject",
        "module": "glance"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "glance.db.sqlalchemy.models_metadef.GlanceMetadefBase"
        ],
        "class_docstring": "Represents a metadata-schema namespace-property in the datastore.",
        "klass": "glance.db.sqlalchemy.models_metadef.MetadefProperty",
        "module": "glance"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "glance.db.sqlalchemy.models_metadef.GlanceMetadefBase"
        ],
        "class_docstring": "Represents a metadata-schema resource type in the datastore.",
        "klass": "glance.db.sqlalchemy.models_metadef.MetadefResourceType",
        "module": "glance"
    },
    {
        "base_classes": [
            "sqlalchemy.ext.declarative.api.Base",
            "glance.db.sqlalchemy.models_metadef.GlanceMetadefBase"
        ],
        "class_docstring": "Represents a metadata-schema tag in the data store.",
        "klass": "glance.db.sqlalchemy.models_metadef.MetadefTag",
        "module": "glance"
    },
    {
        "base_classes": [
            "autobahn.asyncio.rawsocket.WampRawSocketFactory"
        ],
        "class_docstring": "\n    asyncio-based WAMP-over-RawSocket client factory.\n    ",
        "klass": "autobahn.asyncio.rawsocket.WampRawSocketClientFactory",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "autobahn.asyncio.rawsocket.WampRawSocketFactory"
        ],
        "class_docstring": "\n    asyncio-based WAMP-over-RawSocket server protocol factory.\n    ",
        "klass": "autobahn.asyncio.rawsocket.WampRawSocketServerFactory",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents the collection of fields in an index. Maps field names to\n    FieldType objects which define the behavior of each field.\n\n    Low-level parts of the index use field numbers instead of field names for\n    compactness. This class has several methods for converting between the\n    field name, field number, and field object itself.\n    ",
        "klass": "whoosh.fields.Schema",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.filedb.filestore.Storage"
        ],
        "class_docstring": "Storage object that stores the index as files in a directory on disk.\n\n    Prior to version 3, the initializer would raise an IOError if the directory\n    did not exist. As of version 3, the object does not check if the\n    directory exists at initialization. This change is to support using the\n    :meth:`FileStorage.create` method.\n    ",
        "klass": "whoosh.filedb.filestore.FileStorage",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.filedb.filestore.Storage"
        ],
        "class_docstring": "Storage object that keeps the index in memory.\n    ",
        "klass": "whoosh.filedb.filestore.RamStorage",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Reader for the fast on-disk key-value files created by\n    :class:`HashWriter`.\n    ",
        "klass": "whoosh.filedb.filetables.HashReader",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Implements a fast on-disk key-value store. This hash uses a two-level\n    hashing scheme, where a key is hashed, the low eight bits of the hash value\n    are used to index into one of 256 hash tables. This is basically the CDB\n    algorithm, but unlike CDB this object writes all data serially (it doesn't\n    seek backwards to overwrite information at the end).\n\n    Also unlike CDB, this format uses 64-bit file pointers, so the file length\n    is essentially unlimited. However, each key and value must be less than\n    2 GB in length.\n    ",
        "klass": "whoosh.filedb.filetables.HashWriter",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.filedb.filetables.HashWriter"
        ],
        "class_docstring": "Implements an on-disk hash, but requires that keys be added in order.\n    An :class:`OrderedHashReader` can then look up \"nearest keys\" based on\n    the ordering.\n    ",
        "klass": "whoosh.filedb.filetables.OrderedHashWriter",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.formats.Characters"
        ],
        "class_docstring": "A format that stores positions, character start and end, and\n    per-position boost information in each posting.\n\n    Supports: frequency, weight, positions, position_boosts, characters,\n    character_boosts.\n    ",
        "klass": "whoosh.formats.CharacterBoosts",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.formats.Positions"
        ],
        "class_docstring": "Stores token position and character start and end information for each\n    posting.\n\n    Supports: frequency, weight, positions, position_boosts (always reports\n    position boost = 1.0), characters.\n    ",
        "klass": "whoosh.formats.Characters",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.formats.Format"
        ],
        "class_docstring": "Only indexes whether a given term occurred in a given document; it does\n    not store frequencies or positions. This is useful for fields that should\n    be searchable but not scorable, such as file path.\n\n    Supports: frequency, weight (always reports frequency = 1).\n    ",
        "klass": "whoosh.formats.Existence",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.formats.Positions"
        ],
        "class_docstring": "A format that stores positions and per-position boost information\n    in each posting.\n\n    Supports: frequency, weight, positions, position_boosts.\n    ",
        "klass": "whoosh.formats.PositionBoosts",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.formats.Format"
        ],
        "class_docstring": "Stores position information in each posting, to allow phrase searching\n    and \"near\" queries.\n\n    Supports: frequency, weight, positions, position_boosts (always reports\n    position boost = 1.0).\n    ",
        "klass": "whoosh.formats.Positions",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.binary.AdditiveBiMatcher"
        ],
        "class_docstring": "Matches postings in the first sub-matcher, and if the same posting is\n    in the second sub-matcher, adds their scores.\n    ",
        "klass": "whoosh.matching.AndMaybeMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.binary.BiMatcher"
        ],
        "class_docstring": "Matches the postings in the first sub-matcher that are NOT present in\n    the second sub-matcher.\n    ",
        "klass": "whoosh.matching.AndNotMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.wrappers.WrappingMatcher"
        ],
        "class_docstring": "Filters the postings from the wrapped based on whether the IDs are\n    present in or absent from a set.\n    ",
        "klass": "whoosh.matching.FilterMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.binary.AdditiveBiMatcher"
        ],
        "class_docstring": "Matches the intersection (AND) of the postings in the two sub-matchers.\n    ",
        "klass": "whoosh.matching.IntersectionMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.wrappers.WrappingMatcher"
        ],
        "class_docstring": "Synthetic matcher, generates postings that are NOT present in the\n    wrapped matcher.\n    ",
        "klass": "whoosh.matching.InverseMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.mcore.Matcher"
        ],
        "class_docstring": "Synthetic matcher backed by a list of IDs.\n    ",
        "klass": "whoosh.matching.ListMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.wrappers.WrappingMatcher"
        ],
        "class_docstring": "Matches postings that are in both sub-matchers, but only uses scores\n    from the first.\n    ",
        "klass": "whoosh.matching.RequireMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.binary.AdditiveBiMatcher"
        ],
        "class_docstring": "Matches the union (OR) of the postings in the two sub-matchers.\n    ",
        "klass": "whoosh.matching.UnionMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "whoosh.matching.mcore.Matcher"
        ],
        "class_docstring": "Base class for matchers that wrap sub-matchers.\n    ",
        "klass": "whoosh.matching.WrappingMatcher",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A hand-written query parser built on modular plug-ins. The default\n    configuration implements a powerful fielded query language similar to\n    Lucene's.\n\n    You can use the ``plugins`` argument when creating the object to override\n    the default list of plug-ins, and/or use ``add_plugin()`` and/or\n    ``remove_plugin_class()`` to change the plug-ins included in the parser.\n\n    >>> from whoosh import qparser\n    >>> parser = qparser.QueryParser(\"content\", schema)\n    >>> parser.remove_plugin_class(qparser.WildcardPlugin)\n    >>> parser.add_plugin(qparser.PrefixPlugin())\n    >>> parser.parse(u\"hello there\")\n    And([Term(\"content\", u\"hello\"), Term(\"content\", u\"there\")])\n    ",
        "klass": "whoosh.qparser.default.QueryParser",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A hand-written query parser built on modular plug-ins. The default\n    configuration implements a powerful fielded query language similar to\n    Lucene's.\n\n    You can use the ``plugins`` argument when creating the object to override\n    the default list of plug-ins, and/or use ``add_plugin()`` and/or\n    ``remove_plugin_class()`` to change the plug-ins included in the parser.\n\n    >>> from whoosh import qparser\n    >>> parser = qparser.QueryParser(\"content\", schema)\n    >>> parser.remove_plugin_class(qparser.WildcardPlugin)\n    >>> parser.add_plugin(qparser.PrefixPlugin())\n    >>> parser.parse(u\"hello there\")\n    And([Term(\"content\", u\"hello\"), Term(\"content\", u\"there\")])\n    ",
        "klass": "whoosh.qparser.QueryParser",
        "module": "whoosh"
    },
    {
        "base_classes": [
            "storops.unity.resource.storage_resource.UnityStorageResource"
        ],
        "class_docstring": "\n    Create a consistency group (former LUN group).\n    ",
        "klass": "storops.unity.resource.cg.UnityConsistencyGroup",
        "module": "storops"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager to share parameter instances when creating\n    multiple Parameterized objects of the same type. Parameter default\n    values are instantiated once and cached to be reused when another\n    Parameterized object of the same type is instantiated.\n    Can be useful to easily modify large collections of Parameterized\n    objects at once and can provide a significant speedup.\n    ",
        "klass": "param.parameterized.shared_parameters",
        "module": "param"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Element generator factory.\n\n    Unlike the ordinary Element factory, the E factory allows you to pass in\n    more than just a tag and some optional attributes; you can also pass in\n    text and other elements.  The text is added as either text or tail\n    attributes, and elements are inserted at the right spot.  Some small\n    examples::\n\n        >>> from lxml import etree as ET\n        >>> from lxml.builder import E\n\n        >>> ET.tostring(E(\"tag\"))\n        '<tag/>'\n        >>> ET.tostring(E(\"tag\", \"text\"))\n        '<tag>text</tag>'\n        >>> ET.tostring(E(\"tag\", \"text\", key=\"value\"))\n        '<tag key=\"value\">text</tag>'\n        >>> ET.tostring(E(\"tag\", E(\"subtag\", \"text\"), \"tail\"))\n        '<tag><subtag>text</subtag>tail</tag>'\n\n    For simple tags, the factory also allows you to write ``E.tag(...)`` instead\n    of ``E('tag', ...)``::\n\n        >>> ET.tostring(E.tag())\n        '<tag/>'\n        >>> ET.tostring(E.tag(\"text\"))\n        '<tag>text</tag>'\n        >>> ET.tostring(E.tag(E.subtag(\"text\"), \"tail\"))\n        '<tag><subtag>text</subtag>tail</tag>'\n\n    Here's a somewhat larger example; this shows how to generate HTML\n    documents, using a mix of prepared factory functions for inline elements,\n    nested ``E.tag`` calls, and embedded XHTML fragments::\n\n        # some common inline elements\n        A = E.a\n        I = E.i\n        B = E.b\n\n        def CLASS(v):\n            # helper function, 'class' is a reserved word\n            return {'class': v}\n\n        page = (\n            E.html(\n                E.head(\n                    E.title(\"This is a sample document\")\n                ),\n                E.body(\n                    E.h1(\"Hello!\", CLASS(\"title\")),\n                    E.p(\"This is a paragraph with \", B(\"bold\"), \" text in it!\"),\n                    E.p(\"This is another paragraph, with a \",\n                        A(\"link\", href=\"http://www.python.org\"), \".\"),\n                    E.p(\"Here are some reserved characters: <spam&egg>.\"),\n                    ET.XML(\"<p>And finally, here is an embedded XHTML fragment.</p>\"),\n                )\n            )\n        )\n\n        print ET.tostring(page)\n\n    Here's a prettyprinted version of the output from the above script::\n\n        <html>\n          <head>\n            <title>This is a sample document</title>\n          </head>\n          <body>\n            <h1 class=\"title\">Hello!</h1>\n            <p>This is a paragraph with <b>bold</b> text in it!</p>\n            <p>This is another paragraph, with <a href=\"http://www.python.org\">link</a>.</p>\n            <p>Here are some reserved characters: &lt;spam&amp;egg&gt;.</p>\n            <p>And finally, here is an embedded XHTML fragment.</p>\n          </body>\n        </html>\n\n    For namespace support, you can pass a namespace map (``nsmap``)\n    and/or a specific target ``namespace`` to the ElementMaker class::\n\n        >>> E = ElementMaker(namespace=\"http://my.ns/\")\n        >>> print(ET.tostring( E.test ))\n        <test xmlns=\"http://my.ns/\"/>\n\n        >>> E = ElementMaker(namespace=\"http://my.ns/\", nsmap={'p':'http://my.ns/'})\n        >>> print(ET.tostring( E.test ))\n        <p:test xmlns:p=\"http://my.ns/\"/>\n    ",
        "klass": "lxml.builder.ElementMaker",
        "module": "lxml"
    },
    {
        "base_classes": [
            "lxml.etree.HTMLParser"
        ],
        "class_docstring": "HTMLPullParser(self, events=None, *, tag=None, base_url=None, **kwargs)\n\n    HTML parser that collects parse events in an iterator.\n\n    The collected events are the same as for iterparse(), but the\n    parser itself is non-blocking in the sense that it receives\n    data chunks incrementally through its .feed() method, instead\n    of reading them directly from a file(-like) object all by itself.\n\n    By default, it collects Element end events.  To change that,\n    pass any subset of the available events into the ``events``\n    argument: ``'start'``, ``'end'``, ``'start-ns'``,\n    ``'end-ns'``, ``'comment'``, ``'pi'``.\n\n    To support loading external dependencies relative to the input\n    source, you can pass the ``base_url``.\n    ",
        "klass": "lxml.etree.HTMLPullParser",
        "module": "lxml"
    },
    {
        "base_classes": [
            "lxml.etree._Validator"
        ],
        "class_docstring": "RelaxNG(self, etree=None, file=None)\n    Turn a document into a Relax NG validator.\n\n    Either pass a schema as Element or ElementTree, or pass a file or\n    filename through the ``file`` keyword argument.\n    ",
        "klass": "lxml.etree.RelaxNG",
        "module": "lxml"
    },
    {
        "base_classes": [
            "lxml.etree._Validator"
        ],
        "class_docstring": "XMLSchema(self, etree=None, file=None)\n    Turn a document into an XML Schema validator.\n\n    Either pass a schema as Element or ElementTree, or pass a file or\n    filename through the ``file`` keyword argument.\n\n    Passing the ``attribute_defaults`` boolean option will make the\n    schema insert default/fixed attributes into validated documents.\n    ",
        "klass": "lxml.etree.XMLSchema",
        "module": "lxml"
    },
    {
        "base_classes": [
            "lxml.etree._XPathEvaluatorBase"
        ],
        "class_docstring": "XPath(self, path, namespaces=None, extensions=None, regexp=True, smart_strings=True)\n    A compiled XPath expression that can be called on Elements and ElementTrees.\n\n    Besides the XPath expression, you can pass prefix-namespace\n    mappings and extension functions to the constructor through the\n    keyword arguments ``namespaces`` and ``extensions``.  EXSLT\n    regular expression support can be disabled with the 'regexp'\n    boolean keyword (defaults to True).  Smart strings will be\n    returned for string results unless you pass\n    ``smart_strings=False``.\n    ",
        "klass": "lxml.etree.XPath",
        "module": "lxml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "XSLT(self, xslt_input, extensions=None, regexp=True, access_control=None)\n\n    Turn an XSL document into an XSLT object.\n\n    Calling this object on a tree or Element will execute the XSLT::\n\n        transform = etree.XSLT(xsl_tree)\n        result = transform(xml_tree)\n\n    Keyword arguments of the constructor:\n\n    - extensions: a dict mapping ``(namespace, name)`` pairs to\n      extension functions or extension elements\n    - regexp: enable exslt regular expression support in XPath\n      (default: True)\n    - access_control: access restrictions for network or file\n      system (see `XSLTAccessControl`)\n\n    Keyword arguments of the XSLT call:\n\n    - profile_run: enable XSLT profiling (default: False)\n\n    Other keyword arguments of the call are passed to the stylesheet\n    as parameters.\n    ",
        "klass": "lxml.etree.XSLT",
        "module": "lxml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "iterparse(self, source, events=(\"end\",), tag=None,                   attribute_defaults=False, dtd_validation=False,                   load_dtd=False, no_network=True, remove_blank_text=False,                   remove_comments=False, remove_pis=False, encoding=None,                   html=False, recover=None, huge_tree=False, schema=None)\n\n    Incremental parser.\n\n    Parses XML into a tree and generates tuples (event, element) in a\n    SAX-like fashion. ``event`` is any of 'start', 'end', 'start-ns',\n    'end-ns'.\n\n    For 'start' and 'end', ``element`` is the Element that the parser just\n    found opening or closing.  For 'start-ns', it is a tuple (prefix, URI) of\n    a new namespace declaration.  For 'end-ns', it is simply None.  Note that\n    all start and end events are guaranteed to be properly nested.\n\n    The keyword argument ``events`` specifies a sequence of event type names\n    that should be generated.  By default, only 'end' events will be\n    generated.\n\n    The additional ``tag`` argument restricts the 'start' and 'end' events to\n    those elements that match the given tag.  The ``tag`` argument can also be\n    a sequence of tags to allow matching more than one tag.  By default,\n    events are generated for all elements.  Note that the 'start-ns' and\n    'end-ns' events are not impacted by this restriction.\n\n    The other keyword arguments in the constructor are mainly based on the\n    libxml2 parser configuration.  A DTD will also be loaded if validation or\n    attribute default values are requested.\n\n    Available boolean keyword arguments:\n     - attribute_defaults: read default attributes from DTD\n     - dtd_validation: validate (if DTD is available)\n     - load_dtd: use DTD for parsing\n     - no_network: prevent network access for related files\n     - remove_blank_text: discard blank text nodes\n     - remove_comments: discard comments\n     - remove_pis: discard processing instructions\n     - strip_cdata: replace CDATA sections by normal text content (default: True)\n     - compact: safe memory for short text content (default: True)\n     - resolve_entities: replace entities by their text value (default: True)\n     - huge_tree: disable security restrictions and support very deep trees\n                  and very long text content (only affects libxml2 2.7+)\n     - html: parse input as HTML (default: XML)\n     - recover: try hard to parse through broken input (default: True for HTML,\n                False otherwise)\n\n    Other keyword arguments:\n     - encoding: override the document encoding\n     - schema: an XMLSchema to validate against\n    ",
        "klass": "lxml.etree.iterparse",
        "module": "lxml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "xmlfile(self, output_file, encoding=None, compression=None, close=False, buffered=True)\n\n    A simple mechanism for incremental XML serialisation.\n\n    Usage example::\n\n         with xmlfile(\"somefile.xml\", encoding='utf-8') as xf:\n             xf.write_declaration(standalone=True)\n             xf.write_doctype('<!DOCTYPE root SYSTEM \"some.dtd\">')\n\n             # generate an element (the root element)\n             with xf.element('root'):\n                  # write a complete Element into the open root element\n                  xf.write(etree.Element('test'))\n\n                  # generate and write more Elements, e.g. through iterparse\n                  for element in generate_some_elements():\n                      # serialise generated elements into the XML file\n                      xf.write(element)\n\n                  # or write multiple Elements or strings at once\n                  xf.write(etree.Element('start'), \"text\", etree.Element('end'))\n\n    If 'output_file' is a file(-like) object, passing ``close=True`` will\n    close it when exiting the context manager.  By default, it is left\n    to the owner to do that.  When a file path is used, lxml will take care\n    of opening and closing the file itself.  Also, when a compression level\n    is set, lxml will deliberately close the file to make sure all data gets\n    compressed and written.\n\n    Setting ``buffered=False`` will flush the output after each operation,\n    such as opening or closing an ``xf.element()`` block or calling\n    ``xf.write()``.  Alternatively, calling ``xf.flush()`` can be used to\n    explicitly flush any pending output when buffering is enabled.\n    ",
        "klass": "lxml.etree.xmlfile",
        "module": "lxml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "ElementMaker(self, namespace=None, nsmap=None, annotate=True, makeelement=None)\n\n    An ElementMaker that can be used for constructing trees.\n\n    Example::\n\n      >>> M = ElementMaker(annotate=False)\n      >>> attributes = {'class': 'par'}\n      >>> html = M.html( M.body( M.p('hello', attributes, M.br, 'objectify', style=\"font-weight: bold\") ) )\n\n      >>> from lxml.etree import tostring\n      >>> print(tostring(html, method='html').decode('ascii'))\n      <html><body><p style=\"font-weight: bold\" class=\"par\">hello<br>objectify</p></body></html>\n\n    To create tags that are not valid Python identifiers, call the factory\n    directly and pass the tag name as first argument::\n\n      >>> root = M('tricky-tag', 'some text')\n      >>> print(root.tag)\n      tricky-tag\n      >>> print(root.text)\n      some text\n\n    Note that this module has a predefined ElementMaker instance called ``E``.\n    ",
        "klass": "lxml.objectify.ElementMaker",
        "module": "lxml"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.SampleFileProfiler"
        ],
        "class_docstring": "The CPU time profiler.",
        "klass": "plaso.engine.profilers.CPUTimeProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.SampleFileProfiler"
        ],
        "class_docstring": "The memory profiler.",
        "klass": "plaso.engine.profilers.MemoryProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.CPUTimeProfiler"
        ],
        "class_docstring": "The processing profiler.",
        "klass": "plaso.engine.profilers.ProcessingProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Shared functionality for sample file-based profilers.",
        "klass": "plaso.engine.profilers.SampleFileProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.CPUTimeProfiler"
        ],
        "class_docstring": "The serializers profiler.",
        "klass": "plaso.engine.profilers.SerializersProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.SampleFileProfiler"
        ],
        "class_docstring": "The storage profiler.",
        "klass": "plaso.engine.profilers.StorageProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.SampleFileProfiler"
        ],
        "class_docstring": "The task queue profiler.",
        "klass": "plaso.engine.profilers.TaskQueueProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.profilers.SampleFileProfiler"
        ],
        "class_docstring": "The tasks profiler.",
        "klass": "plaso.engine.profilers.TasksProfiler",
        "module": "plaso"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Event heap.",
        "klass": "plaso.storage.event_heaps.EventHeap",
        "module": "plaso"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Serialized event heap.\n\n  Attributes:\n    data_size (int): total data size of the serialized events on the heap.\n  ",
        "klass": "plaso.storage.event_heaps.SerializedEventHeap",
        "module": "plaso"
    },
    {
        "base_classes": [
            "joblib.logger.Logger"
        ],
        "class_docstring": " Helper class for readable parallel mapping.\n\n        Read more in the :ref:`User Guide <parallel>`.\n\n        Parameters\n        -----------\n        n_jobs: int, default: None\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n            None is a marker for 'unset' that will be interpreted as n_jobs=1\n            (sequential execution) unless the call is performed under a\n            parallel_backend context manager that sets another value for\n            n_jobs.\n        backend: str, ParallelBackendBase instance or None, default: 'loky'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"loky\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes.\n            - \"multiprocessing\" previous process-based backend based on\n              `multiprocessing.Pool`. Less robust than `loky`.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              register_parallel_backend. This will allow you to implement\n              a backend of your liking.\n\n            It is not recommended to hard-code the backend name in a call to\n            Parallel in a library. Instead it is recommended to set soft hints\n            (prefer) or hard constraints (require) so as to make it possible\n            for library users to change the backend from the outside using the\n            parallel_backend context manager.\n        prefer: str in {'processes', 'threads'} or None, default: None\n            Soft hint to choose the default backend if no specific backend\n            was selected with the parallel_backend context manager. The\n            default process-based backend is 'loky' and the default\n            thread-based backend is 'threading'. Ignored if the ``backend``\n            parameter is specified.\n        require: 'sharedmem' or None, default None\n            Hard constraint to select the backend. If set to 'sharedmem',\n            the selected backend will be single-host and thread-based even\n            if the user asked for a non-thread based backend with\n            parallel_backend.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the workers should never starve.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, dispatching\n            calls to workers can be slower than sequential computation because\n            of the overhead. Batching fast computations together can mitigate\n            this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmapping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAM disk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmapping of large arrays.\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}\n            Memmapping mode for numpy arrays passed to workers.\n            See 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses workers to compute in parallel the application of a\n        function to many different arguments. The main functionality it brings\n        in addition to using the raw multiprocessing or concurrent.futures API\n        are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    ",
        "klass": "joblib.Parallel",
        "module": "joblib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the ``register_parallel_backend`` function.\n\n    By default the following backends are available:\n\n    - 'loky': single-host, process-based parallelism (used by default),\n    - 'threading': single-host, thread-based parallelism,\n    - 'multiprocessing': legacy single-host, process-based parallelism.\n\n    'loky' is recommended to run functions that manipulate Python objects.\n    'threading' is a low-overhead alternative that is most efficient for\n    functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n    CPU-bound code in a few calls to native code that explicitly releases the\n    GIL.\n\n    In addition, if the `dask` and `distributed` Python packages are installed,\n    it is possible to use the 'dask' backend for better scheduling of nested\n    parallel calls without over-subscription and potentially distribute\n    parallel calls over a networked cluster of several hosts.\n\n    Alternatively the backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the ``Parallel`` class constructor. It is particularly useful when calling\n    into library code that uses joblib internally but does not expose the\n    backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    Joblib also tries to limit the oversubscription by limiting the number of\n    threads usable in some third-party library threadpools like OpenBLAS, MKL\n    or OpenMP. The default limit in each worker is set to\n    ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n    overwritten with the ``inner_max_num_threads`` argument which will be used\n    to set this limit in the child processes.\n\n    .. versionadded:: 0.10\n\n    ",
        "klass": "joblib.parallel_backend",
        "module": "joblib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Windows: mmap(fileno, length[, tagname[, access[, offset]]])\n\nMaps length bytes from the file specified by the file handle fileno,\nand returns a mmap object.  If length is larger than the current size\nof the file, the file is extended to contain length bytes.  If length\nis 0, the maximum length of the map is the current size of the file,\nexcept that if the file is empty Windows raises an exception (you cannot\ncreate an empty mapping on Windows).\n\nUnix: mmap(fileno, length[, flags[, prot[, access[, offset]]]])\n\nMaps length bytes from the file specified by the file descriptor fileno,\nand returns a mmap object.  If length is 0, the maximum length of the map\nwill be the current size of the file when mmap is called.\nflags specifies the nature of the mapping. MAP_PRIVATE creates a\nprivate copy-on-write mapping, so changes to the contents of the mmap\nobject will be private to this process, and MAP_SHARED creates a mapping\nthat's shared with all other processes mapping the same areas of the file.\nThe default value is MAP_SHARED.\n\nTo map anonymous memory, pass -1 as the fileno (both versions).",
        "klass": "mmap.mmap",
        "module": "mmap"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Provides a file-like object that takes care of all the things you\n       commonly want to do when processing a text file that has some\n       line-by-line syntax: strip comments (as long as \"#\" is your\n       comment character), skip blank lines, join adjacent lines by\n       escaping the newline (ie. backslash at end of line), strip\n       leading and/or trailing whitespace.  All of these are optional\n       and independently controllable.\n\n       Provides a 'warn()' method so you can generate warning messages that\n       report physical line number, even if the logical line in question\n       spans multiple physical lines.  Also provides 'unreadline()' for\n       implementing line-at-a-time lookahead.\n\n       Constructor is called as:\n\n           TextFile (filename=None, file=None, **options)\n\n       It bombs (RuntimeError) if both 'filename' and 'file' are None;\n       'filename' should be a string, and 'file' a file object (or\n       something that provides 'readline()' and 'close()' methods).  It is\n       recommended that you supply at least 'filename', so that TextFile\n       can include it in warning messages.  If 'file' is not supplied,\n       TextFile creates its own using 'io.open()'.\n\n       The options are all boolean, and affect the value returned by\n       'readline()':\n         strip_comments [default: true]\n           strip from \"#\" to end-of-line, as well as any whitespace\n           leading up to the \"#\" -- unless it is escaped by a backslash\n         lstrip_ws [default: false]\n           strip leading whitespace from each line before returning it\n         rstrip_ws [default: true]\n           strip trailing whitespace (including line terminator!) from\n           each line before returning it\n         skip_blanks [default: true}\n           skip lines that are empty *after* stripping comments and\n           whitespace.  (If both lstrip_ws and rstrip_ws are false,\n           then some lines may consist of solely whitespace: these will\n           *not* be skipped, even if 'skip_blanks' is true.)\n         join_lines [default: false]\n           if a backslash is the last non-newline character on a line\n           after stripping comments and whitespace, join the following line\n           to it to form one \"logical line\"; if N consecutive lines end\n           with a backslash, then N+1 physical lines will be joined to\n           form one logical line.\n         collapse_join [default: false]\n           strip leading whitespace from lines that are joined to their\n           predecessor; only matters if (join_lines and not lstrip_ws)\n         errors [default: 'strict']\n           error handler used to decode the file content\n\n       Note that since 'rstrip_ws' can strip the trailing newline, the\n       semantics of 'readline()' must differ from those of the builtin file\n       object's 'readline()' method!  In particular, 'readline()' returns\n       None for end-of-file: an empty string might just be a blank line (or\n       an all-whitespace line), if 'rstrip_ws' is true but 'skip_blanks' is\n       not.",
        "klass": "distutils.text_file.TextFile",
        "module": "distutils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Representation of a socket with a mysql server.\n\n    The proper way to get an instance of this class is to call\n    connect().\n\n    Establish a connection to the MySQL database. Accepts several\n    arguments:\n\n    :param host: Host where the database server is located\n    :param user: Username to log in as\n    :param password: Password to use.\n    :param database: Database to use, None to not use a particular one.\n    :param port: MySQL port to use, default is usually OK. (default: 3306)\n    :param bind_address: When the client has multiple network interfaces, specify\n        the interface from which to connect to the host. Argument can be\n        a hostname or an IP address.\n    :param unix_socket: Optionally, you can use a unix socket rather than TCP/IP.\n    :param read_timeout: The timeout for reading from the connection in seconds (default: None - no timeout)\n    :param write_timeout: The timeout for writing to the connection in seconds (default: None - no timeout)\n    :param charset: Charset you want to use.\n    :param sql_mode: Default SQL_MODE to use.\n    :param read_default_file:\n        Specifies  my.cnf file to read these parameters from under the [client] section.\n    :param conv:\n        Conversion dictionary to use instead of the default one.\n        This is used to provide custom marshalling and unmarshaling of types.\n        See converters.\n    :param use_unicode:\n        Whether or not to default to unicode strings.\n        This option defaults to true for Py3k.\n    :param client_flag: Custom flags to send to MySQL. Find potential values in constants.CLIENT.\n    :param cursorclass: Custom cursor class to use.\n    :param init_command: Initial SQL statement to run when connection is established.\n    :param connect_timeout: Timeout before throwing an exception when connecting.\n        (default: 10, min: 1, max: 31536000)\n    :param ssl:\n        A dict of arguments similar to mysql_ssl_set()'s parameters.\n    :param read_default_group: Group to read from in the configuration file.\n    :param compress: Not supported\n    :param named_pipe: Not supported\n    :param autocommit: Autocommit mode. None means use server default. (default: False)\n    :param local_infile: Boolean to enable the use of LOAD DATA LOCAL command. (default: False)\n    :param max_allowed_packet: Max size of packet sent to server in bytes. (default: 16MB)\n        Only used to limit size of \"LOAD LOCAL INFILE\" data packet smaller than default (16KB).\n    :param defer_connect: Don't explicitly connect on contruction - wait for connect call.\n        (default: False)\n    :param auth_plugin_map: A dict of plugin names to a class that processes that plugin.\n        The class will take the Connection object as the argument to the constructor.\n        The class needs an authenticate method taking an authentication packet as\n        an argument.  For the dialog plugin, a prompt(echo, prompt) method can be used\n        (if no authenticate method) for returning a string from the user. (experimental)\n    :param server_public_key: SHA256 authenticaiton plugin public key value. (default: None)\n    :param db: Alias for database. (for compatibility to MySQLdb)\n    :param passwd: Alias for password. (for compatibility to MySQLdb)\n    :param binary_prefix: Add _binary prefix on bytes and bytearray. (default: False)\n\n    See `Connection <https://www.python.org/dev/peps/pep-0249/#connection-objects>`_ in the\n    specification.\n    ",
        "klass": "pymysql.connections.Connection",
        "module": "pymysql"
    },
    {
        "base_classes": [
            "tkinter.ttk.Entry"
        ],
        "class_docstring": "Ttk Combobox widget combines a text field with a pop-down list of\n    values.",
        "klass": "tkinter.ttk.Combobox",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "tkinter.ttk.Widget"
        ],
        "class_docstring": "Ttk Progressbar widget shows the status of a long-running\n    operation. They can operate in two modes: determinate mode shows the\n    amount completed relative to the total amount of work to be done, and\n    indeterminate mode provides an animated display to let the user know\n    that something is happening.",
        "klass": "tkinter.ttk.Progressbar",
        "module": "tkinter"
    },
    {
        "base_classes": [
            "h2o.base.Keyed"
        ],
        "class_docstring": "\n    Primary data store for H2O.\n\n    H2OFrame is similar to pandas' ``DataFrame``, or R's ``data.frame``. One of the critical distinction is that the\n    data is generally not held in memory, instead it is located on a (possibly remote) H2O cluster, and thus\n    ``H2OFrame`` represents a mere handle to that data.\n\n    Create a new H2OFrame object, possibly from some other object.\n\n    :param python_obj: object that will be converted to an ``H2OFrame``. This could have multiple types:\n\n        - None: create an empty H2OFrame\n        - A list/tuple of strings or numbers: create a single-column H2OFrame containing the contents of this list.\n        - A dictionary of ``{name: list}`` pairs: create an H2OFrame with multiple columns, each column having the\n          provided ``name`` and contents from ``list``. If the source dictionary is not an OrderedDict, then the\n          columns in the H2OFrame may appear shuffled.\n        - A list of lists of strings/numbers: construct an H2OFrame from a rectangular table of values, with inner\n          lists treated as rows of the table. I.e. ``H2OFrame([[1, 'a'], [2, 'b'], [3, 'c']])`` will create a\n          frame with 3 rows and 2 columns, one numeric and one string.\n        - A Pandas dataframe, or a Numpy ndarray: create a matching H2OFrame.\n        - A Scipy sparse matrix: create a matching sparse H2OFrame.\n\n    :param int header: if ``python_obj`` is a list of lists, this parameter can be used to indicate whether the\n        first row of the data represents headers. The value of -1 means the first row is data, +1 means the first\n        row is the headers, 0 (default) allows H2O to guess whether the first row contains data or headers.\n    :param List[str] column_names: explicit list of column names for the new H2OFrame. This will override any\n        column names derived from the data. If the python_obj does not contain explicit column names, and this\n        parameter is not given, then the columns will be named \"C1\", \"C2\", \"C3\", etc.\n    :param column_types: explicit column types for the new H2OFrame. This could be either a list of types for\n        each column, or a dictionary of {column name: column type} pairs. In the latter case you may override\n        types for only few columns, and let H2O choose the types of the rest.\n    :param na_strings: List of strings in the input data that should be interpreted as missing values. This could\n        be given on a per-column basis, either as a list-of-lists, or as a dictionary {column name: list of nas}.\n    :param str destination_frame: (internal) name of the target DKV key in the H2O backend.\n    :param str separator: (deprecated)\n\n    :example:\n    >>> python_obj = [1, 2, 2.5, -100.9, 0]\n    >>> frame = h2o.H2OFrame(python_obj)\n    >>> pyunit_utils.check_dims_values(python_obj, the_frame, rows=5, cols=1)\n    ",
        "klass": "h2o.frame.H2OFrame",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.base.Keyed"
        ],
        "class_docstring": "\n    Primary data store for H2O.\n\n    H2OFrame is similar to pandas' ``DataFrame``, or R's ``data.frame``. One of the critical distinction is that the\n    data is generally not held in memory, instead it is located on a (possibly remote) H2O cluster, and thus\n    ``H2OFrame`` represents a mere handle to that data.\n\n    Create a new H2OFrame object, possibly from some other object.\n\n    :param python_obj: object that will be converted to an ``H2OFrame``. This could have multiple types:\n\n        - None: create an empty H2OFrame\n        - A list/tuple of strings or numbers: create a single-column H2OFrame containing the contents of this list.\n        - A dictionary of ``{name: list}`` pairs: create an H2OFrame with multiple columns, each column having the\n          provided ``name`` and contents from ``list``. If the source dictionary is not an OrderedDict, then the\n          columns in the H2OFrame may appear shuffled.\n        - A list of lists of strings/numbers: construct an H2OFrame from a rectangular table of values, with inner\n          lists treated as rows of the table. I.e. ``H2OFrame([[1, 'a'], [2, 'b'], [3, 'c']])`` will create a\n          frame with 3 rows and 2 columns, one numeric and one string.\n        - A Pandas dataframe, or a Numpy ndarray: create a matching H2OFrame.\n        - A Scipy sparse matrix: create a matching sparse H2OFrame.\n\n    :param int header: if ``python_obj`` is a list of lists, this parameter can be used to indicate whether the\n        first row of the data represents headers. The value of -1 means the first row is data, +1 means the first\n        row is the headers, 0 (default) allows H2O to guess whether the first row contains data or headers.\n    :param List[str] column_names: explicit list of column names for the new H2OFrame. This will override any\n        column names derived from the data. If the python_obj does not contain explicit column names, and this\n        parameter is not given, then the columns will be named \"C1\", \"C2\", \"C3\", etc.\n    :param column_types: explicit column types for the new H2OFrame. This could be either a list of types for\n        each column, or a dictionary of {column name: column type} pairs. In the latter case you may override\n        types for only few columns, and let H2O choose the types of the rest.\n    :param na_strings: List of strings in the input data that should be interpreted as missing values. This could\n        be given on a per-column basis, either as a list-of-lists, or as a dictionary {column name: list of nas}.\n    :param str destination_frame: (internal) name of the target DKV key in the H2O backend.\n    :param str separator: (deprecated)\n\n    :example:\n    >>> python_obj = [1, 2, 2.5, -100.9, 0]\n    >>> frame = h2o.H2OFrame(python_obj)\n    >>> pyunit_utils.check_dims_values(python_obj, the_frame, rows=5, cols=1)\n    ",
        "klass": "h2o.H2OFrame",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.base.Keyed"
        ],
        "class_docstring": "\n    Automatic Machine Learning\n\n    The Automatic Machine Learning (AutoML) function automates the supervised machine learning model training process.\n    The current version of AutoML trains and cross-validates \n    a Random Forest (DRF), \n    an Extremely-Randomized Forest (DRF/XRT),\n    a random grid of Generalized Linear Models (GLM)\n    a random grid of XGBoost (XGBoost),\n    a random grid of Gradient Boosting Machines (GBM), \n    a random grid of Deep Neural Nets (DeepLearning), \n    and 2 Stacked Ensembles, one of all the models, and one of only the best models of each kind.\n\n    :examples:\n    >>> import h2o\n    >>> from h2o.automl import H2OAutoML\n    >>> h2o.init()\n    >>> # Import a sample binary outcome train/test set into H2O\n    >>> train = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv\")\n    >>> test = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv\")\n    >>> # Identify the response and set of predictors\n    >>> y = \"response\"\n    >>> x = list(train.columns)  #if x is defined as all columns except the response, then x is not required\n    >>> x.remove(y)\n    >>> # For binary classification, response should be a factor\n    >>> train[y] = train[y].asfactor()\n    >>> test[y] = test[y].asfactor()\n    >>> # Run AutoML for 30 seconds\n    >>> aml = H2OAutoML(max_runtime_secs = 30)\n    >>> aml.train(x = x, y = y, training_frame = train)\n    >>> # Print Leaderboard (ranked by xval metrics)\n    >>> aml.leaderboard\n    >>> # (Optional) Evaluate performance on a test set\n    >>> perf = aml.leader.model_performance(test)\n    >>> perf.auc()\n    ",
        "klass": "h2o.automl.H2OAutoML",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Cox Proportional Hazards\n\n    Trains a Cox Proportional Hazards Model (CoxPH) on an H2O dataset.\n    ",
        "klass": "h2o.estimators.coxph.H2OCoxProportionalHazardsEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.deeplearning.H2ODeepLearningEstimator"
        ],
        "class_docstring": "\n    :examples:\n\n    >>> import h2o as ml\n    >>> from h2o.estimators.deeplearning import H2OAutoEncoderEstimator\n    >>> ml.init()\n    >>> rows = [[1,2,3,4,0]*50, [2,1,2,4,1]*50, [2,1,4,2,1]*50, [0,1,2,34,1]*50, [2,3,4,1,0]*50]\n    >>> fr = ml.H2OFrame(rows)\n    >>> fr[4] = fr[4].asfactor()\n    >>> model = H2OAutoEncoderEstimator()\n    >>> model.train(x=range(4), training_frame=fr)\n    ",
        "klass": "h2o.estimators.deeplearning.H2OAutoEncoderEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Deep Water\n\n    Build a Deep Learning model using multiple native GPU backends\n    Builds a deep neural network on an H2OFrame containing various data sources\n    ",
        "klass": "h2o.estimators.deepwater.H2ODeepWaterEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class that represents the group by operation on an H2OFrame.\n\n    The returned groups are sorted by the natural group-by column sort.\n\n    :param H2OFrame fr: H2OFrame that you want the group by operation to be performed on.\n    :param by: by can be a column name (str) or an index (int) of a single column,  or a list for multiple columns\n        denoting the set of columns to group by.\n\n    Sample usage:\n\n    >>> my_frame = ...  # some existing H2OFrame\n    >>> grouped = my_frame.group_by(by=[\"C1\", \"C2\"])\n    >>> grouped.sum(col=\"X1\", na=\"all\").mean(col=\"X5\", na=\"all\").max()\n    >>> grouped.get_frame()\n\n\n    Any number of aggregations may be chained together in this manner.  Note that once the aggregation operations\n    are complete, calling the GroupBy object with a new set of aggregations will yield no effect.  You must generate\n    a new GroupBy object in order to apply a new aggregation on it.  In addition, certain aggregations are only\n    defined for numerical or categorical columns.  An error will be thrown for calling aggregation on the wrong\n    data types.\n\n    If no arguments are given to the aggregation (e.g. \"max\" in the above example), then it is assumed that the\n    aggregation should apply to all columns but the group by columns.  However, operations will not be performed\n    on String columns.  They will be skipped.\n\n    All GroupBy aggregations take parameter na, which controls treatment of NA values during the calculation.\n    It can be one of:\n\n        - \"all\" (default) -- any NAs are used in the calculation as-is; which usually results in the final result\n          being NA too.\n        - \"ignore\" -- NA entries are not included in calculations, but the total number of entries is taken as the\n          total number of rows. For example, mean([1, 2, 3, nan], na=\"ignore\") will produce 1.5.  In addition,\n          median([1, 2, 3, nan], na=\"ignore\") will first sort the row as [nan, 1, 2, 3].  Next, the median is the\n          mean of the two middle values in this case producing a median of 1.5.\n        - \"rm\" entries are skipped during the calculations, reducing the total effective count of entries. For\n          example, mean([1, 2, 3, nan], na=\"rm\") will produce 2.  The median in this case will be 2 as the middle\n          value.\n\n    Variance (var) and standard deviation (sd) are the sample (not population) statistics.\n    ",
        "klass": "h2o.group_by.GroupBy",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.utils.backward_compatibility.BackwardsCompatibleBase"
        ],
        "class_docstring": "\n    Grid Search of a Hyper-Parameter Space for a Model\n\n    :param model: The type of model to be explored initialized with optional parameters that will be\n        unchanged across explored models.\n    :param hyper_params: A dictionary of string parameters (keys) and a list of values to be explored by grid\n        search (values).\n    :param str grid_id: The unique id assigned to the resulting grid object. If none is given, an id will\n        automatically be generated.\n    :param search_criteria:  The optional dictionary of directives which control the search of the hyperparameter space.\n        The dictionary can include values for: ``strategy``, ``max_models``, ``max_runtime_secs``, ``stopping_metric``, \n        ``stopping_tolerance``, ``stopping_rounds`` and ``seed``. The default strategy, \"Cartesian\", covers the entire space of \n        hyperparameter combinations. If you want to use cartesian grid search, you can leave the search_criteria \n        argument unspecified. Specify the \"RandomDiscrete\" strategy to get random search of all the combinations of \n        your hyperparameters with three ways of specifying when to stop the search: max number of models, max time, and \n        metric-based early stopping (e.g., stop if MSE hasn\u2019t improved by 0.0001 over the 5 best models). \n        Examples below::\n\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"max_runtime_secs\": 600,\n            ...             \"max_models\": 100, \"stopping_metric\": \"AUTO\",\n            ...             \"stopping_tolerance\": 0.00001, \"stopping_rounds\": 5,\n            ...             \"seed\": 123456}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 42,\n            ...             \"max_runtime_secs\": 28800, \"seed\": 1234}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"stopping_metric\": \"AUTO\",\n            ...             \"stopping_tolerance\": 0.001, \"stopping_rounds\": 10}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"stopping_rounds\": 5,\n            ...             \"stopping_metric\": \"misclassification\",\n            ...             \"stopping_tolerance\": 0.00001}\n    :returns: a new H2OGridSearch instance\n\n    Examples\n    --------\n        >>> from h2o.grid.grid_search import H2OGridSearch\n        >>> from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n        >>> hyper_parameters = {'alpha': [0.01,0.5], 'lambda': [1e-5,1e-6]}\n        >>> gs = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial'), hyper_parameters)\n        >>> training_data = h2o.import_file(\"smalldata/logreg/benign.csv\")\n        >>> gs.train(x=range(3) + range(4,11),y=3, training_frame=training_data)\n        >>> gs.show()\n    ",
        "klass": "h2o.grid.grid_search.H2OGridSearch",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.utils.backward_compatibility.BackwardsCompatibleBase"
        ],
        "class_docstring": "\n    Grid Search of a Hyper-Parameter Space for a Model\n\n    :param model: The type of model to be explored initialized with optional parameters that will be\n        unchanged across explored models.\n    :param hyper_params: A dictionary of string parameters (keys) and a list of values to be explored by grid\n        search (values).\n    :param str grid_id: The unique id assigned to the resulting grid object. If none is given, an id will\n        automatically be generated.\n    :param search_criteria:  The optional dictionary of directives which control the search of the hyperparameter space.\n        The dictionary can include values for: ``strategy``, ``max_models``, ``max_runtime_secs``, ``stopping_metric``, \n        ``stopping_tolerance``, ``stopping_rounds`` and ``seed``. The default strategy, \"Cartesian\", covers the entire space of \n        hyperparameter combinations. If you want to use cartesian grid search, you can leave the search_criteria \n        argument unspecified. Specify the \"RandomDiscrete\" strategy to get random search of all the combinations of \n        your hyperparameters with three ways of specifying when to stop the search: max number of models, max time, and \n        metric-based early stopping (e.g., stop if MSE hasn\u2019t improved by 0.0001 over the 5 best models). \n        Examples below::\n\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"max_runtime_secs\": 600,\n            ...             \"max_models\": 100, \"stopping_metric\": \"AUTO\",\n            ...             \"stopping_tolerance\": 0.00001, \"stopping_rounds\": 5,\n            ...             \"seed\": 123456}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 42,\n            ...             \"max_runtime_secs\": 28800, \"seed\": 1234}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"stopping_metric\": \"AUTO\",\n            ...             \"stopping_tolerance\": 0.001, \"stopping_rounds\": 10}\n            >>> criteria = {\"strategy\": \"RandomDiscrete\", \"stopping_rounds\": 5,\n            ...             \"stopping_metric\": \"misclassification\",\n            ...             \"stopping_tolerance\": 0.00001}\n    :returns: a new H2OGridSearch instance\n\n    Examples\n    --------\n        >>> from h2o.grid.grid_search import H2OGridSearch\n        >>> from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n        >>> hyper_parameters = {'alpha': [0.01,0.5], 'lambda': [1e-5,1e-6]}\n        >>> gs = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial'), hyper_parameters)\n        >>> training_data = h2o.import_file(\"smalldata/logreg/benign.csv\")\n        >>> gs.train(x=range(3) + range(4,11),y=3, training_frame=training_data)\n        >>> gs.show()\n    ",
        "klass": "h2o.grid.H2OGridSearch",
        "module": "h2o"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Representation of a MOJO Pipeline. This is currently an experimental feature.\n    ",
        "klass": "h2o.pipeline.H2OMojoPipeline",
        "module": "h2o"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Deprecated API. Please use H2OTargetencoderEstimator instead.\n\n    This is a main class that provides Python's API to the Java implementation of the target encoding.\n\n    In general target encoding could be applied to three types of problems, namely:\n    \n         1) Binary classification (supported)\n         2) Multi-class classification (not supported yet)\n         3) Regression (not supported yet)\n\n    :param List[str]-or-List[int] x: List of categorical column names or indices that we want apply target encoding to.\n    :param str-or-int y: the name or column index of the response variable in the data.\n    :param str-or-int fold_column: the name or column index of the fold column in the data.\n    :param boolean blending_avg: (deprecated) whether to perform blended average. Defaults to TRUE.\n    :param boolean blended_avg: whether to perform blended average. Defaults to TRUE.\n    :param double inflection_point: parameter for blending. Used to calculate `lambda`. Determines half of the minimal sample size\n        for which we completely trust the estimate based on the sample in the particular level of categorical variable. Default value is 10.\n    :param double smoothing: parameter for blending. Used to calculate `lambda`. Controls the rate of transition between\n        the particular level's posterior probability and the prior probability. For smoothing values approaching infinity it becomes a hard\n        threshold between the posterior and the prior probability. Default value is 20.\n\n    :examples:\n\n    >>> targetEncoder = TargetEncoder(x=te_columns, y=responseColumnName, blended_avg=True, inflection_point=10, smoothing=20)\n    >>> targetEncoder.fit(trainFrame) \n    >>> encodedTrain = targetEncoder.transform(frame=trainFrame, holdout_type=\"kfold\", seed=1234, is_train_or_valid=True)\n    >>> encodedValid = targetEncoder.transform(frame=validFrame, holdout_type=\"none\", noise=0.0, is_train_or_valid=True)\n    >>> encodedTest = targetEncoder.transform(frame=testFrame, holdout_type=\"none\", noise=0.0, is_train_or_valid=False)\n    ",
        "klass": "h2o.targetencoder.TargetEncoder",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    XGBoost\n\n    Builds a eXtreme Gradient Boosting model using the native XGBoost backend.\n    ",
        "klass": "h2o.estimators.xgboost.H2OXGBoostEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "odl.operator.operator.Operator"
        ],
        "class_docstring": "Broadcast argument to set of operators.\n\n    An argument is broadcast by evaluating several operators in the same\n    point::\n\n        BroadcastOperator(op1, op2)(x) = [op1(x), op2(x)]\n\n    See Also\n    --------\n    ProductSpaceOperator : More general case, used as backend.\n    ReductionOperator : Calculates sum of operator results.\n    DiagonalOperator : Case where each operator should have its own argument.\n    ",
        "klass": "odl.BroadcastOperator",
        "module": "odl"
    },
    {
        "base_classes": [
            "odl.operator.default_ops.ScalingOperator"
        ],
        "class_docstring": "Operator mapping each element to itself.\n\n    Implements::\n\n        IdentityOperator()(x) == x\n    ",
        "klass": "odl.IdentityOperator",
        "module": "odl"
    },
    {
        "base_classes": [
            "odl.operator.operator.Operator"
        ],
        "class_docstring": "A matrix acting as a linear operator.\n\n    This operator uses a matrix to represent an operator, and get its\n    adjoint and inverse by doing computations on the matrix. This is in\n    general a rather slow and memory-inefficient approach, and users are\n    recommended to use other alternatives if possible.\n    ",
        "klass": "odl.MatrixOperator",
        "module": "odl"
    },
    {
        "base_classes": [
            "odl.discr.discr_ops.ResizingOperatorBase"
        ],
        "class_docstring": "Operator mapping a discretized function to a new domain.\n\n    This operator is a mapping between uniformly discretized\n    `DiscreteLp` spaces with the same `DiscreteLp.cell_sides`,\n    but different `DiscreteLp.shape`. The underlying operation is array\n    resizing, i.e. no resampling is performed.\n    In axes where the domain is enlarged, the new entries are filled\n    (\"padded\") according to a provided parameter ``pad_mode``.\n\n    All resizing operator variants are linear, except constant padding\n    with constant != 0.\n\n    See `the online documentation\n    <https://odlgroup.github.io/odl/math/resizing_ops.html>`_\n    on resizing operators for mathematical details.\n    ",
        "klass": "odl.ResizingOperator",
        "module": "odl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Reader for uncompressed binary files using an optional header.\n\n    This class can be used to read header and data from files that contain\n    a single binary header followed by a single block of uncompressed binary\n    data.\n    Alternatively, the header can be bypassed and data blocks can be\n    read directly using `read_data` with start and end byte values, which\n    allows to read arbitrary portions.\n\n    An instance of this class can also be used as context manager, i.e.::\n\n        with FileReaderRawBinaryWithHeader(file, header_fields) as reader:\n            header, data = reader.read()\n    ",
        "klass": "odl.contrib.mrc.FileReaderRawBinaryWithHeader",
        "module": "odl"
    },
    {
        "base_classes": [
            "odl.contrib.mrc.mrc.MRCHeaderProperties",
            "odl.contrib.mrc.uncompr_bin.FileWriterRawBinaryWithHeader"
        ],
        "class_docstring": "Writer for the MRC file format.\n\n    See [Che+2015] or the `explanations on the CCP4 homepage\n    <http://www.ccpem.ac.uk/mrc_format/mrc2014.php>`_ for the\n    text of the specification.\n\n    References\n    ----------\n    [Che+2015] Cheng, A et al. *MRC2014: Extensions to the MRC format header\n    for electron cryo-microscopy and tomography*. Journal of Structural\n    Biology, 129 (2015), pp 146--150.\n    ",
        "klass": "odl.contrib.mrc.FileWriterMRC",
        "module": "odl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Writer for uncompressed binary files using an optional header.\n\n    This class can be used to write a single binary header followed by a\n    single block of uncompressed binary data to a file.\n    Alternatively, the header can be bypassed and data blocks can be\n    written directly using `write_data`, which allows to write arbitrary\n    portions.\n    ",
        "klass": "odl.contrib.mrc.FileWriterRawBinaryWithHeader",
        "module": "odl"
    },
    {
        "base_classes": [
            "odl.trafos.fourier.FourierTransformBase"
        ],
        "class_docstring": "Discretized Fourier transform between discrete L^p spaces.\n\n    This operator is the discretized variant of the continuous\n    `Fourier Transform\n    <https://en.wikipedia.org/wiki/Fourier_Transform>`_ between\n    Lebesgue L^p spaces. It applies a three-step procedure consisting\n    of a pre-processing step of the data, an FFT evaluation and\n    a post-processing step. Pre- and post-processing account for\n    the shift and scaling of the real-space and Fourier-space grids.\n\n    The sign convention ('-' vs. '+') can be changed with the ``sign``\n    parameter.\n\n    See Also\n    --------\n    DiscreteFourierTransform\n    FourierTransformInverse\n    odl.trafos.util.ft_utils.dft_preprocess_data\n    odl.trafos.backends.pyfftw_bindings.pyfftw_call\n    odl.trafos.util.ft_utils.dft_postprocess_data\n    ",
        "klass": "odl.trafos.FourierTransform",
        "module": "odl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager that casts obj to a `numpy.array` and saves changes.",
        "klass": "odl.util.writable_array",
        "module": "odl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An HTTP client that handles:\n\n    - all methods\n    - caching\n    - ETags\n    - compression,\n    - HTTPS\n    - Basic\n    - Digest\n    - WSSE\n\n    and more.\n    ",
        "klass": "httplib2.Http",
        "module": "httplib2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    ",
        "klass": "warnings.catch_warnings",
        "module": "warnings"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Daemonize the helper application, putting it in a forked background\n    process.\n\n    ",
        "klass": "helper.unix.Daemon",
        "module": "helper"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Daemonize the helper application, putting it in a forked background\n    process.\n\n    ",
        "klass": "helper.windows.Daemon",
        "module": "helper"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The core of the Distutils.  Most of the work hiding behind 'setup'\n    is really done within a Distribution instance, which farms the work out\n    to the Distutils commands specified on the command line.\n\n    Setup scripts will almost never instantiate Distribution directly,\n    unless the 'setup()' function is totally inadequate to their needs.\n    However, it is conceivable that a setup script might wish to subclass\n    Distribution for some specialized purpose, and then pass the subclass\n    to 'setup()' as the 'distclass' keyword argument.  If so, it is\n    necessary to respect the expectations that 'setup' has of Distribution.\n    See the code for 'setup()', in core.py, for details.\n    ",
        "klass": "distutils.core.Distribution",
        "module": "distutils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Just a collection of attributes that describes an extension\n    module and everything needed to build it (hopefully in a portable\n    way, but there are hooks that let you be as unportable as you need).\n\n    Instance attributes:\n      name : string\n        the full name of the extension, including any packages -- ie.\n        *not* a filename or pathname, but Python dotted name\n      sources : [string]\n        list of source filenames, relative to the distribution root\n        (where the setup script lives), in Unix form (slash-separated)\n        for portability.  Source files may be C, C++, SWIG (.i),\n        platform-specific resource files, or whatever else is recognized\n        by the \"build_ext\" command as source for a Python extension.\n      include_dirs : [string]\n        list of directories to search for C/C++ header files (in Unix\n        form for portability)\n      define_macros : [(name : string, value : string|None)]\n        list of macros to define; each macro is defined using a 2-tuple,\n        where 'value' is either the string to define it to or None to\n        define it without a particular value (equivalent of \"#define\n        FOO\" in source or -DFOO on Unix C compiler command line)\n      undef_macros : [string]\n        list of macros to undefine explicitly\n      library_dirs : [string]\n        list of directories to search for C/C++ libraries at link time\n      libraries : [string]\n        list of library names (not filenames or paths) to link against\n      runtime_library_dirs : [string]\n        list of directories to search for C/C++ libraries at run time\n        (for shared extensions, this is when the extension is loaded)\n      extra_objects : [string]\n        list of extra files to link with (eg. object files not implied\n        by 'sources', static library that must be explicitly specified,\n        binary resource files, etc.)\n      extra_compile_args : [string]\n        any extra platform- and compiler-specific information to use\n        when compiling the source files in 'sources'.  For platforms and\n        compilers where \"command line\" makes sense, this is typically a\n        list of command-line arguments, but for other platforms it could\n        be anything.\n      extra_link_args : [string]\n        any extra platform- and compiler-specific information to use\n        when linking object files together to create the extension (or\n        to create a new static Python interpreter).  Similar\n        interpretation as for 'extra_compile_args'.\n      export_symbols : [string]\n        list of symbols to be exported from a shared extension.  Not\n        used on all platforms, and not generally necessary for Python\n        extensions, which typically export exactly one symbol: \"init\" +\n        extension_name.\n      swig_opts : [string]\n        any extra options to pass to SWIG if a source file has the .i\n        extension.\n      depends : [string]\n        list of files that the extension depends on\n      language : string\n        extension language (i.e. \"c\", \"c++\", \"objc\"). Will be detected\n        from the source extensions if not provided.\n      optional : boolean\n        specifies that a build failure in the extension should not abort the\n        build process, but simply not install the failing extension.\n    ",
        "klass": "distutils.core.Extension",
        "module": "distutils"
    },
    {
        "base_classes": [
            "optparse.OptionContainer"
        ],
        "class_docstring": "\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    ",
        "klass": "jcvi.apps.base.OptionParser",
        "module": "optparse"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "\n    Write and execute makefile.\n    ",
        "klass": "jcvi.apps.grid.MakeManager",
        "module": "jcvi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A SeqRecord object holds a sequence and information about it.\n\n    Main attributes:\n     - id          - Identifier such as a locus tag (string)\n     - seq         - The sequence itself (Seq object or similar)\n\n    Additional attributes:\n     - name        - Sequence name, e.g. gene name (string)\n     - description - Additional text (string)\n     - dbxrefs     - List of database cross references (list of strings)\n     - features    - Any (sub)features defined (list of SeqFeature objects)\n     - annotations - Further information about the whole sequence (dictionary).\n       Most entries are strings, or lists of strings.\n     - letter_annotations - Per letter/symbol annotation (restricted\n       dictionary). This holds Python sequences (lists, strings\n       or tuples) whose length matches that of the sequence.\n       A typical use would be to hold a list of integers\n       representing sequencing quality scores, or a string\n       representing the secondary structure.\n\n    You will typically use Bio.SeqIO to read in sequences from files as\n    SeqRecord objects.  However, you may want to create your own SeqRecord\n    objects directly (see the __init__ method for further details):\n\n    >>> from Bio.Seq import Seq\n    >>> from Bio.SeqRecord import SeqRecord\n    >>> from Bio.Alphabet import IUPAC\n    >>> record = SeqRecord(Seq(\"MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF\",\n    ...                         IUPAC.protein),\n    ...                    id=\"YP_025292.1\", name=\"HokC\",\n    ...                    description=\"toxic membrane protein\")\n    >>> print(record)\n    ID: YP_025292.1\n    Name: HokC\n    Description: toxic membrane protein\n    Number of features: 0\n    Seq('MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF', IUPACProtein())\n\n    If you want to save SeqRecord objects to a sequence file, use Bio.SeqIO\n    for this.  For the special case where you want the SeqRecord turned into\n    a string in a particular file format there is a format method which uses\n    Bio.SeqIO internally:\n\n    >>> print(record.format(\"fasta\"))\n    >YP_025292.1 toxic membrane protein\n    MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF\n    <BLANKLINE>\n\n    You can also do things like slicing a SeqRecord, checking its length, etc\n\n    >>> len(record)\n    44\n    >>> edited = record[:10] + record[11:]\n    >>> print(edited.seq)\n    MKQHKAMIVAIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF\n    >>> print(record.seq)\n    MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF\n\n    ",
        "klass": "Bio.SeqRecord.SeqRecord",
        "module": "Bio"
    },
    {
        "base_classes": [
            "matplotlib.patches.Patch"
        ],
        "class_docstring": "\n    A fancy arrow patch. It draws an arrow using the :class:`ArrowStyle`.\n\n    The head and tail positions are fixed at the specified start and end points\n    of the arrow, but the size and shape (in display coordinates) of the arrow\n    does not change when the axis is moved or zoomed.\n    ",
        "klass": "matplotlib.patches.FancyArrowPatch",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "graphviz.dot.Dot"
        ],
        "class_docstring": "Graph source code in the DOT language.\n\n    Args:\n        name: Graph name used in the source code.\n        comment: Comment added to the first line of the source.\n        filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``).\n        directory: (Sub)directory for source saving and rendering.\n        format: Rendering output format (``'pdf'``, ``'png'``, ...).\n        engine: Layout command used (``'dot'``, ``'neato'``, ...).\n        encoding: Encoding for saving the source.\n        graph_attr: Mapping of ``(attribute, value)`` pairs for the graph.\n        node_attr: Mapping of ``(attribute, value)`` pairs set for all nodes.\n        edge_attr: Mapping of ``(attribute, value)`` pairs set for all edges.\n        body: Iterable of verbatim lines to add to the graph ``body``.\n        strict (bool): Rendering should merge multi-edges.\n\n    Note:\n        All parameters are optional and can be changed under their\n        corresponding attribute name after instance creation.\n    ",
        "klass": "graphviz.dot.Graph",
        "module": "graphviz"
    },
    {
        "base_classes": [
            "bitarray._bitarray"
        ],
        "class_docstring": "bitarray(initial=0, /, endian='big')\n\nReturn a new bitarray object whose items are bits initialized from\nthe optional initial object, and endianness.\nIf no initial object is provided, an empty bitarray (length zero) is created.\nThe initial object may be of the following types:\n\n`int`: Create a bitarray of given integer length.  The initial values are\narbitrary.  If you want all values to be set, use the .setall() method.\n\n`str`: Create bitarray from a string of `0` and `1`.\n\n`list`, `tuple`, `iterable`: Create bitarray from a sequence, each\nelement in the sequence is converted to a bit using its truth value.\n\n`bitarray`: Create bitarray from another bitarray.  This is done by\ncopying the memory holding the bitarray data, and is hence very fast.\n\nThe optional keyword arguments `endian` specifies the bit endianness of the\ncreated bitarray object.\nAllowed values are the strings `big` and `little` (default is `big`).\n\nNote that setting the bit endianness only has an effect when accessing the\nmachine representation of the bitarray, i.e. when using the methods: tofile,\nfromfile, tobytes, frombytes.",
        "klass": "bitarray.bitarray",
        "module": "bitarray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for the running of commands\n\n    When the command is run the output is copied to a LogFile and\n    (optionally) standard-out\n\n    The argument list assumes for the first three elements the\n    OpenFOAM-convention:\n\n    <cmd> <dir> <case>\n\n    The directory name for outputs is therefor created from <dir> and\n    <case>\n\n    Provides some handle-methods that are to be overloaded for\n    additional functionality",
        "klass": "PyFoam.Execution.BasicRunner.BasicRunner",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "PyFoam.Execution.AnalyzedRunner.AnalyzedRunner"
        ],
        "class_docstring": "It is assumed that the provided solver is a steady state\n    solver. After all the linear solvers have initial residuals below\n    their limits the run is assumed to be convergent and the run is\n    stopped by setting\n\n    stopAt nextWrite;\n    writeInterval 1;\n\n    in the controlDict",
        "klass": "PyFoam.Execution.ConvergenceRunner.ConvergenceRunner",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "PyFoam.Execution.AnalyzedRunner.AnalyzedRunner"
        ],
        "class_docstring": "To this runner regular expressions can be added. Each line is\n    checked against each regular expression and saved with the\n    corresponding time.\n\n    Each RegEx has a name\n\n    For each pattern group in the RegEx one data value is stored",
        "klass": "PyFoam.Execution.UtilityRunner.UtilityRunner",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Collects data that could go into a spreadsheet. The focus of this class is on\n    storing all the data at once\n    ",
        "klass": "PyFoam.Basics.SpreadsheetData.SpreadsheetData",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "PyFoam.Basics.TemplateFile.TemplateFileOldFormat"
        ],
        "class_docstring": "Works on template files. Does calculations between $$.\n    Lines that start with $$ contain definitions",
        "klass": "PyFoam.Basics.TemplateFile.TemplateFile",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Works on template files. Does calculations between $$.\n    Lines that start with $$ contain definitions",
        "klass": "PyFoam.Basics.TemplateFile.TemplateFileOldFormat",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "numpy.signedinteger"
        ],
        "class_docstring": "Signed integer type, compatible with C ``char``.\n    Character code: ``'b'``.\n    Canonical name: ``np.byte``.\n    Alias *on this platform*: ``np.int8``: 8-bit signed integer (-128 to 127).",
        "klass": "numpy.int8",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.signedinteger"
        ],
        "class_docstring": "Signed integer type, compatible with C ``short``.\n    Character code: ``'h'``.\n    Canonical name: ``np.short``.\n    Alias *on this platform*: ``np.int16``: 16-bit signed integer (-32768 to 32767).",
        "klass": "numpy.int16",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.signedinteger"
        ],
        "class_docstring": "Signed integer type, compatible with C ``int``.\n    Character code: ``'i'``.\n    Canonical name: ``np.intc``.\n    Alias *on this platform*: ``np.int32``: 32-bit signed integer (-2147483648 to 2147483647).",
        "klass": "numpy.int32",
        "module": "numpy"
    },
    {
        "base_classes": [
            "luigi.parameter.Parameter"
        ],
        "class_docstring": "\n    A parameter which takes two values:\n        1. an instance of :class:`~collections.Iterable` and\n        2. the class of the variables to convert to.\n\n    In the task definition, use\n\n    .. code-block:: python\n\n        class MyTask(luigi.Task):\n            my_param = luigi.ChoiceParameter(choices=[0.1, 0.2, 0.3], var_type=float)\n\n    At the command line, use\n\n    .. code-block:: console\n\n        $ luigi --module my_tasks MyTask --my-param 0.1\n\n    Consider using :class:`~luigi.EnumParameter` for a typed, structured\n    alternative.  This class can perform the same role when all choices are the\n    same type and transparency of parameter value on the command line is\n    desired.\n    ",
        "klass": "luigi.ChoiceParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter._DatetimeParameterBase"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.datetime` specified to the hour.\n\n    A DateHourParameter is a `ISO 8601 <http://en.wikipedia.org/wiki/ISO_8601>`_ formatted\n    date and time specified to the hour. For example, ``2013-07-10T19`` specifies July 10, 2013 at\n    19:00.\n    ",
        "klass": "luigi.DateHourParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter._DatetimeParameterBase"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.datetime` specified to the minute.\n\n    A DateMinuteParameter is a `ISO 8601 <http://en.wikipedia.org/wiki/ISO_8601>`_ formatted\n    date and time specified to the minute. For example, ``2013-07-10T1907`` specifies July 10, 2013 at\n    19:07.\n\n    The interval parameter can be used to clamp this parameter to every N minutes, instead of every minute.\n    ",
        "klass": "luigi.DateMinuteParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter._DateParameterBase"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.date`.\n\n    A DateParameter is a Date string formatted ``YYYY-MM-DD``. For example, ``2013-07-10`` specifies\n    July 10, 2013.\n\n    DateParameters are 90% of the time used to be interpolated into file system paths or the like.\n    Here is a gentle reminder of how to interpolate date parameters into strings:\n\n    .. code:: python\n\n        class MyTask(luigi.Task):\n            date = luigi.DateParameter()\n\n            def run(self):\n                templated_path = \"/my/path/to/my/dataset/{date:%Y/%m/%d}/\"\n                instantiated_path = templated_path.format(date=self.date)\n                # print(instantiated_path) --> /my/path/to/my/dataset/2016/06/09/\n                # ... use instantiated_path ...\n\n    To set this parameter to default to the current day. You can write code like this:\n\n    .. code:: python\n\n        import datetime\n\n        class MyTask(luigi.Task):\n            date = luigi.DateParameter(default=datetime.date.today())\n    ",
        "klass": "luigi.DateParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter._DatetimeParameterBase"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.datetime` specified to the second.\n\n    A DateSecondParameter is a `ISO 8601 <http://en.wikipedia.org/wiki/ISO_8601>`_ formatted\n    date and time specified to the second. For example, ``2013-07-10T190738`` specifies July 10, 2013 at\n    19:07:38.\n\n    The interval parameter can be used to clamp this parameter to every N seconds, instead of every second.\n    ",
        "klass": "luigi.DateSecondParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter.Parameter"
        ],
        "class_docstring": "\n    Parameter whose value is a ``dict``.\n\n    In the task definition, use\n\n    .. code-block:: python\n\n        class MyTask(luigi.Task):\n          tags = luigi.DictParameter()\n\n            def run(self):\n                logging.info(\"Find server with role: %s\", self.tags['role'])\n                server = aws.ec2.find_my_resource(self.tags)\n\n\n    At the command line, use\n\n    .. code-block:: console\n\n        $ luigi --module my_tasks MyTask --tags <JSON string>\n\n    Simple example with two tags:\n\n    .. code-block:: console\n\n        $ luigi --module my_tasks MyTask --tags '{\"role\": \"web\", \"env\": \"staging\"}'\n\n    It can be used to define dynamic parameters, when you do not know the exact list of your parameters (e.g. list of\n    tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related\n    values (like a database connection config).\n    ",
        "klass": "luigi.DictParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter.DateParameter"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.date`, specified to the month\n    (day of :py:class:`~datetime.date` is \"rounded\" to first of the month).\n\n    A MonthParameter is a Date string formatted ``YYYY-MM``. For example, ``2013-07`` specifies\n    July of 2013. Task objects constructed from code accept :py:class:`~datetime.date` (ignoring the day value) or\n    :py:class:`~luigi.date_interval.Month`.\n    ",
        "klass": "luigi.MonthParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.parameter.DateParameter"
        ],
        "class_docstring": "\n    Parameter whose value is a :py:class:`~datetime.date`, specified to the year\n    (day and month of :py:class:`~datetime.date` is \"rounded\" to first day of the year).\n\n    A YearParameter is a Date string formatted ``YYYY``. Task objects constructed from code accept\n    :py:class:`~datetime.date` (ignoring the month and day values) or :py:class:`~luigi.date_interval.Year`.\n    ",
        "klass": "luigi.YearParameter",
        "module": "luigi"
    },
    {
        "base_classes": [
            "luigi.target.Target"
        ],
        "class_docstring": " Target for a resource in Redis.",
        "klass": "luigi.contrib.redis_store.RedisTarget",
        "module": "luigi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Async scheduler that can handle multiple workers, etc.\n\n    Can be run locally or on a server (using RemoteScheduler + server.Server).\n    ",
        "klass": "luigi.scheduler.Scheduler",
        "module": "luigi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Twitter API",
        "klass": "tweepy.API",
        "module": "tweepy"
    },
    {
        "base_classes": [
            "tweepy.auth.AuthHandler"
        ],
        "class_docstring": "OAuth authentication handler",
        "klass": "tweepy.auth.OAuthHandler",
        "module": "tweepy"
    },
    {
        "base_classes": [
            "tweepy.auth.AuthHandler"
        ],
        "class_docstring": "OAuth authentication handler",
        "klass": "tweepy.OAuthHandler",
        "module": "tweepy"
    },
    {
        "base_classes": [
            "testtools.testresult.real.StreamResult"
        ],
        "class_docstring": "Copies all event it receives to multiple results.\n\n    This provides an easy facility for combining multiple StreamResults.\n\n    For TestResult the equivalent class was ``MultiTestResult``.\n    ",
        "klass": "testtools.CopyStreamResult",
        "module": "testtools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager to handle expected exceptions.\n\n      def test_foo(self):\n          with ExpectedException(ValueError, 'fo.*'):\n              raise ValueError('foo')\n\n    will pass.  If the raised exception has a type other than the specified\n    type, it will be re-raised.  If it has a 'str()' that does not match the\n    given regular expression, an AssertionError will be raised.  If no\n    exception is raised, an AssertionError will be raised.\n    ",
        "klass": "testtools.testcase.ExpectedException",
        "module": "testtools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager to handle expected exceptions.\n\n      def test_foo(self):\n          with ExpectedException(ValueError, 'fo.*'):\n              raise ValueError('foo')\n\n    will pass.  If the raised exception has a type other than the specified\n    type, it will be re-raised.  If it has a 'str()' that does not match the\n    given regular expression, an AssertionError will be raised.  If no\n    exception is raised, an AssertionError will be raised.\n    ",
        "klass": "testtools.ExpectedException",
        "module": "testtools"
    },
    {
        "base_classes": [
            "testtools.testresult.real.StreamResult"
        ],
        "class_docstring": "A StreamResult that routes events.\n\n    StreamResultRouter forwards received events to another StreamResult object,\n    selected by a dynamic forwarding policy. Events where no destination is\n    found are forwarded to the fallback StreamResult, or an error is raised.\n\n    Typical use is to construct a router with a fallback and then either\n    create up front mapping rules, or create them as-needed from the fallback\n    handler::\n\n      >>> router = StreamResultRouter()\n      >>> sink = doubles.StreamResult()\n      >>> router.add_rule(sink, 'route_code_prefix', route_prefix='0',\n      ...     consume_route=True)\n      >>> router.status(\n      ...     test_id='foo', route_code='0/1', test_status='uxsuccess')\n\n    StreamResultRouter has no buffering.\n\n    When adding routes (and for the fallback) whether to call startTestRun and\n    stopTestRun or to not call them is controllable by passing\n    'do_start_stop_run'. The default is to call them for the fallback only.\n    If a route is added after startTestRun has been called, and\n    do_start_stop_run is True then startTestRun is called immediately on the\n    new route sink.\n\n    There is no a-priori defined lookup order for routes: if they are ambiguous\n    the behaviour is undefined. Only a single route is chosen for any event.\n    ",
        "klass": "testtools.StreamResultRouter",
        "module": "testtools"
    },
    {
        "base_classes": [
            "testtools.testresult.real.StreamResult"
        ],
        "class_docstring": "A specialised StreamResult that summarises a stream.\n\n    The summary uses the same representation as the original\n    unittest.TestResult contract, allowing it to be consumed by any test\n    runner.\n    ",
        "klass": "testtools.StreamSummary",
        "module": "testtools"
    },
    {
        "base_classes": [
            "testtools.testresult.real.StreamResult"
        ],
        "class_docstring": "A specialised StreamResult that emits a callback as tests complete.\n\n    Top level file attachments are simply discarded. Hung tests are detected\n    by stopTestRun and notified there and then.\n\n    The callback is passed a dict with the following keys:\n\n      * id: the test id.\n      * tags: The tags for the test. A set of unicode strings.\n      * details: A dict of file attachments - ``testtools.content.Content``\n        objects.\n      * status: One of the StreamResult status codes (including inprogress) or\n        'unknown' (used if only file events for a test were received...)\n      * timestamps: A pair of timestamps - the first one received with this\n        test id, and the one in the event that triggered the notification.\n        Hung tests have a None for the second end event. Timestamps are not\n        compared - their ordering is purely order received in the stream.\n\n    Only the most recent tags observed in the stream are reported.\n    ",
        "klass": "testtools.StreamToDict",
        "module": "testtools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Context manager to capture log messages. Useful for testing.\n    Usage:\n\n    .. code-block:: python\n\n        with capture_log(level, match) as log:\n            ...\n        # log is a list strings (as they would have appeared in the console)\n    ",
        "klass": "flexx.util.logging.capture_log",
        "module": "flexx"
    },
    {
        "base_classes": [
            "unittest.runner.TextTestRunner"
        ],
        "class_docstring": "Test runner that uses nose's TextTestResult to enable errorClasses,\n    as well as providing hooks for plugins to override or replace the test\n    output stream, results, and the test case itself.\n    ",
        "klass": "nose.core.TextTestRunner",
        "module": "nose"
    },
    {
        "base_classes": [
            "nose.plugins.base.Plugin"
        ],
        "class_docstring": "\n    Log capture plugin. Enabled by default. Disable with --nologcapture.\n    This plugin captures logging statements issued during test execution,\n    appending any output captured to the error or failure output,\n    should the test fail or raise an error.\n    ",
        "klass": "nose.plugins.logcapture.LogCapture",
        "module": "nose"
    },
    {
        "base_classes": [
            "unittest.suite.TestSuite"
        ],
        "class_docstring": "A simple test suite that doesn't provide class or module shared fixtures.\n    ",
        "klass": "unittest2.BaseTestSuite",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest2.case.TestCase"
        ],
        "class_docstring": "A test case that wraps a test function.\n\n    This is useful for slipping pre-existing test functions into the\n    unittest framework. Optionally, set-up and tidy-up functions can be\n    supplied. As with TestCase, the tidy-up ('tearDown') function will\n    always be called if the set-up ('setUp') function ran successfully.\n    ",
        "klass": "unittest2.FunctionTestCase",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest.case.TestCase"
        ],
        "class_docstring": "A class whose instances are single test cases.\n\n    By default, the test code itself should be placed in a method named\n    'runTest'.\n\n    If the fixture may be used for many test cases, create as\n    many test methods as are needed. When instantiating such a TestCase\n    subclass, specify in the constructor arguments the name of the test method\n    that the instance is to execute.\n\n    Test authors should subclass TestCase for their own tests. Construction\n    and deconstruction of the test's environment ('fixture') can be\n    implemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\n    If it is necessary to override the __init__ method, the base class\n    __init__ method must always be called. It is important that subclasses\n    should not change the signature of their __init__ method, since instances\n    of the classes are instantiated automatically by parts of the framework\n    in order to be run.\n\n    When subclassing TestCase, you can set these attributes:\n    * failureException: determines which exception will be raised when\n        the instance's assertion methods fail; test methods raising this\n        exception will be deemed to have 'failed' rather than 'errored'.\n    * longMessage: determines whether long messages (including repr of\n        objects used in assert methods) will be printed on failure in *addition*\n        to any explicit message passed.\n    * maxDiff: sets the maximum length of a diff in failure messages\n        by assert methods using difflib. It is looked up as an instance\n        attribute so can be configured by individual tests if required.\n    ",
        "klass": "unittest2.TestCase",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest.loader.TestLoader"
        ],
        "class_docstring": "\n    This class is responsible for loading tests according to various criteria\n    and returning them wrapped in a TestSuite\n    ",
        "klass": "unittest2.TestLoader",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest.result.TestResult"
        ],
        "class_docstring": "Holder for test result information.\n\n    Test results are automatically managed by the TestCase and TestSuite\n    classes, and do not need to be explicitly manipulated by writers of tests.\n\n    Each instance holds the total number of tests run, and collections of\n    failures and errors that occurred among those test runs. The collections\n    contain tuples of (testcase, exceptioninfo), where exceptioninfo is the\n    formatted traceback of the error that occurred.\n    ",
        "klass": "unittest2.TestResult",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest2.suite.BaseTestSuite"
        ],
        "class_docstring": "A test suite is a composite test consisting of a number of TestCases.\n\n    For use, create an instance of TestSuite, then add test case instances.\n    When all tests have been added, the suite can be passed to a test\n    runner, such as TextTestRunner. It will run the individual test cases\n    in the order in which they were added, aggregating the results. When\n    subclassing, do not forget to call the base class constructor.\n    ",
        "klass": "unittest2.TestSuite",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "unittest.runner.TextTestRunner"
        ],
        "class_docstring": "A test runner class that displays results in textual form.\n\n    It prints out the names of tests as they are run, errors as they\n    occur, and a summary of the results at the end of the test run.\n    ",
        "klass": "unittest2.TextTestRunner",
        "module": "unittest2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    ",
        "klass": "unittest2.compatibility.catch_warnings",
        "module": "warnings"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The core of the Distutils.  Most of the work hiding behind 'setup'\n    is really done within a Distribution instance, which farms the work out\n    to the Distutils commands specified on the command line.\n\n    Setup scripts will almost never instantiate Distribution directly,\n    unless the 'setup()' function is totally inadequate to their needs.\n    However, it is conceivable that a setup script might wish to subclass\n    Distribution for some specialized purpose, and then pass the subclass\n    to 'setup()' as the 'distclass' keyword argument.  If so, it is\n    necessary to respect the expectations that 'setup' has of Distribution.\n    See the code for 'setup()', in core.py, for details.\n    ",
        "klass": "distutils.dist.Distribution",
        "module": "distutils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Built-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.",
        "klass": "list",
        "module": "list"
    },
    {
        "base_classes": [
            "pexpect.spawnbase.SpawnBase"
        ],
        "class_docstring": "This is the main class interface for Pexpect. Use this class to start\n    and control child applications. ",
        "klass": "pexpect.spawn",
        "module": "pexpect"
    },
    {
        "base_classes": [
            "pexpect.pty_spawn.spawn"
        ],
        "class_docstring": "This class extends pexpect.spawn to specialize setting up SSH\n    connections. This adds methods for login, logout, and expecting the shell\n    prompt. It does various tricky things to handle many situations in the SSH\n    login process. For example, if the session is your first login, then pxssh\n    automatically accepts the remote certificate; or if you have public key\n    authentication setup then pxssh won't wait for the password prompt.\n\n    pxssh uses the shell prompt to synchronize output from the remote host. In\n    order to make this more robust it sets the shell prompt to something more\n    unique than just $ or #. This should work on most Borne/Bash or Csh style\n    shells.\n\n    Example that runs a few commands on a remote server and prints the result::\n\n        from pexpect import pxssh\n        import getpass\n        try:\n            s = pxssh.pxssh()\n            hostname = raw_input('hostname: ')\n            username = raw_input('username: ')\n            password = getpass.getpass('password: ')\n            s.login(hostname, username, password)\n            s.sendline('uptime')   # run a command\n            s.prompt()             # match the prompt\n            print(s.before)        # print everything before the prompt.\n            s.sendline('ls -l')\n            s.prompt()\n            print(s.before)\n            s.sendline('df')\n            s.prompt()\n            print(s.before)\n            s.logout()\n        except pxssh.ExceptionPxssh as e:\n            print(\"pxssh failed on login.\")\n            print(e)\n\n    Example showing how to specify SSH options::\n\n        from pexpect import pxssh\n        s = pxssh.pxssh(options={\n                            \"StrictHostKeyChecking\": \"no\",\n                            \"UserKnownHostsFile\": \"/dev/null\"})\n        ...\n\n    Note that if you have ssh-agent running while doing development with pxssh\n    then this can lead to a lot of confusion. Many X display managers (xdm,\n    gdm, kdm, etc.) will automatically start a GUI agent. You may see a GUI\n    dialog box popup asking for a password during development. You should turn\n    off any key agents during testing. The 'force_password' attribute will turn\n    off public key authentication. This will only work if the remote SSH server\n    is configured to allow password logins. Example of using 'force_password'\n    attribute::\n\n            s = pxssh.pxssh()\n            s.force_password = True\n            hostname = raw_input('hostname: ')\n            username = raw_input('username: ')\n            password = getpass.getpass('password: ')\n            s.login (hostname, username, password)\n\n    `debug_command_string` is only for the test suite to confirm that the string\n    generated for SSH is correct, using this will not allow you to do\n    anything other than get a string back from `pxssh.pxssh.login()`.\n    ",
        "klass": "pexpect.pxssh.pxssh",
        "module": "pexpect"
    },
    {
        "base_classes": [
            "mistral.actions.base.Action"
        ],
        "class_docstring": "Constructs an HTTP action.\n\n    :param url: URL for the new HTTP request.\n    :param method: (optional, 'GET' by default) method for the new HTTP\n        request.\n    :param params: (optional) Dictionary or bytes to be sent in the\n        query string for the HTTP request.\n    :param body: (optional) Dictionary, bytes, or file-like object to send\n        in the body of the HTTP request.\n    :param headers: (optional) Dictionary of HTTP Headers to send with\n        the HTTP request.\n    :param cookies: (optional) Dict or CookieJar object to send with\n        the HTTP request.\n    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom\n        HTTP Auth.\n    :param timeout: (optional) Float describing the timeout of the request\n        in seconds.\n    :param allow_redirects: (optional) Boolean. Set to True if POST/PUT/DELETE\n        redirect following is allowed.\n    :param proxies: (optional) Dictionary mapping protocol to the URL of\n        the proxy.\n    :param verify: (optional) if ``True``, the SSL cert will be verified.\n        A CA_BUNDLE path can also be provided.\n    ",
        "klass": "mistral.actions.std_actions.HTTPAction",
        "module": "mistral"
    },
    {
        "base_classes": [
            "mistral.actions.base.Action"
        ],
        "class_docstring": "Runs Secure Shell (SSH) command on provided single or multiple hosts.\n\n    It is allowed to provide either a single host or a list of hosts in\n    action parameter 'host'. In case of a single host the action result\n    will be a single value, otherwise a list of results provided in the\n    same order as provided hosts.\n    ",
        "klass": "mistral.actions.std_actions.SSHAction",
        "module": "mistral"
    },
    {
        "base_classes": [
            "click.core.BaseCommand"
        ],
        "class_docstring": "Commands are the basic building block of command line interfaces in\n    Click.  A basic command handles command line parsing and might dispatch\n    more parsing to commands nested below it.\n\n    .. versionchanged:: 2.0\n       Added the `context_settings` parameter.\n\n    :param name: the name of the command to use unless a group overrides it.\n    :param context_settings: an optional dictionary with defaults that are\n                             passed to the context object.\n    :param callback: the callback to invoke.  This is optional.\n    :param params: the parameters to register with this command.  This can\n                   be either :class:`Option` or :class:`Argument` objects.\n    :param help: the help string to use for this command.\n    :param epilog: like the help string but it's printed at the end of the\n                   help page after everything else.\n    :param short_help: the short help to use for this command.  This is\n                       shown on the command listing of the parent command.\n    :param add_help_option: by default each command registers a ``--help``\n                            option.  This can be disabled by this parameter.\n    :param hidden: hide this command from help outputs.\n\n    :param deprecated: issues a message indicating that\n                             the command is deprecated.\n    ",
        "klass": "click.Command",
        "module": "click"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The context is a special internal object that holds state relevant\n    for the script execution at every single level.  It's normally invisible\n    to commands unless they opt-in to getting access to it.\n\n    The context is useful as it can pass internal objects around and can\n    control special execution features such as reading data from\n    environment variables.\n\n    A context can be used as context manager in which case it will call\n    :meth:`close` on teardown.\n\n    .. versionadded:: 2.0\n       Added the `resilient_parsing`, `help_option_names`,\n       `token_normalize_func` parameters.\n\n    .. versionadded:: 3.0\n       Added the `allow_extra_args` and `allow_interspersed_args`\n       parameters.\n\n    .. versionadded:: 4.0\n       Added the `color`, `ignore_unknown_options`, and\n       `max_content_width` parameters.\n\n    :param command: the command class for this context.\n    :param parent: the parent context.\n    :param info_name: the info name for this invocation.  Generally this\n                      is the most descriptive name for the script or\n                      command.  For the toplevel script it is usually\n                      the name of the script, for commands below it it's\n                      the name of the script.\n    :param obj: an arbitrary object of user data.\n    :param auto_envvar_prefix: the prefix to use for automatic environment\n                               variables.  If this is `None` then reading\n                               from environment variables is disabled.  This\n                               does not affect manually set environment\n                               variables which are always read.\n    :param default_map: a dictionary (like object) with default values\n                        for parameters.\n    :param terminal_width: the width of the terminal.  The default is\n                           inherit from parent context.  If no context\n                           defines the terminal width then auto\n                           detection will be applied.\n    :param max_content_width: the maximum width for content rendered by\n                              Click (this currently only affects help\n                              pages).  This defaults to 80 characters if\n                              not overridden.  In other words: even if the\n                              terminal is larger than that, Click will not\n                              format things wider than 80 characters by\n                              default.  In addition to that, formatters might\n                              add some safety mapping on the right.\n    :param resilient_parsing: if this flag is enabled then Click will\n                              parse without any interactivity or callback\n                              invocation.  Default values will also be\n                              ignored.  This is useful for implementing\n                              things such as completion support.\n    :param allow_extra_args: if this is set to `True` then extra arguments\n                             at the end will not raise an error and will be\n                             kept on the context.  The default is to inherit\n                             from the command.\n    :param allow_interspersed_args: if this is set to `False` then options\n                                    and arguments cannot be mixed.  The\n                                    default is to inherit from the command.\n    :param ignore_unknown_options: instructs click to ignore options it does\n                                   not know and keeps them for later\n                                   processing.\n    :param help_option_names: optionally a list of strings that define how\n                              the default help parameter is named.  The\n                              default is ``['--help']``.\n    :param token_normalize_func: an optional function that is used to\n                                 normalize tokens (options, choices,\n                                 etc.).  This for instance can be used to\n                                 implement case insensitive behavior.\n    :param color: controls if the terminal supports ANSI colors or not.  The\n                  default is autodetection.  This is only needed if ANSI\n                  codes are used in texts that Click prints which is by\n                  default not the case.  This for instance would affect\n                  help output.\n    ",
        "klass": "click.Context",
        "module": "click"
    },
    {
        "base_classes": [
            "click.core.MultiCommand"
        ],
        "class_docstring": "A group allows a command to have subcommands attached.  This is the\n    most common way to implement nesting in Click.\n\n    :param commands: a dictionary of commands.\n    ",
        "klass": "click.Group",
        "module": "click"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class helps with formatting text-based help pages.  It's\n    usually just needed for very special internal cases, but it's also\n    exposed so that developers can write their own fancy outputs.\n\n    At present, it always writes into memory.\n\n    :param indent_increment: the additional increment for each level.\n    :param width: the width for the text.  This defaults to the terminal\n                  width clamped to a maximum of 78.\n    ",
        "klass": "click.formatting.HelpFormatter",
        "module": "click"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "This class helps with formatting text-based help pages.  It's\n    usually just needed for very special internal cases, but it's also\n    exposed so that developers can write their own fancy outputs.\n\n    At present, it always writes into memory.\n\n    :param indent_increment: the additional increment for each level.\n    :param width: the width for the text.  This defaults to the terminal\n                  width clamped to a maximum of 78.\n    ",
        "klass": "click.HelpFormatter",
        "module": "click"
    },
    {
        "base_classes": [
            "pymongo.common.BaseObject"
        ],
        "class_docstring": "\n    A client-side representation of a MongoDB cluster.\n\n    Instances can represent either a standalone MongoDB server, a replica\n    set, or a sharded cluster. Instances of this class are responsible for\n    maintaining up-to-date state of the cluster, and possibly cache\n    resources related to this, including background threads for monitoring,\n    and connection pools.\n    ",
        "klass": "pymongo.MongoClient",
        "module": "pymongo"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "SON data.\n\n    A subclass of dict that maintains ordering of keys and provides a\n    few extra niceties for dealing with SON. SON provides an API\n    similar to collections.OrderedDict from Python 2.7+.\n    ",
        "klass": "bson.son.SON",
        "module": "bson"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "BSON's JavaScript code type.\n\n    Raises :class:`TypeError` if `code` is not an instance of\n    :class:`basestring` (:class:`str` in python 3) or `scope`\n    is not ``None`` or an instance of :class:`dict`.\n\n    Scope variables can be set by passing a dictionary as the `scope`\n    argument or by using keyword arguments. If a variable is set as a\n    keyword argument it will override any setting for that variable in\n    the `scope` dictionary.\n\n    :Parameters:\n      - `code`: A string containing JavaScript code to be evaluated or another\n        instance of Code. In the latter case, the scope of `code` becomes this\n        Code's :attr:`scope`.\n      - `scope` (optional): dictionary representing the scope in which\n        `code` should be evaluated - a mapping from identifiers (as\n        strings) to values. Defaults to ``None``. This is applied after any\n        scope associated with a given `code` above.\n      - `**kwargs` (optional): scope variables can also be passed as\n        keyword arguments. These are applied after `scope` and `code`.\n\n    .. versionchanged:: 3.4\n      The default value for :attr:`scope` is ``None`` instead of ``{}``.\n\n    ",
        "klass": "bson.code.Code",
        "module": "bson"
    },
    {
        "base_classes": [
            "ppci.graph.graph.BaseGraph"
        ],
        "class_docstring": " Directed graph. ",
        "klass": "ppci.graph.DiGraph",
        "module": "ppci"
    },
    {
        "base_classes": [
            "ppci.graph.graph.BaseGraph"
        ],
        "class_docstring": " Generic graph base class.\n\n    Can dump to graphviz dot format for example!\n    ",
        "klass": "ppci.graph.Graph",
        "module": "ppci"
    },
    {
        "base_classes": [
            "ppci.graph.graph.Graph"
        ],
        "class_docstring": " A graph that allows masking nodes temporarily ",
        "klass": "ppci.graph.MaskableGraph",
        "module": "ppci"
    },
    {
        "base_classes": [
            "ppci.utils.graph.Graph"
        ],
        "class_docstring": " Directed graph. ",
        "klass": "ppci.utils.graph.DiGraph",
        "module": "ppci"
    },
    {
        "base_classes": [
            "urllib3.poolmanager.PoolManager"
        ],
        "class_docstring": "\n    Behaves just like :class:`PoolManager`, but sends all requests through\n    the defined proxy, using the CONNECT method for HTTPS URLs.\n\n    :param proxy_url:\n        The URL of the proxy to be used.\n\n    :param proxy_headers:\n        A dictionary containing headers that will be sent to the proxy. In case\n        of HTTP they are being sent with each request, while in the\n        HTTPS/CONNECT case they are sent only once. Could be used for proxy\n        authentication.\n\n    Example:\n        >>> proxy = urllib3.ProxyManager('http://localhost:3128/')\n        >>> r1 = proxy.request('GET', 'http://google.com/')\n        >>> r2 = proxy.request('GET', 'http://httpbin.org/')\n        >>> len(proxy.pools)\n        1\n        >>> r3 = proxy.request('GET', 'https://httpbin.org/')\n        >>> r4 = proxy.request('GET', 'https://twitter.com/')\n        >>> len(proxy.pools)\n        3\n\n    ",
        "klass": "urllib3.poolmanager.ProxyManager",
        "module": "urllib3"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This class will allow you to stub out requests so you don't have to hit\n    an endpoint to write tests. Responses are returned first in, first out.\n    If operations are called out of order, or are called with no remaining\n    queued responses, an error will be raised.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': 'test-bucket'}\n\n        stubber.add_response('list_objects', response, expected_params)\n        stubber.activate()\n\n        service_response = s3.list_objects(Bucket='test-bucket')\n        assert service_response == response\n\n\n    This class can also be called as a context manager, which will handle\n    activation / deactivation for you.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n\n        response = {\n            \"Owner\": {\n                \"ID\": \"foo\",\n                \"DisplayName\": \"bar\"\n            },\n            \"Buckets\": [{\n                \"CreationDate\": datetime.datetime(2016, 1, 20, 22, 9),\n                \"Name\": \"baz\"\n            }]\n        }\n\n\n        with Stubber(s3) as stubber:\n            stubber.add_response('list_buckets', response, {})\n            service_response = s3.list_buckets()\n\n        assert service_response == response\n\n\n    If you have an input parameter that is a randomly generated value, or you\n    otherwise don't care about its value, you can use ``stub.ANY`` to ignore\n    it in validation.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber, ANY\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': ANY}\n        stubber.add_response('list_objects', response, expected_params)\n\n        with stubber:\n            service_response = s3.list_objects(Bucket='test-bucket')\n\n        assert service_response == response\n    ",
        "klass": "botocore.stub.Stubber",
        "module": "botocore"
    },
    {
        "base_classes": [
            "thrift.Thrift.TException"
        ],
        "class_docstring": "Application level thrift exceptions.",
        "klass": "thrift.Thrift.TApplicationException",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.protocol.TProtocol.TProtocolBase"
        ],
        "class_docstring": "Binary implementation of the Thrift protocol driver.",
        "klass": "thrift.protocol.TBinaryProtocol.TBinaryProtocol",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.protocol.TProtocol.TProtocolBase"
        ],
        "class_docstring": "Compact implementation of the Thrift protocol driver.",
        "klass": "thrift.protocol.TCompactProtocol.TCompactProtocol",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.transport.TTransport.TTransportBase",
            "thrift.transport.TTransport.CReadableTransport"
        ],
        "class_docstring": "Wraps a cBytesIO object as a TTransport.\n\n    NOTE: Unlike the C++ version of this class, you cannot write to it\n          then immediately read from it.  If you want to read from a\n          TMemoryBuffer, you must either pass a string to the constructor.\n    TODO(dreiss): Make this work like the C++ version.\n    ",
        "klass": "thrift.transport.TTransport.TMemoryBuffer",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.transport.TSocket.TSocketBase"
        ],
        "class_docstring": "Socket implementation of TTransport base.",
        "klass": "thrift.transport.TSocket.TSocket",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.transport.TTransport.TTransportBase",
            "thrift.transport.TTransport.CReadableTransport"
        ],
        "class_docstring": "Class that wraps another transport and buffers its I/O.\n\n    The implementation uses a (configurable) fixed-size read buffer\n    but buffers all writes until a flush is performed.\n    ",
        "klass": "thrift.transport.TTransport.TBufferedTransport",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.transport.TTransport.TTransportBase"
        ],
        "class_docstring": "Wraps a file-like object to make it work as a Thrift transport.",
        "klass": "thrift.transport.TTransport.TFileObjectTransport",
        "module": "thrift"
    },
    {
        "base_classes": [
            "thrift.transport.TTransport.TTransportBase",
            "thrift.transport.TTransport.CReadableTransport"
        ],
        "class_docstring": "Class that wraps another transport and frames its I/O when writing.",
        "klass": "thrift.transport.TTransport.TFramedTransport",
        "module": "thrift"
    },
    {
        "base_classes": [
            "jedi.api.Script"
        ],
        "class_docstring": "\n    Jedi API for Python REPLs.\n\n    In addition to completion of simple attribute access, Jedi\n    supports code completion based on static code analysis.\n    Jedi can complete attributes of object which is not initialized\n    yet.\n\n    >>> from os.path import join\n    >>> namespace = locals()\n    >>> script = Interpreter('join(\"\").up', [namespace])\n    >>> print(script.completions()[0].name)\n    upper\n    ",
        "klass": "jedi.Interpreter",
        "module": "jedi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A Script is the base for completions, goto or whatever you want to do with\n    |jedi|.\n\n    You can either use the ``source`` parameter or ``path`` to read a file.\n    Usually you're going to want to use both of them (in an editor).\n\n    The script might be analyzed in a different ``sys.path`` than |jedi|:\n\n    - if `sys_path` parameter is not ``None``, it will be used as ``sys.path``\n      for the script;\n\n    - if `sys_path` parameter is ``None`` and ``VIRTUAL_ENV`` environment\n      variable is defined, ``sys.path`` for the specified environment will be\n      guessed (see :func:`jedi.evaluate.sys_path.get_venv_path`) and used for\n      the script;\n\n    - otherwise ``sys.path`` will match that of |jedi|.\n\n    :param source: The source code of the current file, separated by newlines.\n    :type source: str\n    :param line: The line to perform actions on (starting with 1).\n    :type line: int\n    :param column: The column of the cursor (starting with 0).\n    :type column: int\n    :param path: The path of the file in the file system, or ``''`` if\n        it hasn't been saved yet.\n    :type path: str or None\n    :param encoding: The encoding of ``source``, if it is not a\n        ``unicode`` object (default ``'utf-8'``).\n    :type encoding: str\n    :param sys_path: ``sys.path`` to use during analysis of the script\n    :type sys_path: list\n    :param environment: TODO\n    :type environment: Environment\n    ",
        "klass": "jedi.api.Script",
        "module": "jedi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A Script is the base for completions, goto or whatever you want to do with\n    |jedi|.\n\n    You can either use the ``source`` parameter or ``path`` to read a file.\n    Usually you're going to want to use both of them (in an editor).\n\n    The script might be analyzed in a different ``sys.path`` than |jedi|:\n\n    - if `sys_path` parameter is not ``None``, it will be used as ``sys.path``\n      for the script;\n\n    - if `sys_path` parameter is ``None`` and ``VIRTUAL_ENV`` environment\n      variable is defined, ``sys.path`` for the specified environment will be\n      guessed (see :func:`jedi.evaluate.sys_path.get_venv_path`) and used for\n      the script;\n\n    - otherwise ``sys.path`` will match that of |jedi|.\n\n    :param source: The source code of the current file, separated by newlines.\n    :type source: str\n    :param line: The line to perform actions on (starting with 1).\n    :type line: int\n    :param column: The column of the cursor (starting with 0).\n    :type column: int\n    :param path: The path of the file in the file system, or ``''`` if\n        it hasn't been saved yet.\n    :type path: str or None\n    :param encoding: The encoding of ``source``, if it is not a\n        ``unicode`` object (default ``'utf-8'``).\n    :type encoding: str\n    :param sys_path: ``sys.path`` to use during analysis of the script\n    :type sys_path: list\n    :param environment: TODO\n    :type environment: Environment\n    ",
        "klass": "jedi.Script",
        "module": "jedi"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This object (the `Network` object) handles keeping track of all the\n    graph's nodes, and links between the nodes.\n\n    The `Network' object is mostly used to topologically sort the nodes,\n    to handle dependency resolution.\n    ",
        "klass": "pupa.utils.topsort.Network",
        "module": "pupa"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Return a context manager for managing rc settings.\n\n    This allows one to do::\n\n        with mpl.rc_context(fname='screen.rc'):\n            plt.plot(x, a)\n            with mpl.rc_context(fname='print.rc'):\n                plt.plot(x, b)\n            plt.plot(x, c)\n\n    The 'a' vs 'x' and 'c' vs 'x' plots would have settings from\n    'screen.rc', while the 'b' vs 'x' plot would have settings from\n    'print.rc'.\n\n    A dictionary can also be passed to the context manager::\n\n        with mpl.rc_context(rc={'text.usetex': True}, fname='screen.rc'):\n            plt.plot(x, a)\n\n    The 'rc' dictionary takes precedence over the settings loaded from\n    'fname'.  Passing a dictionary only is also valid. For example a\n    common usage is::\n\n        with mpl.rc_context(rc={'interactive': False}):\n            fig, ax = plt.subplots()\n            ax.plot(range(3), range(3))\n            fig.savefig('A.png', format='png')\n            plt.close(fig)\n    ",
        "klass": "matplotlib.rc_context",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.animation.TimedAnimation"
        ],
        "class_docstring": "\n    Makes an animation by repeatedly calling a function *func*.\n\n    Parameters\n    ----------\n    fig : `~matplotlib.figure.Figure`\n       The figure object that is used to get draw, resize, and any\n       other needed events.\n\n    func : callable\n       The function to call at each frame.  The first argument will\n       be the next value in *frames*.   Any additional positional\n       arguments can be supplied via the *fargs* parameter.\n\n       The required signature is::\n\n          def func(frame, *fargs) -> iterable_of_artists\n\n       If ``blit == True``, *func* must return an iterable of all artists\n       that were modified or created. This information is used by the blitting\n       algorithm to determine which parts of the figure have to be updated.\n       The return value is unused if ``blit == False`` and may be omitted in\n       that case.\n\n    frames : iterable, int, generator function, or None, optional\n        Source of data to pass *func* and each frame of the animation\n\n        - If an iterable, then simply use the values provided.  If the\n          iterable has a length, it will override the *save_count* kwarg.\n\n        - If an integer, then equivalent to passing ``range(frames)``\n\n        - If a generator function, then must have the signature::\n\n             def gen_function() -> obj\n\n        - If *None*, then equivalent to passing ``itertools.count``.\n\n        In all of these cases, the values in *frames* is simply passed through\n        to the user-supplied *func* and thus can be of any type.\n\n    init_func : callable, optional\n       A function used to draw a clear frame. If not given, the\n       results of drawing from the first item in the frames sequence\n       will be used. This function will be called once before the\n       first frame.\n\n       The required signature is::\n\n          def init_func() -> iterable_of_artists\n\n       If ``blit == True``, *init_func* must return an iterable of artists\n       to be re-drawn. This information is used by the blitting\n       algorithm to determine which parts of the figure have to be updated.\n       The return value is unused if ``blit == False`` and may be omitted in\n       that case.\n\n    fargs : tuple or None, optional\n       Additional arguments to pass to each call to *func*.\n\n    save_count : int, optional\n       The number of values from *frames* to cache.\n\n    interval : number, optional\n       Delay between frames in milliseconds.  Defaults to 200.\n\n    repeat_delay : number, optional\n       If the animation in repeated, adds a delay in milliseconds\n       before repeating the animation.  Defaults to *None*.\n\n    repeat : bool, optional\n       Controls whether the animation should repeat when the sequence\n       of frames is completed.  Defaults to *True*.\n\n    blit : bool, optional\n       Controls whether blitting is used to optimize drawing. Note: when using\n       blitting any animated artists will be drawn according to their zorder.\n       However, they will be drawn on top of any previous artists, regardless\n       of their zorder.  Defaults to *False*.\n\n    cache_frame_data : bool, optional\n       Controls whether frame data is cached. Defaults to *True*.\n       Disabling cache might be helpful when frames contain large objects.\n    ",
        "klass": "matplotlib.animation.FuncAnimation",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This is a mixin class to support scalar data to RGBA mapping.\n    The ScalarMappable makes use of data normalization before returning\n    RGBA colors from the given colormap.\n\n    ",
        "klass": "matplotlib.cm.ScalarMappable",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.collections.Collection"
        ],
        "class_docstring": "A collection of ellipses, drawn using splines.",
        "klass": "matplotlib.collections.EllipseCollection",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.collections.LineCollection"
        ],
        "class_docstring": "\n    A collection of discrete events.\n\n    The events are given by a 1-dimensional array, usually the position of\n    something along an axis, such as time or length.  They do not have an\n    amplitude and are displayed as vertical or horizontal parallel bars.\n    ",
        "klass": "matplotlib.collections.EventCollection",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Execute a child program in a new process.\n\n    For a complete description of the arguments see the Python documentation.\n\n    Arguments:\n      args: A string, or a sequence of program arguments.\n\n      bufsize: supplied as the buffering argument to the open() function when\n          creating the stdin/stdout/stderr pipe file objects\n\n      executable: A replacement program to execute.\n\n      stdin, stdout and stderr: These specify the executed programs' standard\n          input, standard output and standard error file handles, respectively.\n\n      preexec_fn: (POSIX only) An object to be called in the child process\n          just before the child is executed.\n\n      close_fds: Controls closing or inheriting of file descriptors.\n\n      shell: If true, the command will be executed through the shell.\n\n      cwd: Sets the current directory before the child is executed.\n\n      env: Defines the environment variables for the new process.\n\n      text: If true, decode stdin, stdout and stderr using the given encoding\n          (if set) or the system default otherwise.\n\n      universal_newlines: Alias of text, provided for backwards compatibility.\n\n      startupinfo and creationflags (Windows only)\n\n      restore_signals (POSIX only)\n\n      start_new_session (POSIX only)\n\n      pass_fds (POSIX only)\n\n      encoding and errors: Text mode encoding and error handling to use for\n          file objects stdin, stdout and stderr.\n\n    Attributes:\n        stdin, stdout, stderr, pid, returncode\n    ",
        "klass": "matplotlib.compat.subprocess.Popen",
        "module": "subprocess"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class for storing and manipulating font properties.\n\n    The font properties are those described in the `W3C Cascading\n    Style Sheet, Level 1\n    <http://www.w3.org/TR/1998/REC-CSS2-19980512/>`_ font\n    specification.  The six properties are:\n\n      - family: A list of font names in decreasing order of priority.\n        The items may include a generic font family name, either\n        'serif', 'sans-serif', 'cursive', 'fantasy', or 'monospace'.\n        In that case, the actual font to be used will be looked up\n        from the associated rcParam.\n\n      - style: Either 'normal', 'italic' or 'oblique'.\n\n      - variant: Either 'normal' or 'small-caps'.\n\n      - stretch: A numeric value in the range 0-1000 or one of\n        'ultra-condensed', 'extra-condensed', 'condensed',\n        'semi-condensed', 'normal', 'semi-expanded', 'expanded',\n        'extra-expanded' or 'ultra-expanded'\n\n      - weight: A numeric value in the range 0-1000 or one of\n        'ultralight', 'light', 'normal', 'regular', 'book', 'medium',\n        'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy',\n        'extra bold', 'black'\n\n      - size: Either an relative value of 'xx-small', 'x-small',\n        'small', 'medium', 'large', 'x-large', 'xx-large' or an\n        absolute font size, e.g., 12\n\n    The default font property for TrueType fonts (as specified in the\n    default rcParams) is::\n\n      sans-serif, normal, normal, normal, normal, scalable.\n\n    Alternatively, a font may be specified using an absolute path to a\n    .ttf file, by using the *fname* kwarg.\n\n    The preferred usage of font sizes is to use the relative values,\n    e.g.,  'large', instead of absolute font sizes, e.g., 12.  This\n    approach allows all text sizes to be made larger or smaller based\n    on the font manager's default font size.\n\n    This class will also accept a fontconfig_ pattern_, if it is the only\n    argument provided.  This support does not require fontconfig to be\n    installed.  We are merely borrowing its pattern syntax for use here.\n\n    .. _fontconfig: https://www.freedesktop.org/wiki/Software/fontconfig/\n    .. _pattern:\n       https://www.freedesktop.org/software/fontconfig/fontconfig-user.html\n\n    Note that Matplotlib's internal font manager and fontconfig use a\n    different algorithm to lookup fonts, so the results of the same pattern\n    may be different in Matplotlib than in other applications that use\n    fontconfig.\n    ",
        "klass": "matplotlib.font_manager.FontProperties",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.transforms.Affine2DBase"
        ],
        "class_docstring": "\n    A mutable 2D affine transformation.\n    ",
        "klass": "matplotlib.transforms.Affine2D",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "An empty token, will always match.\n    ",
        "klass": "pyparsing.Empty",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.Token"
        ],
        "class_docstring": "\n    Token for matching strings that are delimited by quoting characters.\n\n    Defined with the following parameters:\n\n        - quoteChar - string of one or more characters defining the\n          quote delimiting string\n        - escChar - character to escape quotes, typically backslash\n          (default= ``None``)\n        - escQuote - special quote sequence to escape an embedded quote\n          string (such as SQL's ``\"\"`` to escape an embedded ``\"``)\n          (default= ``None``)\n        - multiline - boolean indicating whether quotes can span\n          multiple lines (default= ``False``)\n        - unquoteResults - boolean indicating whether the matched text\n          should be unquoted (default= ``True``)\n        - endQuoteChar - string of one or more characters defining the\n          end of the quote delimited string (default= ``None``  => same as\n          quoteChar)\n        - convertWhitespaceEscapes - convert escaped whitespace\n          (``'\\t'``, ``'\\n'``, etc.) to actual whitespace\n          (default= ``True``)\n\n    Example::\n\n        qs = QuotedString('\"')\n        print(qs.searchString('lsjdf \"This is the quote\" sldjf'))\n        complex_qs = QuotedString('{{', endQuoteChar='}}')\n        print(complex_qs.searchString('lsjdf {{This is the \"quote\"}} sldjf'))\n        sql_qs = QuotedString('\"', escQuote='\"\"')\n        print(sql_qs.searchString('lsjdf \"This is the quote with \"\"embedded\"\" quotes\" sldjf'))\n\n    prints::\n\n        [['This is the quote']]\n        [['This is the \"quote\"']]\n        [['This is the quote with \"embedded\" quotes']]\n    ",
        "klass": "pyparsing.QuotedString",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "matplotlib.patches.Patch"
        ],
        "class_docstring": "\n    A scale-free ellipse.\n    ",
        "klass": "matplotlib.patches.Ellipse",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.patches.Patch"
        ],
        "class_docstring": "an axis spine -- the line noting the data area boundaries\n\n    Spines are the lines connecting the axis tick marks and noting the\n    boundaries of the data area. They can be placed at arbitrary\n    positions. See function:`~matplotlib.spines.Spine.set_position`\n    for more information.\n\n    The default position is ``('outward',0)``.\n\n    Spines are subclasses of class:`~matplotlib.patches.Patch`, and\n    inherit much of their behavior.\n\n    Spines draw a line, a circle, or an arc depending if\n    function:`~matplotlib.spines.Spine.set_patch_line`,\n    function:`~matplotlib.spines.Spine.set_patch_circle`, or\n    function:`~matplotlib.spines.Spine.set_patch_arc` has been called.\n    Line-like is the default.\n\n    ",
        "klass": "matplotlib.spines.Spine",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.text.Text",
            "matplotlib.text._AnnotationBase"
        ],
        "class_docstring": "\n    An `.Annotation` is a `.Text` that can refer to a specific position *xy*.\n    Optionally an arrow pointing from the text to *xy* can be drawn.\n\n    Attributes\n    ----------\n    xy\n        The annotated position.\n    xycoords\n        The coordinate system for *xy*.\n    arrow_patch\n        A `.FancyArrowPatch` to point from *xytext* to *xy*.\n    ",
        "klass": "matplotlib.text.Annotation",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.ticker.Locator"
        ],
        "class_docstring": "\n    Select no more than N intervals at nice locations.\n    ",
        "klass": "matplotlib.ticker.MaxNLocator",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "elasticsearch_dsl.search.Request"
        ],
        "class_docstring": "\n    Combine multiple :class:`~elasticsearch_dsl.Search` objects into a single\n    request.\n    ",
        "klass": "elasticsearch_dsl.search.MultiSearch",
        "module": "elasticsearch_dsl"
    },
    {
        "base_classes": [
            "elasticsearch_dsl.search.Request"
        ],
        "class_docstring": "\n    Combine multiple :class:`~elasticsearch_dsl.Search` objects into a single\n    request.\n    ",
        "klass": "elasticsearch_dsl.MultiSearch",
        "module": "elasticsearch_dsl"
    },
    {
        "base_classes": [
            "watchdog.observers.api.EventDispatcher"
        ],
        "class_docstring": "Base observer.",
        "klass": "watchdog.observers.api.BaseObserver",
        "module": "watchdog"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An scheduled watch.\n\n    :param path:\n        Path string.\n    :param recursive:\n        ``True`` if watch is recursive; ``False`` otherwise.\n    ",
        "klass": "watchdog.observers.api.ObservedWatch",
        "module": "watchdog"
    },
    {
        "base_classes": [
            "queue.Queue"
        ],
        "class_docstring": "Thread-safe implementation of an special queue where a\n    put of the last-item put'd will be dropped.\n\n    The implementation leverages locking already implemented in the base class\n    redefining only the primitives.\n\n    Queued items must be immutable and hashable so that they can be used\n    as dictionary keys. You must implement **only read-only properties** and\n    the :meth:`Item.__hash__()`, :meth:`Item.__eq__()`, and\n    :meth:`Item.__ne__()` methods for items to be hashable.\n\n    An example implementation follows::\n\n        class Item(object):\n            def __init__(self, a, b):\n                self._a = a\n                self._b = b\n\n            @property\n            def a(self):\n                return self._a\n\n            @property\n            def b(self):\n                return self._b\n\n            def _key(self):\n                return (self._a, self._b)\n\n            def __eq__(self, item):\n                return self._key() == item._key()\n\n            def __ne__(self, item):\n                return self._key() != item._key()\n\n            def __hash__(self):\n                return hash(self._key())\n\n    based on the OrderedSetQueue below\n    ",
        "klass": "watchdog.utils.bricks.SkipRepeatsQueue",
        "module": "watchdog"
    },
    {
        "base_classes": [
            "watchdog.observers.api.BaseObserver"
        ],
        "class_docstring": "\n    Platform-independent observer that polls a directory to detect file\n    system changes.\n    ",
        "klass": "watchdog.observers.polling.PollingObserver",
        "module": "watchdog"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An :class:`Arrow <arrow.arrow.Arrow>` object.\n\n    Implements the ``datetime`` interface, behaving as an aware ``datetime`` while implementing\n    additional functionality.\n\n    :param year: the calendar year.\n    :param month: the calendar month.\n    :param day: the calendar day.\n    :param hour: (optional) the hour. Defaults to 0.\n    :param minute: (optional) the minute, Defaults to 0.\n    :param second: (optional) the second, Defaults to 0.\n    :param microsecond: (optional) the microsecond. Defaults 0.\n    :param tzinfo: (optional) A timezone expression.  Defaults to UTC.\n\n    .. _tz-expr:\n\n    Recognized timezone expressions:\n\n        - A ``tzinfo`` object.\n        - A ``str`` describing a timezone, similar to 'US/Pacific', or 'Europe/Berlin'.\n        - A ``str`` in ISO 8601 style, as in '+07:00'.\n        - A ``str``, one of the following:  'local', 'utc', 'UTC'.\n\n    Usage::\n\n        >>> import arrow\n        >>> arrow.Arrow(2013, 5, 5, 12, 30, 45)\n        <Arrow [2013-05-05T12:30:45+00:00]>\n\n    ",
        "klass": "arrow.Arrow",
        "module": "arrow"
    },
    {
        "base_classes": [
            "sympy.tensor.array.dense_ndim_array.DenseNDimArray",
            "sympy.tensor.array.ndim_array.ImmutableNDimArray"
        ],
        "class_docstring": "\n\n    ",
        "klass": "sympy.tensor.array.dense_ndim_array.ImmutableDenseNDimArray",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.symbol.Symbol"
        ],
        "class_docstring": "Dummy symbols are each unique, even if they have the same name:\n\n    >>> from sympy import Dummy\n    >>> Dummy(\"x\") == Dummy(\"x\")\n    False\n\n    If a name is not supplied then a string value of an internal count will be\n    used. This is useful when a temporary variable is needed and the name\n    of the variable used in the expression is not important.\n\n    >>> Dummy() #doctest: +SKIP\n    _Dummy_10\n\n    ",
        "klass": "sympy.core.symbol.Dummy",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.expr.Expr",
            "sympy.core.compatibility.NotIterable"
        ],
        "class_docstring": "Represent the base or stem of an indexed object\n\n    The IndexedBase class represent an array that contains elements. The main purpose\n    of this class is to allow the convenient creation of objects of the Indexed\n    class.  The __getitem__ method of IndexedBase returns an instance of\n    Indexed.  Alone, without indices, the IndexedBase class can be used as a\n    notation for e.g. matrix equations, resembling what you could do with the\n    Symbol class.  But, the IndexedBase class adds functionality that is not\n    available for Symbol instances:\n\n      -  An IndexedBase object can optionally store shape information.  This can\n         be used in to check array conformance and conditions for numpy\n         broadcasting.  (TODO)\n      -  An IndexedBase object implements syntactic sugar that allows easy symbolic\n         representation of array operations, using implicit summation of\n         repeated indices.\n      -  The IndexedBase object symbolizes a mathematical structure equivalent\n         to arrays, and is recognized as such for code generation and automatic\n         compilation and wrapping.\n\n    >>> from sympy.tensor import IndexedBase, Idx\n    >>> from sympy import symbols\n    >>> A = IndexedBase('A'); A\n    A\n    >>> type(A)\n    <class 'sympy.tensor.indexed.IndexedBase'>\n\n    When an IndexedBase object receives indices, it returns an array with named\n    axes, represented by an Indexed object:\n\n    >>> i, j = symbols('i j', integer=True)\n    >>> A[i, j, 2]\n    A[i, j, 2]\n    >>> type(A[i, j, 2])\n    <class 'sympy.tensor.indexed.Indexed'>\n\n    The IndexedBase constructor takes an optional shape argument.  If given,\n    it overrides any shape information in the indices. (But not the index\n    ranges!)\n\n    >>> m, n, o, p = symbols('m n o p', integer=True)\n    >>> i = Idx('i', m)\n    >>> j = Idx('j', n)\n    >>> A[i, j].shape\n    (m, n)\n    >>> B = IndexedBase('B', shape=(o, p))\n    >>> B[i, j].shape\n    (o, p)\n\n    Assumptions can be specified with keyword arguments the same way as for Symbol:\n\n    >>> A_real = IndexedBase('A', real=True)\n    >>> A_real.is_real\n    True\n    >>> A != A_real\n    True\n\n    Assumptions can also be inherited if a Symbol is used to initialize the IndexedBase:\n\n    >>> I = symbols('I', integer=True)\n    >>> C_inherit = IndexedBase(I)\n    >>> C_explicit = IndexedBase('I', integer=True)\n    >>> C_inherit == C_explicit\n    True\n    ",
        "klass": "sympy.IndexedBase",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.expr.Expr"
        ],
        "class_docstring": "\n    Generic class for representing and operating on polynomial expressions.\n    Subclasses Expr class.\n\n    Examples\n    ========\n\n    >>> from sympy import Poly\n    >>> from sympy.abc import x, y\n\n    Create a univariate polynomial:\n\n    >>> Poly(x*(x**2 + x - 1)**2)\n    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n\n    Create a univariate polynomial with specific domain:\n\n    >>> from sympy import sqrt\n    >>> Poly(x**2 + 2*x + sqrt(3), domain='R')\n    Poly(1.0*x**2 + 2.0*x + 1.73205080756888, x, domain='RR')\n\n    Create a multivariate polynomial:\n\n    >>> Poly(y*x**2 + x*y + 1)\n    Poly(x**2*y + x*y + 1, x, y, domain='ZZ')\n\n    Create a univariate polynomial, where y is a constant:\n\n    >>> Poly(y*x**2 + x*y + 1,x)\n    Poly(y*x**2 + y*x + 1, x, domain='ZZ[y]')\n\n    You can evaluate the above polynomial as a function of y:\n\n    >>> Poly(y*x**2 + x*y + 1,x).eval(2)\n    6*y + 1\n\n    See Also\n    ========\n\n    sympy.core.expr.Expr\n\n    ",
        "klass": "sympy.polys.polytools.Poly",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.expr.Expr"
        ],
        "class_docstring": "\n    Generic class for representing and operating on polynomial expressions.\n    Subclasses Expr class.\n\n    Examples\n    ========\n\n    >>> from sympy import Poly\n    >>> from sympy.abc import x, y\n\n    Create a univariate polynomial:\n\n    >>> Poly(x*(x**2 + x - 1)**2)\n    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n\n    Create a univariate polynomial with specific domain:\n\n    >>> from sympy import sqrt\n    >>> Poly(x**2 + 2*x + sqrt(3), domain='R')\n    Poly(1.0*x**2 + 2.0*x + 1.73205080756888, x, domain='RR')\n\n    Create a multivariate polynomial:\n\n    >>> Poly(y*x**2 + x*y + 1)\n    Poly(x**2*y + x*y + 1, x, y, domain='ZZ')\n\n    Create a univariate polynomial, where y is a constant:\n\n    >>> Poly(y*x**2 + x*y + 1,x)\n    Poly(y*x**2 + y*x + 1, x, domain='ZZ[y]')\n\n    You can evaluate the above polynomial as a function of y:\n\n    >>> Poly(y*x**2 + x*y + 1,x).eval(2)\n    6*y + 1\n\n    See Also\n    ========\n\n    sympy.core.expr.Expr\n\n    ",
        "klass": "sympy.Poly",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.polys.polytools.Poly"
        ],
        "class_docstring": "Class for representing pure polynomials. ",
        "klass": "sympy.polys.polytools.PurePoly",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.concrete.expr_with_limits.AddWithLimits",
            "sympy.concrete.expr_with_intlimits.ExprWithIntLimits"
        ],
        "class_docstring": "Represents unevaluated summation.\n\n    ``Sum`` represents a finite or infinite series, with the first argument\n    being the general form of terms in the series, and the second argument\n    being ``(dummy_variable, start, end)``, with ``dummy_variable`` taking\n    all integer values from ``start`` through ``end``. In accordance with\n    long-standing mathematical convention, the end term is included in the\n    summation.\n\n    Finite sums\n    ===========\n\n    For finite sums (and sums with symbolic limits assumed to be finite) we\n    follow the summation convention described by Karr [1], especially\n    definition 3 of section 1.4. The sum:\n\n    .. math::\n\n        \\sum_{m \\leq i < n} f(i)\n\n    has *the obvious meaning* for `m < n`, namely:\n\n    .. math::\n\n        \\sum_{m \\leq i < n} f(i) = f(m) + f(m+1) + \\ldots + f(n-2) + f(n-1)\n\n    with the upper limit value `f(n)` excluded. The sum over an empty set is\n    zero if and only if `m = n`:\n\n    .. math::\n\n        \\sum_{m \\leq i < n} f(i) = 0  \\quad \\mathrm{for} \\quad  m = n\n\n    Finally, for all other sums over empty sets we assume the following\n    definition:\n\n    .. math::\n\n        \\sum_{m \\leq i < n} f(i) = - \\sum_{n \\leq i < m} f(i)  \\quad \\mathrm{for} \\quad  m > n\n\n    It is important to note that Karr defines all sums with the upper\n    limit being exclusive. This is in contrast to the usual mathematical notation,\n    but does not affect the summation convention. Indeed we have:\n\n    .. math::\n\n        \\sum_{m \\leq i < n} f(i) = \\sum_{i = m}^{n - 1} f(i)\n\n    where the difference in notation is intentional to emphasize the meaning,\n    with limits typeset on the top being inclusive.\n\n    Examples\n    ========\n\n    >>> from sympy.abc import i, k, m, n, x\n    >>> from sympy import Sum, factorial, oo, IndexedBase, Function\n    >>> Sum(k, (k, 1, m))\n    Sum(k, (k, 1, m))\n    >>> Sum(k, (k, 1, m)).doit()\n    m**2/2 + m/2\n    >>> Sum(k**2, (k, 1, m))\n    Sum(k**2, (k, 1, m))\n    >>> Sum(k**2, (k, 1, m)).doit()\n    m**3/3 + m**2/2 + m/6\n    >>> Sum(x**k, (k, 0, oo))\n    Sum(x**k, (k, 0, oo))\n    >>> Sum(x**k, (k, 0, oo)).doit()\n    Piecewise((1/(1 - x), Abs(x) < 1), (Sum(x**k, (k, 0, oo)), True))\n    >>> Sum(x**k/factorial(k), (k, 0, oo)).doit()\n    exp(x)\n\n    Here are examples to do summation with symbolic indices.  You\n    can use either Function of IndexedBase classes:\n\n    >>> f = Function('f')\n    >>> Sum(f(n), (n, 0, 3)).doit()\n    f(0) + f(1) + f(2) + f(3)\n    >>> Sum(f(n), (n, 0, oo)).doit()\n    Sum(f(n), (n, 0, oo))\n    >>> f = IndexedBase('f')\n    >>> Sum(f[n]**2, (n, 0, 3)).doit()\n    f[0]**2 + f[1]**2 + f[2]**2 + f[3]**2\n\n    An example showing that the symbolic result of a summation is still\n    valid for seemingly nonsensical values of the limits. Then the Karr\n    convention allows us to give a perfectly valid interpretation to\n    those sums by interchanging the limits according to the above rules:\n\n    >>> S = Sum(i, (i, 1, n)).doit()\n    >>> S\n    n**2/2 + n/2\n    >>> S.subs(n, -4)\n    6\n    >>> Sum(i, (i, 1, -4)).doit()\n    6\n    >>> Sum(-i, (i, -3, 0)).doit()\n    6\n\n    An explicit example of the Karr summation convention:\n\n    >>> S1 = Sum(i**2, (i, m, m+n-1)).doit()\n    >>> S1\n    m**2*n + m*n**2 - m*n + n**3/3 - n**2/2 + n/6\n    >>> S2 = Sum(i**2, (i, m+n, m-1)).doit()\n    >>> S2\n    -m**2*n - m*n**2 + m*n - n**3/3 + n**2/2 - n/6\n    >>> S1 + S2\n    0\n    >>> S3 = Sum(i, (i, m, m-1)).doit()\n    >>> S3\n    0\n\n    See Also\n    ========\n\n    summation\n    Product, sympy.concrete.products.product\n\n    References\n    ==========\n\n    .. [1] Michael Karr, \"Summation in Finite Terms\", Journal of the ACM,\n           Volume 28 Issue 2, April 1981, Pages 305-350\n           http://dl.acm.org/citation.cfm?doid=322248.322255\n    .. [2] https://en.wikipedia.org/wiki/Summation#Capital-sigma_notation\n    .. [3] https://en.wikipedia.org/wiki/Empty_sum\n    ",
        "klass": "sympy.Sum",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.expr.AtomicExpr",
            "sympy.logic.boolalg.Boolean"
        ],
        "class_docstring": "\n    Assumptions:\n       commutative = True\n\n    You can override the default assumptions in the constructor:\n\n    >>> from sympy import symbols\n    >>> A,B = symbols('A,B', commutative = False)\n    >>> bool(A*B != B*A)\n    True\n    >>> bool(A*B*2 == 2*A*B) == True # multiplication by scalars is commutative\n    True\n\n    ",
        "klass": "sympy.Symbol",
        "module": "sympy"
    },
    {
        "base_classes": [
            "sympy.core.basic.Basic"
        ],
        "class_docstring": "\n    Wrapper around the builtin tuple object\n\n    The Tuple is a subclass of Basic, so that it works well in the\n    SymPy framework.  The wrapped tuple is available as self.args, but\n    you can also access elements or slices with [:] syntax.\n\n    Parameters\n    ==========\n\n    sympify : bool\n        If ``False``, ``sympify`` is not called on ``args``. This\n        can be used for speedups for very large tuples where the\n        elements are known to already be sympy objects.\n\n    Example\n    =======\n\n    >>> from sympy import symbols\n    >>> from sympy.core.containers import Tuple\n    >>> a, b, c, d = symbols('a b c d')\n    >>> Tuple(a, b, c)[1:]\n    (b, c)\n    >>> Tuple(a, b, c).subs(a, d)\n    (d, b, c)\n\n    ",
        "klass": "sympy.Tuple",
        "module": "sympy"
    },
    {
        "base_classes": [
            "TrigonometricFunction"
        ],
        "class_docstring": "\n    The cosine function.\n\n    Returns the cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cos, pi\n    >>> from sympy.abc import x\n    >>> cos(x**2).diff(x)\n    -2*x*sin(x**2)\n    >>> cos(1).diff(x)\n    0\n    >>> cos(pi)\n    -1\n    >>> cos(pi/2)\n    0\n    >>> cos(2*pi/3)\n    -1/2\n    >>> cos(pi/12)\n    sqrt(2)/4 + sqrt(6)/4\n\n    See Also\n    ========\n\n    sin, csc, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cos\n\n    ",
        "klass": "cos",
        "module": "cos"
    },
    {
        "base_classes": [
            "TrigonometricFunction"
        ],
        "class_docstring": "\n    The cosine function.\n\n    Returns the cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cos, pi\n    >>> from sympy.abc import x\n    >>> cos(x**2).diff(x)\n    -2*x*sin(x**2)\n    >>> cos(1).diff(x)\n    0\n    >>> cos(pi)\n    -1\n    >>> cos(pi/2)\n    0\n    >>> cos(2*pi/3)\n    -1/2\n    >>> cos(pi/12)\n    sqrt(2)/4 + sqrt(6)/4\n\n    See Also\n    ========\n\n    sin, csc, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cos\n\n    ",
        "klass": "sympy.cos",
        "module": "cos"
    },
    {
        "base_classes": [
            "ExpBase"
        ],
        "class_docstring": "\n    The exponential function, :math:`e^x`.\n\n    See Also\n    ========\n\n    log\n    ",
        "klass": "sympy.exp",
        "module": "exp"
    },
    {
        "base_classes": [
            "Function"
        ],
        "class_docstring": "\n    The gamma function\n\n    .. math::\n        \\Gamma(x) := \\int^{\\infty}_{0} t^{x-1} e^{-t} \\mathrm{d}t.\n\n    The ``gamma`` function implements the function which passes through the\n    values of the factorial function, i.e. `\\Gamma(n) = (n - 1)!` when n is\n    an integer. More general, `\\Gamma(z)` is defined in the whole complex\n    plane except at the negative integers where there are simple poles.\n\n    Examples\n    ========\n\n    >>> from sympy import S, I, pi, oo, gamma\n    >>> from sympy.abc import x\n\n    Several special values are known:\n\n    >>> gamma(1)\n    1\n    >>> gamma(4)\n    6\n    >>> gamma(S(3)/2)\n    sqrt(pi)/2\n\n    The Gamma function obeys the mirror symmetry:\n\n    >>> from sympy import conjugate\n    >>> conjugate(gamma(x))\n    gamma(conjugate(x))\n\n    Differentiation with respect to x is supported:\n\n    >>> from sympy import diff\n    >>> diff(gamma(x), x)\n    gamma(x)*polygamma(0, x)\n\n    Series expansion is also supported:\n\n    >>> from sympy import series\n    >>> series(gamma(x), x, 0, 3)\n    1/x - EulerGamma + x*(EulerGamma**2/2 + pi**2/12) + x**2*(-EulerGamma*pi**2/12 + polygamma(2, 1)/6 - EulerGamma**3/6) + O(x**3)\n\n    We can numerically evaluate the gamma function to arbitrary precision\n    on the whole complex plane:\n\n    >>> gamma(pi).evalf(40)\n    2.288037795340032417959588909060233922890\n    >>> gamma(1+I).evalf(20)\n    0.49801566811835604271 - 0.15494982830181068512*I\n\n    See Also\n    ========\n\n    lowergamma: Lower incomplete gamma function.\n    uppergamma: Upper incomplete gamma function.\n    polygamma: Polygamma function.\n    loggamma: Log Gamma function.\n    digamma: Digamma function.\n    trigamma: Trigamma function.\n    sympy.functions.special.beta_functions.beta: Euler Beta function.\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Gamma_function\n    .. [2] http://dlmf.nist.gov/5\n    .. [3] http://mathworld.wolfram.com/GammaFunction.html\n    .. [4] http://functions.wolfram.com/GammaBetaErf/Gamma/\n    ",
        "klass": "gamma",
        "module": "gamma"
    },
    {
        "base_classes": [
            "TupleParametersBase"
        ],
        "class_docstring": "\n    The (generalized) hypergeometric function is defined by a series where\n    the ratios of successive terms are a rational function of the summation\n    index. When convergent, it is continued analytically to the largest\n    possible domain.\n\n    The hypergeometric function depends on two vectors of parameters, called\n    the numerator parameters :math:`a_p`, and the denominator parameters\n    :math:`b_q`. It also has an argument :math:`z`. The series definition is\n\n    .. math ::\n        {}_pF_q\\left(\\begin{matrix} a_1, \\cdots, a_p \\\\ b_1, \\cdots, b_q \\end{matrix}\n                     \\middle| z \\right)\n        = \\sum_{n=0}^\\infty \\frac{(a_1)_n \\cdots (a_p)_n}{(b_1)_n \\cdots (b_q)_n}\n                            \\frac{z^n}{n!},\n\n    where :math:`(a)_n = (a)(a+1)\\cdots(a+n-1)` denotes the rising factorial.\n\n    If one of the :math:`b_q` is a non-positive integer then the series is\n    undefined unless one of the `a_p` is a larger (i.e. smaller in\n    magnitude) non-positive integer. If none of the :math:`b_q` is a\n    non-positive integer and one of the :math:`a_p` is a non-positive\n    integer, then the series reduces to a polynomial. To simplify the\n    following discussion, we assume that none of the :math:`a_p` or\n    :math:`b_q` is a non-positive integer. For more details, see the\n    references.\n\n    The series converges for all :math:`z` if :math:`p \\le q`, and thus\n    defines an entire single-valued function in this case. If :math:`p =\n    q+1` the series converges for :math:`|z| < 1`, and can be continued\n    analytically into a half-plane. If :math:`p > q+1` the series is\n    divergent for all :math:`z`.\n\n    Note: The hypergeometric function constructor currently does *not* check\n    if the parameters actually yield a well-defined function.\n\n    Examples\n    ========\n\n    The parameters :math:`a_p` and :math:`b_q` can be passed as arbitrary\n    iterables, for example:\n\n    >>> from sympy.functions import hyper\n    >>> from sympy.abc import x, n, a\n    >>> hyper((1, 2, 3), [3, 4], x)\n    hyper((1, 2, 3), (3, 4), x)\n\n    There is also pretty printing (it looks better using unicode):\n\n    >>> from sympy import pprint\n    >>> pprint(hyper((1, 2, 3), [3, 4], x), use_unicode=False)\n      _\n     |_  /1, 2, 3 |  \\\n     |   |        | x|\n    3  2 \\  3, 4  |  /\n\n    The parameters must always be iterables, even if they are vectors of\n    length one or zero:\n\n    >>> hyper((1, ), [], x)\n    hyper((1,), (), x)\n\n    But of course they may be variables (but if they depend on x then you\n    should not expect much implemented functionality):\n\n    >>> hyper((n, a), (n**2,), x)\n    hyper((n, a), (n**2,), x)\n\n    The hypergeometric function generalizes many named special functions.\n    The function hyperexpand() tries to express a hypergeometric function\n    using named special functions.\n    For example:\n\n    >>> from sympy import hyperexpand\n    >>> hyperexpand(hyper([], [], x))\n    exp(x)\n\n    You can also use expand_func:\n\n    >>> from sympy import expand_func\n    >>> expand_func(x*hyper([1, 1], [2], -x))\n    log(x + 1)\n\n    More examples:\n\n    >>> from sympy import S\n    >>> hyperexpand(hyper([], [S(1)/2], -x**2/4))\n    cos(x)\n    >>> hyperexpand(x*hyper([S(1)/2, S(1)/2], [S(3)/2], x**2))\n    asin(x)\n\n    We can also sometimes hyperexpand parametric functions:\n\n    >>> from sympy.abc import a\n    >>> hyperexpand(hyper([-a], [], x))\n    (1 - x)**a\n\n    See Also\n    ========\n\n    sympy.simplify.hyperexpand\n    sympy.functions.special.gamma_functions.gamma\n    meijerg\n\n    References\n    ==========\n\n    .. [1] Luke, Y. L. (1969), The Special Functions and Their Approximations,\n           Volume 1\n    .. [2] https://en.wikipedia.org/wiki/Generalized_hypergeometric_function\n    ",
        "klass": "hyper",
        "module": "hyper"
    },
    {
        "base_classes": [
            "Function"
        ],
        "class_docstring": "\n    The natural logarithm function `\\ln(x)` or `\\log(x)`.\n\n    Logarithms are taken with the natural base, `e`. To get\n    a logarithm of a different base ``b``, use ``log(x, b)``,\n    which is essentially short-hand for ``log(x)/log(b)``.\n\n    ``log`` represents the principal branch of the natural\n    logarithm. As such it has a branch cut along the negative\n    real axis and returns values having a complex argument in\n    `(-\\pi, \\pi]`.\n\n    Examples\n    ========\n\n    >>> from sympy import log, sqrt, S, I\n    >>> log(8, 2)\n    3\n    >>> log(S(8)/3, 2)\n    -log(3)/log(2) + 3\n    >>> log(-1 + I*sqrt(3))\n    log(2) + 2*I*pi/3\n\n    See Also\n    ========\n\n    exp\n\n    ",
        "klass": "log",
        "module": "log"
    },
    {
        "base_classes": [
            "Function"
        ],
        "class_docstring": "\n    The ``loggamma`` function implements the logarithm of the\n    gamma function i.e, `\\log\\Gamma(x)`.\n\n    Examples\n    ========\n\n    Several special values are known. For numerical integral\n    arguments we have:\n\n    >>> from sympy import loggamma\n    >>> loggamma(-2)\n    oo\n    >>> loggamma(0)\n    oo\n    >>> loggamma(1)\n    0\n    >>> loggamma(2)\n    0\n    >>> loggamma(3)\n    log(2)\n\n    and for symbolic values:\n\n    >>> from sympy import Symbol\n    >>> n = Symbol(\"n\", integer=True, positive=True)\n    >>> loggamma(n)\n    log(gamma(n))\n    >>> loggamma(-n)\n    oo\n\n    for half-integral values:\n\n    >>> from sympy import S, pi\n    >>> loggamma(S(5)/2)\n    log(3*sqrt(pi)/4)\n    >>> loggamma(n/2)\n    log(2**(1 - n)*sqrt(pi)*gamma(n)/gamma(n/2 + 1/2))\n\n    and general rational arguments:\n\n    >>> from sympy import expand_func\n    >>> L = loggamma(S(16)/3)\n    >>> expand_func(L).doit()\n    -5*log(3) + loggamma(1/3) + log(4) + log(7) + log(10) + log(13)\n    >>> L = loggamma(S(19)/4)\n    >>> expand_func(L).doit()\n    -4*log(4) + loggamma(3/4) + log(3) + log(7) + log(11) + log(15)\n    >>> L = loggamma(S(23)/7)\n    >>> expand_func(L).doit()\n    -3*log(7) + log(2) + loggamma(2/7) + log(9) + log(16)\n\n    The loggamma function has the following limits towards infinity:\n\n    >>> from sympy import oo\n    >>> loggamma(oo)\n    oo\n    >>> loggamma(-oo)\n    zoo\n\n    The loggamma function obeys the mirror symmetry\n    if `x \\in \\mathbb{C} \\setminus \\{-\\infty, 0\\}`:\n\n    >>> from sympy.abc import x\n    >>> from sympy import conjugate\n    >>> conjugate(loggamma(x))\n    loggamma(conjugate(x))\n\n    Differentiation with respect to x is supported:\n\n    >>> from sympy import diff\n    >>> diff(loggamma(x), x)\n    polygamma(0, x)\n\n    Series expansion is also supported:\n\n    >>> from sympy import series\n    >>> series(loggamma(x), x, 0, 4)\n    -log(x) - EulerGamma*x + pi**2*x**2/12 + x**3*polygamma(2, 1)/6 + O(x**4)\n\n    We can numerically evaluate the gamma function to arbitrary precision\n    on the whole complex plane:\n\n    >>> from sympy import I\n    >>> loggamma(5).evalf(30)\n    3.17805383034794561964694160130\n    >>> loggamma(I).evalf(20)\n    -0.65092319930185633889 - 1.8724366472624298171*I\n\n    See Also\n    ========\n\n    gamma: Gamma function.\n    lowergamma: Lower incomplete gamma function.\n    uppergamma: Upper incomplete gamma function.\n    polygamma: Polygamma function.\n    digamma: Digamma function.\n    trigamma: Trigamma function.\n    sympy.functions.special.beta_functions.beta: Euler Beta function.\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Gamma_function\n    .. [2] http://dlmf.nist.gov/5\n    .. [3] http://mathworld.wolfram.com/LogGammaFunction.html\n    .. [4] http://functions.wolfram.com/GammaBetaErf/LogGamma/\n    ",
        "klass": "sympy.loggamma",
        "module": "loggamma"
    },
    {
        "base_classes": [
            "Function"
        ],
        "class_docstring": "\n    Partition numbers\n\n    The Partition numbers are a sequence of integers `p_n` that represent the\n    number of distinct ways of representing `n` as a sum of natural numbers\n    (with order irrelevant). The generating function for `p_n` is given by:\n\n    .. math:: \\sum_{n=0}^\\infty p_n x^n = \\prod_{k=1}^\\infty (1 - x^k)^{-1}\n\n    Examples\n    ========\n\n    >>> from sympy import Symbol\n    >>> from sympy.functions import partition\n    >>> [partition(n) for n in range(9)]\n    [1, 1, 2, 3, 5, 7, 11, 15, 22]\n    >>> n = Symbol('n', integer=True, negative=True)\n    >>> partition(n)\n    0\n\n    See Also\n    ========\n\n    bell, bernoulli, catalan, euler, fibonacci, harmonic, lucas, genocchi, tribonacci\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Partition_(number_theory%29\n    .. [2] https://en.wikipedia.org/wiki/Pentagonal_number_theorem\n\n    ",
        "klass": "partition",
        "module": "partition"
    },
    {
        "base_classes": [
            "ReciprocalTrigonometricFunction"
        ],
        "class_docstring": "\n    The secant function.\n\n    Returns the secant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import sec\n    >>> from sympy.abc import x\n    >>> sec(x**2).diff(x)\n    2*x*tan(x**2)*sec(x**2)\n    >>> sec(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, csc, cos, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sec\n\n    ",
        "klass": "sec",
        "module": "sec"
    },
    {
        "base_classes": [
            "TrigonometricFunction"
        ],
        "class_docstring": "\n    The sine function.\n\n    Returns the sine of x (measured in radians).\n\n    Notes\n    =====\n\n    This function will evaluate automatically in the\n    case x/pi is some rational number [4]_.  For example,\n    if x is a multiple of pi, pi/2, pi/3, pi/4 and pi/6.\n\n    Examples\n    ========\n\n    >>> from sympy import sin, pi\n    >>> from sympy.abc import x\n    >>> sin(x**2).diff(x)\n    2*x*cos(x**2)\n    >>> sin(1).diff(x)\n    0\n    >>> sin(pi)\n    0\n    >>> sin(pi/2)\n    1\n    >>> sin(pi/6)\n    1/2\n    >>> sin(pi/12)\n    -sqrt(2)/4 + sqrt(6)/4\n\n\n    See Also\n    ========\n\n    csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sin\n    .. [4] http://mathworld.wolfram.com/TrigonometryAngles.html\n\n    ",
        "klass": "sympy.sin",
        "module": "sin"
    },
    {
        "base_classes": [
            "F95Function"
        ],
        "class_docstring": " Fortran merge function ",
        "klass": "merge",
        "module": "merge"
    },
    {
        "base_classes": [
            "_io._TextIOBase"
        ],
        "class_docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.",
        "klass": "sympy.core.compatibility.StringIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "range(stop) -> range object\nrange(start, stop[, step]) -> range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).",
        "klass": "sympy.core.compatibility.range",
        "module": "range"
    },
    {
        "base_classes": [
            "sympy.core.expr.Expr"
        ],
        "class_docstring": "\n    Generic class for representing and operating on polynomial expressions.\n    Subclasses Expr class.\n\n    Examples\n    ========\n\n    >>> from sympy import Poly\n    >>> from sympy.abc import x, y\n\n    Create a univariate polynomial:\n\n    >>> Poly(x*(x**2 + x - 1)**2)\n    Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n\n    Create a univariate polynomial with specific domain:\n\n    >>> from sympy import sqrt\n    >>> Poly(x**2 + 2*x + sqrt(3), domain='R')\n    Poly(1.0*x**2 + 2.0*x + 1.73205080756888, x, domain='RR')\n\n    Create a multivariate polynomial:\n\n    >>> Poly(y*x**2 + x*y + 1)\n    Poly(x**2*y + x*y + 1, x, y, domain='ZZ')\n\n    Create a univariate polynomial, where y is a constant:\n\n    >>> Poly(y*x**2 + x*y + 1,x)\n    Poly(y*x**2 + y*x + 1, x, domain='ZZ[y]')\n\n    You can evaluate the above polynomial as a function of y:\n\n    >>> Poly(y*x**2 + x*y + 1,x).eval(2)\n    6*y + 1\n\n    See Also\n    ========\n\n    sympy.core.expr.Expr\n\n    ",
        "klass": "sympy.polys.Poly",
        "module": "sympy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Has methods to ``enumerate`` and ``count`` the partitions of a multiset.\n\n    This implements a refactored and extended version of Knuth's algorithm\n    7.1.2.5M [AOCP]_.\"\n\n    The enumeration methods of this class are generators and return\n    data structures which can be interpreted by the same visitor\n    functions used for the output of ``multiset_partitions_taocp``.\n\n    Examples\n    ========\n\n    >>> from sympy.utilities.enumerative import MultisetPartitionTraverser\n    >>> m = MultisetPartitionTraverser()\n    >>> m.count_partitions([4,4,4,2])\n    127750\n    >>> m.count_partitions([3,3,3])\n    686\n\n    See Also\n    ========\n\n    multiset_partitions_taocp\n    sympy.utilities.iterables.multiset_partitions\n\n    References\n    ==========\n\n    .. [AOCP] Algorithm 7.1.2.5M in Volume 4A, Combinatoral Algorithms,\n           Part 1, of The Art of Computer Programming, by Donald Knuth.\n\n    .. [Factorisatio] On a Problem of Oppenheim concerning\n           \"Factorisatio Numerorum\" E. R. Canfield, Paul Erdos, Carl\n           Pomerance, JOURNAL OF NUMBER THEORY, Vol. 17, No. 1. August\n           1983.  See section 7 for a description of an algorithm\n           similar to Knuth's.\n\n    .. [Yorgey] Generating Multiset Partitions, Brent Yorgey, The\n           Monad.Reader, Issue 8, September 2007.\n\n    ",
        "klass": "sympy.utilities.enumerative.MultisetPartitionTraverser",
        "module": "sympy"
    },
    {
        "base_classes": [
            "ginga.BaseImage.BaseImage"
        ],
        "class_docstring": "\n    Abstraction of an astronomical data (image).\n\n    NOTE: this module is NOT thread-safe!\n    ",
        "klass": "ginga.AstroImage.AstroImage",
        "module": "ginga"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to represent tables of heterogeneous data.\n\n    `~astropy.table.Table` provides a class for heterogeneous tabular data.\n    A key enhancement provided by the `~astropy.table.Table` class over\n    e.g. a `numpy` structured array is the ability to easily modify the\n    structure of the table by adding or removing columns, or adding new\n    rows of data.  In addition table and column metadata are fully supported.\n\n    `~astropy.table.Table` differs from `~astropy.nddata.NDData` by the\n    assumption that the input data consists of columns of homogeneous data,\n    where each column has a unique identifier and may contain additional\n    metadata such as the data unit, format, and description.\n\n    See also: http://docs.astropy.org/en/stable/table/\n\n    Parameters\n    ----------\n    data : numpy ndarray, dict, list, Table, or table-like object, optional\n        Data to initialize table.\n    masked : bool, optional\n        Specify whether the table is masked.\n    names : list, optional\n        Specify column names.\n    dtype : list, optional\n        Specify column data types.\n    meta : dict, optional\n        Metadata associated with the table.\n    copy : bool, optional\n        Copy the input data. If the input is a Table the ``meta`` is always\n        copied regardless of the ``copy`` parameter.\n        Default is True.\n    rows : numpy ndarray, list of lists, optional\n        Row-oriented data for table instead of ``data`` argument.\n    copy_indices : bool, optional\n        Copy any indices in the input data. Default is True.\n    **kwargs : dict, optional\n        Additional keyword args when converting table-like object.\n    ",
        "klass": "astropy.table.table.Table",
        "module": "astropy"
    },
    {
        "base_classes": [
            "pyscf.lib.misc.StreamObject"
        ],
        "class_docstring": "restricted CCSD\n\n    Attributes:\n        verbose : int\n            Print level.  Default value equals to :class:`Mole.verbose`\n        max_memory : float or int\n            Allowed memory in MB.  Default value equals to :class:`Mole.max_memory`\n        conv_tol : float\n            converge threshold.  Default is 1e-7.\n        conv_tol_normt : float\n            converge threshold for norm(t1,t2).  Default is 1e-5.\n        max_cycle : int\n            max number of iterations.  Default is 50.\n        diis_space : int\n            DIIS space size.  Default is 6.\n        diis_start_cycle : int\n            The step to start DIIS.  Default is 0.\n        iterative_damping : float\n            The self consistent damping parameter.\n        direct : bool\n            AO-direct CCSD. Default is False.\n        async_io : bool\n            Allow for asynchronous function execution. Default is True.\n        incore_complete : bool\n            Avoid all I/O (also for DIIS). Default is False.\n        level_shift : float\n            A shift on virtual orbital energies to stablize the CCSD iteration\n        frozen : int or list\n            If integer is given, the inner-most orbitals are frozen from CC\n            amplitudes.  Given the orbital indices (0-based) in a list, both\n            occupied and virtual orbitals can be frozen in CC calculation.\n\n            >>> mol = gto.M(atom = 'H 0 0 0; F 0 0 1.1', basis = 'ccpvdz')\n            >>> mf = scf.RHF(mol).run()\n            >>> # freeze 2 core orbitals\n            >>> mycc = cc.CCSD(mf).set(frozen = 2).run()\n            >>> # freeze 2 core orbitals and 3 high lying unoccupied orbitals\n            >>> mycc.set(frozen = [0,1,16,17,18]).run()\n\n    Saved results\n\n        converged : bool\n            CCSD converged or not\n        e_corr : float\n            CCSD correlation correction\n        e_tot : float\n            Total CCSD energy (HF + correlation)\n        t1, t2 :\n            T amplitudes t1[i,a], t2[i,j,a,b]  (i,j in occ, a,b in virt)\n        l1, l2 :\n            Lambda amplitudes l1[i,a], l2[i,j,a,b]  (i,j in occ, a,b in virt)\n    ",
        "klass": "pyscf.cc.ccsd.CCSD",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Direct inversion in the iterative subspace method.\n\n    Attributes:\n        space : int\n            DIIS subspace size. The maximum number of the vectors to be stored.\n        min_space\n            The minimal size of subspace before DIIS extrapolation.\n\n    Functions:\n        update(x, xerr=None) :\n            If xerr the error vector is given, this function will push the target\n            vector and error vector in the DIIS subspace, and use the error vector\n            to extrapolate the vector and return the extrapolated vector.\n            If xerr is None, this function will take the difference between\n            the current given vector and the last given vector as the error\n            vector to extrapolate the vector.\n\n    Examples:\n\n    >>> from pyscf import gto, scf, lib\n    >>> mol = gto.M(atom='H 0 0 0; H 0 0 1', basis='ccpvdz')\n    >>> mf = scf.RHF(mol)\n    >>> h = mf.get_hcore()\n    >>> s = mf.get_ovlp()\n    >>> e, c = mf.eig(h, s)\n    >>> occ = mf.get_occ(e, c)\n    >>> # DIIS without error vector\n    >>> adiis = lib.diis.DIIS()\n    >>> for i in range(7):\n    ...     dm = mf.make_rdm1(c, occ)\n    ...     f = h + mf.get_veff(mol, dm)\n    ...     if i > 1:\n    ...         f = adiis.update(f)\n    ...     e, c = mf.eig(f, s)\n    ...     print('E_%d = %.12f' % (i, mf.energy_tot(dm, h, mf.get_veff(mol, dm))))\n    E_0 = -1.050329433306\n    E_1 = -1.098566175145\n    E_2 = -1.100103795287\n    E_3 = -1.100152104615\n    E_4 = -1.100153706922\n    E_5 = -1.100153764848\n    E_6 = -1.100153764878\n\n    >>> # Take Hartree-Fock gradients as the error vector\n    >>> adiis = lib.diis.DIIS()\n    >>> for i in range(7):\n    ...     dm = mf.make_rdm1(c, occ)\n    ...     f = h + mf.get_veff(mol, dm)\n    ...     if i > 1:\n    ...         f = adiis.update(f, mf.get_grad(c, occ, f))\n    ...     e, c = mf.eig(f, s)\n    ...     print('E_%d = %.12f' % (i, mf.energy_tot(dm, h, mf.get_veff(mol, dm))))\n    E_0 = -1.050329433306\n    E_1 = -1.098566175145\n    E_2 = -1.100103795287\n    E_3 = -1.100152104615\n    E_4 = -1.100153763813\n    E_5 = -1.100153764878\n    E_6 = -1.100153764878\n    ",
        "klass": "pyscf.lib.diis.DIIS",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.lib.misc.StreamObject"
        ],
        "class_docstring": "restricted CISD\n\n    Attributes:\n        verbose : int\n            Print level.  Default value equals to :class:`Mole.verbose`\n        max_memory : float or int\n            Allowed memory in MB.  Default value equals to :class:`Mole.max_memory`\n        conv_tol : float\n            converge threshold.  Default is 1e-9.\n        max_cycle : int\n            max number of iterations.  Default is 50.\n        max_space : int\n            Davidson diagonalization space size.  Default is 12.\n        direct : bool\n            AO-direct CISD. Default is False.\n        async_io : bool\n            Allow for asynchronous function execution. Default is True.\n        frozen : int or list\n            If integer is given, the inner-most orbitals are frozen from CI\n            amplitudes.  Given the orbital indices (0-based) in a list, both\n            occupied and virtual orbitals can be frozen in CI calculation.\n\n            >>> mol = gto.M(atom = 'H 0 0 0; F 0 0 1.1', basis = 'ccpvdz')\n            >>> mf = scf.RHF(mol).run()\n            >>> # freeze 2 core orbitals\n            >>> myci = ci.CISD(mf).set(frozen = 2).run()\n            >>> # freeze 2 core orbitals and 3 high lying unoccupied orbitals\n            >>> myci.set(frozen = [0,1,16,17,18]).run()\n\n    Saved results\n\n        converged : bool\n            CISD converged or not\n        e_corr : float\n            CISD correlation correction\n        e_tot : float\n            Total CCSD energy (HF + correlation)\n        ci :\n            CI wavefunction coefficients\n    ",
        "klass": "pyscf.ci.cisd.CISD",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.lib.misc.StreamObject"
        ],
        "class_docstring": "Basic class to hold molecular structure and global options\n\n    Attributes:\n        verbose : int\n            Print level\n        output : str or None\n            Output file, default is None which dumps msg to sys.stdout\n        max_memory : int, float\n            Allowed memory in MB\n        charge : int\n            Charge of molecule. It affects the electron numbers\n        spin : int\n            2S, num. alpha electrons - num. beta electrons\n        symmetry : bool or str\n            Whether to use symmetry.  When this variable is set to True, the\n            molecule will be rotated and the highest rotation axis will be\n            placed z-axis.\n            If a string is given as the name of point group, the given point\n            group symmetry will be used.  Note that the input molecular\n            coordinates will not be changed in this case.\n        symmetry_subgroup : str\n            subgroup\n\n        atom : list or str\n            To define molecluar structure.  The internal format is\n\n            | atom = [[atom1, (x, y, z)],\n            |         [atom2, (x, y, z)],\n            |         ...\n            |         [atomN, (x, y, z)]]\n\n        unit : str\n            Angstrom or Bohr\n        basis : dict or str\n            To define basis set.\n        nucmod : dict or str or [function(nuc_charge, nucprop) => zeta]\n            Nuclear model.  0 or None means point nuclear model.  Other\n            values will enable Gaussian nuclear model.  If a function is\n            assigned to this attribute, the function will be called to\n            generate the nuclear charge distribution value \"zeta\" and the\n            relevant nuclear model will be set to Gaussian model.\n            Default is point nuclear model.\n        nucprop : dict\n            Nuclear properties (like g-factor 'g', quadrupole moments 'Q').\n            It is needed by pyscf.prop module and submodules.\n        cart : boolean\n            Using Cartesian GTO basis and integrals (6d,10f,15g)\n\n        ** Following attributes are generated by :func:`Mole.build` **\n\n        stdout : file object\n            Default is sys.stdout if :attr:`Mole.output` is not set\n        topgroup : str\n            Point group of the system.\n        groupname : str\n            The supported subgroup of the point group. It can be one of Dooh,\n            Coov, D2h, C2h, C2v, D2, Cs, Ci, C2, C1\n        nelectron : int\n            sum of nuclear charges - :attr:`Mole.charge`\n        symm_orb : a list of numpy.ndarray\n            Symmetry adapted basis.  Each element is a set of symm-adapted orbitals\n            for one irreducible representation.  The list index does **not** correspond\n            to the id of irreducible representation.\n        irrep_id : a list of int\n            Each element is one irreducible representation id associated with the basis\n            stored in symm_orb.  One irrep id stands for one irreducible representation\n            symbol.  The irrep symbol and the relevant id are defined in\n            :attr:`symm.param.IRREP_ID_TABLE`\n        irrep_name : a list of str\n            Each element is one irreducible representation symbol associated with the basis\n            stored in symm_orb.  The irrep symbols are defined in\n            :attr:`symm.param.IRREP_ID_TABLE`\n        _built : bool\n            To label whether :func:`Mole.build` has been called.  It is to\n            ensure certain functions being initialized only once.\n        _basis : dict\n            like :attr:`Mole.basis`, the internal format which is returned from the\n            parser :func:`format_basis`\n        _keys : a set of str\n            Store the keys appeared in the module.  It is used to check misinput attributes\n\n        ** Following attributes are arguments used by ``libcint`` library **\n\n        _atm :\n            :code:`[[charge, ptr-of-coord, nuc-model, ptr-zeta, 0, 0], [...]]`\n            each element reperesents one atom\n        natm :\n            number of atoms\n        _bas :\n            :code:`[[atom-id, angular-momentum, num-primitive-GTO, num-contracted-GTO, 0, ptr-of-exps, ptr-of-contract-coeff, 0], [...]]`\n            each element reperesents one shell\n        nbas :\n            number of shells\n        _env :\n            list of floats to store the coordinates, GTO exponents, contract-coefficients\n\n    Examples:\n\n    >>> mol = Mole(atom='H^2 0 0 0; H 0 0 1.1', basis='sto3g').build()\n    >>> print(mol.atom_symbol(0))\n    H^2\n    >>> print(mol.atom_pure_symbol(0))\n    H\n    >>> print(mol.nao_nr())\n    2\n    >>> print(mol.intor('int1e_ovlp_sph'))\n    [[ 0.99999999  0.43958641]\n     [ 0.43958641  0.99999999]]\n    >>> mol.charge = 1\n    >>> mol.build()\n    <class 'pyscf.gto.mole.Mole'> has no attributes Charge\n\n    ",
        "klass": "pyscf.gto.Mole",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.lib.misc.StreamObject"
        ],
        "class_docstring": "Non-relativistic restricted Hartree-Fock hessian",
        "klass": "pyscf.hessian.rhf.Hessian",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.hessian.rhf.Hessian"
        ],
        "class_docstring": "Non-relativistic UHF hessian",
        "klass": "pyscf.hessian.uhf.Hessian",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.hessian.rhf.Hessian"
        ],
        "class_docstring": "Non-relativistic RKS hessian",
        "klass": "pyscf.hessian.rks.Hessian",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.hessian.uhf.Hessian"
        ],
        "class_docstring": "Non-relativistic UKS hessian",
        "klass": "pyscf.hessian.uks.Hessian",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Within this macro, function(s) can be executed asynchronously (the\n    given functions are executed in background).\n\n    Attributes:\n        sync (bool): Whether to run in synchronized mode.  The default value\n            is False (asynchoronized mode).\n\n    Examples:\n\n    >>> with call_in_background(fun) as async_fun:\n    ...     async_fun(a, b)  # == fun(a, b)\n    ...     do_something_else()\n\n    >>> with call_in_background(fun1, fun2) as (afun1, afun2):\n    ...     afun2(a, b)\n    ...     do_something_else()\n    ...     afun2(a, b)\n    ...     do_something_else()\n    ...     afun1(a, b)\n    ...     do_something_else()\n    ",
        "klass": "pyscf.lib.call_in_background",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.lib.misc.temporary_env"
        ],
        "class_docstring": "Within the context of this macro, the environment varialbe LIGHT_SPEED\n    can be customized.\n\n    Examples:\n\n    >>> with light_speed(15.):\n    ...     print(lib.param.LIGHT_SPEED)\n    15.\n    >>> print(lib.param.LIGHT_SPEED)\n    137.03599967994\n    ",
        "klass": "pyscf.lib.light_speed",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Within the context of this macro, the attributes of the object are\n    temporarily updated. When the program goes out of the scope of the\n    context, the original value of each attribute will be restored.\n\n    Examples:\n\n    >>> with temporary_env(lib.param, LIGHT_SPEED=15., BOHR=2.5):\n    ...     print(lib.param.LIGHT_SPEED, lib.param.BOHR)\n    15. 2.5\n    >>> print(lib.param.LIGHT_SPEED, lib.param.BOHR)\n    137.03599967994 0.52917721092\n    ",
        "klass": "pyscf.lib.temporary_env",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Attributes:\n        stdout : file object or sys.stdout\n            The file to dump output message.\n        verbose : int\n            Large value means more noise in the output file.\n    ",
        "klass": "pyscf.lib.logger.Logger",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.pbc.scf.khf.KRHF"
        ],
        "class_docstring": "RKS class adapted for PBCs with k-point sampling.\n    ",
        "klass": "pyscf.pbc.dft.KRKS",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.dft.numint.NumInt"
        ],
        "class_docstring": "Generalization of pyscf's NumInt class for k-point sampling and\n    periodic images.\n    ",
        "klass": "pyscf.pbc.dft.numint.KNumInt",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.dft.numint.NumInt"
        ],
        "class_docstring": "Generalization of pyscf's NumInt class for a single k-point shift and\n    periodic images.\n    ",
        "klass": "pyscf.pbc.dft.numint.NumInt",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.gto.mole.Mole"
        ],
        "class_docstring": "A Cell object holds the basic information of a crystal.\n\n    Attributes:\n        a : (3,3) ndarray\n            Lattice primitive vectors. Each row represents a lattice vector\n            Reciprocal lattice vectors are given by  b1,b2,b3 = 2 pi inv(a).T\n        mesh : (3,) list of ints\n            The number G-vectors along each direction.\n            The default value is estimated based on :attr:`precision`\n        pseudo : dict or str\n            To define pseudopotential.\n        precision : float\n            To control Ewald sums and lattice sums accuracy\n        rcut : float\n            Cutoff radius (unit Bohr) in lattice summation. The default value\n            is estimated based on the required :attr:`precision`.\n        ke_cutoff : float\n            If set, defines a spherical cutoff of planewaves, with .5 * G**2 < ke_cutoff\n            The default value is estimated based on :attr:`precision`\n        dimension : int\n            Default is 3\n\n        ** Following attributes (for experts) are automatically generated. **\n\n        ew_eta, ew_cut : float\n            The Ewald 'eta' and 'cut' parameters.  See :func:`get_ewald_params`\n\n    (See other attributes in :class:`Mole`)\n\n    Examples:\n\n    >>> mol = Mole(atom='H^2 0 0 0; H 0 0 1.1', basis='sto3g')\n    >>> cl = Cell()\n    >>> cl.build(a='3 0 0; 0 3 0; 0 0 3', atom='C 1 1 1', basis='sto3g')\n    >>> print(cl.atom_symbol(0))\n    C\n    ",
        "klass": "pyscf.pbc.gto.Cell",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.pbc.scf.ghf.GHF",
            "pyscf.pbc.scf.khf.KSCF"
        ],
        "class_docstring": "GHF class for PBCs.\n    ",
        "klass": "pyscf.pbc.scf.KGHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.pbc.scf.uhf.UHF",
            "pyscf.pbc.scf.khf.KSCF"
        ],
        "class_docstring": "UHF class with k-point sampling.\n    ",
        "klass": "pyscf.pbc.scf.kuhf.KUHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.pbc.scf.uhf.UHF",
            "pyscf.pbc.scf.khf.KSCF"
        ],
        "class_docstring": "UHF class with k-point sampling.\n    ",
        "klass": "pyscf.pbc.scf.KUHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.scf.rohf.ROHF",
            "pyscf.pbc.scf.hf.RHF"
        ],
        "class_docstring": "ROHF class for PBCs.\n    ",
        "klass": "pyscf.pbc.scf.rohf.ROHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.scf.uhf.UHF",
            "pyscf.pbc.scf.hf.SCF"
        ],
        "class_docstring": "UHF class for PBCs.\n    ",
        "klass": "pyscf.pbc.scf.uhf.UHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.scf.uhf.UHF",
            "pyscf.pbc.scf.hf.SCF"
        ],
        "class_docstring": "UHF class for PBCs.\n    ",
        "klass": "pyscf.pbc.scf.UHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.scf.hf.SCF"
        ],
        "class_docstring": "SCF base class.   non-relativistic RHF.\n\n    Attributes:\n        verbose : int\n            Print level.  Default value equals to :class:`Mole.verbose`\n        max_memory : float or int\n            Allowed memory in MB.  Default equals to :class:`Mole.max_memory`\n        chkfile : str\n            checkpoint file to save MOs, orbital energies etc.  Writing to\n            chkfile can be disabled if this attribute is set to None or False.\n        conv_tol : float\n            converge threshold.  Default is 1e-9\n        conv_tol_grad : float\n            gradients converge threshold.  Default is sqrt(conv_tol)\n        max_cycle : int\n            max number of iterations.  Default is 50\n        init_guess : str\n            initial guess method.  It can be one of 'minao', 'atom', 'hcore', '1e', 'chkfile'.\n            Default is 'minao'\n        DIIS : DIIS class\n            The class to generate diis object.  It can be one of\n            diis.SCF_DIIS, diis.ADIIS, diis.EDIIS.\n        diis : boolean or object of DIIS class defined in :mod:`scf.diis`.\n            Default is the object associated to the attribute :attr:`self.DIIS`.\n            Set it to None/False to turn off DIIS.\n            Note if this attribute is inialized as a DIIS object, the SCF driver\n            will use this object in the iteration. The DIIS informations (vector\n            basis and error vector) will be held inside this object. When kernel\n            function is called again, the old states (vector basis and error\n            vector) will be reused.\n        diis_space : int\n            DIIS space size.  By default, 8 Fock matrices and errors vector are stored.\n        diis_start_cycle : int\n            The step to start DIIS.  Default is 1.\n        diis_file: 'str'\n            File to store DIIS vectors and error vectors.\n        level_shift : float or int\n            Level shift (in AU) for virtual space.  Default is 0.\n        direct_scf : bool\n            Direct SCF is used by default.\n        direct_scf_tol : float\n            Direct SCF cutoff threshold.  Default is 1e-13.\n        callback : function(envs_dict) => None\n            callback function takes one dict as the argument which is\n            generated by the builtin function :func:`locals`, so that the\n            callback function can access all local variables in the current\n            envrionment.\n        conv_check : bool\n            An extra cycle to check convergence after SCF iterations.\n        check_convergence : function(envs) => bool\n            A hook for overloading convergence criteria in SCF iterations.\n\n    Saved results:\n\n        converged : bool\n            SCF converged or not\n        e_tot : float\n            Total HF energy (electronic energy plus nuclear repulsion)\n        mo_energy :\n            Orbital energies\n        mo_occ\n            Orbital occupancy\n        mo_coeff\n            Orbital coefficients\n\n    Examples:\n\n    >>> mol = gto.M(atom='H 0 0 0; H 0 0 1.1', basis='cc-pvdz')\n    >>> mf = scf.hf.SCF(mol)\n    >>> mf.verbose = 0\n    >>> mf.level_shift = .4\n    >>> mf.scf()\n    -1.0811707843775884\n    \n    Attributes for Dirac-Hartree-Fock\n        with_ssss : bool, for Dirac-Hartree-Fock only\n            If False, ignore small component integrals (SS|SS).  Default is True.\n        with_gaunt : bool, for Dirac-Hartree-Fock only\n            Default is False.\n        with_breit : bool, for Dirac-Hartree-Fock only\n            Gaunt + gauge term.  Default is False.\n\n    Examples:\n\n    >>> mol = gto.M(atom='H 0 0 0; H 0 0 1', basis='ccpvdz', verbose=0)\n    >>> mf = scf.RHF(mol)\n    >>> e0 = mf.scf()\n    >>> mf = scf.DHF(mol)\n    >>> e1 = mf.scf()\n    >>> print('Relativistic effects = %.12f' % (e1-e0))\n    Relativistic effects = -0.000008854205\n    ",
        "klass": "pyscf.scf.dhf.UHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.tdscf.rhf.TDA"
        ],
        "class_docstring": "Time-dependent Hartree-Fock\n\n    Attributes:\n        conv_tol : float\n            Diagonalization convergence tolerance.  Default is 1e-9.\n        nstates : int\n            Number of TD states to be computed. Default is 3.\n\n    Saved results:\n\n        converged : bool\n            Diagonalization converged or not\n        e : 1D array\n            excitation energy for each excited state.\n        xy : A list of two 2D arrays\n            The two 2D arrays are Excitation coefficients X (shape [nocc,nvir])\n            and de-excitation coefficients Y (shape [nocc,nvir]) for each\n            excited state.  (X,Y) are normalized to 1/2 in RHF/RKS methods and\n            normalized to 1 for UHF/UKS methods. In the TDA calculation, Y = 0.\n    ",
        "klass": "pyscf.tddft.TDHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.tdscf.rhf.TDA"
        ],
        "class_docstring": "Time-dependent Hartree-Fock\n\n    Attributes:\n        conv_tol : float\n            Diagonalization convergence tolerance.  Default is 1e-9.\n        nstates : int\n            Number of TD states to be computed. Default is 3.\n\n    Saved results:\n\n        converged : bool\n            Diagonalization converged or not\n        e : 1D array\n            excitation energy for each excited state.\n        xy : A list of two 2D arrays\n            The two 2D arrays are Excitation coefficients X (shape [nocc,nvir])\n            and de-excitation coefficients Y (shape [nocc,nvir]) for each\n            excited state.  (X,Y) are normalized to 1/2 in RHF/RKS methods and\n            normalized to 1 for UHF/UKS methods. In the TDA calculation, Y = 0.\n    ",
        "klass": "pyscf.tdscf.rhf.TDHF",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.tdscf.rks.TDA"
        ],
        "class_docstring": " Solve (A-B)(A+B)(X+Y) = (X+Y)w^2\n    ",
        "klass": "pyscf.tdscf.rks.TDDFTNoHybrid",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "pyscf.tdscf.uks.TDA"
        ],
        "class_docstring": " Solve (A-B)(A+B)(X+Y) = (X+Y)w^2\n    ",
        "klass": "pyscf.tdscf.uks.TDDFTNoHybrid",
        "module": "pyscf"
    },
    {
        "base_classes": [
            "asynctest.mock.Mock"
        ],
        "class_docstring": "\n    Enhance :class:`~asynctest.mock.Mock` with features allowing to mock\n    a coroutine function.\n\n    The :class:`~asynctest.CoroutineMock` object will behave so the object is\n    recognized as coroutine function, and the result of a call as a coroutine:\n\n    >>> mock = CoroutineMock()\n    >>> asyncio.iscoroutinefunction(mock)\n    True\n    >>> asyncio.iscoroutine(mock())\n    True\n\n\n    The result of ``mock()`` is a coroutine which will have the outcome of\n    ``side_effect`` or ``return_value``:\n\n    - if ``side_effect`` is a function, the coroutine will return the result\n      of that function,\n    - if ``side_effect`` is an exception, the coroutine will raise the\n      exception,\n    - if ``side_effect`` is an iterable, the coroutine will return the next\n      value of the iterable, however, if the sequence of result is exhausted,\n      ``StopIteration`` is raised immediately,\n    - if ``side_effect`` is not defined, the coroutine will return the value\n      defined by ``return_value``, hence, by default, the coroutine returns\n      a new :class:`~asynctest.CoroutineMock` object.\n\n    If the outcome of ``side_effect`` or ``return_value`` is a coroutine, the\n    mock coroutine obtained when the mock object is called will be this\n    coroutine itself (and not a coroutine returning a coroutine).\n\n    The test author can also specify a wrapped object with ``wraps``. In this\n    case, the :class:`~asynctest.Mock` object behavior is the same as with an\n    :class:`unittest.mock.Mock` object: the wrapped object may have methods\n    defined as coroutine functions.\n    ",
        "klass": "asynctest.mock.CoroutineMock",
        "module": "asynctest"
    },
    {
        "base_classes": [
            "asynctest.mock.Mock"
        ],
        "class_docstring": "\n    Enhance :class:`~asynctest.mock.Mock` with features allowing to mock\n    a coroutine function.\n\n    The :class:`~asynctest.CoroutineMock` object will behave so the object is\n    recognized as coroutine function, and the result of a call as a coroutine:\n\n    >>> mock = CoroutineMock()\n    >>> asyncio.iscoroutinefunction(mock)\n    True\n    >>> asyncio.iscoroutine(mock())\n    True\n\n\n    The result of ``mock()`` is a coroutine which will have the outcome of\n    ``side_effect`` or ``return_value``:\n\n    - if ``side_effect`` is a function, the coroutine will return the result\n      of that function,\n    - if ``side_effect`` is an exception, the coroutine will raise the\n      exception,\n    - if ``side_effect`` is an iterable, the coroutine will return the next\n      value of the iterable, however, if the sequence of result is exhausted,\n      ``StopIteration`` is raised immediately,\n    - if ``side_effect`` is not defined, the coroutine will return the value\n      defined by ``return_value``, hence, by default, the coroutine returns\n      a new :class:`~asynctest.CoroutineMock` object.\n\n    If the outcome of ``side_effect`` or ``return_value`` is a coroutine, the\n    mock coroutine obtained when the mock object is called will be this\n    coroutine itself (and not a coroutine returning a coroutine).\n\n    The test author can also specify a wrapped object with ``wraps``. In this\n    case, the :class:`~asynctest.Mock` object behavior is the same as with an\n    :class:`unittest.mock.Mock` object: the wrapped object may have methods\n    defined as coroutine functions.\n    ",
        "klass": "asynctest.CoroutineMock",
        "module": "asynctest"
    },
    {
        "base_classes": [
            "asynctest.mock.AsyncMagicMixin",
            "unittest.mock.MagicMock"
        ],
        "class_docstring": "\n    Enhance :class:`unittest.mock.MagicMock` so it returns\n    a :class:`~asynctest.CoroutineMock` object instead of\n    a :class:`~asynctest.Mock` object where a method on a ``spec`` or\n    ``spec_set`` object is a coroutine.\n\n    If you want to mock a coroutine function, use :class:`CoroutineMock`\n    instead.\n\n    :class:`MagicMock` allows to mock ``__aenter__``, ``__aexit__``,\n    ``__aiter__`` and ``__anext__``.\n\n    When mocking an asynchronous iterator, you can set the\n    ``return_value`` of ``__aiter__`` to an iterable to define the list of\n    values to be returned during iteration.\n\n    You can not mock ``__await__``. If you want to mock an object implementing\n    __await__, :class:`CoroutineMock` will likely be sufficient.\n\n    see :class:`~asynctest.Mock`.\n\n    .. versionadded:: 0.11\n\n        support of asynchronous iterators and asynchronous context managers.\n    ",
        "klass": "asynctest.MagicMock",
        "module": "asynctest"
    },
    {
        "base_classes": [
            "unittest.mock.Mock"
        ],
        "class_docstring": "\n    Enhance :class:`unittest.mock.Mock` so it returns\n    a :class:`~asynctest.CoroutineMock` object instead of\n    a :class:`~asynctest.Mock` object where a method on a ``spec`` or\n    ``spec_set`` object is a coroutine.\n\n    For instance:\n\n    >>> class Foo:\n    ...     @asyncio.coroutine\n    ...     def foo(self):\n    ...         pass\n    ...\n    ...     def bar(self):\n    ...         pass\n\n    >>> type(asynctest.mock.Mock(Foo()).foo)\n    <class 'asynctest.mock.CoroutineMock'>\n\n    >>> type(asynctest.mock.Mock(Foo()).bar)\n    <class 'asynctest.mock.Mock'>\n\n    The test author can also specify a wrapped object with ``wraps``. In this\n    case, the :class:`~asynctest.Mock` object behavior is the same as with an\n    :class:`unittest.mock.Mock` object: the wrapped object may have methods\n    defined as coroutine functions.\n\n    If you want to mock a coroutine function, use :class:`CoroutineMock`\n    instead.\n\n    See :class:`~asynctest.NonCallableMock` for details about :mod:`asynctest`\n    features, and :mod:`unittest.mock` for the comprehensive documentation\n    about mocking.\n    ",
        "klass": "asynctest.mock.Mock",
        "module": "asynctest"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "HTML parser\n\n    Generates a tree structure from a stream of (possibly malformed) HTML.\n\n    ",
        "klass": "html5lib.html5parser.HTMLParser",
        "module": "html5lib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "HTML parser\n\n    Generates a tree structure from a stream of (possibly malformed) HTML.\n\n    ",
        "klass": "html5lib.HTMLParser",
        "module": "html5lib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Buffering for streams that do not have buffering of their own\n\n    The buffer is implemented as a list of chunks on the assumption that\n    joining many strings will be slow since it is O(n**2)\n    ",
        "klass": "html5lib._inputstream.BufferedStream",
        "module": "html5lib"
    },
    {
        "base_classes": [
            "abydos.distance._distance._Distance"
        ],
        "class_docstring": "Distance between the Eudex hashes of two terms.\n\n    Cf. :cite:`Ticki:2016`.\n\n    .. versionadded:: 0.3.6\n    ",
        "klass": "abydos.distance.Eudex",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._needleman_wunsch.NeedlemanWunsch"
        ],
        "class_docstring": "Gotoh score.\n\n    The Gotoh score :cite:`Gotoh:1982` is essentially Needleman-Wunsch with\n    affine gap penalties.\n\n    .. versionadded:: 0.3.6\n    ",
        "klass": "abydos.distance.Gotoh",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._distance._Distance"
        ],
        "class_docstring": "Meta-Levenshtein distance.\n\n    Meta-Levenshtein distance :cite:`Moreau:2008` combines Soft-TFIDF with\n    Levenshtein alignment.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.distance.MetaLevenshtein",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._distance._Distance"
        ],
        "class_docstring": "Needleman-Wunsch score.\n\n    The Needleman-Wunsch score :cite:`Needleman:1970` is a standard edit\n    distance measure.\n\n\n    .. versionadded:: 0.3.6\n    ",
        "klass": "abydos.distance.NeedlemanWunsch",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._needleman_wunsch.NeedlemanWunsch"
        ],
        "class_docstring": "Smith-Waterman score.\n\n    The Smith-Waterman score :cite:`Smith:1981` is a standard edit distance\n    measure, differing from Needleman-Wunsch in that it focuses on local\n    alignment and disallows negative scores.\n\n    .. versionadded:: 0.3.6\n    ",
        "klass": "abydos.distance.SmithWaterman",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._token_distance._TokenDistance"
        ],
        "class_docstring": "SoftTF-IDF similarity.\n\n    For two sets X and Y and a population N, SoftTF-IDF similarity\n    :cite:`Cohen:2003` is\n\n        .. math::\n\n            \\begin{array}{ll}\n            sim_{SoftTF-IDF}(X, Y) &= \\sum_{w \\in \\{sim_{metric}(x, y) \\ge\n            \\theta | x \\in X, y \\in Y \\}} V(w, S) \\cdot V(w, X) \\cdot V(w, Y)\n            \\\\\n            \\\\\n            V(w, S) &= \\frac{V'(w, S)}{\\sqrt{\\sum_{w \\in S} V'(w, S)^2}}\n            \\\\\n            \\\\\n            V'(w, S) &= log(1+TF_{w,S}) \\cdot log(1+IDF_w)\n            \\end{array}\n\n    Notes\n    -----\n    One is added to both the TF & IDF values before taking the logarithm to\n    ensure the logarithms do not fall to 0, which will tend to result in 0.0\n    similarities even when there is a degree of matching.\n\n    Rather than needing to exceed the threshold value, as in :cite:`Cohen:2003`\n    the similarity must be greater than or equal to the threshold.\n\n    .. versionadded:: 0.4.0\n\n    ",
        "klass": "abydos.distance.SoftTFIDF",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.distance._token_distance._TokenDistance"
        ],
        "class_docstring": "Tversky index.\n\n    The Tversky index :cite:`Tversky:1977` is defined as:\n    For two sets X and Y:\n\n        .. math::\n\n            sim_{Tversky}(X, Y) = \\frac{|X \\cap Y|}\n            {|X \\cap Y| + \\alpha|X - Y| + \\beta|Y - X|}\n\n    :math:`\\alpha = \\beta = 1` is equivalent to the Jaccard & Tanimoto\n    similarity coefficients.\n\n    :math:`\\alpha = \\beta = 0.5` is equivalent to the S\u00f8rensen-Dice\n    similarity coefficient :cite:`Dice:1945,Sorensen:1948`.\n\n    Unequal \u03b1 and \u03b2 will tend to emphasize one or the other set's\n    contributions:\n\n        - :math:`\\alpha > \\beta` emphasizes the contributions of X over Y\n        - :math:`\\alpha < \\beta` emphasizes the contributions of Y over X)\n\n    Parameter values' relation to 1 emphasizes different types of\n    contributions:\n\n        - :math:`\\alpha` and :math:`\\beta > 1` emphsize unique contributions\n          over the intersection\n        - :math:`\\alpha` and :math:`\\beta < 1` emphsize the intersection over\n          unique contributions\n\n    The symmetric variant is defined in :cite:`Jiminez:2013`. This is activated\n    by specifying a bias parameter.\n\n\n    .. versionadded:: 0.3.6\n    ",
        "klass": "abydos.distance.Tversky",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A C- or V-cluster tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.COrVClusterTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A C*V*-cluster tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.CVClusterTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A character tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.CharacterTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "LegaliPy tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.LegaliPyTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "NLTK tokenizer wrapper class.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.NLTKTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A q-gram class, which functions like a bag/multiset.\n\n    A q-gram is here defined as all sequences of q characters. Q-grams are also\n    known as k-grams and n-grams, but the term n-gram more typically refers to\n    sequences of whitespace-delimited words in a string, where q-gram refers\n    to sequences of characters in a word or string.\n\n    .. versionadded:: 0.1.0\n    ",
        "klass": "abydos.tokenizer.QGrams",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A q-skipgram class, which functions like a bag/multiset.\n\n    A q-gram is here defined as all sequences of q characters. Q-grams are also\n    known as k-grams and n-grams, but the term n-gram more typically refers to\n    sequences of whitespace-delimited words in a string, where q-gram refers\n    to sequences of characters in a word or string.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.QSkipgrams",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A regexp tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.RegexpTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "Syllable Alignment Pattern Searching tokenizer.\n\n    This is the syllabifier described on p. 917 of :cite:`Ruibin:2005`.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.SAPSTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "SonoriPy tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.SonoriPyTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._tokenizer._Tokenizer"
        ],
        "class_docstring": "A V*C*-cluster tokenizer.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer.VCClusterTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._regexp.RegexpTokenizer"
        ],
        "class_docstring": "A whitespace tokenizer.\n\n    Examples\n    --------\n    >>> WhitespaceTokenizer().tokenize('a b c f a c g e a b')\n    WhitespaceTokenizer({'a': 3, 'b': 2, 'c': 2, 'f': 1, 'g': 1, 'e': 1})\n\n\n    .. versionadded:: 0.4.0\n\n    ",
        "klass": "abydos.tokenizer.WhitespaceTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "abydos.tokenizer._regexp.RegexpTokenizer"
        ],
        "class_docstring": "A wordpunct tokenizer.\n\n    Examples\n    --------\n    >>> WordpunctTokenizer().tokenize(\"Can't stop the feelin'!\")\n    WordpunctTokenizer({'Can': 1, \"'\": 1, 't': 1, 'stop': 1, 'the': 1,\n    'feelin': 1, \"'!\": 1})\n\n\n    .. versionadded:: 0.4.0\n\n    ",
        "klass": "abydos.tokenizer.WordpunctTokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Abstract _Tokenizer class.\n\n    .. versionadded:: 0.4.0\n    ",
        "klass": "abydos.tokenizer._Tokenizer",
        "module": "abydos"
    },
    {
        "base_classes": [
            "tables.hdf5extension.File",
            "object"
        ],
        "class_docstring": "The in-memory representation of a PyTables file.\n\n    An instance of this class is returned when a PyTables file is\n    opened with the :func:`tables.open_file` function. It offers methods\n    to manipulate (create, rename, delete...) nodes and handle their\n    attributes, as well as methods to traverse the object tree.\n    The *user entry point* to the object tree attached to the HDF5 file\n    is represented in the root_uep attribute.\n    Other attributes are available.\n\n    File objects support an *Undo/Redo mechanism* which can be enabled\n    with the :meth:`File.enable_undo` method. Once the Undo/Redo\n    mechanism is enabled, explicit *marks* (with an optional unique\n    name) can be set on the state of the database using the\n    :meth:`File.mark`\n    method. There are two implicit marks which are always available:\n    the initial mark (0) and the final mark (-1).  Both the identifier\n    of a mark and its name can be used in *undo* and *redo* operations.\n\n    Hierarchy manipulation operations (node creation, movement and\n    removal) and attribute handling operations (setting and deleting)\n    made after a mark can be undone by using the :meth:`File.undo`\n    method, which returns the database to the state of a past mark.\n    If undo() is not followed by operations that modify the hierarchy\n    or attributes, the :meth:`File.redo` method can be used to return\n    the database to the state of a future mark. Else, future states of\n    the database are forgotten.\n\n    Note that data handling operations can not be undone nor redone by\n    now. Also, hierarchy manipulation operations on nodes that do not\n    support the Undo/Redo mechanism issue an UndoRedoWarning *before*\n    changing the database.\n\n    The Undo/Redo mechanism is persistent between sessions and can\n    only be disabled by calling the :meth:`File.disable_undo` method.\n\n    File objects can also act as context managers when using the with\n    statement introduced in Python 2.5.  When exiting a context, the\n    file is automatically closed.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file (supports environment variable expansion).\n        It is suggested that file names have any of the .h5, .hdf or\n        .hdf5 extensions, although this is not mandatory.\n\n    mode : str\n        The mode to open the file. It can be one of the\n        following:\n\n            * *'r'*: Read-only; no data can be modified.\n            * *'w'*: Write; a new file is created (an existing file\n              with the same name would be deleted).\n            * *'a'*: Append; an existing file is opened for reading\n              and writing, and if the file does not exist it is created.\n            * *'r+'*: It is similar to 'a', but the file must already\n              exist.\n\n    title : str\n        If the file is to be created, a TITLE string attribute will be\n        set on the root group with the given value. Otherwise, the\n        title will be read from disk, and this will not have any effect.\n\n    root_uep : str\n        The root User Entry Point. This is a group in the HDF5 hierarchy\n        which will be taken as the starting point to create the object\n        tree. It can be whatever existing group in the file, named by\n        its HDF5 path. If it does not exist, an HDF5ExtError is issued.\n        Use this if you do not want to build the *entire* object tree,\n        but rather only a *subtree* of it.\n\n        .. versionchanged:: 3.0\n           The *rootUEP* parameter has been renamed into *root_uep*.\n\n    filters : Filters\n        An instance of the Filters (see :ref:`FiltersClassDescr`) class that\n        provides information about the desired I/O filters applicable to the\n        leaves that hang directly from the *root group*, unless other filter\n        properties are specified for these leaves. Besides, if you do not\n        specify filter properties for child groups, they will inherit these\n        ones, which will in turn propagate to child nodes.\n\n    Notes\n    -----\n    In addition, it recognizes the (lowercase) names of parameters\n    present in :file:`tables/parameters.py` as additional keyword\n    arguments.\n    See :ref:`parameter_files` for a detailed info on the supported\n    parameters.\n\n\n    .. rubric:: File attributes\n\n    .. attribute:: filename\n\n        The name of the opened file.\n\n    .. attribute:: format_version\n\n        The PyTables version number of this file.\n\n    .. attribute:: isopen\n\n        True if the underlying file is open, false otherwise.\n\n    .. attribute:: mode\n\n        The mode in which the file was opened.\n\n    .. attribute:: root\n\n        The *root* of the object tree hierarchy (a Group instance).\n\n    .. attribute:: root_uep\n\n        The UEP (user entry point) group name in the file (see\n        the :func:`open_file` function).\n\n        .. versionchanged:: 3.0\n           The *rootUEP* attribute has been renamed into *root_uep*.\n\n    ",
        "klass": "tables.File",
        "module": "tables"
    },
    {
        "base_classes": [
            "pyglet.event.EventDispatcher"
        ],
        "class_docstring": "High-level sound and video player.",
        "klass": "pyglet.media.Player",
        "module": "pyglet"
    },
    {
        "base_classes": [
            "pyramid.config.actions.ActionConfiguratorMixin",
            "pyramid.config.predicates.PredicateConfiguratorMixin",
            "pyramid.config.testing.TestingConfiguratorMixin",
            "pyramid.config.tweens.TweensConfiguratorMixin",
            "pyramid.config.security.SecurityConfiguratorMixin",
            "pyramid.config.views.ViewsConfiguratorMixin",
            "pyramid.config.routes.RoutesConfiguratorMixin",
            "pyramid.config.zca.ZCAConfiguratorMixin",
            "pyramid.config.i18n.I18NConfiguratorMixin",
            "pyramid.config.rendering.RenderingConfiguratorMixin",
            "pyramid.config.assets.AssetsConfiguratorMixin",
            "pyramid.config.settings.SettingsConfiguratorMixin",
            "pyramid.config.factories.FactoriesConfiguratorMixin",
            "pyramid.config.adapters.AdaptersConfiguratorMixin"
        ],
        "class_docstring": "\n    A Configurator is used to configure a :app:`Pyramid`\n    :term:`application registry`.\n\n    The Configurator lifecycle can be managed by using a context manager to\n    automatically handle calling :meth:`pyramid.config.Configurator.begin` and\n    :meth:`pyramid.config.Configurator.end` as well as\n    :meth:`pyramid.config.Configurator.commit`.\n\n    .. code-block:: python\n\n        with Configurator(settings=settings) as config:\n            config.add_route('home', '/')\n            app = config.make_wsgi_app()\n\n    If the ``registry`` argument is not ``None``, it must\n    be an instance of the :class:`pyramid.registry.Registry` class\n    representing the registry to configure.  If ``registry`` is ``None``, the\n    configurator will create a :class:`pyramid.registry.Registry` instance\n    itself; it will also perform some default configuration that would not\n    otherwise be done.  After its construction, the configurator may be used\n    to add further configuration to the registry.\n\n    .. warning:: If ``registry`` is assigned the above-mentioned class\n       instance, all other constructor arguments are ignored,\n       with the exception of ``package``.\n\n    If the ``package`` argument is passed, it must be a reference to a Python\n    :term:`package` (e.g. ``sys.modules['thepackage']``) or a :term:`dotted\n    Python name` to the same.  This value is used as a basis to convert\n    relative paths passed to various configuration methods, such as methods\n    which accept a ``renderer`` argument, into absolute paths.  If ``None``\n    is passed (the default), the package is assumed to be the Python package\n    in which the *caller* of the ``Configurator`` constructor lives.\n\n    If the ``root_package`` is passed, it will propagate through the\n    configuration hierarchy as a way for included packages to locate\n    resources relative to the package in which the main ``Configurator`` was\n    created. If ``None`` is passed (the default), the ``root_package`` will\n    be derived from the ``package`` argument. The ``package`` attribute is\n    always pointing at the package being included when using :meth:`.include`,\n    whereas the ``root_package`` does not change.\n\n    If the ``settings`` argument is passed, it should be a Python dictionary\n    representing the :term:`deployment settings` for this application.  These\n    are later retrievable using the\n    :attr:`pyramid.registry.Registry.settings` attribute (aka\n    ``request.registry.settings``).\n\n    If the ``root_factory`` argument is passed, it should be an object\n    representing the default :term:`root factory` for your application or a\n    :term:`dotted Python name` to the same.  If it is ``None``, a default\n    root factory will be used.\n\n    If ``authentication_policy`` is passed, it should be an instance\n    of an :term:`authentication policy` or a :term:`dotted Python\n    name` to the same.\n\n    If ``authorization_policy`` is passed, it should be an instance of\n    an :term:`authorization policy` or a :term:`dotted Python name` to\n    the same.\n\n    .. note:: A ``ConfigurationError`` will be raised when an\n       authorization policy is supplied without also supplying an\n       authentication policy (authorization requires authentication).\n\n    If ``renderers`` is ``None`` (the default), a default set of\n    :term:`renderer` factories is used. Else, it should be a list of\n    tuples representing a set of renderer factories which should be\n    configured into this application, and each tuple representing a set of\n    positional values that should be passed to\n    :meth:`pyramid.config.Configurator.add_renderer`.\n\n    If ``debug_logger`` is not passed, a default debug logger that logs to a\n    logger will be used (the logger name will be the package name of the\n    *caller* of this configurator).  If it is passed, it should be an\n    instance of the :class:`logging.Logger` (PEP 282) standard library class\n    or a Python logger name.  The debug logger is used by :app:`Pyramid`\n    itself to log warnings and authorization debugging information.\n\n    If ``locale_negotiator`` is passed, it should be a :term:`locale\n    negotiator` implementation or a :term:`dotted Python name` to\n    same.  See :ref:`custom_locale_negotiator`.\n\n    If ``request_factory`` is passed, it should be a :term:`request\n    factory` implementation or a :term:`dotted Python name` to the same.\n    See :ref:`changing_the_request_factory`.  By default it is ``None``,\n    which means use the default request factory.\n\n    If ``response_factory`` is passed, it should be a :term:`response\n    factory` implementation or a :term:`dotted Python name` to the same.\n    See :ref:`changing_the_response_factory`.  By default it is ``None``,\n    which means use the default response factory.\n\n    If ``default_permission`` is passed, it should be a\n    :term:`permission` string to be used as the default permission for\n    all view configuration registrations performed against this\n    Configurator.  An example of a permission string:``'view'``.\n    Adding a default permission makes it unnecessary to protect each\n    view configuration with an explicit permission, unless your\n    application policy requires some exception for a particular view.\n    By default, ``default_permission`` is ``None``, meaning that view\n    configurations which do not explicitly declare a permission will\n    always be executable by entirely anonymous users (any\n    authorization policy in effect is ignored).\n\n    .. seealso::\n\n        See also :ref:`setting_a_default_permission`.\n\n    If ``session_factory`` is passed, it should be an object which\n    implements the :term:`session factory` interface.  If a nondefault\n    value is passed, the ``session_factory`` will be used to create a\n    session object when ``request.session`` is accessed.  Note that\n    the same outcome can be achieved by calling\n    :meth:`pyramid.config.Configurator.set_session_factory`.  By\n    default, this argument is ``None``, indicating that no session\n    factory will be configured (and thus accessing ``request.session``\n    will throw an error) unless ``set_session_factory`` is called later\n    during configuration.\n\n    If ``autocommit`` is ``True``, every method called on the configurator\n    will cause an immediate action, and no configuration conflict detection\n    will be used. If ``autocommit`` is ``False``, most methods of the\n    configurator will defer their action until\n    :meth:`pyramid.config.Configurator.commit` is called.  When\n    :meth:`pyramid.config.Configurator.commit` is called, the actions implied\n    by the called methods will be checked for configuration conflicts unless\n    ``autocommit`` is ``True``.  If a conflict is detected, a\n    ``ConfigurationConflictError`` will be raised.  Calling\n    :meth:`pyramid.config.Configurator.make_wsgi_app` always implies a final\n    commit.\n\n    If ``default_view_mapper`` is passed, it will be used as the default\n    :term:`view mapper` factory for view configurations that don't otherwise\n    specify one (see :class:`pyramid.interfaces.IViewMapperFactory`).  If\n    ``default_view_mapper`` is not passed, a superdefault view mapper will be\n    used.\n\n    If ``exceptionresponse_view`` is passed, it must be a :term:`view\n    callable` or ``None``.  If it is a view callable, it will be used as an\n    exception view callable when an :term:`exception response` is raised. If\n    ``exceptionresponse_view`` is ``None``, no exception response view will\n    be registered, and all raised exception responses will be bubbled up to\n    Pyramid's caller.  By\n    default, the ``pyramid.httpexceptions.default_exceptionresponse_view``\n    function is used as the ``exceptionresponse_view``.\n\n    If ``route_prefix`` is passed, all routes added with\n    :meth:`pyramid.config.Configurator.add_route` will have the specified path\n    prepended to their pattern.\n\n    If ``introspection`` is passed, it must be a boolean value.  If it's\n    ``True``, introspection values during actions will be kept for use\n    for tools like the debug toolbar.  If it's ``False``, introspection\n    values provided by registrations will be ignored.  By default, it is\n    ``True``.\n\n    .. versionadded:: 1.1\n       The ``exceptionresponse_view`` argument.\n\n    .. versionadded:: 1.2\n       The ``route_prefix`` argument.\n\n    .. versionadded:: 1.3\n       The ``introspection`` argument.\n\n    .. versionadded:: 1.6\n       The ``root_package`` argument.\n       The ``response_factory`` argument.\n\n    .. versionadded:: 1.9\n       The ability to use the configurator as a context manager with the\n       ``with``-statement to make threadlocal configuration available for\n       further configuration with an implicit commit.\n    ",
        "klass": "pyramid.config.Configurator",
        "module": "pyramid"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a WSGI response.\n\n    If no arguments are passed, creates a :class:`~Response` that uses a\n    variety of defaults. The defaults may be changed by sub-classing the\n    :class:`~Response`. See the :ref:`sub-classing notes\n    <response_subclassing_notes>`.\n\n    :cvar ~Response.body: If ``body`` is a ``text_type``, then it will be\n        encoded using either ``charset`` when provided or ``default_encoding``\n        when ``charset`` is not provided if the ``content_type`` allows for a\n        ``charset``. This argument is mutually  exclusive with ``app_iter``.\n\n    :vartype ~Response.body: bytes or text_type\n\n    :cvar ~Response.status: Either an :class:`int` or a string that is\n        an integer followed by the status text. If it is an integer, it will be\n        converted to a proper status that also includes the status text.  Any\n        existing status text will be kept. Non-standard values are allowed.\n\n    :vartype ~Response.status: int or str\n\n    :cvar ~Response.headerlist: A list of HTTP headers for the response.\n\n    :vartype ~Response.headerlist: list\n\n    :cvar ~Response.app_iter: An iterator that is used as the body of the\n        response. Should conform to the WSGI requirements and should provide\n        bytes. This argument is mutually exclusive with ``body``.\n\n    :vartype ~Response.app_iter: iterable\n\n    :cvar ~Response.content_type: Sets the ``Content-Type`` header. If no\n        ``content_type`` is provided, and there is no ``headerlist``, the\n        ``default_content_type`` will be automatically set. If ``headerlist``\n        is provided then this value is ignored.\n\n    :vartype ~Response.content_type: str or None\n\n    :cvar conditional_response: Used to change the behavior of the\n        :class:`~Response` to check the original request for conditional\n        response headers. See :meth:`~Response.conditional_response_app` for\n        more information.\n\n    :vartype conditional_response: bool\n\n    :cvar ~Response.charset: Adds a ``charset`` ``Content-Type`` parameter. If\n        no ``charset`` is provided and the ``Content-Type`` is text, then the\n        ``default_charset`` will automatically be added.  Currently the only\n        ``Content-Type``'s that allow for a ``charset`` are defined to be\n        ``text/*``, ``application/xml``, and ``*/*+xml``. Any other\n        ``Content-Type``'s will not have a ``charset`` added. If a\n        ``headerlist`` is provided this value is ignored.\n\n    :vartype ~Response.charset: str or None\n\n    All other response attributes may be set on the response by providing them\n    as keyword arguments. A :exc:`TypeError` will be raised for any unexpected\n    keywords.\n\n    .. _response_subclassing_notes:\n\n    **Sub-classing notes:**\n\n    * The ``default_content_type`` is used as the default for the\n      ``Content-Type`` header that is returned on the response. It is\n      ``text/html``.\n\n    * The ``default_charset`` is used as the default character set to return on\n      the ``Content-Type`` header, if the ``Content-Type`` allows for a\n      ``charset`` parameter. Currently the only ``Content-Type``'s that allow\n      for a ``charset`` are defined to be: ``text/*``, ``application/xml``, and\n      ``*/*+xml``. Any other ``Content-Type``'s will not have a ``charset``\n      added.\n\n    * The ``unicode_errors`` is set to ``strict``, and access on a\n      :attr:`~Response.text` will raise an error if it fails to decode the\n      :attr:`~Response.body`.\n\n    * ``default_conditional_response`` is set to ``False``. This flag may be\n      set to ``True`` so that all ``Response`` objects will attempt to check\n      the original request for conditional response headers. See\n      :meth:`~Response.conditional_response_app` for more information.\n\n    * ``default_body_encoding`` is set to 'UTF-8' by default. It exists to\n      allow users to get/set the ``Response`` object using ``.text``, even if\n      no ``charset`` has been set for the ``Content-Type``.\n    ",
        "klass": "pyramid.response.Response",
        "module": "webob"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Manages an asynchronous process.\n\n    This class spawns a new process via subprocess and uses\n    greenthreads to read stderr and stdout asynchronously into queues\n    that can be read via repeatedly calling iter_stdout() and\n    iter_stderr().\n\n    If respawn_interval is non-zero, any error in communicating with\n    the managed process will result in the process and greenthreads\n    being cleaned up and the process restarted after the specified\n    interval.\n\n    Example usage:\n\n    >>> import time\n    >>> proc = AsyncProcess(['ping'])\n    >>> proc.start()\n    >>> time.sleep(5)\n    >>> proc.stop()\n    >>> for line in proc.iter_stdout():\n    ...     print(line)\n    ",
        "klass": "neutron.agent.common.async_process.AsyncProcess",
        "module": "neutron"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Manager for access to a resource for processing\n\n    This class controls access to a resource in a non-blocking way.  The first\n    instance to be created for a given ID is granted exclusive access to\n    the resource.\n\n    Other instances may be created for the same ID while the first\n    instance has exclusive access.  If that happens then it doesn't block and\n    wait for access.  Instead, it signals to the master instance that an update\n    came in with the timestamp.\n\n    This way, a thread will not block to wait for access to a resource.\n    Instead it effectively signals to the thread that is working on the\n    resource that something has changed since it started working on it.\n    That thread will simply finish its current iteration and then repeat.\n\n    This class keeps track of the last time that resource data was fetched and\n    processed.  The timestamp that it keeps must be before when the data used\n    to process the resource last was fetched from the database.  But, as close\n    as possible.  The timestamp should not be recorded, however, until the\n    resource has been processed using the fetch data.\n    ",
        "klass": "neutron.agent.common.resource_processing_queue.ExclusiveResourceProcessor",
        "module": "neutron"
    },
    {
        "base_classes": [
            "neutron.agent.linux.external_process.MonitoredProcess"
        ],
        "class_docstring": "An external process manager for Neutron spawned processes.\n\n    Note: The manager expects uuid to be in cmdline.\n    ",
        "klass": "neutron.agent.linux.external_process.ProcessManager",
        "module": "neutron"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager class which handles a DB connection.\n\n       An existing connection can be passed as a parameter. When\n       nested block is complete the new connection will be closed.\n       This class is not thread safe.\n    ",
        "klass": "neutron.db.migration.connection.DBConnection",
        "module": "neutron"
    },
    {
        "base_classes": [
            "neutron.worker.NeutronBaseWorker"
        ],
        "class_docstring": "A worker that runs a function at a fixed interval.",
        "klass": "neutron.worker.PeriodicWorker",
        "module": "neutron"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Server class to manage multiple WSGI sockets and applications.",
        "klass": "neutron.wsgi.Server",
        "module": "neutron"
    },
    {
        "base_classes": [
            "parsel.selector.Selector",
            "scrapy.utils.trackref.object_ref"
        ],
        "class_docstring": "\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    ",
        "klass": "scrapy.selector.unified.Selector",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "parsel.selector.Selector",
            "scrapy.utils.trackref.object_ref"
        ],
        "class_docstring": "\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    ",
        "klass": "scrapy.Selector",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "scrapy.crawler.CrawlerRunner"
        ],
        "class_docstring": "\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a Twisted `reactor`_ and handling shutdown signals, like the\n    keyboard interrupt command Ctrl-C. It also configures top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    Twisted `reactor`_ within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    ",
        "klass": "scrapy.crawler.CrawlerProcess",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    TCP client endpoint with an IPv4 configuration.\n    ",
        "klass": "twisted.internet.endpoints.TCP4ClientEndpoint",
        "module": "twisted"
    },
    {
        "base_classes": [
            "scrapy.settings.BaseSettings"
        ],
        "class_docstring": "\n    This object stores Scrapy settings for the configuration of internal\n    components, and can be used for any further customization.\n\n    It is a direct subclass and supports all methods of\n    :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation\n    of this class, the new object will have the global default settings\n    described on :ref:`topics-settings-ref` already populated.\n    ",
        "klass": "scrapy.settings.Settings",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "scrapy.utils.datatypes.CaselessDict"
        ],
        "class_docstring": "Case insensitive http headers dictionary",
        "klass": "scrapy.http.Headers",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "email.mime.base.MIMEBase"
        ],
        "class_docstring": "Base class for MIME non-multipart type messages.",
        "klass": "email.mime.nonmultipart.MIMENonMultipart",
        "module": "email"
    },
    {
        "base_classes": [
            "twisted.internet.defer.Deferred"
        ],
        "class_docstring": "\n    L{DeferredList} is a tool for collecting the results of several Deferreds.\n\n    This tracks a list of L{Deferred}s for their results, and makes a single\n    callback when they have all completed.  By default, the ultimate result is a\n    list of (success, result) tuples, 'success' being a boolean.\n    L{DeferredList} exposes the same API that L{Deferred} does, so callbacks and\n    errbacks can be added to it in the same way.\n\n    L{DeferredList} is implemented by adding callbacks and errbacks to each\n    L{Deferred} in the list passed to it.  This means callbacks and errbacks\n    added to the Deferreds before they are passed to L{DeferredList} will change\n    the result that L{DeferredList} sees (i.e., L{DeferredList} is not special).\n    Callbacks and errbacks can also be added to the Deferreds after they are\n    passed to L{DeferredList} and L{DeferredList} may change the result that\n    they see.\n\n    See the documentation for the C{__init__} arguments for more information.\n\n    @ivar _deferredList: The L{list} of L{Deferred}s to track.\n    ",
        "klass": "twisted.internet.defer.DeferredList",
        "module": "twisted"
    },
    {
        "base_classes": [
            "parsel.selector.Selector",
            "scrapy.utils.trackref.object_ref"
        ],
        "class_docstring": "\n    An instance of :class:`Selector` is a wrapper over response to select\n    certain parts of its content.\n\n    ``response`` is an :class:`~scrapy.http.HtmlResponse` or an\n    :class:`~scrapy.http.XmlResponse` object that will be used for selecting\n    and extracting data.\n\n    ``text`` is a unicode string or utf-8 encoded text for cases when a\n    ``response`` isn't available. Using ``text`` and ``response`` together is\n    undefined behavior.\n\n    ``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"``\n    or ``None`` (default).\n\n    If ``type`` is ``None``, the selector automatically chooses the best type\n    based on ``response`` type (see below), or defaults to ``\"html\"`` in case it\n    is used together with ``text``.\n\n    If ``type`` is ``None`` and a ``response`` is passed, the selector type is\n    inferred from the response type as follows:\n\n    * ``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type\n    * ``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type\n    * ``\"html\"`` for anything else\n\n    Otherwise, if ``type`` is set, the selector type will be forced and no\n    detection will occur.\n    ",
        "klass": "scrapy.selector.Selector",
        "module": "scrapy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Define a web-accessible resource.\n\n    This serves 2 main purposes; one is to provide a standard representation\n    for what HTTP specification calls an 'entity', and the other is to provide\n    an abstract directory structure for URL retrieval.\n    ",
        "klass": "twisted.web.resource.Resource",
        "module": "twisted"
    },
    {
        "base_classes": [
            "twisted.web.http.HTTPFactory"
        ],
        "class_docstring": "\n    A web site: manage log, sessions, and resources.\n\n    @ivar counter: increment value used for generating unique sessions ID.\n    @ivar requestFactory: A factory which is called with (channel)\n        and creates L{Request} instances. Default to L{Request}.\n    @ivar displayTracebacks: If set, unhandled exceptions raised during\n        rendering are returned to the client as HTML. Default to C{False}.\n    @ivar sessionFactory: factory for sessions objects. Default to L{Session}.\n    @ivar sessionCheckTime: Deprecated.  See L{Session.sessionTimeout} instead.\n    ",
        "klass": "twisted.web.server.Site",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    ActionChains are a way to automate low level interactions such as\n    mouse movements, mouse button actions, key press, and context menu interactions.\n    This is useful for doing more complex actions like hover over and drag and drop.\n\n    Generate user actions.\n       When you call methods for actions on the ActionChains object,\n       the actions are stored in a queue in the ActionChains object.\n       When you call perform(), the events are fired in the order they\n       are queued up.\n\n    ActionChains can be used in a chain pattern::\n\n        menu = driver.find_element_by_css_selector(\".nav\")\n        hidden_submenu = driver.find_element_by_css_selector(\".nav #submenu1\")\n\n        ActionChains(driver).move_to_element(menu).click(hidden_submenu).perform()\n\n    Or actions can be queued up one by one, then performed.::\n\n        menu = driver.find_element_by_css_selector(\".nav\")\n        hidden_submenu = driver.find_element_by_css_selector(\".nav #submenu1\")\n\n        actions = ActionChains(driver)\n        actions.move_to_element(menu)\n        actions.click(hidden_submenu)\n        actions.perform()\n\n    Either way, the actions are performed in the order they are called, one after\n    another.\n    ",
        "klass": "selenium.webdriver.common.action_chains.ActionChains",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    ActionChains are a way to automate low level interactions such as\n    mouse movements, mouse button actions, key press, and context menu interactions.\n    This is useful for doing more complex actions like hover over and drag and drop.\n\n    Generate user actions.\n       When you call methods for actions on the ActionChains object,\n       the actions are stored in a queue in the ActionChains object.\n       When you call perform(), the events are fired in the order they\n       are queued up.\n\n    ActionChains can be used in a chain pattern::\n\n        menu = driver.find_element_by_css_selector(\".nav\")\n        hidden_submenu = driver.find_element_by_css_selector(\".nav #submenu1\")\n\n        ActionChains(driver).move_to_element(menu).click(hidden_submenu).perform()\n\n    Or actions can be queued up one by one, then performed.::\n\n        menu = driver.find_element_by_css_selector(\".nav\")\n        hidden_submenu = driver.find_element_by_css_selector(\".nav #submenu1\")\n\n        actions = ActionChains(driver)\n        actions.move_to_element(menu)\n        actions.click(hidden_submenu)\n        actions.perform()\n\n    Either way, the actions are performed in the order they are called, one after\n    another.\n    ",
        "klass": "selenium.webdriver.ActionChains",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Controls a browser by sending commands to a remote server.\n    This server is expected to be running the WebDriver wire protocol\n    as defined at\n    https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol\n\n    :Attributes:\n     - session_id - String ID of the browser session started and controlled by this WebDriver.\n     - capabilities - Dictionaty of effective capabilities of this browser session as returned\n         by the remote server. See https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities\n     - command_executor - remote_connection.RemoteConnection object used to execute commands.\n     - error_handler - errorhandler.ErrorHandler object used to handle errors.\n    ",
        "klass": "selenium.webdriver.chrome.webdriver.WebDriver",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Controls a browser by sending commands to a remote server.\n    This server is expected to be running the WebDriver wire protocol\n    as defined at\n    https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol\n\n    :Attributes:\n     - session_id - String ID of the browser session started and controlled by this WebDriver.\n     - capabilities - Dictionaty of effective capabilities of this browser session as returned\n         by the remote server. See https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities\n     - command_executor - remote_connection.RemoteConnection object used to execute commands.\n     - error_handler - errorhandler.ErrorHandler object used to handle errors.\n    ",
        "klass": "selenium.webdriver.firefox.webdriver.WebDriver",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Controls a browser by sending commands to a remote server.\n    This server is expected to be running the WebDriver wire protocol\n    as defined at\n    https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol\n\n    :Attributes:\n     - session_id - String ID of the browser session started and controlled by this WebDriver.\n     - capabilities - Dictionaty of effective capabilities of this browser session as returned\n         by the remote server. See https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities\n     - command_executor - remote_connection.RemoteConnection object used to execute commands.\n     - error_handler - errorhandler.ErrorHandler object used to handle errors.\n    ",
        "klass": "selenium.webdriver.phantomjs.webdriver.WebDriver",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Controls a browser by sending commands to a remote server.\n    This server is expected to be running the WebDriver wire protocol\n    as defined at\n    https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol\n\n    :Attributes:\n     - session_id - String ID of the browser session started and controlled by this WebDriver.\n     - capabilities - Dictionaty of effective capabilities of this browser session as returned\n         by the remote server. See https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities\n     - command_executor - remote_connection.RemoteConnection object used to execute commands.\n     - error_handler - errorhandler.ErrorHandler object used to handle errors.\n    ",
        "klass": "selenium.webdriver.remote.webdriver.WebDriver",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A wrapper around an arbitrary WebDriver instance which supports firing events\n    ",
        "klass": "selenium.webdriver.support.events.EventFiringWebDriver",
        "module": "selenium"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Programmatic access to coverage.py.\n\n    To use::\n\n        from coverage import Coverage\n\n        cov = Coverage()\n        cov.start()\n        #.. call your code ..\n        cov.stop()\n        cov.html_report(directory='covhtml')\n\n    ",
        "klass": "coverage.Coverage",
        "module": "coverage"
    },
    {
        "base_classes": [
            "coverage.debug.SimpleReprMixin"
        ],
        "class_docstring": "Manages collected coverage data, including file storage.\n\n    This class is the public supported API to the data that coverage.py\n    collects during program execution.  It includes information about what code\n    was executed. It does not include information from the analysis phase, to\n    determine what lines could have been executed, or what lines were not\n    executed.\n\n    .. note::\n\n        The data file is currently a SQLite database file, with a\n        :ref:`documented schema <dbschema>`. The schema is subject to change\n        though, so be careful about querying it directly. Use this API if you\n        can to isolate yourself from changes.\n\n    There are a number of kinds of data that can be collected:\n\n    * **lines**: the line numbers of source lines that were executed.\n      These are always available.\n\n    * **arcs**: pairs of source and destination line numbers for transitions\n      between source lines.  These are only available if branch coverage was\n      used.\n\n    * **file tracer names**: the module names of the file tracer plugins that\n      handled each file in the data.\n\n    Lines, arcs, and file tracer names are stored for each source file. File\n    names in this API are case-sensitive, even on platforms with\n    case-insensitive file systems.\n\n    A data file either stores lines, or arcs, but not both.\n\n    A data file is associated with the data when the :class:`CoverageData`\n    is created, using the parameters `basename`, `suffix`, and `no_disk`. The\n    base name can be queried with :meth:`base_filename`, and the actual file\n    name being used is available from :meth:`data_filename`.\n\n    To read an existing coverage.py data file, use :meth:`read`.  You can then\n    access the line, arc, or file tracer data with :meth:`lines`, :meth:`arcs`,\n    or :meth:`file_tracer`.\n\n    The :meth:`has_arcs` method indicates whether arc data is available.  You\n    can get a set of the files in the data with :meth:`measured_files`.  As\n    with most Python containers, you can determine if there is any data at all\n    by using this object as a boolean value.\n\n    The contexts for each line in a file can be read with\n    :meth:`contexts_by_lineno`.\n\n    To limit querying to certain contexts, use :meth:`set_query_context` or\n    :meth:`set_query_contexts`. These will narrow the focus of subsequent\n    :meth:`lines`, :meth:`arcs`, and :meth:`contexts_by_lineno` calls. The set\n    of all measured context names can be retrieved with\n    :meth:`measured_contexts`.\n\n    Most data files will be created by coverage.py itself, but you can use\n    methods here to create data files if you like.  The :meth:`add_lines`,\n    :meth:`add_arcs`, and :meth:`add_file_tracers` methods add data, in ways\n    that are convenient for coverage.py.\n\n    To record data for contexts, use :meth:`set_context` to set a context to\n    be used for subsequent :meth:`add_lines` and :meth:`add_arcs` calls.\n\n    To add a source file without any measured data, use :meth:`touch_file`.\n\n    Write the data to its file with :meth:`write`.\n\n    You can clear the data in memory with :meth:`erase`.  Two data collections\n    can be combined by using :meth:`update` on one :class:`CoverageData`,\n    passing it the other.\n\n    Data in a :class:`CoverageData` can be serialized and deserialized with\n    :meth:`dumps` and :meth:`loads`.\n\n    ",
        "klass": "coverage.CoverageData",
        "module": "coverage"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A collection of aliases for paths.\n\n    When combining data files from remote machines, often the paths to source\n    code are different, for example, due to OS differences, or because of\n    serialized checkouts on continuous integration machines.\n\n    A `PathAliases` object tracks a list of pattern/result pairs, and can\n    map a path through those aliases to produce a unified path.\n\n    ",
        "klass": "coverage.files.PathAliases",
        "module": "coverage"
    },
    {
        "base_classes": [
            "coverage.debug.SimpleReprMixin"
        ],
        "class_docstring": "Manages collected coverage data, including file storage.\n\n    This class is the public supported API to the data that coverage.py\n    collects during program execution.  It includes information about what code\n    was executed. It does not include information from the analysis phase, to\n    determine what lines could have been executed, or what lines were not\n    executed.\n\n    .. note::\n\n        The data file is currently a SQLite database file, with a\n        :ref:`documented schema <dbschema>`. The schema is subject to change\n        though, so be careful about querying it directly. Use this API if you\n        can to isolate yourself from changes.\n\n    There are a number of kinds of data that can be collected:\n\n    * **lines**: the line numbers of source lines that were executed.\n      These are always available.\n\n    * **arcs**: pairs of source and destination line numbers for transitions\n      between source lines.  These are only available if branch coverage was\n      used.\n\n    * **file tracer names**: the module names of the file tracer plugins that\n      handled each file in the data.\n\n    Lines, arcs, and file tracer names are stored for each source file. File\n    names in this API are case-sensitive, even on platforms with\n    case-insensitive file systems.\n\n    A data file either stores lines, or arcs, but not both.\n\n    A data file is associated with the data when the :class:`CoverageData`\n    is created, using the parameters `basename`, `suffix`, and `no_disk`. The\n    base name can be queried with :meth:`base_filename`, and the actual file\n    name being used is available from :meth:`data_filename`.\n\n    To read an existing coverage.py data file, use :meth:`read`.  You can then\n    access the line, arc, or file tracer data with :meth:`lines`, :meth:`arcs`,\n    or :meth:`file_tracer`.\n\n    The :meth:`has_arcs` method indicates whether arc data is available.  You\n    can get a set of the files in the data with :meth:`measured_files`.  As\n    with most Python containers, you can determine if there is any data at all\n    by using this object as a boolean value.\n\n    The contexts for each line in a file can be read with\n    :meth:`contexts_by_lineno`.\n\n    To limit querying to certain contexts, use :meth:`set_query_context` or\n    :meth:`set_query_contexts`. These will narrow the focus of subsequent\n    :meth:`lines`, :meth:`arcs`, and :meth:`contexts_by_lineno` calls. The set\n    of all measured context names can be retrieved with\n    :meth:`measured_contexts`.\n\n    Most data files will be created by coverage.py itself, but you can use\n    methods here to create data files if you like.  The :meth:`add_lines`,\n    :meth:`add_arcs`, and :meth:`add_file_tracers` methods add data, in ways\n    that are convenient for coverage.py.\n\n    To record data for contexts, use :meth:`set_context` to set a context to\n    be used for subsequent :meth:`add_lines` and :meth:`add_arcs` calls.\n\n    To add a source file without any measured data, use :meth:`touch_file`.\n\n    Write the data to its file with :meth:`write`.\n\n    You can clear the data in memory with :meth:`erase`.  Two data collections\n    can be combined by using :meth:`update` on one :class:`CoverageData`,\n    passing it the other.\n\n    Data in a :class:`CoverageData` can be serialized and deserialized with\n    :meth:`dumps` and :meth:`loads`.\n\n    ",
        "klass": "coverage.data.CoverageData",
        "module": "coverage"
    },
    {
        "base_classes": [
            "projectq.cengines._basics.BasicEngine"
        ],
        "class_docstring": "\n    The MainEngine class provides all functionality of the main compiler\n    engine.\n\n    It initializes all further compiler engines (calls, e.g.,\n    .next_engine=...) and keeps track of measurement results and active\n    qubits (and their IDs).\n\n    Attributes:\n        next_engine (BasicEngine): Next compiler engine (or the back-end).\n        main_engine (MainEngine): Self.\n        active_qubits (WeakSet): WeakSet containing all active qubits\n        dirty_qubits (Set): Containing all dirty qubit ids\n        backend (BasicEngine): Access the back-end.\n        mapper (BasicMapperEngine): Access to the mapper if there is one.\n\n    ",
        "klass": "projectq.MainEngine",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.cengines._basics.BasicEngine"
        ],
        "class_docstring": "\n    CircuitDrawer is a compiler engine which generates TikZ code for drawing\n    quantum circuits.\n\n    The circuit can be modified by editing the settings.json file which is\n    generated upon first execution. This includes adjusting the gate width,\n    height, shadowing, line thickness, and many more options.\n\n    After initializing the CircuitDrawer, it can also be given the mapping\n    from qubit IDs to wire location (via the :meth:`set_qubit_locations`\n    function):\n\n    .. code-block:: python\n\n        circuit_backend = CircuitDrawer()\n        circuit_backend.set_qubit_locations({0: 1, 1: 0}) # swap lines 0 and 1\n        eng = MainEngine(circuit_backend)\n\n        ... # run quantum algorithm on this main engine\n\n        print(circuit_backend.get_latex()) # prints LaTeX code\n\n    To see the qubit IDs in the generated circuit, simply set the `draw_id`\n    option in the settings.json file under \"gates\":\"AllocateQubitGate\" to\n    True:\n\n    .. code-block:: python\n\n        \"gates\": {\n            \"AllocateQubitGate\": {\n                \"draw_id\": True,\n                \"height\": 0.15,\n                \"width\": 0.2,\n                \"pre_offset\": 0.1,\n                \"offset\": 0.1\n            },\n            ...\n\n    The settings.json file has the following structure:\n\n    .. code-block:: python\n\n        {\n            \"control\": { # settings for control \"circle\"\n                    \"shadow\": false,\n                    \"size\": 0.1\n            },\n            \"gate_shadow\": true, # enable/disable shadows for all gates\n            \"gates\": {\n                    \"GateClassString\": {\n                        GATE_PROPERTIES\n                    }\n                    \"GateClassString2\": {\n                        ...\n            },\n            \"lines\": { # settings for qubit lines\n                    \"double_classical\": true, # draw double-lines for\n                                              # classical bits\n                    \"double_lines_sep\": 0.04, # gap between the two lines\n                                              # for double lines\n                    \"init_quantum\": true, # start out with quantum bits\n                    \"style\": \"very thin\" # line style\n            }\n        }\n\n    All gates (except for the ones requiring special treatment) support the\n    following properties:\n\n    .. code-block:: python\n\n        \"GateClassString\": {\n            \"height\": GATE_HEIGHT,\n            \"width\": GATE_WIDTH\n            \"pre_offset\": OFFSET_BEFORE_PLACEMENT,\n            \"offset\": OFFSET_AFTER_PLACEMENT,\n        },\n\n    ",
        "klass": "projectq.backends._circuits._drawer.CircuitDrawer",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.cengines._basics.BasicEngine"
        ],
        "class_docstring": "\n    CircuitDrawer is a compiler engine which generates TikZ code for drawing\n    quantum circuits.\n\n    The circuit can be modified by editing the settings.json file which is\n    generated upon first execution. This includes adjusting the gate width,\n    height, shadowing, line thickness, and many more options.\n\n    After initializing the CircuitDrawer, it can also be given the mapping\n    from qubit IDs to wire location (via the :meth:`set_qubit_locations`\n    function):\n\n    .. code-block:: python\n\n        circuit_backend = CircuitDrawer()\n        circuit_backend.set_qubit_locations({0: 1, 1: 0}) # swap lines 0 and 1\n        eng = MainEngine(circuit_backend)\n\n        ... # run quantum algorithm on this main engine\n\n        print(circuit_backend.get_latex()) # prints LaTeX code\n\n    To see the qubit IDs in the generated circuit, simply set the `draw_id`\n    option in the settings.json file under \"gates\":\"AllocateQubitGate\" to\n    True:\n\n    .. code-block:: python\n\n        \"gates\": {\n            \"AllocateQubitGate\": {\n                \"draw_id\": True,\n                \"height\": 0.15,\n                \"width\": 0.2,\n                \"pre_offset\": 0.1,\n                \"offset\": 0.1\n            },\n            ...\n\n    The settings.json file has the following structure:\n\n    .. code-block:: python\n\n        {\n            \"control\": { # settings for control \"circle\"\n                    \"shadow\": false,\n                    \"size\": 0.1\n            },\n            \"gate_shadow\": true, # enable/disable shadows for all gates\n            \"gates\": {\n                    \"GateClassString\": {\n                        GATE_PROPERTIES\n                    }\n                    \"GateClassString2\": {\n                        ...\n            },\n            \"lines\": { # settings for qubit lines\n                    \"double_classical\": true, # draw double-lines for\n                                              # classical bits\n                    \"double_lines_sep\": 0.04, # gap between the two lines\n                                              # for double lines\n                    \"init_quantum\": true, # start out with quantum bits\n                    \"style\": \"very thin\" # line style\n            }\n        }\n\n    All gates (except for the ones requiring special treatment) support the\n    following properties:\n\n    .. code-block:: python\n\n        \"GateClassString\": {\n            \"height\": GATE_HEIGHT,\n            \"width\": GATE_WIDTH\n            \"pre_offset\": OFFSET_BEFORE_PLACEMENT,\n            \"offset\": OFFSET_AFTER_PLACEMENT,\n        },\n\n    ",
        "klass": "projectq.backends.CircuitDrawer",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Condition an entire code block on the value of qubits being 1.\n\n    Example:\n        .. code-block:: python\n\n            with Control(eng, ctrlqubits):\n                do_something(otherqubits)\n    ",
        "klass": "projectq.meta._control.Control",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Base class of all gates. (Don't use it directly but derive from it)\n    ",
        "klass": "projectq.ops._basics.BasicGate",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Class used as a container to store commands. If a gate is applied to\n    qubits, then the gate and qubits are saved in a command object. Qubits\n    are copied into WeakQubitRefs in order to allow early deallocation (would\n    be kept alive otherwise). WeakQubitRef qubits don't send deallocate gate\n    when destructed.\n\n    Attributes:\n        gate: The gate to execute\n        qubits: Tuple of qubit lists (e.g. Quregs). Interchangeable qubits\n                  are stored in a unique order\n        control_qubits: The Qureg of control qubits in a unique order\n        engine: The engine (usually: MainEngine)\n        tags: The list of tag objects associated with this command\n          (e.g., ComputeTag, UncomputeTag, LoopTag, ...). tag objects need to\n          support ==, != (__eq__ and __ne__) for comparison as used in e.g.\n          TagRemover. New tags should always be added to the end of the list.\n          This means that if there are e.g. two LoopTags in a command, tag[0]\n          is from the inner scope while tag[1] is from the other scope as the\n          other scope receives the command after the inner scope LoopEngine\n          and hence adds its LoopTag to the end.\n        all_qubits: A tuple of control_qubits + qubits\n    ",
        "klass": "projectq.ops._command.Command",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.ops._basics.BasicGate"
        ],
        "class_docstring": "\n    Gate for time evolution under a Hamiltonian (QubitOperator object).\n\n    This gate is the unitary time evolution propagator:\n    exp(-i * H * t),\n    where H is the Hamiltonian of the system and t is the time. Note that -i\n    factor is stored implicitely.\n\n    Example:\n        .. code-block:: python\n\n            wavefunction = eng.allocate_qureg(5)\n            hamiltonian = 0.5 * QubitOperator(\"X0 Z1 Y5\")\n            # Apply exp(-i * H * t) to the wavefunction:\n            TimeEvolution(time=2.0, hamiltonian=hamiltonian) | wavefunction\n\n    Attributes:\n        time(float, int): time t\n        hamiltonian(QubitOperator): hamiltonaian H\n\n    ",
        "klass": "projectq.ops._time_evolution.TimeEvolution",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Invert an entire code block.\n\n    Use it with a with-statement, i.e.,\n\n    .. code-block:: python\n\n        with Dagger(eng):\n            [code to invert]\n\n    Warning:\n        If the code to invert contains allocation of qubits, those qubits have\n        to be deleted prior to exiting the 'with Dagger()' context.\n\n        This code is **NOT VALID**:\n\n        .. code-block:: python\n\n            with Dagger(eng):\n                qb = eng.allocate_qubit()\n                H | qb # qb is still available!!!\n\n        The **correct way** of handling qubit (de-)allocation is as follows:\n\n        .. code-block:: python\n\n            with Dagger(eng):\n                qb = eng.allocate_qubit()\n                ...\n                del qb # sends deallocate gate (which becomes an allocate)\n    ",
        "klass": "projectq.meta._dagger.Dagger",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.cengines._basics.BasicEngine"
        ],
        "class_docstring": "\n    The MainEngine class provides all functionality of the main compiler\n    engine.\n\n    It initializes all further compiler engines (calls, e.g.,\n    .next_engine=...) and keeps track of measurement results and active\n    qubits (and their IDs).\n\n    Attributes:\n        next_engine (BasicEngine): Next compiler engine (or the back-end).\n        main_engine (MainEngine): Self.\n        active_qubits (WeakSet): WeakSet containing all active qubits\n        dirty_qubits (Set): Containing all dirty qubit ids\n        backend (BasicEngine): Access the back-end.\n        mapper (BasicMapperEngine): Access to the mapper if there is one.\n\n    ",
        "klass": "projectq.cengines.MainEngine",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.ops._basics.BasicGate"
        ],
        "class_docstring": "\n    Defines a base class of a rotation gate.\n\n    A rotation gate has a continuous parameter (the angle), labeled 'angle' /\n    self.angle. Its inverse is the same gate with the negated argument.\n    Rotation gates of the same class can be merged by adding the angles.\n    The continuous parameter is modulo 4 * pi, self.angle is in the interval\n    [0, 4 * pi).\n    ",
        "klass": "projectq.ops._basics.BasicRotationGate",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Start a compute-section.\n\n    Example:\n        .. code-block:: python\n\n            with Compute(eng):\n                do_something(qubits)\n            action(qubits)\n            Uncompute(eng) # runs inverse of the compute section\n\n    Warning:\n        If qubits are allocated within the compute section, they must either be\n        uncomputed and deallocated within that section or, alternatively,\n        uncomputed and deallocated in the following uncompute section.\n\n        This means that the following examples are valid:\n\n        .. code-block:: python\n\n            with Compute(eng):\n                anc = eng.allocate_qubit()\n                do_something_with_ancilla(anc)\n                ...\n                uncompute_ancilla(anc)\n                del anc\n\n            do_something_else(qubits)\n\n            Uncompute(eng)  # will allocate a new ancilla (with a different id)\n                            # and then deallocate it again\n\n        .. code-block:: python\n\n            with Compute(eng):\n                anc = eng.allocate_qubit()\n                do_something_with_ancilla(anc)\n                ...\n\n            do_something_else(qubits)\n\n            Uncompute(eng)  # will deallocate the ancilla!\n\n        After the uncompute section, ancilla qubits allocated within the\n        compute section will be invalid (and deallocated). The same holds when\n        using CustomUncompute.\n\n        Failure to comply with these rules results in an exception being\n        thrown.\n    ",
        "klass": "projectq.meta.Compute",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Condition an entire code block on the value of qubits being 1.\n\n    Example:\n        .. code-block:: python\n\n            with Control(eng, ctrlqubits):\n                do_something(otherqubits)\n    ",
        "klass": "projectq.meta.Control",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Start a custom uncompute-section.\n\n    Example:\n        .. code-block:: python\n\n            with Compute(eng):\n                do_something(qubits)\n            action(qubits)\n            with CustomUncompute(eng):\n                do_something_inverse(qubits)\n\n    Raises:\n        QubitManagementError: If qubits are allocated within Compute or within\n                              CustomUncompute context but are not deallocated.\n    ",
        "klass": "projectq.meta.CustomUncompute",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Invert an entire code block.\n\n    Use it with a with-statement, i.e.,\n\n    .. code-block:: python\n\n        with Dagger(eng):\n            [code to invert]\n\n    Warning:\n        If the code to invert contains allocation of qubits, those qubits have\n        to be deleted prior to exiting the 'with Dagger()' context.\n\n        This code is **NOT VALID**:\n\n        .. code-block:: python\n\n            with Dagger(eng):\n                qb = eng.allocate_qubit()\n                H | qb # qb is still available!!!\n\n        The **correct way** of handling qubit (de-)allocation is as follows:\n\n        .. code-block:: python\n\n            with Dagger(eng):\n                qb = eng.allocate_qubit()\n                ...\n                del qb # sends deallocate gate (which becomes an allocate)\n    ",
        "klass": "projectq.meta.Dagger",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Loop n times over an entire code block.\n\n    Example:\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                # [quantum gates to be executed 4 times]\n\n    Warning:\n        If the code in the loop contains allocation of qubits, those qubits\n        have to be deleted prior to exiting the 'with Loop()' context.\n\n        This code is **NOT VALID**:\n\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                qb = eng.allocate_qubit()\n                H | qb # qb is still available!!!\n\n        The **correct way** of handling qubit (de-)allocation is as follows:\n\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                qb = eng.allocate_qubit()\n                ...\n                del qb # sends deallocate gate\n    ",
        "klass": "projectq.meta._loop.Loop",
        "module": "projectq"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Loop n times over an entire code block.\n\n    Example:\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                # [quantum gates to be executed 4 times]\n\n    Warning:\n        If the code in the loop contains allocation of qubits, those qubits\n        have to be deleted prior to exiting the 'with Loop()' context.\n\n        This code is **NOT VALID**:\n\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                qb = eng.allocate_qubit()\n                H | qb # qb is still available!!!\n\n        The **correct way** of handling qubit (de-)allocation is as follows:\n\n        .. code-block:: python\n\n            with Loop(eng, 4):\n                qb = eng.allocate_qubit()\n                ...\n                del qb # sends deallocate gate\n    ",
        "klass": "projectq.meta.Loop",
        "module": "projectq"
    },
    {
        "base_classes": [
            "projectq.ops._basics.BasicGate"
        ],
        "class_docstring": "\n    Defines a base class of a phase gate.\n\n    A phase gate has a continuous parameter (the angle), labeled 'angle' /\n    self.angle. Its inverse is the same gate with the negated argument.\n    Phase gates of the same class can be merged by adding the angles.\n    The continuous parameter is modulo 2 * pi, self.angle is in the interval\n    [0, 2 * pi).\n    ",
        "klass": "projectq.ops._basics.BasicPhaseGate",
        "module": "projectq"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Isolate a specific environment variable.",
        "klass": "fixtures.EnvironmentVariable",
        "module": "fixtures"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Replace a logger and capture its output.",
        "klass": "fixtures.FakeLogger",
        "module": "fixtures"
    },
    {
        "base_classes": [
            "fixtures._fixtures.mockpatch._Base"
        ],
        "class_docstring": "Deal with code around mock.",
        "klass": "fixtures.MockPatchObject",
        "module": "fixtures"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Replace or delete an attribute.",
        "klass": "fixtures.MonkeyPatch",
        "module": "fixtures"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Fixture that aborts the contained code after a number of seconds.\n\n    The interrupt can be either gentle, in which case TimeoutException is\n    raised, or not gentle, in which case the process will typically be aborted\n    by SIGALRM.\n\n    Cautions:\n     * This has no effect on Windows.\n     * Only one Timeout can be used at any time per process.\n    ",
        "klass": "fixtures.Timeout",
        "module": "fixtures"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Elasticsearch low-level client. Provides a straightforward mapping from\n    Python to ES REST endpoints.\n\n    The instance has attributes ``cat``, ``cluster``, ``indices``, ``ingest``,\n    ``nodes``, ``snapshot`` and ``tasks`` that provide access to instances of\n    :class:`~elasticsearch.client.CatClient`,\n    :class:`~elasticsearch.client.ClusterClient`,\n    :class:`~elasticsearch.client.IndicesClient`,\n    :class:`~elasticsearch.client.IngestClient`,\n    :class:`~elasticsearch.client.NodesClient`,\n    :class:`~elasticsearch.client.SnapshotClient` and\n    :class:`~elasticsearch.client.TasksClient` respectively. This is the\n    preferred (and only supported) way to get access to those classes and their\n    methods.\n\n    You can specify your own connection class which should be used by providing\n    the ``connection_class`` parameter::\n\n        # create connection to localhost using the ThriftConnection\n        es = Elasticsearch(connection_class=ThriftConnection)\n\n    If you want to turn on :ref:`sniffing` you have several options (described\n    in :class:`~elasticsearch.Transport`)::\n\n        # create connection that will automatically inspect the cluster to get\n        # the list of active nodes. Start with nodes running on 'esnode1' and\n        # 'esnode2'\n        es = Elasticsearch(\n            ['esnode1', 'esnode2'],\n            # sniff before doing anything\n            sniff_on_start=True,\n            # refresh nodes after a node fails to respond\n            sniff_on_connection_fail=True,\n            # and also every 60 seconds\n            sniffer_timeout=60\n        )\n\n    Different hosts can have different parameters, use a dictionary per node to\n    specify those::\n\n        # connect to localhost directly and another node using SSL on port 443\n        # and an url_prefix. Note that ``port`` needs to be an int.\n        es = Elasticsearch([\n            {'host': 'localhost'},\n            {'host': 'othernode', 'port': 443, 'url_prefix': 'es', 'use_ssl': True},\n        ])\n\n    If using SSL, there are several parameters that control how we deal with\n    certificates (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # make sure we verify SSL certificates\n            verify_certs=True,\n            # provide a path to CA certs on disk\n            ca_certs='/path/to/CA_certs'\n        )\n\n    If using SSL, but don't verify the certs, a warning message is showed\n    optionally (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # no verify SSL certificates\n            verify_certs=False,\n            # don't show warnings about ssl certs verification\n            ssl_show_warn=False\n        )\n\n    SSL client authentication is supported\n    (see :class:`~elasticsearch.Urllib3HttpConnection` for\n    detailed description of the options)::\n\n        es = Elasticsearch(\n            ['localhost:443', 'other_host:443'],\n            # turn on SSL\n            use_ssl=True,\n            # make sure we verify SSL certificates\n            verify_certs=True,\n            # provide a path to CA certs on disk\n            ca_certs='/path/to/CA_certs',\n            # PEM formatted SSL client certificate\n            client_cert='/path/to/clientcert.pem',\n            # PEM formatted SSL client key\n            client_key='/path/to/clientkey.pem'\n        )\n\n    Alternatively you can use RFC-1738 formatted URLs, as long as they are not\n    in conflict with other options::\n\n        es = Elasticsearch(\n            [\n                'http://user:secret@localhost:9200/',\n                'https://user:secret@other_host:443/production'\n            ],\n            verify_certs=True\n        )\n\n    By default, `JSONSerializer\n    <https://github.com/elastic/elasticsearch-py/blob/master/elasticsearch/serializer.py#L24>`_\n    is used to encode all outgoing requests.\n    However, you can implement your own custom serializer::\n\n        from elasticsearch.serializer import JSONSerializer\n\n        class SetEncoder(JSONSerializer):\n            def default(self, obj):\n                if isinstance(obj, set):\n                    return list(obj)\n                if isinstance(obj, Something):\n                    return 'CustomSomethingRepresentation'\n                return JSONSerializer.default(self, obj)\n\n        es = Elasticsearch(serializer=SetEncoder())\n\n    ",
        "klass": "elasticsearch.Elasticsearch",
        "module": "elasticsearch"
    },
    {
        "base_classes": [
            "queue.Queue"
        ],
        "class_docstring": "Variant of Queue that retrieves open entries in priority order (lowest first).\n\n    Entries are typically tuples of the form:  (priority number, data).\n    ",
        "klass": "queue.PriorityQueue",
        "module": "queue"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A plugin can retrieve stream information from the URL specified.\n\n    :param url: URL that the plugin will operate on\n    ",
        "klass": "streamlink.plugin.Plugin",
        "module": "streamlink"
    },
    {
        "base_classes": [
            "logging.Formatter"
        ],
        "class_docstring": "A context.RequestContext aware formatter configured through flags.\n\n    The flags used to set format strings are: logging_context_format_string\n    and logging_default_format_string.  You can also specify\n    logging_debug_format_suffix to append extra formatting if the log level is\n    debug.\n\n    The standard variables available to the formatter are listed at:\n    http://docs.python.org/library/logging.html#formatter\n\n    In addition to the standard variables, one custom variable is\n    available to both formatting string: `isotime` produces a\n    timestamp in ISO8601 format, suitable for producing\n    RFC5424-compliant log messages.\n\n    Furthermore, logging_context_format_string has access to all of\n    the data in a dict representation of the context.\n    ",
        "klass": "oslo_log.formatters.ContextFormatter",
        "module": "oslo_log"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A text-processing pipeline. Usually you'll load this once per process,\n    and pass the instance around your application.\n\n    Defaults (class): Settings, data and factory methods for creating the `nlp`\n        object and processing pipeline.\n    lang (unicode): Two-letter language ID, i.e. ISO code.\n\n    DOCS: https://spacy.io/api/language\n    ",
        "klass": "spacy.language.Language",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Tokenizer(Vocab vocab, rules=None, prefix_search=None, suffix_search=None, infix_finditer=None, token_match=None)\nSegment text, and create Doc objects with the discovered segment\n    boundaries.\n\n    DOCS: https://spacy.io/api/tokenizer\n    ",
        "klass": "spacy.tokenizer.Tokenizer",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A look-up table that allows you to access `Lexeme` objects. The `Vocab`\n    instance also provides access to the `StringStore`, and owns underlying\n    C-data that is shared between `Doc` objects.\n\n    DOCS: https://spacy.io/api/vocab\n    ",
        "klass": "spacy.vocab.Vocab",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Match sequences of tokens, based on pattern rules.\n\n    DOCS: https://spacy.io/api/matcher\n    USAGE: https://spacy.io/usage/rule-based-matching\n    ",
        "klass": "spacy.matcher.Matcher",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Efficiently match large terminology lists. While the `Matcher` matches\n    sequences based on lists of token descriptions, the `PhraseMatcher` accepts\n    match patterns in the form of `Doc` objects.\n\n    DOCS: https://spacy.io/api/phrasematcher\n    USAGE: https://spacy.io/usage/rule-based-matching#phrasematcher\n\n    Adapted from FlashText: https://github.com/vi3k6i5/flashtext\n    MIT License (see `LICENSE`)\n    Copyright (c) 2017 Vikash Singh (vikash.duliajan@gmail.com)\n    ",
        "klass": "spacy.matcher.PhraseMatcher",
        "module": "spacy"
    },
    {
        "base_classes": [
            "spacy.syntax.nn_parser.Parser"
        ],
        "class_docstring": "Pipeline component for dependency parsing.\n\n    DOCS: https://spacy.io/api/dependencyparser\n    ",
        "klass": "spacy.pipeline.DependencyParser",
        "module": "spacy"
    },
    {
        "base_classes": [
            "spacy.syntax.nn_parser.Parser"
        ],
        "class_docstring": "Pipeline component for named entity recognition.\n\n    DOCS: https://spacy.io/api/entityrecognizer\n    ",
        "klass": "spacy.pipeline.EntityRecognizer",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The EntityRuler lets you add spans to the `Doc.ents` using token-based\n    rules or exact phrase matches. It can be combined with the statistical\n    `EntityRecognizer` to boost accuracy, or used on its own to implement a\n    purely rule-based entity recognition system. After initialization, the\n    component is typically added to the pipeline using `nlp.add_pipe`.\n\n    DOCS: https://spacy.io/api/entityruler\n    USAGE: https://spacy.io/usage/rule-based-matching#entityruler\n    ",
        "klass": "spacy.pipeline.EntityRuler",
        "module": "spacy"
    },
    {
        "base_classes": [
            "spacy.pipeline.pipes.Pipe"
        ],
        "class_docstring": "Pipeline component for part-of-speech tagging.\n\n    DOCS: https://spacy.io/api/tagger\n    ",
        "klass": "spacy.pipeline.Tagger",
        "module": "spacy"
    },
    {
        "base_classes": [
            "spacy.pipeline.pipes.Pipe"
        ],
        "class_docstring": "Pre-train position-sensitive vectors for tokens.",
        "klass": "spacy.pipeline.Tensorizer",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Look up strings by 64-bit hashes.\n\n    DOCS: https://spacy.io/api/stringstore\n    ",
        "klass": "spacy.strings.StringStore",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A sequence of Token objects. Access sentences and named entities, export\n    annotations to numpy arrays, losslessly serialize to compressed binary\n    strings. The `Doc` object holds an array of `TokenC` structs. The\n    Python-level `Token` and `Span` objects are views of this array, i.e.\n    they don't own the data themselves.\n\n    EXAMPLE:\n        Construction 1\n        >>> doc = nlp(u'Some text')\n\n        Construction 2\n        >>> from spacy.tokens import Doc\n        >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n        >>>           spaces=[True, False, False])\n\n    DOCS: https://spacy.io/api/doc\n    ",
        "klass": "spacy.tokens.Doc",
        "module": "spacy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Workbook is the container for all other parts of the document.",
        "klass": "openpyxl.Workbook",
        "module": "openpyxl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Workbook is the container for all other parts of the document.",
        "klass": "openpyxl.workbook.Workbook",
        "module": "openpyxl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager that can replace lxml.etree.xmlfile.",
        "klass": "openpyxl.xml.functions.xmlfile",
        "module": "et_xmlfile"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A validation schema.\n\n    The schema is a Python tree-like structure where nodes are pattern\n    matched against corresponding trees of values.\n\n    Nodes can be values, in which case a direct comparison is used, types,\n    in which case an isinstance() check is performed, or callables, which will\n    validate and optionally convert the value.\n\n    We can equate schemas also.\n\n    For Example:\n\n            >>> v = Schema({Required('a'): unicode})\n            >>> v1 = Schema({Required('a'): unicode})\n            >>> v2 = Schema({Required('b'): unicode})\n            >>> assert v == v1\n            >>> assert v != v2\n\n    ",
        "klass": "voluptuous.Schema",
        "module": "voluptuous"
    },
    {
        "base_classes": [
            "statsmodels.base.model.LikelihoodModel"
        ],
        "class_docstring": "\n    Generalized Linear Models class\n\n    GLM inherits from statsmodels.base.model.LikelihoodModel\n\n    Parameters\n    ----------\n    endog : array-like\n        1d array of endogenous response variable.  This array can be 1d or 2d.\n        Binomial family models accept a 2d array with two columns. If\n        supplied, each observation is expected to be [success, failure].\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user (models specified using a formula\n        include an intercept by default). See `statsmodels.tools.add_constant`.\n    family : family class instance\n        The default is Gaussian.  To specify the binomial distribution\n        family = sm.family.Binomial()\n        Each family can take a link instance as an argument.  See\n        statsmodels.family.family for more information.\n    offset : array-like or None\n        An offset to be included in the model.  If provided, must be\n        an array whose length is the number of rows in exog.\n    exposure : array-like or None\n        Log(exposure) will be added to the linear prediction in the model.\n        Exposure is only valid if the log link is used. If provided, it must be\n        an array with the same length as endog.\n    freq_weights : array-like\n        1d array of frequency weights. The default is None. If None is selected\n        or a blank value, then the algorithm will replace with an array of 1's\n        with length equal to the endog.\n        WARNING: Using weights is not verified yet for all possible options\n        and results, see Notes.\n    var_weights : array-like\n        1d array of variance (analytic) weights. The default is None. If None\n        is selected or a blank value, then the algorithm will replace with an\n        array of 1's with length equal to the endog.\n        WARNING: Using weights is not verified yet for all possible options\n        and results, see Notes.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n\n    Attributes\n    ----------\n    df_model : float\n        Model degrees of freedom is equal to p - 1, where p is the number\n        of regressors.  Note that the intercept is not reported as a\n        degree of freedom.\n    df_resid : float\n        Residual degrees of freedom is equal to the number of observation n\n        minus the number of regressors p.\n    endog : array\n        See Notes.  Note that `endog` is a reference to the data so that if\n        data is already an array and it is changed, then `endog` changes\n        as well.\n    exposure : array-like\n        Include ln(exposure) in model with coefficient constrained to 1. Can\n        only be used if the link is the logarithm function.\n    exog : array\n        See Notes.  Note that `exog` is a reference to the data so that if\n        data is already an array and it is changed, then `exog` changes\n        as well.\n    freq_weights : array\n        See Notes. Note that `freq_weights` is a reference to the data so that\n        if data is already an array and it is changed, then `freq_weights`\n        changes as well.\n    var_weights : array\n        See Notes. Note that `var_weights` is a reference to the data so that\n        if data is already an array and it is changed, then `var_weights`\n        changes as well.\n    iteration : int\n        The number of iterations that fit has run.  Initialized at 0.\n    family : family class instance\n        The distribution family of the model. Can be any family in\n        statsmodels.families.  Default is Gaussian.\n    mu : array\n        The mean response of the transformed variable.  `mu` is the value of\n        the inverse of the link function at lin_pred, where lin_pred is the\n        linear predicted value of the WLS fit of the transformed variable.\n        `mu` is only available after fit is called.  See\n        statsmodels.families.family.fitted of the distribution family for more\n        information.\n    n_trials : array\n        See Notes. Note that `n_trials` is a reference to the data so that if\n        data is already an array and it is changed, then `n_trials` changes\n        as well. `n_trials` is the number of binomial trials and only available\n        with that distribution. See statsmodels.families.Binomial for more\n        information.\n    normalized_cov_params : array\n        The p x p normalized covariance of the design / exogenous data.\n        This is approximately equal to (X.T X)^(-1)\n    offset : array-like\n        Include offset in model with coefficient constrained to 1.\n    scale : float\n        The estimate of the scale / dispersion of the model fit.  Only\n        available after fit is called.  See GLM.fit and GLM.estimate_scale\n        for more information.\n    scaletype : str\n        The scaling used for fitting the model.  This is only available after\n        fit is called.  The default is None.  See GLM.fit for more information.\n    weights : array\n        The value of the weights after the last iteration of fit.  Only\n        available after fit is called.  See statsmodels.families.family for\n        the specific distribution weighting functions.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> data = sm.datasets.scotland.load(as_pandas=False)\n    >>> data.exog = sm.add_constant(data.exog)\n\n    Instantiate a gamma family model with the default link function.\n\n    >>> gamma_model = sm.GLM(data.endog, data.exog,\n    ...                      family=sm.families.Gamma())\n\n    >>> gamma_results = gamma_model.fit()\n    >>> gamma_results.params\n    array([-0.01776527,  0.00004962,  0.00203442, -0.00007181,  0.00011185,\n           -0.00000015, -0.00051868, -0.00000243])\n    >>> gamma_results.scale\n    0.0035842831734919055\n    >>> gamma_results.deviance\n    0.087388516416999198\n    >>> gamma_results.pearson_chi2\n    0.086022796163805704\n    >>> gamma_results.llf\n    -83.017202161073527\n\n    See Also\n    --------\n    statsmodels.genmod.families.family.Family\n    :ref:`families`\n    :ref:`links`\n\n    Notes\n    -----\n    Only the following combinations make sense for family and link:\n\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n     Family        ident log logit probit cloglog pow opow nbinom loglog logc\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n     Gaussian      x     x   x     x      x       x   x     x      x\n     inv Gaussian  x     x                        x\n     binomial      x     x   x     x      x       x   x           x      x\n     Poission      x     x                        x\n     neg binomial  x     x                        x        x\n     gamma         x     x                        x\n     Tweedie       x     x                        x\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n\n    Not all of these link functions are currently available.\n\n    Endog and exog are references so that if the data they refer to are already\n    arrays and these arrays are changed, endog and exog will change.\n\n    Statsmodels supports two separte definitions of weights: frequency weights\n    and variance weights.\n\n    Frequency weights produce the same results as repeating observations by the\n    frequencies (if those are integers). Frequency weights will keep the number\n    of observations consistent, but the degrees of freedom will change to\n    reflect the new weights.\n\n    Variance weights (referred to in other packages as analytic weights) are\n    used when ``endog`` represents an an average or mean. This relies on the\n    assumption that that the inverse variance scales proportionally to the\n    weight--an observation that is deemed more credible should have less\n    variance and therefore have more weight. For the ``Poisson`` family--which\n    assumes that occurences scale proportionally with time--a natural practice\n    would be to use the amount of time as the variance weight and set ``endog``\n    to be a rate (occurrances per period of time). Similarly, using a\n    compound Poisson family, namely ``Tweedie``, makes a similar assumption\n    about the rate (or frequency) of occurences having variance proportional to\n    time.\n\n    Both frequency and variance weights are verified for all basic results with\n    nonrobust or heteroscedasticity robust ``cov_type``. Other robust\n    covariance types have not yet been verified, and at least the small sample\n    correction is currently not based on the correct total frequency count.\n\n    Currently, all residuals are not weighted by frequency, although they may\n    incorporate ``n_trials`` for ``Binomial`` and ``var_weights``\n\n    +---------------+----------------------------------+\n    | Residual Type | Applicable weights               |\n    +===============+==================================+\n    | Anscombe      | ``var_weights``                  |\n    +---------------+----------------------------------+\n    | Deviance      | ``var_weights``                  |\n    +---------------+----------------------------------+\n    | Pearson       | ``var_weights`` and ``n_trials`` |\n    +---------------+----------------------------------+\n    | Reponse       | ``n_trials``                     |\n    +---------------+----------------------------------+\n    | Working       | ``n_trials``                     |\n    +---------------+----------------------------------+\n\n    WARNING: Loglikelihood and deviance are not valid in models where\n    scale is equal to 1 (i.e., ``Binomial``, ``NegativeBinomial``, and\n    ``Poisson``). If variance weights are specified, then results such as\n    ``loglike`` and ``deviance`` are based on a quasi-likelihood\n    interpretation. The loglikelihood is not correctly specified in this case,\n    and statistics based on it, such AIC or likelihood ratio tests, are not\n    appropriate.\n\n    ",
        "klass": "statsmodels.genmod.generalized_linear_model.GLM",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.base.model.LikelihoodModel"
        ],
        "class_docstring": "\n    Generalized Linear Models class\n\n    GLM inherits from statsmodels.base.model.LikelihoodModel\n\n    Parameters\n    ----------\n    endog : array-like\n        1d array of endogenous response variable.  This array can be 1d or 2d.\n        Binomial family models accept a 2d array with two columns. If\n        supplied, each observation is expected to be [success, failure].\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user (models specified using a formula\n        include an intercept by default). See `statsmodels.tools.add_constant`.\n    family : family class instance\n        The default is Gaussian.  To specify the binomial distribution\n        family = sm.family.Binomial()\n        Each family can take a link instance as an argument.  See\n        statsmodels.family.family for more information.\n    offset : array-like or None\n        An offset to be included in the model.  If provided, must be\n        an array whose length is the number of rows in exog.\n    exposure : array-like or None\n        Log(exposure) will be added to the linear prediction in the model.\n        Exposure is only valid if the log link is used. If provided, it must be\n        an array with the same length as endog.\n    freq_weights : array-like\n        1d array of frequency weights. The default is None. If None is selected\n        or a blank value, then the algorithm will replace with an array of 1's\n        with length equal to the endog.\n        WARNING: Using weights is not verified yet for all possible options\n        and results, see Notes.\n    var_weights : array-like\n        1d array of variance (analytic) weights. The default is None. If None\n        is selected or a blank value, then the algorithm will replace with an\n        array of 1's with length equal to the endog.\n        WARNING: Using weights is not verified yet for all possible options\n        and results, see Notes.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n\n    Attributes\n    ----------\n    df_model : float\n        Model degrees of freedom is equal to p - 1, where p is the number\n        of regressors.  Note that the intercept is not reported as a\n        degree of freedom.\n    df_resid : float\n        Residual degrees of freedom is equal to the number of observation n\n        minus the number of regressors p.\n    endog : array\n        See Notes.  Note that `endog` is a reference to the data so that if\n        data is already an array and it is changed, then `endog` changes\n        as well.\n    exposure : array-like\n        Include ln(exposure) in model with coefficient constrained to 1. Can\n        only be used if the link is the logarithm function.\n    exog : array\n        See Notes.  Note that `exog` is a reference to the data so that if\n        data is already an array and it is changed, then `exog` changes\n        as well.\n    freq_weights : array\n        See Notes. Note that `freq_weights` is a reference to the data so that\n        if data is already an array and it is changed, then `freq_weights`\n        changes as well.\n    var_weights : array\n        See Notes. Note that `var_weights` is a reference to the data so that\n        if data is already an array and it is changed, then `var_weights`\n        changes as well.\n    iteration : int\n        The number of iterations that fit has run.  Initialized at 0.\n    family : family class instance\n        The distribution family of the model. Can be any family in\n        statsmodels.families.  Default is Gaussian.\n    mu : array\n        The mean response of the transformed variable.  `mu` is the value of\n        the inverse of the link function at lin_pred, where lin_pred is the\n        linear predicted value of the WLS fit of the transformed variable.\n        `mu` is only available after fit is called.  See\n        statsmodels.families.family.fitted of the distribution family for more\n        information.\n    n_trials : array\n        See Notes. Note that `n_trials` is a reference to the data so that if\n        data is already an array and it is changed, then `n_trials` changes\n        as well. `n_trials` is the number of binomial trials and only available\n        with that distribution. See statsmodels.families.Binomial for more\n        information.\n    normalized_cov_params : array\n        The p x p normalized covariance of the design / exogenous data.\n        This is approximately equal to (X.T X)^(-1)\n    offset : array-like\n        Include offset in model with coefficient constrained to 1.\n    scale : float\n        The estimate of the scale / dispersion of the model fit.  Only\n        available after fit is called.  See GLM.fit and GLM.estimate_scale\n        for more information.\n    scaletype : str\n        The scaling used for fitting the model.  This is only available after\n        fit is called.  The default is None.  See GLM.fit for more information.\n    weights : array\n        The value of the weights after the last iteration of fit.  Only\n        available after fit is called.  See statsmodels.families.family for\n        the specific distribution weighting functions.\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> data = sm.datasets.scotland.load(as_pandas=False)\n    >>> data.exog = sm.add_constant(data.exog)\n\n    Instantiate a gamma family model with the default link function.\n\n    >>> gamma_model = sm.GLM(data.endog, data.exog,\n    ...                      family=sm.families.Gamma())\n\n    >>> gamma_results = gamma_model.fit()\n    >>> gamma_results.params\n    array([-0.01776527,  0.00004962,  0.00203442, -0.00007181,  0.00011185,\n           -0.00000015, -0.00051868, -0.00000243])\n    >>> gamma_results.scale\n    0.0035842831734919055\n    >>> gamma_results.deviance\n    0.087388516416999198\n    >>> gamma_results.pearson_chi2\n    0.086022796163805704\n    >>> gamma_results.llf\n    -83.017202161073527\n\n    See Also\n    --------\n    statsmodels.genmod.families.family.Family\n    :ref:`families`\n    :ref:`links`\n\n    Notes\n    -----\n    Only the following combinations make sense for family and link:\n\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n     Family        ident log logit probit cloglog pow opow nbinom loglog logc\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n     Gaussian      x     x   x     x      x       x   x     x      x\n     inv Gaussian  x     x                        x\n     binomial      x     x   x     x      x       x   x           x      x\n     Poission      x     x                        x\n     neg binomial  x     x                        x        x\n     gamma         x     x                        x\n     Tweedie       x     x                        x\n     ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n\n    Not all of these link functions are currently available.\n\n    Endog and exog are references so that if the data they refer to are already\n    arrays and these arrays are changed, endog and exog will change.\n\n    Statsmodels supports two separte definitions of weights: frequency weights\n    and variance weights.\n\n    Frequency weights produce the same results as repeating observations by the\n    frequencies (if those are integers). Frequency weights will keep the number\n    of observations consistent, but the degrees of freedom will change to\n    reflect the new weights.\n\n    Variance weights (referred to in other packages as analytic weights) are\n    used when ``endog`` represents an an average or mean. This relies on the\n    assumption that that the inverse variance scales proportionally to the\n    weight--an observation that is deemed more credible should have less\n    variance and therefore have more weight. For the ``Poisson`` family--which\n    assumes that occurences scale proportionally with time--a natural practice\n    would be to use the amount of time as the variance weight and set ``endog``\n    to be a rate (occurrances per period of time). Similarly, using a\n    compound Poisson family, namely ``Tweedie``, makes a similar assumption\n    about the rate (or frequency) of occurences having variance proportional to\n    time.\n\n    Both frequency and variance weights are verified for all basic results with\n    nonrobust or heteroscedasticity robust ``cov_type``. Other robust\n    covariance types have not yet been verified, and at least the small sample\n    correction is currently not based on the correct total frequency count.\n\n    Currently, all residuals are not weighted by frequency, although they may\n    incorporate ``n_trials`` for ``Binomial`` and ``var_weights``\n\n    +---------------+----------------------------------+\n    | Residual Type | Applicable weights               |\n    +===============+==================================+\n    | Anscombe      | ``var_weights``                  |\n    +---------------+----------------------------------+\n    | Deviance      | ``var_weights``                  |\n    +---------------+----------------------------------+\n    | Pearson       | ``var_weights`` and ``n_trials`` |\n    +---------------+----------------------------------+\n    | Reponse       | ``n_trials``                     |\n    +---------------+----------------------------------+\n    | Working       | ``n_trials``                     |\n    +---------------+----------------------------------+\n\n    WARNING: Loglikelihood and deviance are not valid in models where\n    scale is equal to 1 (i.e., ``Binomial``, ``NegativeBinomial``, and\n    ``Poisson``). If variance weights are specified, then results such as\n    ``loglike`` and ``deviance`` are based on a quasi-likelihood\n    interpretation. The loglikelihood is not correctly specified in this case,\n    and statistics based on it, such AIC or likelihood ratio tests, are not\n    appropriate.\n\n    ",
        "klass": "statsmodels.api.GLM",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.RegressionModel"
        ],
        "class_docstring": "\n    Generalized least squares model with a general covariance structure.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    sigma : scalar or array\n        `sigma` is the weighting matrix of the covariance.\n        The default is None for no scaling.  If `sigma` is a scalar, it is\n        assumed that `sigma` is an n x n diagonal matrix with the given\n        scalar, `sigma` as the value of each diagonal element.  If `sigma`\n        is an n-length vector, then `sigma` is assumed to be a diagonal\n        matrix with the given `sigma` on the diagonal.  This should be the\n        same as WLS.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Attributes\n    ----------\n    pinv_wexog : array\n        `pinv_wexog` is the p x n Moore-Penrose pseudoinverse of `wexog`.\n    cholsimgainv : array\n        The transpose of the Cholesky decomposition of the pseudoinverse.\n    df_model : float\n        p - 1, where p is the number of regressors including the intercept.\n        of freedom.\n    df_resid : float\n        Number of observations n less the number of parameters p.\n    llf : float\n        The value of the likelihood function of the fitted model.\n    nobs : float\n        The number of observations n.\n    normalized_cov_params : array\n        p x p array :math:`(X^{T}\\Sigma^{-1}X)^{-1}`\n    results : RegressionResults instance\n        A property that returns the RegressionResults class if fit.\n    sigma : array\n        `sigma` is the n x n covariance structure of the error terms.\n    wexog : array\n        Design matrix whitened by `cholsigmainv`\n    wendog : array\n        Response variable whitened by `cholsigmainv`\n\n    Notes\n    -----\n    If sigma is a function of the data making one of the regressors\n    a constant, then the current postestimation statistics will not be correct.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n    >>> data = sm.datasets.longley.load(as_pandas=False)\n    >>> data.exog = sm.add_constant(data.exog)\n    >>> ols_resid = sm.OLS(data.endog, data.exog).fit().resid\n    >>> res_fit = sm.OLS(ols_resid[1:], ols_resid[:-1]).fit()\n    >>> rho = res_fit.params\n\n    `rho` is a consistent estimator of the correlation of the residuals from\n    an OLS fit of the longley data.  It is assumed that this is the true rho\n    of the AR process data.\n\n    >>> from scipy.linalg import toeplitz\n    >>> order = toeplitz(np.arange(16))\n    >>> sigma = rho**order\n\n    `sigma` is an n x n matrix of the autocorrelation structure of the\n    data.\n\n    >>> gls_model = sm.GLS(data.endog, data.exog, sigma=sigma)\n    >>> gls_results = gls_model.fit()\n    >>> print(gls_results.summary())\n\n    ",
        "klass": "statsmodels.api.GLS",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.GLS"
        ],
        "class_docstring": "\n    A regression model with an AR(p) covariance structure.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    rho : int\n        Order of the autoregressive covariance\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> X = range(1,8)\n    >>> X = sm.add_constant(X)\n    >>> Y = [1,3,4,5,8,10,9]\n    >>> model = sm.GLSAR(Y, X, rho=2)\n    >>> for i in range(6):\n    ...     results = model.fit()\n    ...     print(\"AR coefficients: {0}\".format(model.rho))\n    ...     rho, sigma = sm.regression.yule_walker(results.resid,\n    ...                                            order=model.order)\n    ...     model = sm.GLSAR(Y, X, rho)\n    ...\n    AR coefficients: [ 0.  0.]\n    AR coefficients: [-0.52571491 -0.84496178]\n    AR coefficients: [-0.6104153  -0.86656458]\n    AR coefficients: [-0.60439494 -0.857867  ]\n    AR coefficients: [-0.6048218  -0.85846157]\n    AR coefficients: [-0.60479146 -0.85841922]\n    >>> results.params\n    array([-0.66661205,  1.60850853])\n    >>> results.tvalues\n    array([ -2.10304127,  21.8047269 ])\n    >>> print(results.t_test([1, 0]))\n    <T test: effect=array([-0.66661205]), sd=array([[ 0.31697526]]), t=array([[-2.10304127]]), p=array([[ 0.06309969]]), df_denom=3>\n    >>> print(results.f_test(np.identity(2)))\n    <F test: F=array([[ 1815.23061844]]), p=[[ 0.00002372]], df_denom=3, df_num=2>\n\n    Or, equivalently\n\n    >>> model2 = sm.GLSAR(Y, X, rho=2)\n    >>> res = model2.iterative_fit(maxiter=6)\n    >>> model2.rho\n    array([-0.60479146, -0.85841922])\n\n    Notes\n    -----\n    GLSAR is considered to be experimental.\n    The linear autoregressive process of order p--AR(p)--is defined as:\n    TODO\n    ",
        "klass": "statsmodels.api.GLSAR",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.discrete.discrete_model.BinaryModel"
        ],
        "class_docstring": "\n    Binary choice logit model\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n\n    Attributes\n    ----------\n    endog : array\n        A reference to the endogenous response variable\n    exog : array\n        A reference to the exogenous design.\n    ",
        "klass": "statsmodels.api.Logit",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.WLS"
        ],
        "class_docstring": "\n    A simple ordinary least squares model.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Attributes\n    ----------\n    weights : scalar\n        Has an attribute weights = array(1.0) due to inheritance from WLS.\n\n    See Also\n    --------\n    GLS\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>>\n    >>> import statsmodels.api as sm\n    >>>\n    >>> Y = [1,3,4,5,2,3,4]\n    >>> X = range(1,8)\n    >>> X = sm.add_constant(X)\n    >>>\n    >>> model = sm.OLS(Y,X)\n    >>> results = model.fit()\n    >>> results.params\n    array([ 2.14285714,  0.25      ])\n    >>> results.tvalues\n    array([ 1.87867287,  0.98019606])\n    >>> print(results.t_test([1, 0]))\n    <T test: effect=array([ 2.14285714]), sd=array([[ 1.14062282]]), t=array([[ 1.87867287]]), p=array([[ 0.05953974]]), df_denom=5>\n    >>> print(results.f_test(np.identity(2)))\n    <F test: F=array([[ 19.46078431]]), p=[[ 0.00437251]], df_denom=5, df_num=2>\n\n    Notes\n    -----\n    No constant is added by the model unless you are using formulas.\n    ",
        "klass": "statsmodels.regression.linear_model.OLS",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.WLS"
        ],
        "class_docstring": "\n    A simple ordinary least squares model.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Attributes\n    ----------\n    weights : scalar\n        Has an attribute weights = array(1.0) due to inheritance from WLS.\n\n    See Also\n    --------\n    GLS\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>>\n    >>> import statsmodels.api as sm\n    >>>\n    >>> Y = [1,3,4,5,2,3,4]\n    >>> X = range(1,8)\n    >>> X = sm.add_constant(X)\n    >>>\n    >>> model = sm.OLS(Y,X)\n    >>> results = model.fit()\n    >>> results.params\n    array([ 2.14285714,  0.25      ])\n    >>> results.tvalues\n    array([ 1.87867287,  0.98019606])\n    >>> print(results.t_test([1, 0]))\n    <T test: effect=array([ 2.14285714]), sd=array([[ 1.14062282]]), t=array([[ 1.87867287]]), p=array([[ 0.05953974]]), df_denom=5>\n    >>> print(results.f_test(np.identity(2)))\n    <F test: F=array([[ 19.46078431]]), p=[[ 0.00437251]], df_denom=5, df_num=2>\n\n    Notes\n    -----\n    No constant is added by the model unless you are using formulas.\n    ",
        "klass": "statsmodels.api.OLS",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.discrete.discrete_model.BinaryModel"
        ],
        "class_docstring": "\n    Binary choice Probit model\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n\n    Attributes\n    ----------\n    endog : array\n        A reference to the endogenous response variable\n    exog : array\n        A reference to the exogenous design.\n    ",
        "klass": "statsmodels.api.Probit",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.base.model.LikelihoodModel"
        ],
        "class_docstring": "\n    Robust Linear Models\n\n    Estimate a robust linear model via iteratively reweighted least squares\n    given a robust criterion estimator.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    M : statsmodels.robust.norms.RobustNorm, optional\n        The robust criterion function for downweighting outliers.\n        The current options are LeastSquares, HuberT, RamsayE, AndrewWave,\n        TrimmedMean, Hampel, and TukeyBiweight.  The default is HuberT().\n        See statsmodels.robust.norms for more information.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n\n    Attributes\n    ----------\n\n    df_model : float\n        The degrees of freedom of the model.  The number of regressors p less\n        one for the intercept.  Note that the reported model degrees\n        of freedom does not count the intercept as a regressor, though\n        the model is assumed to have an intercept.\n    df_resid : float\n        The residual degrees of freedom.  The number of observations n\n        less the number of regressors p.  Note that here p does include\n        the intercept as using a degree of freedom.\n    endog : array\n        See above.  Note that endog is a reference to the data so that if\n        data is already an array and it is changed, then `endog` changes\n        as well.\n    exog : array\n        See above.  Note that endog is a reference to the data so that if\n        data is already an array and it is changed, then `endog` changes\n        as well.\n    M : statsmodels.robust.norms.RobustNorm\n         See above.  Robust estimator instance instantiated.\n    nobs : float\n        The number of observations n\n    pinv_wexog : array\n        The pseudoinverse of the design / exogenous data array.  Note that\n        RLM has no whiten method, so this is just the pseudo inverse of the\n        design.\n    normalized_cov_params : array\n        The p x p normalized covariance of the design / exogenous data.\n        This is approximately equal to (X.T X)^(-1)\n\n\n    Examples\n    --------\n    >>> import statsmodels.api as sm\n    >>> data = sm.datasets.stackloss.load(as_pandas=False)\n    >>> data.exog = sm.add_constant(data.exog)\n    >>> rlm_model = sm.RLM(data.endog, data.exog,                            M=sm.robust.norms.HuberT())\n\n    >>> rlm_results = rlm_model.fit()\n    >>> rlm_results.params\n    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835])\n    >>> rlm_results.bse\n    array([ 0.11100521,  0.30293016,  0.12864961,  9.79189854])\n    >>> rlm_results_HC2 = rlm_model.fit(cov=\"H2\")\n    >>> rlm_results_HC2.params\n    array([  0.82938433,   0.92606597,  -0.12784672, -41.02649835])\n    >>> rlm_results_HC2.bse\n    array([ 0.11945975,  0.32235497,  0.11796313,  9.08950419])\n    >>> mod = sm.RLM(data.endog, data.exog, M=sm.robust.norms.Hampel())\n    >>> rlm_hamp_hub = mod.fit(scale_est=sm.robust.scale.HuberScale())\n    >>> rlm_hamp_hub.params\n    array([  0.73175452,   1.25082038,  -0.14794399, -40.27122257])\n    ",
        "klass": "statsmodels.api.RLM",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.RegressionModel"
        ],
        "class_docstring": "\n    A regression model with diagonal but non-identity covariance structure.\n\n    The weights are presumed to be (proportional to) the inverse of\n    the variance of the observations.  That is, if the variables are\n    to be transformed by 1/sqrt(W) you must supply weights = 1/W.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    weights : array-like, optional\n        1d array of weights.  If you supply 1/W then the variables are\n        pre- multiplied by 1/sqrt(W).  If no weights are supplied the\n        default value is 1 and WLS results are the same as OLS.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Attributes\n    ----------\n    weights : array\n        The stored weights supplied as an argument.\n\n    See Also\n    --------\n    regression.GLS\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n    >>> Y = [1,3,4,5,2,3,4]\n    >>> X = range(1,8)\n    >>> X = sm.add_constant(X)\n    >>> wls_model = sm.WLS(Y,X, weights=list(range(1,8)))\n    >>> results = wls_model.fit()\n    >>> results.params\n    array([ 2.91666667,  0.0952381 ])\n    >>> results.tvalues\n    array([ 2.0652652 ,  0.35684428])\n    >>> print(results.t_test([1, 0]))\n    <T test: effect=array([ 2.91666667]), sd=array([[ 1.41224801]]), t=array([[ 2.0652652]]), p=array([[ 0.04690139]]), df_denom=5>\n    >>> print(results.f_test([0, 1]))\n    <F test: F=array([[ 0.12733784]]), p=[[ 0.73577409]], df_denom=5, df_num=1>\n\n    Notes\n    -----\n    If the weights are a function of the data, then the post estimation\n    statistics such as fvalue and mse_model might not be correct, as the\n    package does not yet support no-constant regression.\n    ",
        "klass": "statsmodels.regression.linear_model.WLS",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.regression.linear_model.RegressionModel"
        ],
        "class_docstring": "\n    A regression model with diagonal but non-identity covariance structure.\n\n    The weights are presumed to be (proportional to) the inverse of\n    the variance of the observations.  That is, if the variables are\n    to be transformed by 1/sqrt(W) you must supply weights = 1/W.\n\n    \n    Parameters\n    ----------\n    endog : array-like\n        1-d endogenous response variable. The dependent variable.\n    exog : array-like\n        A nobs x k array where `nobs` is the number of observations and `k`\n        is the number of regressors. An intercept is not included by default\n        and should be added by the user. See\n        :func:`statsmodels.tools.add_constant`.\n    weights : array-like, optional\n        1d array of weights.  If you supply 1/W then the variables are\n        pre- multiplied by 1/sqrt(W).  If no weights are supplied the\n        default value is 1 and WLS results are the same as OLS.\n    missing : str\n        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n        checking is done. If 'drop', any observations with nans are dropped.\n        If 'raise', an error is raised. Default is 'none.'\n    hasconst : None or bool\n        Indicates whether the RHS includes a user-supplied constant. If True,\n        a constant is not checked for and k_constant is set to 1 and all\n        result statistics are calculated as if a constant is present. If\n        False, a constant is not checked for and k_constant is set to 0.\n\n\n    Attributes\n    ----------\n    weights : array\n        The stored weights supplied as an argument.\n\n    See Also\n    --------\n    regression.GLS\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n    >>> Y = [1,3,4,5,2,3,4]\n    >>> X = range(1,8)\n    >>> X = sm.add_constant(X)\n    >>> wls_model = sm.WLS(Y,X, weights=list(range(1,8)))\n    >>> results = wls_model.fit()\n    >>> results.params\n    array([ 2.91666667,  0.0952381 ])\n    >>> results.tvalues\n    array([ 2.0652652 ,  0.35684428])\n    >>> print(results.t_test([1, 0]))\n    <T test: effect=array([ 2.91666667]), sd=array([[ 1.41224801]]), t=array([[ 2.0652652]]), p=array([[ 0.04690139]]), df_denom=5>\n    >>> print(results.f_test([0, 1]))\n    <F test: F=array([[ 0.12733784]]), p=[[ 0.73577409]], df_denom=5, df_num=1>\n\n    Notes\n    -----\n    If the weights are a function of the data, then the post estimation\n    statistics such as fvalue and mse_model might not be correct, as the\n    package does not yet support no-constant regression.\n    ",
        "klass": "statsmodels.api.WLS",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "pandas.core.generic.NDFrame"
        ],
        "class_docstring": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    ",
        "klass": "pandas.core.frame.DataFrame",
        "module": "pandas"
    },
    {
        "base_classes": [
            "statsmodels.tsa.arima_model.ARMA"
        ],
        "class_docstring": "\n    Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n\n    Parameters\n    ----------\n    endog : array-like\n        The endogenous variable.\n    order : iterable\n        The (p,d,q) order of the model for the number of AR parameters,\n        differences, and MA parameters to use.\n    exog : array-like, optional\n        An optional array of exogenous variables. This should *not* include a\n        constant or trend. You can specify this in the `fit` method.\n    dates : array-like of datetime, optional\n        An array-like object of datetime objects. If a pandas object is given\n        for endog or exog, it is assumed to have a DateIndex.\n    freq : str, optional\n        The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',\n        'M', 'A', or 'Q'. This is optional if dates are given.\n    \n    \n    Notes\n    -----\n    If exogenous variables are given, then the model that is fit is\n\n    .. math::\n\n       \\phi(L)(y_t - X_t\\beta) = \\theta(L)\\epsilon_t\n\n    where :math:`\\phi` and :math:`\\theta` are polynomials in the lag\n    operator, :math:`L`. This is the regression model with ARMA errors,\n    or ARMAX model. This specification is used, whether or not the model\n    is fit using conditional sum of square or maximum-likelihood, using\n    the `method` argument in\n    :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for\n    now, `css` and `mle` refer to estimation methods only. This may\n    change for the case of the `css` model in future versions.\n\n",
        "klass": "statsmodels.tsa.arima_model.ARIMA",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "statsmodels.tsa.base.tsa_model.TimeSeriesModel"
        ],
        "class_docstring": "\n    Autoregressive Moving Average ARMA(p,q) Model\n\n    Parameters\n    ----------\n    endog : array-like\n        The endogenous variable.\n    order : iterable\n        The (p,q) order of the model for the number of AR parameters,\n        differences, and MA parameters to use.\n    exog : array-like, optional\n        An optional array of exogenous variables. This should *not* include a\n        constant or trend. You can specify this in the `fit` method.\n    dates : array-like of datetime, optional\n        An array-like object of datetime objects. If a pandas object is given\n        for endog or exog, it is assumed to have a DateIndex.\n    freq : str, optional\n        The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',\n        'M', 'A', or 'Q'. This is optional if dates are given.\n    \n    \n    Notes\n    -----\n    If exogenous variables are given, then the model that is fit is\n\n    .. math::\n\n       \\phi(L)(y_t - X_t\\beta) = \\theta(L)\\epsilon_t\n\n    where :math:`\\phi` and :math:`\\theta` are polynomials in the lag\n    operator, :math:`L`. This is the regression model with ARMA errors,\n    or ARMAX model. This specification is used, whether or not the model\n    is fit using conditional sum of square or maximum-likelihood, using\n    the `method` argument in\n    :meth:`statsmodels.tsa.arima_model.ARMA.fit`. Therefore, for\n    now, `css` and `mle` refer to estimation methods only. This may\n    change for the case of the `css` model in future versions.\n\n",
        "klass": "statsmodels.tsa.arima_model.ARMA",
        "module": "statsmodels"
    },
    {
        "base_classes": [
            "weboob.capabilities.image.BaseImage"
        ],
        "class_docstring": "\n    Represents a video.\n\n    This object has to be inherited to specify how to calculate the URL of the video from its ID.\n    \n:var duration: (:class:`timedelta`) file duration",
        "klass": "weboob.capabilities.video.BaseVideo",
        "module": "weboob"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a repository.\n    ",
        "klass": "weboob.core.repositories.Repository",
        "module": "weboob"
    },
    {
        "base_classes": [
            "weboob.browser.filters.base.Filter"
        ],
        "class_docstring": "\n    Get a cleaned text from an element.\n\n    It first replaces all tabs and multiple spaces\n    (including newlines if ``newlines`` is True)\n    to one space and strips the result string.\n\n    The result is coerced into unicode, and optionally normalized\n    according to the ``normalize`` argument.\n\n    Then it replaces all symbols given in the ``symbols`` argument.\n\n    >>> CleanText().filter('coucou ') == u'coucou'\n    True\n    >>> CleanText().filter(u'coucou\u00a0coucou') == u'coucou coucou'\n    True\n    >>> CleanText(newlines=True).filter(u'coucou\\r\\n coucou ') == u'coucou coucou'\n    True\n    >>> CleanText(newlines=False).filter(u'coucou\\r\\n coucou ') == u'coucou\\ncoucou'\n    True\n    ",
        "klass": "weboob.browser.filters.standard.CleanText",
        "module": "weboob"
    },
    {
        "base_classes": [
            "weboob.browser.filters.base.Filter"
        ],
        "class_docstring": "Get the text value of an HTML attribute.\n\n    Get value from attribute `attr` of HTML element matched by `selector`.\n\n    For example::\n\n        obj_foo = Attr('//img[@id=\"thumbnail\"]', 'src')\n\n    will take the \"src\" attribute of ``<img>`` whose \"id\" is \"thumbnail\".\n    ",
        "klass": "weboob.browser.filters.html.Attr",
        "module": "weboob"
    },
    {
        "base_classes": [
            "weboob.browser.filters.base.Filter"
        ],
        "class_docstring": "\n    Apply a regex.\n\n    >>> from lxml.html import etree\n    >>> doc = etree.fromstring('<html><body><p>Date: <span>13/08/1988</span></p></body></html>')\n    >>> Regexp(CleanText('//p'), r'Date: (\\d+)/(\\d+)/(\\d+)', '\\\\3-\\\\2-\\\\1')(doc) == u'1988-08-13'\n    True\n\n    >>> (Regexp(CleanText('//body'), r'(\\d+)', nth=1))(doc) == u'08'\n    True\n    >>> (Regexp(CleanText('//body'), r'(\\d+)', nth=-1))(doc) == u'1988'\n    True\n    >>> (Regexp(CleanText('//body'), r'(\\d+)', template='[\\\\1]', nth='*'))(doc) == [u'[13]', u'[08]', u'[1988]']\n    True\n    >>> (Regexp(CleanText('//body'), r'Date:.*'))(doc) == u'Date: 13/08/1988'\n    True\n    >>> (Regexp(CleanText('//body'), r'^(?!Date:).*', default=None))(doc)\n    >>>\n    ",
        "klass": "weboob.browser.filters.standard.Regexp",
        "module": "weboob"
    },
    {
        "base_classes": [
            "weboob.browser.filters.base._Filter"
        ],
        "class_docstring": "\n    Get the attribute of object.\n\n    Example::\n\n        obj_foo = CleanText('//h1')\n        obj_bar = Field('foo')\n\n    will make \"bar\" field equal to \"foo\" field.\n    ",
        "klass": "weboob.browser.filters.standard.Field",
        "module": "weboob"
    },
    {
        "base_classes": [
            "rx.core.typing.Observer",
            "rx.core.typing.Disposable"
        ],
        "class_docstring": "Base class for implementations of the Observer class. This base\n    class enforces the grammar of observers where OnError and\n    OnCompleted are terminal messages.\n    ",
        "klass": "rx.core.Observer",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.core.typing.Disposable"
        ],
        "class_docstring": "Represents a group of disposable resources that are disposed\n    together",
        "klass": "rx.disposable.CompositeDisposable",
        "module": "rx"
    },
    {
        "base_classes": [
            "typing.Generic"
        ],
        "class_docstring": "Priority queue for scheduling. Note that methods aren't thread-safe.",
        "klass": "rx.internal.PriorityQueue",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.trampolinescheduler.TrampolineScheduler"
        ],
        "class_docstring": "Represents an object that schedules units of work on the current thread.\n    You should never schedule timeouts using the *CurrentThreadScheduler*, as\n    that will block the thread while waiting.\n\n    Each instance manages a number of trampolines (and queues), one for each\n    thread that calls a *schedule* method. These trampolines are automatically\n    garbage-collected when threads disappear, because they're stored in a weak\n    key dictionary.\n    ",
        "klass": "rx.scheduler.CurrentThreadScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler",
            "rx.core.typing.Disposable"
        ],
        "class_docstring": "Creates an object that schedules units of work on a designated thread.",
        "klass": "rx.scheduler.EventLoopScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.scheduler.Scheduler"
        ],
        "class_docstring": "Represents an object that schedules units of work to run immediately,\n    on the current thread. You're not allowed to schedule timeouts using the\n    ImmediateScheduler since that will block the current thread while waiting.\n    Attempts to do so will raise a :class:`WouldBlockException`.\n    ",
        "klass": "rx.scheduler.ImmediateScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "Creates an object that schedules each unit of work on a separate thread.\n    ",
        "klass": "rx.scheduler.NewThreadScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via a timed callback.",
        "klass": "rx.scheduler.TimeoutScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.scheduler.Scheduler"
        ],
        "class_docstring": "Represents an object that schedules units of work on the trampoline.\n    You should never schedule timeouts using the *TrampolineScheduler*, as\n    it will block the thread while waiting.\n\n    Each instance has its own trampoline (and queue), and you can schedule work\n    on it from different threads. Beware though, that the first thread to call\n    a *schedule* method while the trampoline is idle will then remain occupied\n    until the queue is empty.\n    ",
        "klass": "rx.scheduler.TrampolineScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the asyncio mainloop. This class\n    does not use the asyncio threadsafe methods, if you need those please use\n    the AsyncIOThreadSafeScheduler class.",
        "klass": "rx.scheduler.eventloop.AsyncIOScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the eventlet event loop.\n\n    http://eventlet.net/\n    ",
        "klass": "rx.scheduler.eventloop.EventletScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the GEvent event loop.\n\n    http://www.gevent.org/\n    ",
        "klass": "rx.scheduler.eventloop.GEventScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the Tornado I/O main event loop.\n\n    Note, as of Tornado 6, this is just a wrapper around the asyncio loop.\n\n    http://tornado.readthedocs.org/en/latest/ioloop.html",
        "klass": "rx.scheduler.eventloop.IOLoopScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the Twisted reactor mainloop.",
        "klass": "rx.scheduler.eventloop.TwistedScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules works for PyGame.\n\n    Note that this class expects the caller to invoke run() repeatedly.\n\n    http://www.pygame.org/docs/ref/time.html\n    http://www.pygame.org/docs/ref/event.html",
        "klass": "rx.scheduler.mainloop.PyGameScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler for a PyQt5/PySide2 event loop.",
        "klass": "rx.scheduler.mainloop.QtScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler that schedules work via the Tkinter main event loop.\n\n    http://infohost.nmt.edu/tcc/help/pubs/tkinter/web/universal.html\n    http://effbot.org/tkinterbook/widget.htm",
        "klass": "rx.scheduler.mainloop.TkinterScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.scheduler.periodicscheduler.PeriodicScheduler"
        ],
        "class_docstring": "A scheduler for a wxPython event loop.",
        "klass": "rx.scheduler.mainloop.WxScheduler",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.subject.subject.Subject"
        ],
        "class_docstring": "Represents the result of an asynchronous operation. The last value\n    before the close notification, or the error received through\n    on_error, is sent to all subscribed observers.",
        "klass": "rx.subject.AsyncSubject",
        "module": "rx"
    },
    {
        "base_classes": [
            "rx.core.observable.observable.Observable",
            "rx.core.observer.observer.Observer",
            "rx.core.typing.Subject"
        ],
        "class_docstring": "Represents an object that is both an observable sequence as well\n    as an observer. Each notification is broadcasted to all subscribed\n    observers.\n    ",
        "klass": "rx.subject.Subject",
        "module": "rx"
    },
    {
        "base_classes": [
            "asgiref.testing.ApplicationCommunicator"
        ],
        "class_docstring": "\n    ApplicationCommunicator subclass that has WebSocket shortcut methods.\n\n    It will construct the scope for you, so you need to pass the application\n    (uninstantiated) along with the initial connection parameters.\n    ",
        "klass": "channels.testing.WebsocketCommunicator",
        "module": "channels"
    },
    {
        "base_classes": [
            "pyramid.authentication.CallbackAuthenticationPolicy"
        ],
        "class_docstring": "A :app:`Pyramid` :term:`authentication policy` which\n    obtains data from a Pyramid \"auth ticket\" cookie.\n\n    Constructor Arguments\n\n    ``secret``\n\n       The secret (a string) used for auth_tkt cookie signing.  This value\n       should be unique across all values provided to Pyramid for various\n       subsystem secrets (see :ref:`admonishment_against_secret_sharing`).\n       Required.\n\n    ``callback``\n\n       Default: ``None``.  A callback passed the userid and the\n       request, expected to return ``None`` if the userid doesn't\n       exist or a sequence of principal identifiers (possibly empty) if\n       the user does exist.  If ``callback`` is ``None``, the userid\n       will be assumed to exist with no principals.  Optional.\n\n    ``cookie_name``\n\n       Default: ``auth_tkt``.  The cookie name used\n       (string).  Optional.\n\n    ``secure``\n\n       Default: ``False``.  Only send the cookie back over a secure\n       conn.  Optional.\n\n    ``include_ip``\n\n       Default: ``False``.  Make the requesting IP address part of\n       the authentication data in the cookie.  Optional.\n\n       For IPv6 this option is not recommended. The ``mod_auth_tkt``\n       specification does not specify how to handle IPv6 addresses, so using\n       this option in combination with IPv6 addresses may cause an\n       incompatible cookie. It ties the authentication ticket to that\n       individual's IPv6 address.\n\n    ``timeout``\n\n       Default: ``None``.  Maximum number of seconds which a newly\n       issued ticket will be considered valid.  After this amount of\n       time, the ticket will expire (effectively logging the user\n       out).  If this value is ``None``, the ticket never expires.\n       Optional.\n\n    ``reissue_time``\n\n       Default: ``None``.  If this parameter is set, it represents the number\n       of seconds that must pass before an authentication token cookie is\n       automatically reissued as the result of a request which requires\n       authentication.  The duration is measured as the number of seconds\n       since the last auth_tkt cookie was issued and 'now'.  If this value is\n       ``0``, a new ticket cookie will be reissued on every request which\n       requires authentication.\n\n       A good rule of thumb: if you want auto-expired cookies based on\n       inactivity: set the ``timeout`` value to 1200 (20 mins) and set the\n       ``reissue_time`` value to perhaps a tenth of the ``timeout`` value\n       (120 or 2 mins).  It's nonsensical to set the ``timeout`` value lower\n       than the ``reissue_time`` value, as the ticket will never be reissued\n       if so.  However, such a configuration is not explicitly prevented.\n\n       Optional.\n\n    ``max_age``\n\n       Default: ``None``.  The max age of the auth_tkt cookie, in\n       seconds.  This differs from ``timeout`` inasmuch as ``timeout``\n       represents the lifetime of the ticket contained in the cookie,\n       while this value represents the lifetime of the cookie itself.\n       When this value is set, the cookie's ``Max-Age`` and\n       ``Expires`` settings will be set, allowing the auth_tkt cookie\n       to last between browser sessions.  It is typically nonsensical\n       to set this to a value that is lower than ``timeout`` or\n       ``reissue_time``, although it is not explicitly prevented.\n       Optional.\n\n    ``path``\n\n       Default: ``/``. The path for which the auth_tkt cookie is valid.\n       May be desirable if the application only serves part of a domain.\n       Optional.\n\n    ``http_only``\n\n       Default: ``False``. Hide cookie from JavaScript by setting the\n       HttpOnly flag. Not honored by all browsers.\n       Optional.\n\n    ``wild_domain``\n\n       Default: ``True``. An auth_tkt cookie will be generated for the\n       wildcard domain. If your site is hosted as ``example.com`` this\n       will make the cookie available for sites underneath ``example.com``\n       such as ``www.example.com``.\n       Optional.\n\n    ``parent_domain``\n\n       Default: ``False``. An auth_tkt cookie will be generated for the\n       parent domain of the current site. For example if your site is\n       hosted under ``www.example.com`` a cookie will be generated for\n       ``.example.com``. This can be useful if you have multiple sites\n       sharing the same domain. This option supercedes the ``wild_domain``\n       option.\n       Optional.\n\n    ``domain``\n\n       Default: ``None``. If provided the auth_tkt cookie will only be\n       set for this domain. This option is not compatible with ``wild_domain``\n       and ``parent_domain``.\n       Optional.\n\n    ``hashalg``\n\n       Default: ``sha512`` (the literal string).\n\n       Any hash algorithm supported by Python's ``hashlib.new()`` function\n       can be used as the ``hashalg``.\n\n       Cookies generated by different instances of AuthTktAuthenticationPolicy\n       using different ``hashalg`` options are not compatible. Switching the\n       ``hashalg`` will imply that all existing users with a valid cookie will\n       be required to re-login.\n\n       Optional.\n\n    ``debug``\n\n        Default: ``False``.  If ``debug`` is ``True``, log messages to the\n        Pyramid debug logger about the results of various authentication\n        steps.  The output from debugging is useful for reporting to maillist\n        or IRC channels when asking for support.\n\n    ``samesite``\n\n        Default: ``'Lax'``.  The 'samesite' option of the session cookie. Set\n        the value to ``None`` to turn off the samesite option.\n\n        This option is available as of :app:`Pyramid` 1.10.\n\n    .. versionchanged:: 1.4\n\n       Added the ``hashalg`` option, defaulting to ``sha512``.\n\n    .. versionchanged:: 1.5\n\n       Added the ``domain`` option.\n\n       Added the ``parent_domain`` option.\n\n    .. versionchanged:: 1.10\n\n       Added the ``samesite`` option and made the default ``'Lax'``.\n\n    Objects of this class implement the interface described by\n    :class:`pyramid.interfaces.IAuthenticationPolicy`.\n\n    ",
        "klass": "pyramid.authentication.AuthTktAuthenticationPolicy",
        "module": "pyramid"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateTime_base",
            "datetime.datetime"
        ],
        "class_docstring": "XMLSchema datatype U{dateTime<http://www.w3.org/TR/xmlschema-2/#dateTime>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation.  Unless L{pyxb.PreserveInputTimeZone()}\n    is used, all timezoned dateTime objects are in UTC.  Presence of\n    time zone information in the lexical space is preserved by a\n    non-empty tzinfo field, which should always be zero minutes offset\n    from UTC unless the input time zone was preserved.\n\n    @warning: The value space of Python's C{datetime.datetime} class\n    is more restricted than that of C{xs:datetime}.  As a specific\n    example, Python does not support negative years or years with more\n    than four digits.  For now, the convenience of having an object\n    that is compatible with Python is more important than supporting\n    the full value space.  In the future, the choice may be left up to\n    the developer.\n    ",
        "klass": "pyxb.binding.datatypes.dateTime",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateOnly_base"
        ],
        "class_docstring": "XMLSchema datatype U{gDay<http://www.w3.org/TR/xmlschema-2/#gDay>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation; fields not relevant to this type are\n    derived from 1900-01-01T00:00:00.\n    ",
        "klass": "pyxb.binding.datatypes.gDay",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateOnly_base"
        ],
        "class_docstring": "XMLSchema datatype U{gMonth<http://www.w3.org/TR/xmlschema-2/#gMonth>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation; fields not relevant to this type are\n    derived from 1900-01-01T00:00:00.\n    ",
        "klass": "pyxb.binding.datatypes.gMonth",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateOnly_base"
        ],
        "class_docstring": "XMLSchema datatype U{gMonthDay<http://www.w3.org/TR/xmlschema-2/#gMonthDay>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation; fields not relevant to this type are\n    derived from 1900-01-01T00:00:00.\n    ",
        "klass": "pyxb.binding.datatypes.gMonthDay",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateOnly_base"
        ],
        "class_docstring": "XMLSchema datatype U{gYear<http://www.w3.org/TR/xmlschema-2/#gYear>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation; fields not relevant to this type are\n    derived from 1900-01-01T00:00:00.\n    ",
        "klass": "pyxb.binding.datatypes.gYear",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "pyxb.binding.datatypes._PyXBDateOnly_base"
        ],
        "class_docstring": "XMLSchema datatype U{gYearMonth<http://www.w3.org/TR/xmlschema-2/#gYearMonth>}.\n\n    This class uses the Python C{datetime.datetime} class as its\n    underlying representation; fields not relevant to this type are\n    derived from 1900-01-01T00:00:00.\n    ",
        "klass": "pyxb.binding.datatypes.gYearMonth",
        "module": "pyxb"
    },
    {
        "base_classes": [
            "h5py._hl.group.Group"
        ],
        "class_docstring": "\n        Represents an HDF5 file.\n    ",
        "klass": "h5py.File",
        "module": "h5py"
    },
    {
        "base_classes": [
            "distutils.dist.Distribution"
        ],
        "class_docstring": "Distribution with support for features, tests, and package data\n\n    This is an enhanced version of 'distutils.dist.Distribution' that\n    effectively adds the following new optional keyword arguments to 'setup()':\n\n     'install_requires' -- a string or sequence of strings specifying project\n        versions that the distribution requires when installed, in the format\n        used by 'pkg_resources.require()'.  They will be installed\n        automatically when the package is installed.  If you wish to use\n        packages that are not available in PyPI, or want to give your users an\n        alternate download location, you can add a 'find_links' option to the\n        '[easy_install]' section of your project's 'setup.cfg' file, and then\n        setuptools will scan the listed web pages for links that satisfy the\n        requirements.\n\n     'extras_require' -- a dictionary mapping names of optional \"extras\" to the\n        additional requirement(s) that using those extras incurs. For example,\n        this::\n\n            extras_require = dict(reST = [\"docutils>=0.3\", \"reSTedit\"])\n\n        indicates that the distribution can optionally provide an extra\n        capability called \"reST\", but it can only be used if docutils and\n        reSTedit are installed.  If the user installs your package using\n        EasyInstall and requests one of your extras, the corresponding\n        additional requirements will be installed if needed.\n\n     'features' **deprecated** -- a dictionary mapping option names to\n        'setuptools.Feature'\n        objects.  Features are a portion of the distribution that can be\n        included or excluded based on user options, inter-feature dependencies,\n        and availability on the current system.  Excluded features are omitted\n        from all setup commands, including source and binary distributions, so\n        you can create multiple distributions from the same source tree.\n        Feature names should be valid Python identifiers, except that they may\n        contain the '-' (minus) sign.  Features can be included or excluded\n        via the command line options '--with-X' and '--without-X', where 'X' is\n        the name of the feature.  Whether a feature is included by default, and\n        whether you are allowed to control this from the command line, is\n        determined by the Feature object.  See the 'Feature' class for more\n        information.\n\n     'test_suite' -- the name of a test suite to run for the 'test' command.\n        If the user runs 'python setup.py test', the package will be installed,\n        and the named test suite will be run.  The format is the same as\n        would be used on a 'unittest.py' command line.  That is, it is the\n        dotted name of an object to import and call to generate a test suite.\n\n     'package_data' -- a dictionary mapping package names to lists of filenames\n        or globs to use to find data files contained in the named packages.\n        If the dictionary has filenames or globs listed under '\"\"' (the empty\n        string), those names will be searched for in every package, in addition\n        to any names for the specific package.  Data files found using these\n        names/globs will be installed along with the package, in the same\n        location as the package.  Note that globs are allowed to reference\n        the contents of non-package subdirectories, as long as you use '/' as\n        a path separator.  (Globs are automatically converted to\n        platform-specific paths at runtime.)\n\n    In addition to these new keywords, this class also has several new methods\n    for manipulating the distribution's contents.  For example, the 'include()'\n    and 'exclude()' methods can be thought of as in-place add and subtract\n    commands that add or remove packages, modules, extensions, and so on from\n    the distribution.  They are used by the feature subsystem to configure the\n    distribution for the included and excluded features.\n    ",
        "klass": "setuptools.dist.Distribution",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "distutils.dist.Distribution"
        ],
        "class_docstring": "Distribution with support for features, tests, and package data\n\n    This is an enhanced version of 'distutils.dist.Distribution' that\n    effectively adds the following new optional keyword arguments to 'setup()':\n\n     'install_requires' -- a string or sequence of strings specifying project\n        versions that the distribution requires when installed, in the format\n        used by 'pkg_resources.require()'.  They will be installed\n        automatically when the package is installed.  If you wish to use\n        packages that are not available in PyPI, or want to give your users an\n        alternate download location, you can add a 'find_links' option to the\n        '[easy_install]' section of your project's 'setup.cfg' file, and then\n        setuptools will scan the listed web pages for links that satisfy the\n        requirements.\n\n     'extras_require' -- a dictionary mapping names of optional \"extras\" to the\n        additional requirement(s) that using those extras incurs. For example,\n        this::\n\n            extras_require = dict(reST = [\"docutils>=0.3\", \"reSTedit\"])\n\n        indicates that the distribution can optionally provide an extra\n        capability called \"reST\", but it can only be used if docutils and\n        reSTedit are installed.  If the user installs your package using\n        EasyInstall and requests one of your extras, the corresponding\n        additional requirements will be installed if needed.\n\n     'features' **deprecated** -- a dictionary mapping option names to\n        'setuptools.Feature'\n        objects.  Features are a portion of the distribution that can be\n        included or excluded based on user options, inter-feature dependencies,\n        and availability on the current system.  Excluded features are omitted\n        from all setup commands, including source and binary distributions, so\n        you can create multiple distributions from the same source tree.\n        Feature names should be valid Python identifiers, except that they may\n        contain the '-' (minus) sign.  Features can be included or excluded\n        via the command line options '--with-X' and '--without-X', where 'X' is\n        the name of the feature.  Whether a feature is included by default, and\n        whether you are allowed to control this from the command line, is\n        determined by the Feature object.  See the 'Feature' class for more\n        information.\n\n     'test_suite' -- the name of a test suite to run for the 'test' command.\n        If the user runs 'python setup.py test', the package will be installed,\n        and the named test suite will be run.  The format is the same as\n        would be used on a 'unittest.py' command line.  That is, it is the\n        dotted name of an object to import and call to generate a test suite.\n\n     'package_data' -- a dictionary mapping package names to lists of filenames\n        or globs to use to find data files contained in the named packages.\n        If the dictionary has filenames or globs listed under '\"\"' (the empty\n        string), those names will be searched for in every package, in addition\n        to any names for the specific package.  Data files found using these\n        names/globs will be installed along with the package, in the same\n        location as the package.  Note that globs are allowed to reference\n        the contents of non-package subdirectories, as long as you use '/' as\n        a path separator.  (Globs are automatically converted to\n        platform-specific paths at runtime.)\n\n    In addition to these new keywords, this class also has several new methods\n    for manipulating the distribution's contents.  For example, the 'include()'\n    and 'exclude()' methods can be thought of as in-place add and subtract\n    commands that add or remove packages, modules, extensions, and so on from\n    the distribution.  They are used by the feature subsystem to configure the\n    distribution for the included and excluded features.\n    ",
        "klass": "setuptools.Distribution",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "distutils.extension.Extension"
        ],
        "class_docstring": "Extension that uses '.c' files in place of '.pyx' files",
        "klass": "setuptools.Extension",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "setuptools.namespaces.DevelopInstaller",
            "setuptools.command.easy_install.easy_install"
        ],
        "class_docstring": "Set up package for development",
        "klass": "setuptools.command.develop.develop",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "setuptools.Command"
        ],
        "class_docstring": "Manage a download/build/install process",
        "klass": "setuptools.command.easy_install.easy_install",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "distutils.command.install_scripts.install_scripts"
        ],
        "class_docstring": "Do normal script install, plus any egg_info wrapper scripts",
        "klass": "setuptools.command.install_scripts.install_scripts",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "setuptools.Command"
        ],
        "class_docstring": "Command to run unit tests after in-place build",
        "klass": "setuptools.command.test.test",
        "module": "setuptools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for all estimators in scikit-learn\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n    ",
        "klass": "sklearn.base.BaseEstimator",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.ClassifierMixin",
            "sklearn.base.MetaEstimatorMixin"
        ],
        "class_docstring": "Probability calibration with isotonic regression or sigmoid.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    With this class, the base_estimator is fit on the train set of the\n    cross-validation generator and the test set is used for calibration.\n    The probabilities for each of the folds are then averaged\n    for prediction. In case that cv=\"prefit\" is passed to __init__,\n    it is assumed that base_estimator has been fitted already and all\n    data is used for calibration. Note that data for fitting the\n    classifier and for calibrating it must be disjoint.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    base_estimator : instance BaseEstimator\n        The classifier whose output decision function needs to be calibrated\n        to offer more accurate predict_proba outputs. If cv=prefit, the\n        classifier must have been fit already on data.\n\n    method : 'sigmoid' or 'isotonic'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach. It is not advised to use isotonic calibration\n        with too few calibration samples ``(<<1000)`` since it tends to\n        overfit.\n        Use sigmoids (Platt's calibration) in this case.\n\n    cv : integer, cross-validation generator, iterable or \"prefit\", optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`\n        is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that base_estimator has been\n        fitted already and all data is used for calibration.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    Attributes\n    ----------\n    classes_ : array, shape (n_classes)\n        The class labels.\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if cv == \"prefit\")\n        The list of calibrated classifiers, one for each crossvalidation fold,\n        which has been fitted on all but the validation fold and calibrated\n        on the validation fold.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n    ",
        "klass": "sklearn.calibration.CalibratedClassifierCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Isotonic regression model.\n\n    The isotonic regression optimization problem is defined by::\n\n        min sum w_i (y[i] - y_[i]) ** 2\n\n        subject to y_[i] <= y_[j] whenever X[i] <= X[j]\n        and min(y_) = y_min, max(y_) = y_max\n\n    where:\n        - ``y[i]`` are inputs (real numbers)\n        - ``y_[i]`` are fitted\n        - ``X`` specifies the order.\n          If ``X`` is non-decreasing then ``y_`` is non-decreasing.\n        - ``w[i]`` are optional strictly positive weights (default to 1.0)\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : optional, default: None\n        If not None, set the lowest value of the fit to y_min.\n\n    y_max : optional, default: None\n        If not None, set the highest value of the fit to y_max.\n\n    increasing : boolean or string, optional, default: True\n        If boolean, whether or not to fit the isotonic regression with y\n        increasing or decreasing.\n\n        The string value \"auto\" determines whether y should\n        increase or decrease based on the Spearman correlation estimate's\n        sign.\n\n    out_of_bounds : string, optional, default: \"nan\"\n        The ``out_of_bounds`` parameter handles how x-values outside of the\n        training domain are handled.  When set to \"nan\", predicted y-values\n        will be NaN.  When set to \"clip\", predicted y-values will be\n        set to the value corresponding to the nearest train interval endpoint.\n        When set to \"raise\", allow ``interp1d`` to throw ValueError.\n\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    ",
        "klass": "sklearn.isotonic.IsotonicRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    ",
        "klass": "sklearn.calibration._SigmoidCalibration",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : boolean, optional, default: True\n        Make a copy of input data.\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : string, optional, default=``euclidean``\n        Which affinity to use. At the moment ``precomputed`` and\n        ``euclidean`` are supported. ``euclidean`` uses the\n        negative squared euclidean distance between points.\n\n    verbose : boolean, optional, default: False\n        Whether to be verbose.\n\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : array, shape (n_clusters,)\n        Indices of cluster centers\n\n    cluster_centers_ : array, shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    affinity_matrix_ : array, shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation().fit(X)\n    >>> clustering\n    AffinityPropagation()\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    ",
        "klass": "sklearn.cluster.AffinityPropagation",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "\n    Agglomerative Clustering\n\n    Recursively merges the pair of clusters that minimally increases\n    a given linkage distance.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, optional (default=2)\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : string or callable, default: \"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or \"precomputed\".\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n        If \"precomputed\", a distance matrix (instead of a similarity matrix)\n        is needed as input for the fit method.\n\n    memory : None, str or object with the joblib.Memory interface, optional\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, optional\n        Connectivity matrix. Defines for each sample the neighboring\n        samples following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is None, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : bool or 'auto' (optional)\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. This option is\n        useful only when specifying a connectivity matrix. Note also that\n        when varying the number of clusters and using caching, it may\n        be advantageous to compute the full tree. It must be ``True`` if\n        ``distance_threshold`` is not ``None``.\n\n    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional             (default=\"ward\")\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of observation. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - ward minimizes the variance of the clusters being merged.\n        - average uses the average of the distances of each observation of\n          the two sets.\n        - complete or maximum linkage uses the maximum distances between\n          all observations of the two sets.\n        - single uses the minimum of the distances between all observations\n          of the two sets.\n\n    distance_threshold : float, optional (default=None)\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array [n_samples]\n        cluster labels for each point\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n    children_ : array-like, shape (n_samples-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AgglomerativeClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AgglomerativeClustering().fit(X)\n    >>> clustering\n    AgglomerativeClustering()\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n\n    ",
        "klass": "sklearn.cluster.AgglomerativeClustering",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Implements the Birch clustering algorithm.\n\n    It is a memory-efficient, online-learning algorithm provided as an\n    alternative to :class:`MiniBatchKMeans`. It constructs a tree\n    data structure with the cluster centroids being read off the leaf.\n    These can be either the final cluster centroids or can be provided as input\n    to another clustering algorithm such as :class:`AgglomerativeClustering`.\n\n    Read more in the :ref:`User Guide <birch>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    threshold : float, default 0.5\n        The radius of the subcluster obtained by merging a new sample and the\n        closest subcluster should be lesser than the threshold. Otherwise a new\n        subcluster is started. Setting this value to be very low promotes\n        splitting and vice-versa.\n\n    branching_factor : int, default 50\n        Maximum number of CF subclusters in each node. If a new samples enters\n        such that the number of subclusters exceed the branching_factor then\n        that node is split into two nodes with the subclusters redistributed\n        in each. The parent subcluster of that node is removed and two new\n        subclusters are added as parents of the 2 split nodes.\n\n    n_clusters : int, instance of sklearn.cluster model, default 3\n        Number of clusters after the final clustering step, which treats the\n        subclusters from the leaves as new samples.\n\n        - `None` : the final clustering step is not performed and the\n          subclusters are returned as they are.\n\n        - :mod:`sklearn.cluster` Estimator : If a model is provided, the model\n          is fit treating the subclusters as new samples and the initial data\n          is mapped to the label of the closest subcluster.\n\n        - `int` : the model fit is :class:`AgglomerativeClustering` with\n          `n_clusters` set to be equal to the int.\n\n    compute_labels : bool, default True\n        Whether or not to compute labels for each fit.\n\n    copy : bool, default True\n        Whether or not to make a copy of the given data. If set to False,\n        the initial data will be overwritten.\n\n    Attributes\n    ----------\n    root_ : _CFNode\n        Root of the CFTree.\n\n    dummy_leaf_ : _CFNode\n        Start pointer to all the leaves.\n\n    subcluster_centers_ : ndarray,\n        Centroids of all subclusters read directly from the leaves.\n\n    subcluster_labels_ : ndarray,\n        Labels assigned to the centroids of the subclusters after\n        they are clustered globally.\n\n    labels_ : ndarray, shape (n_samples,)\n        Array of labels assigned to the input data.\n        if partial_fit is used instead of fit, they are assigned to the\n        last batch of data.\n\n    See Also\n    --------\n\n    MiniBatchKMeans\n        Alternative  implementation that does incremental updates\n        of the centers' positions using mini-batches.\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(n_clusters=None)\n    >>> brc.fit(X)\n    Birch(n_clusters=None)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])\n    ",
        "klass": "sklearn.cluster.Birch",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, optional\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, optional\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`Glossary <sparse graph>`, in which\n        case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, optional\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, optional\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : array, shape = [n_core_samples]\n        Indices of core samples.\n\n    components_ : array, shape = [n_core_samples, n_features]\n        Copy of each core sample found by training.\n\n    labels_ : array, shape = [n_samples]\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)\n\n    See also\n    --------\n    OPTICS\n        A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    ",
        "klass": "sklearn.cluster.DBSCAN",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cluster._hierarchical.AgglomerativeClustering",
            "sklearn.cluster._feature_agglomeration.AgglomerationTransform"
        ],
        "class_docstring": "Agglomerate features.\n\n    Similar to AgglomerativeClustering, but recursively merges features\n    instead of samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : int or None, optional (default=2)\n        The number of clusters to find. It must be ``None`` if\n        ``distance_threshold`` is not ``None``.\n\n    affinity : string or callable, default \"euclidean\"\n        Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\n        \"manhattan\", \"cosine\", or 'precomputed'.\n        If linkage is \"ward\", only \"euclidean\" is accepted.\n\n    memory : None, str or object with the joblib.Memory interface, optional\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    connectivity : array-like or callable, optional\n        Connectivity matrix. Defines for each feature the neighboring\n        features following a given structure of the data.\n        This can be a connectivity matrix itself or a callable that transforms\n        the data into a connectivity matrix, such as derived from\n        kneighbors_graph. Default is None, i.e, the\n        hierarchical clustering algorithm is unstructured.\n\n    compute_full_tree : bool or 'auto', optional, default \"auto\"\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of features. This option is\n        useful only when specifying a connectivity matrix. Note also that\n        when varying the number of clusters and using caching, it may\n        be advantageous to compute the full tree. It must be ``True`` if\n        ``distance_threshold`` is not ``None``.\n\n    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional            (default=\"ward\")\n        Which linkage criterion to use. The linkage criterion determines which\n        distance to use between sets of features. The algorithm will merge\n        the pairs of cluster that minimize this criterion.\n\n        - ward minimizes the variance of the clusters being merged.\n        - average uses the average of the distances of each feature of\n          the two sets.\n        - complete or maximum linkage uses the maximum distances between\n          all features of the two sets.\n        - single uses the minimum of the distances between all observations\n          of the two sets.\n\n    pooling_func : callable, default np.mean\n        This combines the values of agglomerated features into a single\n        value, and should accept an array of shape [M, N] and the keyword\n        argument `axis=1`, and reduce it to an array of size [M].\n\n    distance_threshold : float, optional (default=None)\n        The linkage distance threshold above which, clusters will not be\n        merged. If not ``None``, ``n_clusters`` must be ``None`` and\n        ``compute_full_tree`` must be ``True``.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    n_clusters_ : int\n        The number of clusters found by the algorithm. If\n        ``distance_threshold=None``, it will be equal to the given\n        ``n_clusters``.\n\n    labels_ : array-like, (n_features,)\n        cluster labels for each feature.\n\n    n_leaves_ : int\n        Number of leaves in the hierarchical tree.\n\n    n_connected_components_ : int\n        The estimated number of connected components in the graph.\n\n    children_ : array-like, shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_features`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_features` is a non-leaf\n        node and has children `children_[i - n_features]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_features + i`\n\n    distances_ : array-like, shape (n_nodes-1,)\n        Distances between nodes in the corresponding place in `children_`.\n        Only computed if distance_threshold is not None.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, cluster\n    >>> digits = datasets.load_digits()\n    >>> images = digits.images\n    >>> X = np.reshape(images, (len(images), -1))\n    >>> agglo = cluster.FeatureAgglomeration(n_clusters=32)\n    >>> agglo.fit(X)\n    FeatureAgglomeration(n_clusters=32)\n    >>> X_reduced = agglo.transform(X)\n    >>> X_reduced.shape\n    (1797, 32)\n    ",
        "klass": "sklearn.cluster.FeatureAgglomeration",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "K-Means clustering.\n\n    Read more in the :ref:`User Guide <k_means>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, optional, default: 8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random' or an ndarray}\n        Method for initialization, defaults to 'k-means++':\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose k observations (rows) at random from data for\n        the initial centroids.\n\n        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n    n_init : int, default: 10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    max_iter : int, default: 300\n        Maximum number of iterations of the k-means algorithm for a\n        single run.\n\n    tol : float, default: 1e-4\n        Relative tolerance with regards to inertia to declare convergence.\n\n    precompute_distances : {'auto', True, False}\n        Precompute distances (faster but takes more memory).\n\n        'auto' : do not precompute distances if n_samples * n_clusters > 12\n        million. This corresponds to about 100MB overhead per job using\n        double precision.\n\n        True : always precompute distances.\n\n        False : never precompute distances.\n\n    verbose : int, default 0\n        Verbosity mode.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for centroid initialization. Use\n        an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    copy_x : bool, optional\n        When pre-computing distances it is more numerically accurate to center\n        the data first.  If copy_x is True (default), then the original data is\n        not modified, ensuring X is C-contiguous.  If False, the original data\n        is modified, and put back before the function returns, but small\n        numerical differences may be introduced by subtracting and then adding\n        the data mean, in this case it will also not ensure that data is\n        C-contiguous which may cause a significant slowdown.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n        The \"elkan\" variation is more efficient by using the triangle\n        inequality, but currently doesn't support sparse data. \"auto\" chooses\n        \"elkan\" for dense data and \"full\" for sparse data.\n\n    Attributes\n    ----------\n    cluster_centers_ : array, [n_clusters, n_features]\n        Coordinates of cluster centers. If the algorithm stops before fully\n        converging (see ``tol`` and ``max_iter``), these will not be\n        consistent with ``labels_``.\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    inertia_ : float\n        Sum of squared distances of samples to their closest cluster center.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n\n    MiniBatchKMeans\n        Alternative online implementation that does incremental updates\n        of the centers positions using mini-batches.\n        For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n        probably much faster than the default batch implementation.\n\n    Notes\n    -----\n    The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n\n    The average complexity is given by O(k n T), were n is the number of\n    samples and T is the number of iteration.\n\n    The worst case complexity is given by O(n^(k+2/p)) with\n    n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n    'How slow is the k-means method?' SoCG2006)\n\n    In practice, the k-means algorithm is very fast (one of the fastest\n    clustering algorithms available), but it falls in local minima. That's why\n    it can be useful to restart it several times.\n\n    If the algorithm stops before fully converging (because of ``tol`` or\n    ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n    i.e. the ``cluster_centers_`` will not be the means of the points in each\n    cluster. Also, the estimator will reassign ``labels_`` after the last\n    iteration to make ``labels_`` consistent with ``predict`` on the training\n    set.\n\n    Examples\n    --------\n\n    >>> from sklearn.cluster import KMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [10, 2], [10, 4], [10, 0]])\n    >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n    >>> kmeans.labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> kmeans.predict([[0, 0], [12, 3]])\n    array([1, 0], dtype=int32)\n    >>> kmeans.cluster_centers_\n    array([[10.,  2.],\n           [ 1.,  2.]])\n    ",
        "klass": "sklearn.cluster.KMeans",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Mean shift clustering using a flat kernel.\n\n    Mean shift clustering aims to discover \"blobs\" in a smooth density of\n    samples. It is a centroid-based algorithm, which works by updating\n    candidates for centroids to be the mean of the points within a given\n    region. These candidates are then filtered in a post-processing stage to\n    eliminate near-duplicates to form the final set of centroids.\n\n    Seeding is performed using a binning technique for scalability.\n\n    Read more in the :ref:`User Guide <mean_shift>`.\n\n    Parameters\n    ----------\n    bandwidth : float, optional\n        Bandwidth used in the RBF kernel.\n\n        If not given, the bandwidth is estimated using\n        sklearn.cluster.estimate_bandwidth; see the documentation for that\n        function for hints on scalability (see also the Notes, below).\n\n    seeds : array, shape=[n_samples, n_features], optional\n        Seeds used to initialize kernels. If not set,\n        the seeds are calculated by clustering.get_bin_seeds\n        with bandwidth as the grid size and default values for\n        other parameters.\n\n    bin_seeding : boolean, optional\n        If true, initial kernel locations are not locations of all\n        points, but rather the location of the discretized version of\n        points, where points are binned onto a grid whose coarseness\n        corresponds to the bandwidth. Setting this option to True will speed\n        up the algorithm because fewer seeds will be initialized.\n        default value: False\n        Ignored if seeds argument is not None.\n\n    min_bin_freq : int, optional\n       To speed up the algorithm, accept only those bins with at least\n       min_bin_freq points as seeds. If not defined, set to 1.\n\n    cluster_all : boolean, default True\n        If true, then all points are clustered, even those orphans that are\n        not within any kernel. Orphans are assigned to the nearest kernel.\n        If false, then orphans are given cluster label -1.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by computing\n        each of the n_init runs in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    max_iter : int, default=300\n        Maximum number of iterations, per seed point before the clustering\n        operation terminates (for that seed point), if has not converged yet.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    cluster_centers_ : array, [n_clusters, n_features]\n        Coordinates of cluster centers.\n\n    labels_ :\n        Labels of each point.\n\n    n_iter_ : int\n        Maximum number of iterations performed on each seed.\n\n        .. versionadded:: 0.22\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MeanShift\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = MeanShift(bandwidth=2).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering.predict([[0, 0], [5, 5]])\n    array([1, 0])\n    >>> clustering\n    MeanShift(bandwidth=2)\n\n    Notes\n    -----\n\n    Scalability:\n\n    Because this implementation uses a flat kernel and\n    a Ball Tree to look up members of each kernel, the complexity will tend\n    towards O(T*n*log(n)) in lower dimensions, with n the number of samples\n    and T the number of points. In higher dimensions the complexity will\n    tend towards O(T*n^2).\n\n    Scalability can be boosted by using fewer seeds, for example by using\n    a higher value of min_bin_freq in the get_bin_seeds function.\n\n    Note that the estimate_bandwidth function is much less scalable than the\n    mean shift algorithm and will be the bottleneck if it is used.\n\n    References\n    ----------\n\n    Dorin Comaniciu and Peter Meer, \"Mean Shift: A robust approach toward\n    feature space analysis\". IEEE Transactions on Pattern Analysis and\n    Machine Intelligence. 2002. pp. 603-619.\n\n    ",
        "klass": "sklearn.cluster.MeanShift",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cluster._k_means.KMeans"
        ],
        "class_docstring": "\n    Mini-Batch K-Means clustering.\n\n    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n\n    Parameters\n    ----------\n\n    n_clusters : int, optional, default: 8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n\n    init : {'k-means++', 'random' or an ndarray}, default: 'k-means++'\n        Method for initialization, defaults to 'k-means++':\n\n        'k-means++' : selects initial cluster centers for k-mean\n        clustering in a smart way to speed up convergence. See section\n        Notes in k_init for more details.\n\n        'random': choose k observations (rows) at random from data for\n        the initial centroids.\n\n        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centers.\n\n    max_iter : int, optional\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n    batch_size : int, optional, default: 100\n        Size of the mini batches.\n\n    verbose : bool, optional\n        Verbosity mode.\n\n    compute_labels : bool, default=True\n        Compute label assignment and inertia for the complete dataset\n        once the minibatch optimization has converged in fit.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for centroid initialization and\n        random reassignment. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default: 0.0\n        Control early stopping based on the relative center changes as\n        measured by a smoothed, variance-normalized of the mean center\n        squared position changes. This early stopping heuristics is\n        closer to the one used for the batch variant of the algorithms\n        but induces a slight computational and memory overhead over the\n        inertia heuristic.\n\n        To disable convergence detection based on normalized center\n        change, set tol to 0.0 (default).\n\n    max_no_improvement : int, default: 10\n        Control early stopping based on the consecutive number of mini\n        batches that does not yield an improvement on the smoothed inertia.\n\n        To disable convergence detection based on inertia, set\n        max_no_improvement to None.\n\n    init_size : int, optional, default: 3 * batch_size\n        Number of samples to randomly sample for speeding up the\n        initialization (sometimes at the expense of accuracy): the\n        only algorithm is initialized by running a batch KMeans on a\n        random subset of the data. This needs to be larger than n_clusters.\n\n    n_init : int, default=3\n        Number of random initializations that are tried.\n        In contrast to KMeans, the algorithm is only run once, using the\n        best of the ``n_init`` initializations as measured by inertia.\n\n    reassignment_ratio : float, default: 0.01\n        Control the fraction of the maximum number of counts for a\n        center to be reassigned. A higher value means that low count\n        centers are more easily reassigned, which means that the\n        model will take longer to converge, but should converge in a\n        better clustering.\n\n    Attributes\n    ----------\n\n    cluster_centers_ : array, [n_clusters, n_features]\n        Coordinates of cluster centers\n\n    labels_ :\n        Labels of each point (if compute_labels is set to True).\n\n    inertia_ : float\n        The value of the inertia criterion associated with the chosen\n        partition (if compute_labels is set to True). The inertia is\n        defined as the sum of square distances of samples to their nearest\n        neighbor.\n\n    See Also\n    --------\n    KMeans\n        The classic implementation of the clustering method based on the\n        Lloyd's algorithm. It consumes the whole set of input data at each\n        iteration.\n\n    Notes\n    -----\n    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    Examples\n    --------\n    >>> from sklearn.cluster import MiniBatchKMeans\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 0], [4, 4],\n    ...               [4, 5], [0, 1], [2, 2],\n    ...               [3, 2], [5, 5], [1, -1]])\n    >>> # manually fit on batches\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6)\n    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n    >>> kmeans.cluster_centers_\n    array([[2. , 1. ],\n           [3.5, 4.5]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([0, 1], dtype=int32)\n    >>> # fit on the whole data\n    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n    ...                          random_state=0,\n    ...                          batch_size=6,\n    ...                          max_iter=10).fit(X)\n    >>> kmeans.cluster_centers_\n    array([[3.95918367, 2.40816327],\n           [1.12195122, 1.3902439 ]])\n    >>> kmeans.predict([[0, 0], [4, 4]])\n    array([1, 0], dtype=int32)\n    ",
        "klass": "sklearn.cluster.MiniBatchKMeans",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClusterMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Apply clustering to a projection of the normalized Laplacian.\n\n    In practice Spectral Clustering is very useful when the structure of\n    the individual clusters is highly non-convex or more generally when\n    a measure of the center and spread of the cluster is not a suitable\n    description of the complete cluster. For instance when clusters are\n    nested circles on the 2D plane.\n\n    If affinity is the adjacency matrix of a graph, this method can be\n    used to find normalized graph cuts.\n\n    When calling ``fit``, an affinity matrix is constructed using either\n    kernel function such the Gaussian (aka RBF) kernel of the euclidean\n    distanced ``d(X, X)``::\n\n            np.exp(-gamma * d(X,X) ** 2)\n\n    or a k-nearest neighbors connectivity matrix.\n\n    Alternatively, using ``precomputed``, a user-provided affinity\n    matrix can be used.\n\n    Read more in the :ref:`User Guide <spectral_clustering>`.\n\n    Parameters\n    ----------\n    n_clusters : integer, optional\n        The dimension of the projection subspace.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities.\n\n    n_components : integer, optional, default=n_clusters\n        Number of eigen vectors to use for the spectral embedding\n\n    random_state : int, RandomState instance or None (default)\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigen vectors decomposition when ``eigen_solver='amg'`` and by\n        the K-Means initialization. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_init : int, optional, default: 10\n        Number of time the k-means algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of inertia.\n\n    gamma : float, default=1.0\n        Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.\n        Ignored for ``affinity='nearest_neighbors'``.\n\n    affinity : string or callable, default 'rbf'\n        How to construct the affinity matrix.\n         - 'nearest_neighbors' : construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf' : construct the affinity matrix using a radial basis function\n           (RBF) kernel.\n         - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n         - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n           of precomputed nearest neighbors, and constructs the affinity matrix\n           by selecting the ``n_neighbors`` nearest neighbors.\n         - one of the kernels supported by\n           :func:`~sklearn.metrics.pairwise_kernels`.\n\n        Only kernels that produce similarity scores (non-negative values that\n        increase with similarity) should be used. This property is not checked\n        by the clustering algorithm.\n\n    n_neighbors : integer\n        Number of neighbors to use when constructing the affinity matrix using\n        the nearest neighbors method. Ignored for ``affinity='rbf'``.\n\n    eigen_tol : float, optional, default: 0.0\n        Stopping criterion for eigendecomposition of the Laplacian matrix\n        when ``eigen_solver='arpack'``.\n\n    assign_labels : {'kmeans', 'discretize'}, default: 'kmeans'\n        The strategy to use to assign labels in the embedding\n        space. There are two ways to assign labels after the laplacian\n        embedding. k-means can be applied and is a popular choice. But it can\n        also be sensitive to initialization. Discretization is another approach\n        which is less sensitive to random initialization.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dictionary of string to any, optional\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    affinity_matrix_ : array-like, shape (n_samples, n_samples)\n        Affinity matrix used for clustering. Available only if after calling\n        ``fit``.\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralClustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralClustering(n_clusters=2,\n    ...         assign_labels=\"discretize\",\n    ...         random_state=0).fit(X)\n    >>> clustering.labels_\n    array([1, 1, 1, 0, 0, 0])\n    >>> clustering\n    SpectralClustering(assign_labels='discretize', n_clusters=2,\n        random_state=0)\n\n    Notes\n    -----\n    If you have an affinity matrix, such as a distance matrix,\n    for which 0 means identical elements, and high values means\n    very dissimilar elements, it can be transformed in a\n    similarity matrix that is well suited for the algorithm by\n    applying the Gaussian (RBF, heat) kernel::\n\n        np.exp(- dist_matrix ** 2 / (2. * delta ** 2))\n\n    Where ``delta`` is a free parameter representing the width of the Gaussian\n    kernel.\n\n    Another alternative is to take a symmetric version of the k\n    nearest neighbors connectivity matrix of the points.\n\n    If the pyamg package is installed, it is used: this greatly\n    speeds up computation.\n\n    References\n    ----------\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n    ",
        "klass": "sklearn.cluster.SpectralClustering",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.OutlierMixin",
            "sklearn.covariance._robust_covariance.MinCovDet"
        ],
        "class_docstring": "An object for detecting outliers in a Gaussian distributed dataset.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    store_precision : boolean, optional (default=True)\n        Specify if the estimated precision is stored.\n\n    assume_centered : boolean, optional (default=False)\n        If True, the support of robust location and covariance estimates\n        is computed, and a covariance estimate is recomputed from it,\n        without centering the data.\n        Useful to work with data whose mean is significantly equal to\n        zero but is not exactly zero.\n        If False, the robust location and covariance are directly computed\n        with the FastMCD algorithm without additional treatment.\n\n    support_fraction : float in (0., 1.), optional (default=None)\n        The proportion of points to be included in the support of the raw\n        MCD estimate. If None, the minimum value of support_fraction will\n        be used within the algorithm: `[n_sample + n_features + 1] / 2`.\n\n    contamination : float in (0., 0.5), optional (default=0.1)\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    Attributes\n    ----------\n    location_ : array-like, shape (n_features,)\n        Estimated robust location\n\n    covariance_ : array-like, shape (n_features, n_features)\n        Estimated robust covariance matrix\n\n    precision_ : array-like, shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    support_ : array-like, shape (n_samples,)\n        A mask of the observations that have been used to compute the\n        robust estimates of location and shape.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        The offset depends on the contamination parameter and is defined in\n        such a way we obtain the expected number of outliers (samples with\n        decision function < 0) in training.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EllipticEnvelope\n    >>> true_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> X = np.random.RandomState(0).multivariate_normal(mean=[0, 0],\n    ...                                                  cov=true_cov,\n    ...                                                  size=500)\n    >>> cov = EllipticEnvelope(random_state=0).fit(X)\n    >>> # predict returns 1 for an inlier and -1 for an outlier\n    >>> cov.predict([[0, 0],\n    ...              [3, 3]])\n    array([ 1, -1])\n    >>> cov.covariance_\n    array([[0.7411..., 0.2535...],\n           [0.2535..., 0.3053...]])\n    >>> cov.location_\n    array([0.0813... , 0.0427...])\n\n    See Also\n    --------\n    EmpiricalCovariance, MinCovDet\n\n    Notes\n    -----\n    Outlier detection from covariance estimation may break or not\n    perform well in high-dimensional settings. In particular, one will\n    always take care to work with ``n_samples > n_features ** 2``.\n\n    References\n    ----------\n    .. [1] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the\n       minimum covariance determinant estimator\" Technometrics 41(3), 212\n       (1999)\n\n    ",
        "klass": "sklearn.covariance.EllipticEnvelope",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Maximum likelihood covariance estimator\n\n    Read more in the :ref:`User Guide <covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool\n        Specifies if the estimated precision is stored.\n\n    assume_centered : bool\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : array-like, shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : 2D ndarray, shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : 2D ndarray, shape (n_features, n_features)\n        Estimated pseudo-inverse matrix.\n        (stored only if store_precision is True)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n\n    ",
        "klass": "sklearn.covariance.EmpiricalCovariance",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.covariance._empirical_covariance.EmpiricalCovariance"
        ],
        "class_docstring": "LedoitWolf Estimator\n\n    Ledoit-Wolf is a particular form of shrinkage, where the shrinkage\n    coefficient is computed using O. Ledoit and M. Wolf's formula as\n    described in \"A Well-Conditioned Estimator for Large-Dimensional\n    Covariance Matrices\", Ledoit and Wolf, Journal of Multivariate\n    Analysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    block_size : int, default=1000\n        Size of the blocks into which the covariance matrix will be split\n        during its Ledoit-Wolf estimation. This is purely a memory\n        optimization and does not affect results.\n\n    Attributes\n    ----------\n    location_ : array-like, shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : array-like, shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : array-like, shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float, 0 <= shrinkage <= 1\n        Coefficient in the convex combination used for the computation\n        of the shrunk estimate.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import LedoitWolf\n    >>> real_cov = np.array([[.4, .2],\n    ...                      [.2, .8]])\n    >>> np.random.seed(0)\n    >>> X = np.random.multivariate_normal(mean=[0, 0],\n    ...                                   cov=real_cov,\n    ...                                   size=50)\n    >>> cov = LedoitWolf().fit(X)\n    >>> cov.covariance_\n    array([[0.4406..., 0.1616...],\n           [0.1616..., 0.8022...]])\n    >>> cov.location_\n    array([ 0.0595... , -0.0075...])\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the Ledoit and Wolf formula (see References)\n\n    References\n    ----------\n    \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\",\n    Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\n    February 2004, pages 365-411.\n\n    ",
        "klass": "sklearn.covariance.LedoitWolf",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.covariance._empirical_covariance.EmpiricalCovariance"
        ],
        "class_docstring": "Oracle Approximating Shrinkage Estimator\n\n    Read more in the :ref:`User Guide <shrunk_covariance>`.\n\n    OAS is a particular form of shrinkage described in\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\n    The formula used here does not correspond to the one given in the\n    article. In the original article, formula (23) states that 2/p is\n    multiplied by Trace(cov*cov) in both the numerator and denominator, but\n    this operation is omitted because for a large p, the value of 2/p is\n    so small that it doesn't affect the value of the estimator.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specify if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data will not be centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data will be centered before computation.\n\n    Attributes\n    ----------\n    covariance_ : array-like, shape (n_features, n_features)\n        Estimated covariance matrix.\n\n    precision_ : array-like, shape (n_features, n_features)\n        Estimated pseudo inverse matrix.\n        (stored only if store_precision is True)\n\n    shrinkage_ : float, 0 <= shrinkage <= 1\n      coefficient in the convex combination used for the computation\n      of the shrunk estimate.\n\n    Notes\n    -----\n    The regularised covariance is:\n\n    (1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\n    where mu = trace(cov) / n_features\n    and shrinkage is given by the OAS formula (see References)\n\n    References\n    ----------\n    \"Shrinkage Algorithms for MMSE Covariance Estimation\"\n    Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\n    ",
        "klass": "sklearn.covariance.OAS",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base._UnstableArchMixin",
            "sklearn.cross_decomposition._pls._PLS"
        ],
        "class_docstring": "CCA Canonical Correlation Analysis.\n\n    CCA inherits from PLS with mode=\"B\" and deflation_mode=\"canonical\".\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2).\n        number of components to keep.\n\n    scale : boolean, (default True)\n        whether to scale the data?\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop\n\n    tol : non-negative real, default 1e-06.\n        the tolerance used in the iterative algorithm\n\n    copy : boolean\n        Whether the deflation be done on a copy. Let the default value\n        to True unless you don't care about side effects\n\n    Attributes\n    ----------\n    x_weights_ : array, [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, [q, n_components]\n        Y block to latents rotations.\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component.\n\n    Notes\n    -----\n    For each component k, find the weights u, v that maximizes\n    max corr(Xk u, Yk v), such that ``|u| = |v| = 1``\n\n    Note that it maximizes only the correlations between the scores.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on the\n    current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current Y score.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import CCA\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> cca = CCA(n_components=1)\n    >>> cca.fit(X, Y)\n    CCA(n_components=1)\n    >>> X_c, Y_c = cca.transform(X, Y)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    In french but still a reference:\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n\n    See also\n    --------\n    PLSCanonical\n    PLSSVD\n    ",
        "klass": "sklearn.cross_decomposition.CCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cross_decomposition._pls._PLS"
        ],
        "class_docstring": " PLSCanonical implements the 2 blocks canonical PLS of the original Wold\n    algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\n\n    This class inherits from PLS with mode=\"A\" and deflation_mode=\"canonical\",\n    norm_y_weights=True and algorithm=\"nipals\", but svd should provide similar\n    results up to numerical errors.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2).\n        Number of components to keep\n\n    scale : boolean, (default True)\n        Option to scale data\n\n    algorithm : string, \"nipals\" or \"svd\"\n        The algorithm used to estimate the weights. It will be called\n        n_components times, i.e. once for each iteration of the outer loop.\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop (used\n        only if algorithm=\"nipals\")\n\n    tol : non-negative real, default 1e-06\n        the tolerance used in the iterative algorithm\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effect\n\n    Attributes\n    ----------\n    x_weights_ : array, shape = [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, shape = [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, shape = [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, shape = [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, shape = [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, shape = [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, shape = [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, shape = [q, n_components]\n        Y block to latents rotations.\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component. Not useful if the algorithm provided is \"svd\".\n\n    Notes\n    -----\n    Matrices::\n\n        T: x_scores_\n        U: y_scores_\n        W: x_weights_\n        C: y_weights_\n        P: x_loadings_\n        Q: y_loadings__\n\n    Are computed such that::\n\n        X = T P.T + Err and Y = U Q.T + Err\n        T[:, k] = Xk W[:, k] for k in range(n_components)\n        U[:, k] = Yk C[:, k] for k in range(n_components)\n        x_rotations_ = W (P.T W)^(-1)\n        y_rotations_ = C (Q.T C)^(-1)\n\n    where Xk and Yk are residual matrices at iteration k.\n\n    `Slides explaining PLS\n    <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\n\n    For each component k, find weights u, v that optimize::\n\n        max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``\n\n    Note that it maximizes both the correlations between the scores and the\n    intra-block variances.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on the\n    current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current Y score. This performs a canonical symmetric version of the PLS\n    regression. But slightly different than the CCA. This is mostly used\n    for modeling.\n\n    This implementation provides the same results that the \"plspm\" package\n    provided in the R language (R-project), using the function plsca(X, Y).\n    Results are equal or collinear with the function\n    ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The difference\n    relies in the fact that mixOmics implementation does not exactly implement\n    the Wold algorithm since it does not normalize y_weights to one.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, Y)\n    PLSCanonical()\n    >>> X_c, Y_c = plsca.transform(X, Y)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n\n    See also\n    --------\n    CCA\n    PLSSVD\n    ",
        "klass": "sklearn.cross_decomposition.PLSCanonical",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cross_decomposition._pls._PLS"
        ],
        "class_docstring": "PLS regression\n\n    PLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1\n    in case of one dimensional response.\n    This class inherits from _PLS with mode=\"A\", deflation_mode=\"regression\",\n    norm_y_weights=False and algorithm=\"nipals\".\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2)\n        Number of components to keep.\n\n    scale : boolean, (default True)\n        whether to scale the data\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop (used\n        only if algorithm=\"nipals\")\n\n    tol : non-negative real\n        Tolerance used in the iterative algorithm default 1e-06.\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effect\n\n    Attributes\n    ----------\n    x_weights_ : array, [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, [q, n_components]\n        Y block to latents rotations.\n\n    coef_ : array, [p, q]\n        The coefficients of the linear model: ``Y = X coef_ + Err``\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component.\n\n    Notes\n    -----\n    Matrices::\n\n        T: x_scores_\n        U: y_scores_\n        W: x_weights_\n        C: y_weights_\n        P: x_loadings_\n        Q: y_loadings_\n\n    Are computed such that::\n\n        X = T P.T + Err and Y = U Q.T + Err\n        T[:, k] = Xk W[:, k] for k in range(n_components)\n        U[:, k] = Yk C[:, k] for k in range(n_components)\n        x_rotations_ = W (P.T W)^(-1)\n        y_rotations_ = C (Q.T C)^(-1)\n\n    where Xk and Yk are residual matrices at iteration k.\n\n    `Slides explaining\n    PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\n\n\n    For each component k, find weights u, v that optimizes:\n    ``max corr(Xk u, Yk v) * std(Xk u) std(Yk u)``, such that ``|u| = 1``\n\n    Note that it maximizes both the correlations between the scores and the\n    intra-block variances.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on\n    the current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current X score. This performs the PLS regression known as PLS2. This\n    mode is prediction oriented.\n\n    This implementation provides the same results that 3 PLS packages\n    provided in the R language (R-project):\n\n        - \"mixOmics\" with function pls(X, Y, mode = \"regression\")\n        - \"plspm \" with function plsreg2(X, Y)\n        - \"pls\" with function oscorespls.fit(X, Y)\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSRegression\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> pls2 = PLSRegression(n_components=2)\n    >>> pls2.fit(X, Y)\n    PLSRegression()\n    >>> Y_pred = pls2.predict(X)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    In french but still a reference:\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n    ",
        "klass": "sklearn.cross_decomposition.PLSRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_extraction.text._VectorizerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Convert a collection of text documents to a matrix of token counts\n\n    This implementation produces a sparse representation of the counts using\n    scipy.sparse.csr_matrix.\n\n    If you do not provide an a-priori dictionary and you do not use an analyzer\n    that does some kind of feature selection then the number of features will\n    be equal to the vocabulary size found by analyzing the data.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : string {'filename', 'file', 'content'}\n        If 'filename', the sequence passed as an argument to fit is\n        expected to be a list of filenames that need reading to fetch\n        the raw content to analyze.\n\n        If 'file', the sequence items must have a 'read' method (file-like\n        object) that is called to fetch the bytes in memory.\n\n        Otherwise the input is expected to be a sequence of items that\n        can be of type string or byte.\n\n    encoding : string, 'utf-8' by default.\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode', None}\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : boolean, True by default\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable or None (default)\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable or None (default)\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : string {'english'}, list, or None (default)\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    token_pattern : string\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp select tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        word n-grams or char n-grams to be extracted. All values of n such\n        such that min_n <= n <= max_n will be used. For example an\n        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    analyzer : string, {'word', 'char', 'char_wb'} or callable\n        Whether the feature should be made of word n-gram or character\n        n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n\n        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n        first read from the file and then passed to the given callable\n        analyzer.\n\n    max_df : float in range [0.0, 1.0] or int, default=1.0\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float in range [0.0, 1.0] or int, default=1\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int or None, default=None\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, optional\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents. Indices\n        in the mapping should not be repeated and should not have any gap\n        between 0 and the largest index.\n\n    binary : boolean, default=False\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    dtype : type, optional\n        Type of the matrix returned by fit_transform() or transform().\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    fixed_vocabulary_: boolean\n        True if a fixed vocabulary of term to indices mapping\n        is provided by the user\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = CountVectorizer()\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(vectorizer.get_feature_names())\n    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n    >>> print(X.toarray())\n    [[0 1 1 1 0 0 1 0 1]\n     [0 2 0 1 0 1 1 0 1]\n     [1 0 0 1 1 0 1 1 1]\n     [0 1 1 1 0 0 1 0 1]]\n    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n    >>> X2 = vectorizer2.fit_transform(corpus)\n    >>> print(vectorizer2.get_feature_names())\n    ['and this', 'document is', 'first document', 'is the', 'is this',\n    'second document', 'the first', 'the second', 'the third', 'third one',\n     'this document', 'this is', 'this the']\n     >>> print(X2.toarray())\n     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n     [0 1 0 1 0 1 0 1 0 0 1 0 0]\n     [1 0 0 1 0 0 0 0 1 1 0 1 0]\n     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n\n    See Also\n    --------\n    HashingVectorizer, TfidfVectorizer\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n    ",
        "klass": "sklearn.feature_extraction.text.CountVectorizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.decomposition._dict_learning.SparseCodingMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Dictionary learning\n\n    Finds a dictionary (a set of atoms) that can best be used to represent data\n    using a sparse code.\n\n    Solves the optimization problem::\n\n        (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n                    (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=n_features\n        number of dictionary elements to extract\n\n    alpha : float, default=1.0\n        sparsity controlling parameter\n\n    max_iter : int, default=1000\n        maximum number of iterations to perform\n\n    tol : float, default=1e-8\n        tolerance for numerical error\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n        .. versionadded:: 0.17\n           *cd* coordinate descent method to improve speed.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}, default='omp'\n        Algorithm used to transform the data\n        lars: uses the least angle regression method (linear_model.lars_path)\n        lasso_lars: uses Lars to compute the Lasso solution\n        lasso_cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n        the estimated components are sparse.\n        omp: uses orthogonal matching pursuit to estimate the sparse solution\n        threshold: squashes to zero all coefficients less than alpha from\n        the projection ``dictionary * X'``\n\n        .. versionadded:: 0.17\n           *lasso_cd* coordinate descent method to improve speed.\n\n    transform_n_nonzero_coefs : int, default=0.1*n_features\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case.\n\n    transform_alpha : float, default=1.0\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n\n    n_jobs : int or None, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    code_init : array of shape (n_samples, n_components), default=None\n        initial value for the code, for warm restart\n\n    dict_init : array of shape (n_components, n_features), default=None\n        initial values for the dictionary, for warm restart\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `lasso_lars`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        dictionary atoms extracted from the data\n\n    error_ : array\n        vector of errors at each iteration\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Notes\n    -----\n    **References:**\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    See also\n    --------\n    SparseCoder\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    ",
        "klass": "sklearn.decomposition.DictionaryLearning",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Factor Analysis (FA)\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PPCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int | None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int\n        Maximum number of iterations.\n\n    noise_variance_init : None | array, shape=(n_features,)\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features)\n\n    svd_method : {'lapack', 'randomized'}\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, optional\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'\n\n    random_state : int, RandomState instance or None, optional (default=0)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Only used when ``svd_method`` equals 'randomized'.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Components with maximum variance.\n\n    loglike_ : list, [n_iterations]\n        The log likelihood at each iteration.\n\n    noise_variance_ : array, shape=(n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    .. David Barber, Bayesian Reasoning and Machine Learning,\n        Algorithm 21.1\n\n    .. Christopher M. Bishop: Pattern Recognition and Machine Learning,\n        Chapter 12.2.4\n\n    See also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n    ",
        "klass": "sklearn.decomposition.FactorAnalysis",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "FastICA: a fast algorithm for Independent Component Analysis.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    n_components : int, optional\n        Number of components to use. If none is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}\n        Apply parallel or deflational algorithm for FastICA.\n\n    whiten : boolean, optional\n        If whiten is false, the data is already considered to be\n        whitened, and no whitening is performed.\n\n    fun : string or function, optional. Default: 'logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. Example:\n\n        def my_g(x):\n            return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dictionary, optional\n        Arguments to send to the functional form.\n        If empty and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, optional\n        Maximum number of iterations during fit.\n\n    tol : float, optional\n        Tolerance on update at each iteration.\n\n    w_init : None of an (n_components, n_components) ndarray\n        The mixing matrix to be used to initialize the algorithm.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    components_ : 2D array, shape (n_components, n_features)\n        The linear operator to apply to the data to get the independent\n        sources. This is equal to the unmixing matrix when ``whiten`` is\n        False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n        ``whiten`` is True.\n\n    mixing_ : array, shape (n_features, n_components)\n        The pseudo-inverse of ``components_``. It is the linear operator\n        that maps independent sources to the data.\n\n    mean_ : array, shape(n_features)\n        The mean over features. Only set if `self.whiten` is True.\n\n    n_iter_ : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge.\n\n    whitening_ : array, shape (n_components, n_features)\n        Only set if whiten is 'True'. This is the pre-whitening matrix\n        that projects data onto the first `n_components` principal components.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Implementation based on\n    *A. Hyvarinen and E. Oja, Independent Component Analysis:\n    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n    pp. 411-430*\n\n    ",
        "klass": "sklearn.decomposition.FastICA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.decomposition._base._BasePCA"
        ],
        "class_docstring": "Incremental principal components analysis (IPCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of\n    the data, keeping only the most significant singular vectors to\n    project the data to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    Depending on the size of the input data, this algorithm can be much more\n    memory efficient than a PCA, and allows sparse input.\n\n    This algorithm has constant memory complexity, on the order\n    of ``batch_size * n_features``, enabling use of np.memmap files without\n    loading the entire file into memory. For sparse matrices, the input\n    is converted to dense in batches (in order to be able to subtract the\n    mean) which avoids storing the entire dense matrix at any one time.\n\n    The computational overhead of each SVD is\n    ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n    remain in memory at a time. There will be ``n_samples / batch_size`` SVD\n    computations to get the principal components, versus 1 large SVD of\n    complexity ``O(n_samples * n_features ** 2)`` for PCA.\n\n    Read more in the :ref:`User Guide <IncrementalPCA>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    n_components : int or None, (default=None)\n        Number of components to keep. If ``n_components `` is ``None``,\n        then ``n_components`` is set to ``min(n_samples, n_features)``.\n\n    whiten : bool, optional\n        When True (False by default) the ``components_`` vectors are divided\n        by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n        with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometimes\n        improve the predictive accuracy of the downstream estimators by\n        making data respect some hard-wired assumptions.\n\n    copy : bool, (default=True)\n        If False, X will be overwritten. ``copy=False`` can be used to\n        save memory but is unsafe for general use.\n\n    batch_size : int or None, (default=None)\n        The number of samples to use for each batch. Only used when calling\n        ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n        is inferred from the data and set to ``5 * n_features``, to provide a\n        balance between approximation accuracy and memory consumption.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Components with maximum variance.\n\n    explained_variance_ : array, shape (n_components,)\n        Variance explained by each of the selected components.\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If all components are stored, the sum of explained variances is equal\n        to 1.0.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\n    var_ : array, shape (n_features,)\n        Per-feature empirical variance, aggregate over calls to\n        ``partial_fit``.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf.\n\n    n_components_ : int\n        The estimated number of components. Relevant when\n        ``n_components=None``.\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator. Will be reset on\n        new calls to fit, but increments across ``partial_fit`` calls.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    *A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000.*\n    See https://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.\n\n    See also\n    --------\n    PCA\n    KernelPCA\n    SparsePCA\n    TruncatedSVD\n    ",
        "klass": "sklearn.decomposition.IncrementalPCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Kernel Principal component analysis (KPCA)\n\n    Non-linear dimensionality reduction through the use of kernels (see\n    :ref:`metrics`).\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : \"linear\" | \"poly\" | \"rbf\" | \"sigmoid\" | \"cosine\" | \"precomputed\"\n        Kernel. Default=\"linear\".\n\n    gamma : float, default=1/n_features\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels.\n\n    degree : int, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : mapping of string to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    alpha : int, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels.\n        (i.e. learn to find the pre-image of a point)\n\n    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'\n        Select eigensolver to use. If n_components is much less than\n        the number of training samples, arpack may be more efficient\n        than the dense eigensolver.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : boolean, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n        .. versionadded:: 0.18\n\n    copy_X : boolean, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : array, (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : array, (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : array, (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : array, (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    ",
        "klass": "sklearn.decomposition.KernelPCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Latent Dirichlet Allocation with online variational Bayes algorithm\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\n    Parameters\n    ----------\n    n_components : int, optional (default=10)\n        Number of topics.\n\n    doc_topic_prior : float, optional (default=None)\n        Prior of document topic distribution `theta`. If the value is None,\n        defaults to `1 / n_components`.\n        In [1]_, this is called `alpha`.\n\n    topic_word_prior : float, optional (default=None)\n        Prior of topic word distribution `beta`. If the value is None, defaults\n        to `1 / n_components`.\n        In [1]_, this is called `eta`.\n\n    learning_method : 'batch' | 'online', default='batch'\n        Method used to update `_component`. Only used in :meth:`fit` method.\n        In general, if the data size is large, the online update will be much\n        faster than the batch update.\n\n        Valid options::\n\n            'batch': Batch variational Bayes method. Use all training data in\n                each EM update.\n                Old `components_` will be overwritten in each iteration.\n            'online': Online variational Bayes method. In each EM update, use\n                mini-batch of training data to update the ``components_``\n                variable incrementally. The learning rate is controlled by the\n                ``learning_decay`` and the ``learning_offset`` parameters.\n\n        .. versionchanged:: 0.20\n            The default learning method is now ``\"batch\"``.\n\n    learning_decay : float, optional (default=0.7)\n        It is a parameter that control learning rate in the online learning\n        method. The value should be set between (0.5, 1.0] to guarantee\n        asymptotic convergence. When the value is 0.0 and batch_size is\n        ``n_samples``, the update method is same as batch learning. In the\n        literature, this is called kappa.\n\n    learning_offset : float, optional (default=10.)\n        A (positive) parameter that downweights early iterations in online\n        learning.  It should be greater than 1.0. In the literature, this is\n        called tau_0.\n\n    max_iter : integer, optional (default=10)\n        The maximum number of iterations.\n\n    batch_size : int, optional (default=128)\n        Number of documents to use in each EM iteration. Only used in online\n        learning.\n\n    evaluate_every : int, optional (default=0)\n        How often to evaluate perplexity. Only used in `fit` method.\n        set it to 0 or negative number to not evalute perplexity in\n        training at all. Evaluating perplexity can help you check convergence\n        in training process, but it will also increase total training time.\n        Evaluating perplexity in every iteration might increase training time\n        up to two-fold.\n\n    total_samples : int, optional (default=1e6)\n        Total number of documents. Only used in the :meth:`partial_fit` method.\n\n    perp_tol : float, optional (default=1e-1)\n        Perplexity tolerance in batch learning. Only used when\n        ``evaluate_every`` is greater than 0.\n\n    mean_change_tol : float, optional (default=1e-3)\n        Stopping tolerance for updating document topic distribution in E-step.\n\n    max_doc_update_iter : int (default=100)\n        Max number of iterations for updating document topic distribution in\n        the E-step.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use in the E-step.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, optional (default=0)\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Variational parameters for topic word distribution. Since the complete\n        conditional for topic word distribution is a Dirichlet,\n        ``components_[i, j]`` can be viewed as pseudocount that represents the\n        number of times word `j` was assigned to topic `i`.\n        It can also be viewed as distribution over the words for each topic\n        after normalization:\n        ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\n    n_batch_iter_ : int\n        Number of iterations of the EM step.\n\n    n_iter_ : int\n        Number of passes over the dataset.\n\n    bound_ : float\n        Final perplexity score on training set.\n\n    doc_topic_prior_ : float\n        Prior of document topic distribution `theta`. If the value is None,\n        it is `1 / n_components`.\n\n    topic_word_prior_ : float\n        Prior of topic word distribution `beta`. If the value is None, it is\n        `1 / n_components`.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import LatentDirichletAllocation\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> # This produces a feature matrix of token counts, similar to what\n    >>> # CountVectorizer would produce on text.\n    >>> X, _ = make_multilabel_classification(random_state=0)\n    >>> lda = LatentDirichletAllocation(n_components=5,\n    ...     random_state=0)\n    >>> lda.fit(X)\n    LatentDirichletAllocation(...)\n    >>> # get topics for some given samples:\n    >>> lda.transform(X[-2:])\n    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n\n    References\n    ----------\n    .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n        Hoffman, David M. Blei, Francis Bach, 2010\n\n    [2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n        Chong Wang, John Paisley, 2013\n\n    [3] Matthew D. Hoffman's onlineldavb code. Link:\n        https://github.com/blei-lab/onlineldavb\n\n    ",
        "klass": "sklearn.decomposition.LatentDirichletAllocation",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.decomposition._dict_learning.SparseCodingMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Mini-batch dictionary learning\n\n    Finds a dictionary (a set of atoms) that can best be used to represent data\n    using a sparse code.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n                    (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int,\n        number of dictionary elements to extract\n\n    alpha : float,\n        sparsity controlling parameter\n\n    n_iter : int,\n        total number of iterations to perform\n\n    fit_algorithm : {'lars', 'cd'}\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int or None, optional (default=None)\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int,\n        number of samples in each mini-batch\n\n    shuffle : bool,\n        whether to shuffle the samples before forming batches\n\n    dict_init : array of shape (n_components, n_features),\n        initial value of the dictionary for warm restart scenarios\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'}\n        Algorithm used to transform the data.\n        lars: uses the least angle regression method (linear_model.lars_path)\n        lasso_lars: uses Lars to compute the Lasso solution\n        lasso_cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n        the estimated components are sparse.\n        omp: uses orthogonal matching pursuit to estimate the sparse solution\n        threshold: squashes to zero all coefficients less than alpha from\n        the projection dictionary * X'\n\n    transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case.\n\n    transform_alpha : float, 1. by default\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n\n    verbose : bool, optional (default: False)\n        To control the verbosity of the procedure.\n\n    split_sign : bool, False by default\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    positive_code : bool\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, optional (default=1000)\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `lasso_lars`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        components extracted from the data\n\n    inner_stats_ : tuple of (A, B) ndarrays\n        Internal sufficient statistics that are kept by the algorithm.\n        Keeping them is useful in online settings, to avoid losing the\n        history of the evolution, but they shouldn't have any use for the\n        end user.\n        A (n_components, n_components) is the dictionary covariance matrix.\n        B (n_features, n_components) is the data approximation matrix\n\n    n_iter_ : int\n        Number of iterations run.\n\n    iter_offset_ : int\n        The number of iteration on data batches that has been\n        performed before.\n\n    random_state_ : RandomState\n        RandomState instance that is generated either from a seed, the random\n        number generattor or by `np.random`.\n\n    Notes\n    -----\n    **References:**\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n\n    See also\n    --------\n    SparseCoder\n    DictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n\n    ",
        "klass": "sklearn.decomposition.MiniBatchDictionaryLearning",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Non-Negative Matrix Factorization (NMF)\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is::\n\n        0.5 * ||X - WH||_Fro^2\n        + alpha * l1_ratio * ||vec(W)||_1\n        + alpha * l1_ratio * ||vec(H)||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\n    Where::\n\n        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)\n        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\n    For multiplicative-update ('mu') solver, the Frobenius norm\n    (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,\n    by changing the beta_loss parameter.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or None\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init : None | 'random' | 'nndsvd' |  'nndsvda' | 'nndsvdar' | 'custom'\n        Method used to initialize the procedure.\n        Default: None.\n        Valid options:\n\n        - None: 'nndsvd' if n_components <= min(n_samples, n_features),\n            otherwise random.\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H\n\n    solver : 'cd' | 'mu'\n        Numerical solver to use:\n        'cd' is a Coordinate Descent solver.\n        'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or string, default 'frobenius'\n        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default: 1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : integer, default: 200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms. Set it to zero to\n        have no regularization.\n\n        .. versionadded:: 0.17\n           *alpha* used in the Coordinate Descent solver.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    shuffle : boolean, default: False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : integer\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : number\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    ",
        "klass": "sklearn.decomposition.NMF",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.decomposition._base._BasePCA"
        ],
        "class_docstring": "Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float, None or str\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, optional (default False)\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : str {'auto', 'full', 'arpack', 'randomized'}\n        If auto :\n            The solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        If full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        If arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        If randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float >= 0, optional (default .0)\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int >= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    random_state : int, RandomState instance or None, optional (default None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. The components are sorted by\n        ``explained_variance_``.\n\n    explained_variance_ : array, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_features_ : int\n        Number of features in the training data.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method of *Minka, T. P.\n    \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n\n    Implements the probabilistic PCA model from:\n    Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    via the score and score_samples methods.\n    See http://www.miketipping.com/papers/met-mppca.pdf\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    *Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.* and also\n    *Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]\n    ",
        "klass": "sklearn.decomposition.PCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Sparse Principal Components Analysis (SparsePCA)\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int,\n        Number of sparse atoms to extract.\n\n    alpha : float,\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float,\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int,\n        Maximum number of iterations to perform.\n\n    tol : float,\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int or None, optional (default=None)\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    U_init : array of shape (n_samples, n_components),\n        Initial values for the loadings for warm restart scenarios.\n\n    V_init : array of shape (n_components, n_features),\n        Initial values for the components for warm restart scenarios.\n\n    verbose : int\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    normalize_components : 'deprecated'\n        This parameter does not have any effect. The components are always\n        normalized.\n\n        .. versionadded:: 0.20\n\n        .. deprecated:: 0.22\n           ``normalize_components`` is deprecated in 0.22 and will be removed\n           in 0.24.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Sparse components extracted from the data.\n\n    error_ : array\n        Vector of errors at each iteration.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import SparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = SparsePCA(n_components=5, random_state=0)\n    >>> transformer.fit(X)\n    SparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    0.9666...\n\n    See also\n    --------\n    PCA\n    MiniBatchSparsePCA\n    DictionaryLearning\n    ",
        "klass": "sklearn.decomposition.SparsePCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with scipy.sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in sklearn.feature_extraction.text. In that\n    context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n    (X.T * X), whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default = 2\n        Desired dimensionality of output data.\n        Must be strictly less than the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : string, default = \"randomized\"\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, optional (default 5)\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        `~sklearn.utils.extmath.randomized_svd` to handle sparse matrices that\n        may have large slowly decaying spectrum.\n\n    random_state : int, RandomState instance or None, optional, default = None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    tol : float, optional\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n\n    explained_variance_ : array, shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import random as sparse_random\n    >>> from sklearn.random_projection import sparse_random_matrix\n    >>> X = sparse_random(100, 100, density=0.01, format='csr',\n    ...                   random_state=42)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0646... 0.0633... 0.0639... 0.0535... 0.0406...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.286...\n    >>> print(svd.singular_values_)\n    [1.553... 1.512...  1.510... 1.370... 1.199...]\n\n    See also\n    --------\n    PCA\n\n    References\n    ----------\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    ",
        "klass": "sklearn.decomposition.TruncatedSVD",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.linear_model._base.LinearClassifierMixin",
            "sklearn.base.TransformerMixin"
        ],
        "class_docstring": "Linear Discriminant Analysis\n\n    A classifier with a linear decision boundary, generated by fitting class\n    conditional densities to the data and using Bayes' rule.\n\n    The model fits a Gaussian density to each class, assuming that all classes\n    share the same covariance matrix.\n\n    The fitted model can also be used to reduce the dimensionality of the input\n    by projecting it to the most discriminative directions.\n\n    .. versionadded:: 0.17\n       *LinearDiscriminantAnalysis*.\n\n    Read more in the :ref:`User Guide <lda_qda>`.\n\n    Parameters\n    ----------\n    solver : string, optional\n        Solver to use, possible values:\n          - 'svd': Singular value decomposition (default).\n            Does not compute the covariance matrix, therefore this solver is\n            recommended for data with a large number of features.\n          - 'lsqr': Least squares solution, can be combined with shrinkage.\n          - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.\n\n    shrinkage : string or float, optional\n        Shrinkage parameter, possible values:\n          - None: no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n        Note that shrinkage works only with 'lsqr' and 'eigen' solvers.\n\n    priors : array, optional, shape (n_classes,)\n        Class priors.\n\n    n_components : int, optional (default=None)\n        Number of components (<= min(n_classes - 1, n_features)) for\n        dimensionality reduction. If None, will be set to\n        min(n_classes - 1, n_features).\n\n    store_covariance : bool, optional\n        Additionally compute class covariance matrix (default False), used\n        only in 'svd' solver.\n\n        .. versionadded:: 0.17\n\n    tol : float, optional, (default 1.0e-4)\n        Threshold used for rank estimation in SVD solver.\n\n        .. versionadded:: 0.17\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) or (n_classes, n_features)\n        Weight vector(s).\n\n    intercept_ : array, shape (n_classes,)\n        Intercept term.\n\n    covariance_ : array-like, shape (n_features, n_features)\n        Covariance matrix (shared by all classes).\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If ``n_components`` is not set then all components are stored and the\n        sum of explained variances is equal to 1.0. Only available when eigen\n        or svd solver is used.\n\n    means_ : array-like, shape (n_classes, n_features)\n        Class means.\n\n    priors_ : array-like, shape (n_classes,)\n        Class priors (sum to 1).\n\n    scalings_ : array-like, shape (rank, n_classes - 1)\n        Scaling of the features in the space spanned by the class centroids.\n\n    xbar_ : array-like, shape (n_features,)\n        Overall mean.\n\n    classes_ : array-like, shape (n_classes,)\n        Unique class labels.\n\n    See also\n    --------\n    sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic\n        Discriminant Analysis\n\n    Notes\n    -----\n    The default solver is 'svd'. It can perform both classification and\n    transform, and it does not rely on the calculation of the covariance\n    matrix. This can be an advantage in situations where the number of features\n    is large. However, the 'svd' solver cannot be used with shrinkage.\n\n    The 'lsqr' solver is an efficient algorithm that only works for\n    classification. It supports shrinkage.\n\n    The 'eigen' solver is based on the optimization of the between class\n    scatter to within class scatter ratio. It can be used for both\n    classification and transform, and it supports shrinkage. However, the\n    'eigen' solver needs to compute the covariance matrix, so it might not be\n    suitable for situations with a high number of features.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = LinearDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    LinearDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    ",
        "klass": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Quadratic Discriminant Analysis\n\n    A classifier with a quadratic decision boundary, generated\n    by fitting class conditional densities to the data\n    and using Bayes' rule.\n\n    The model fits a Gaussian density to each class.\n\n    .. versionadded:: 0.17\n       *QuadraticDiscriminantAnalysis*\n\n    Read more in the :ref:`User Guide <lda_qda>`.\n\n    Parameters\n    ----------\n    priors : array, optional, shape = [n_classes]\n        Priors on classes\n\n    reg_param : float, optional\n        Regularizes the covariance estimate as\n        ``(1-reg_param)*Sigma + reg_param*np.eye(n_features)``\n\n    store_covariance : boolean\n        If True the covariance matrices are computed and stored in the\n        `self.covariance_` attribute.\n\n        .. versionadded:: 0.17\n\n    tol : float, optional, default 1.0e-4\n        Threshold used for rank estimation.\n\n        .. versionadded:: 0.17\n\n    Attributes\n    ----------\n    covariance_ : list of array-like of shape (n_features, n_features)\n        Covariance matrices of each class.\n\n    means_ : array-like of shape (n_classes, n_features)\n        Class means.\n\n    priors_ : array-like of shape (n_classes)\n        Class priors (sum to 1).\n\n    rotations_ : list of arrays\n        For each class k an array of shape [n_features, n_k], with\n        ``n_k = min(n_features, number of elements in class k)``\n        It is the rotation of the Gaussian distribution, i.e. its\n        principal axis.\n\n    scalings_ : list of arrays\n        For each class k an array of shape [n_k]. It contains the scaling\n        of the Gaussian distributions along its principal axes, i.e. the\n        variance in the rotated coordinate system.\n\n    classes_ : array-like, shape (n_classes,)\n        Unique class labels.\n\n    Examples\n    --------\n    >>> from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = QuadraticDiscriminantAnalysis()\n    >>> clf.fit(X, y)\n    QuadraticDiscriminantAnalysis()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear\n        Discriminant Analysis\n    ",
        "klass": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "\n    DummyClassifier is a classifier that makes predictions using simple rules.\n\n    This classifier is useful as a simple baseline to compare with other\n    (real) classifiers. Do not use it for real problems.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    strategy : str, default=\"stratified\"\n        Strategy to use to generate predictions.\n\n        * \"stratified\": generates predictions by respecting the training\n          set's class distribution.\n        * \"most_frequent\": always predicts the most frequent label in the\n          training set.\n        * \"prior\": always predicts the class that maximizes the class prior\n          (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n        * \"uniform\": generates predictions uniformly at random.\n        * \"constant\": always predicts a constant label that is provided by\n          the user. This is useful for metrics that evaluate a non-majority\n          class\n\n          .. versionchanged:: 0.22\n             The default value of `strategy` will change to \"prior\" in version\n             0.24. Starting from version 0.22, a warning will be raised if\n             `strategy` is not explicity set.\n\n          .. versionadded:: 0.17\n             Dummy Classifier now supports prior fitting strategy using\n             parameter *prior*.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    constant : int or str or array-like of shape (n_outputs,)\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    Attributes\n    ----------\n    classes_ : array or list of array of shape (n_classes,)\n        Class labels for each output.\n\n    n_classes_ : array or list of array of shape (n_classes,)\n        Number of label for each output.\n\n    class_prior_ : array or list of array of shape (n_classes,)\n        Probability of each class for each output.\n\n    n_outputs_ : int,\n        Number of outputs.\n\n    sparse_output_ : bool,\n        True if the array returned from predict is to be in sparse CSC format.\n        Is automatically set to True if the input y is passed in sparse format.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.dummy import DummyClassifier\n    >>> X = np.array([-1, 1, 1, 1])\n    >>> y = np.array([0, 1, 1, 1])\n    >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n    >>> dummy_clf.fit(X, y)\n    DummyClassifier(strategy='most_frequent')\n    >>> dummy_clf.predict(X)\n    array([1, 1, 1, 1])\n    >>> dummy_clf.score(X, y)\n    0.75\n    ",
        "klass": "sklearn.dummy.DummyClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "\n    DummyRegressor is a regressor that makes predictions using\n    simple rules.\n\n    This regressor is useful as a simple baseline to compare with other\n    (real) regressors. Do not use it for real problems.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    strategy : str\n        Strategy to use to generate predictions.\n\n        * \"mean\": always predicts the mean of the training set\n        * \"median\": always predicts the median of the training set\n        * \"quantile\": always predicts a specified quantile of the training set,\n          provided with the quantile parameter.\n        * \"constant\": always predicts a constant value that is provided by\n          the user.\n\n    constant : int or float or array-like of shape (n_outputs,)\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    quantile : float in [0.0, 1.0]\n        The quantile to predict using the \"quantile\" strategy. A quantile of\n        0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\n        maximum.\n\n    Attributes\n    ----------\n    constant_ : array, shape (1, n_outputs)\n        Mean or median or quantile of the training targets or constant value\n        given by the user.\n\n    n_outputs_ : int,\n        Number of outputs.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.dummy import DummyRegressor\n    >>> X = np.array([1.0, 2.0, 3.0, 4.0])\n    >>> y = np.array([2.0, 3.0, 5.0, 10.0])\n    >>> dummy_regr = DummyRegressor(strategy=\"mean\")\n    >>> dummy_regr.fit(X, y)\n    DummyRegressor()\n    >>> dummy_regr.predict(X)\n    array([5., 5., 5., 5.])\n    >>> dummy_regr.score(X, y)\n    0.0\n    ",
        "klass": "sklearn.dummy.DummyRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.ensemble._weight_boosting.BaseWeightBoosting"
        ],
        "class_docstring": "An AdaBoost classifier.\n\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost-SAMME [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    base_estimator : object, optional (default=None)\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is ``DecisionTreeClassifier(max_depth=1)``.\n\n    n_estimators : int, optional (default=50)\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, optional (default=1.)\n        Learning rate shrinks the contribution of each classifier by\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n        ``n_estimators``.\n\n    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``base_estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : array of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : array of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances if supported by the ``base_estimator``.\n\n    See Also\n    --------\n    AdaBoostRegressor\n        An AdaBoost regressor that begins by fitting a regressor on the\n        original dataset and then fits additional copies of the regressor\n        on the same dataset but where the weights of instances are\n        adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier\n        GB builds an additive model in a forward stage-wise fashion. Regression\n        trees are fit on the negative gradient of the binomial or multinomial\n        deviance loss function. Binary classification is a special case where\n        only a single regression tree is induced.\n\n    sklearn.tree.DecisionTreeClassifier\n        A non-parametric supervised learning method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.feature_importances_\n    array([0.28..., 0.42..., 0.14..., 0.16...])\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.983...\n    ",
        "klass": "sklearn.ensemble.AdaBoostClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.ensemble._weight_boosting.BaseWeightBoosting"
        ],
        "class_docstring": "An AdaBoost regressor.\n\n    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n    regressor on the original dataset and then fits additional copies of the\n    regressor on the same dataset but where the weights of instances are\n    adjusted according to the error of the current prediction. As such,\n    subsequent regressors focus more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost.R2 [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    base_estimator : object, optional (default=None)\n        The base estimator from which the boosted ensemble is built.\n        If ``None``, then the base estimator is\n        ``DecisionTreeRegressor(max_depth=3)``.\n\n    n_estimators : integer, optional (default=50)\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n\n    learning_rate : float, optional (default=1.)\n        Learning rate shrinks the contribution of each regressor by\n        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n        ``n_estimators``.\n\n    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n        The loss function to use when updating the weights after each\n        boosting iteration.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    estimator_weights_ : array of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : array of floats\n        Regression error for each estimator in the boosted ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances if supported by the ``base_estimator``.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=4, n_informative=2,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n    >>> regr.fit(X, y)\n    AdaBoostRegressor(n_estimators=100, random_state=0)\n    >>> regr.feature_importances_\n    array([0.2788..., 0.7109..., 0.0065..., 0.0036...])\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([4.7972...])\n    >>> regr.score(X, y)\n    0.9771...\n\n    See also\n    --------\n    AdaBoostClassifier, GradientBoostingRegressor,\n    sklearn.tree.DecisionTreeRegressor\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n\n    ",
        "klass": "sklearn.ensemble.AdaBoostRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.ensemble._bagging.BaseBagging"
        ],
        "class_docstring": "A Bagging classifier.\n\n    A Bagging classifier is an ensemble meta-estimator that fits base\n    classifiers each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    base_estimator : object or None, optional (default=None)\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a decision tree.\n\n    n_estimators : int, optional (default=10)\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, optional (default=1.0)\n        The number of samples to draw from X to train each base estimator.\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, optional (default=1.0)\n        The number of features to draw from X to train each base estimator.\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : boolean, optional (default=True)\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : boolean, optional (default=False)\n        Whether features are drawn with replacement.\n\n    oob_score : bool, optional (default=False)\n        Whether to use out-of-bag samples to estimate\n        the generalization error.\n\n    warm_start : bool, optional (default=False)\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* constructor parameter.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    n_features_ : int\n        The number of features when :meth:`fit` is performed.\n\n    estimators_ : list of estimators\n        The collection of fitted base estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int or list\n        The number of classes.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : array of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.ensemble import BaggingClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=100, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = BaggingClassifier(base_estimator=SVC(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n    ",
        "klass": "sklearn.ensemble.BaggingClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.ensemble._bagging.BaseBagging"
        ],
        "class_docstring": "A Bagging regressor.\n\n    A Bagging regressor is an ensemble meta-estimator that fits base\n    regressors each on random subsets of the original dataset and then\n    aggregate their individual predictions (either by voting or by averaging)\n    to form a final prediction. Such a meta-estimator can typically be used as\n    a way to reduce the variance of a black-box estimator (e.g., a decision\n    tree), by introducing randomization into its construction procedure and\n    then making an ensemble out of it.\n\n    This algorithm encompasses several works from the literature. When random\n    subsets of the dataset are drawn as random subsets of the samples, then\n    this algorithm is known as Pasting [1]_. If samples are drawn with\n    replacement, then the method is known as Bagging [2]_. When random subsets\n    of the dataset are drawn as random subsets of the features, then the method\n    is known as Random Subspaces [3]_. Finally, when base estimators are built\n    on subsets of both samples and features, then the method is known as\n    Random Patches [4]_.\n\n    Read more in the :ref:`User Guide <bagging>`.\n\n    .. versionadded:: 0.15\n\n    Parameters\n    ----------\n    base_estimator : object or None, optional (default=None)\n        The base estimator to fit on random subsets of the dataset.\n        If None, then the base estimator is a decision tree.\n\n    n_estimators : int, optional (default=10)\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, optional (default=1.0)\n        The number of samples to draw from X to train each base estimator.\n\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples.\n\n    max_features : int or float, optional (default=1.0)\n        The number of features to draw from X to train each base estimator.\n\n        - If int, then draw `max_features` features.\n        - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : boolean, optional (default=True)\n        Whether samples are drawn with replacement. If False, sampling\n        without replacement is performed.\n\n    bootstrap_features : boolean, optional (default=False)\n        Whether features are drawn with replacement.\n\n    oob_score : bool\n        Whether to use out-of-bag samples to estimate\n        the generalization error.\n\n    warm_start : bool, optional (default=False)\n        When set to True, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit\n        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    Attributes\n    ----------\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n    n_features_ : int\n        The number of features when :meth:`fit` is performed.\n\n    estimators_ : list of estimators\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator. Each subset is defined by an array of the indices selected.\n\n    estimators_features_ : list of arrays\n        The subset of drawn features for each base estimator.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_prediction_` might contain NaN. This attribute exists only\n        when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVR\n    >>> from sklearn.ensemble import BaggingRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=100, n_features=4,\n    ...                        n_informative=2, n_targets=1,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = BaggingRegressor(base_estimator=SVR(),\n    ...                         n_estimators=10, random_state=0).fit(X, y)\n    >>> regr.predict([[0, 0, 0, 0]])\n    array([-2.8720...])\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Pasting small votes for classification in large\n           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n           1996.\n\n    .. [3] T. Ho, \"The random subspace method for constructing decision\n           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n           1998.\n\n    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n    ",
        "klass": "sklearn.ensemble.BaggingRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble._forest.ForestClassifier"
        ],
        "class_docstring": "\n    An extra-trees classifier.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : integer, optional (default=10)\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    bootstrap : boolean, optional (default=False)\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, optional (default=False)\n        Whether to use out-of-bag samples to estimate\n        the generalization accuracy.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : ExtraTreeClassifier\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : array of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : array of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import ExtraTreesClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    ExtraTreesClassifier(random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n\n    References\n    ----------\n\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n           trees\", Machine Learning, 63(1), 3-42, 2006.\n\n    See Also\n    --------\n    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n    RandomForestClassifier : Ensemble Classifier based on trees with optimal\n        splits.\n    ",
        "klass": "sklearn.ensemble.ExtraTreesClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble._forest.ForestRegressor"
        ],
        "class_docstring": "\n    An extra-trees regressor.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : integer, optional (default=10)\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : string, optional (default=\"mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion, and \"mae\" for the mean\n        absolute error.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    bootstrap : boolean, optional (default=False)\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool, optional (default=False)\n        Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : ExtraTreeRegressor\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    n_features_ : int\n        The number of features.\n\n    n_outputs_ : int\n        The number of outputs.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    References\n    ----------\n\n    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n           Machine Learning, 63(1), 3-42, 2006.\n\n    See Also\n    --------\n    sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\n    RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n    ",
        "klass": "sklearn.ensemble.ExtraTreesRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.ensemble._gb.BaseGradientBoosting"
        ],
        "class_docstring": "Gradient Boosting for classification.\n\n    GB builds an additive model in a\n    forward stage-wise fashion; it allows for the optimization of\n    arbitrary differentiable loss functions. In each stage ``n_classes_``\n    regression trees are fit on the negative gradient of the\n    binomial or multinomial deviance loss function. Binary classification\n    is a special case where only a single regression tree is induced.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'deviance', 'exponential'}, optional (default='deviance')\n        loss function to be optimized. 'deviance' refers to\n        deviance (= logistic regression) for classification\n        with probabilistic outputs. For loss 'exponential' gradient\n        boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, optional (default=0.1)\n        learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int (default=100)\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, optional (default=1.0)\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : string, optional (default=\"friedman_mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"friedman_mse\" for the mean squared error with improvement\n        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n        the mean absolute error. The default value of \"friedman_mse\" is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : integer, optional (default=3)\n        maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', optional (default=None)\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default: 0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default: False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    presort : deprecated, default='deprecated'\n        This parameter is deprecated and will be removed in v0.24.\n\n        .. deprecated :: 0.22\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n\n        .. versionadded:: 0.20\n\n    tol : float, optional, default 1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : array, shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    oob_improvement_ : array, shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``\n\n    train_score_ : array, shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, ``loss_.K``)\n        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n        classification, otherwise n_classes.\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    sklearn.ensemble.HistGradientBoostingClassifier,\n    sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n    AdaBoostClassifier\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n    ",
        "klass": "sklearn.ensemble.GradientBoostingClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.ensemble._gb.BaseGradientBoosting"
        ],
        "class_docstring": "Gradient Boosting for regression.\n\n    GB builds an additive model in a forward stage-wise fashion;\n    it allows for the optimization of arbitrary differentiable loss functions.\n    In each stage a regression tree is fit on the negative gradient of the\n    given loss function.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n        loss function to be optimized. 'ls' refers to least squares\n        regression. 'lad' (least absolute deviation) is a highly robust\n        loss function solely based on order information of the input\n        variables. 'huber' is a combination of the two. 'quantile'\n        allows quantile regression (use `alpha` to specify the quantile).\n\n    learning_rate : float, optional (default=0.1)\n        learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int (default=100)\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, optional (default=1.0)\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : string, optional (default=\"friedman_mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"friedman_mse\" for the mean squared error with improvement\n        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n        the mean absolute error. The default value of \"friedman_mse\" is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : integer, optional (default=3)\n        maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', optional (default=None)\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n        initial raw predictions are set to zero. By default a\n        ``DummyEstimator`` is used, predicting either the average target value\n        (for loss='ls'), or a quantile for the other losses.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    alpha : float (default=0.9)\n        The alpha-quantile of the huber loss function and the quantile\n        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n\n    verbose : int, default: 0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default: False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    presort : deprecated, default='deprecated'\n        This parameter is deprecated and will be removed in v0.24.\n\n        .. deprecated :: 0.22\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations.\n\n        .. versionadded:: 0.20\n\n    tol : float, optional, default 1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    feature_importances_ : array, shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    oob_improvement_ : array, shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``\n\n    train_score_ : array, shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n        The collection of fitted sub-estimators.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    sklearn.ensemble.HistGradientBoostingRegressor,\n    sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n    ",
        "klass": "sklearn.ensemble.GradientBoostingRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.OutlierMixin",
            "sklearn.ensemble._bagging.BaseBagging"
        ],
        "class_docstring": "\n    Isolation Forest Algorithm.\n\n    Return the anomaly score of each sample using the IsolationForest algorithm\n\n    The IsolationForest 'isolates' observations by randomly selecting a feature\n    and then randomly selecting a split value between the maximum and minimum\n    values of the selected feature.\n\n    Since recursive partitioning can be represented by a tree structure, the\n    number of splittings required to isolate a sample is equivalent to the path\n    length from the root node to the terminating node.\n\n    This path length, averaged over a forest of such random trees, is a\n    measure of normality and our decision function.\n\n    Random partitioning produces noticeably shorter paths for anomalies.\n    Hence, when a forest of random trees collectively produce shorter path\n    lengths for particular samples, they are highly likely to be anomalies.\n\n    Read more in the :ref:`User Guide <isolation_forest>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    n_estimators : int, optional (default=100)\n        The number of base estimators in the ensemble.\n\n    max_samples : int or float, optional (default=\"auto\")\n        The number of samples to draw from X to train each base estimator.\n            - If int, then draw `max_samples` samples.\n            - If float, then draw `max_samples * X.shape[0]` samples.\n            - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n        If max_samples is larger than the number of samples provided,\n        all samples will be used for all trees (no sampling).\n\n    contamination : 'auto' or float, optional (default='auto')\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Used when fitting to define the threshold\n        on the scores of the samples.\n\n            - If 'auto', the threshold is determined as in the\n              original paper.\n            - If float, the contamination should be in the range [0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    max_features : int or float, optional (default=1.0)\n        The number of features to draw from X to train each base estimator.\n\n            - If int, then draw `max_features` features.\n            - If float, then draw `max_features * X.shape[1]` features.\n\n    bootstrap : bool, optional (default=False)\n        If True, individual trees are fit on random subsets of the training\n        data sampled with replacement. If False, sampling without replacement\n        is performed.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    behaviour : str, default='deprecated'\n        This parameter has not effect, is deprecated, and will be removed.\n\n        .. versionadded:: 0.20\n           ``behaviour`` is added in 0.20 for back-compatibility purpose.\n\n        .. deprecated:: 0.20\n           ``behaviour='old'`` is deprecated in 0.20 and will not be possible\n           in 0.22.\n\n        .. deprecated:: 0.22\n           ``behaviour`` parameter is deprecated in 0.22 and removed in\n           0.24.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : integer\n        The actual number of samples\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores. We\n        have the relation: ``decision_function = score_samples - offset_``.\n        ``offset_`` is defined as follows. When the contamination parameter is\n        set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n        close to 0 and the scores of outliers are close to -1. When a\n        contamination parameter different than \"auto\" is provided, the offset\n        is defined in such a way we obtain the expected number of outliers\n        (samples with decision function < 0) in training.\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    See Also\n    ----------\n    sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n        Gaussian distributed dataset.\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n        Estimate the support of a high-dimensional distribution.\n        The implementation is based on libsvm.\n    sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n        using Local Outlier Factor (LOF).\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import IsolationForest\n    >>> X = [[-1.1], [0.3], [0.5], [100]]\n    >>> clf = IsolationForest(random_state=0).fit(X)\n    >>> clf.predict([[0.1], [0], [90]])\n    array([ 1,  1, -1])\n    ",
        "klass": "sklearn.ensemble.IsolationForest",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble._forest.ForestClassifier"
        ],
        "class_docstring": "\n    A random forest classifier.\n\n    A random forest is a meta estimator that fits a number of decision tree\n    classifiers on various sub-samples of the dataset and uses averaging to\n    improve the predictive accuracy and control over-fitting.\n    The sub-sample size is always the same as the original\n    input sample size but the samples are drawn with replacement if\n    `bootstrap=True` (default).\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : integer, optional (default=100)\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n        Note: this parameter is tree-specific.\n\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n\n    bootstrap : boolean, optional (default=True)\n        Whether bootstrap samples are used when building trees. If False, the\n        whole datset is used to build each tree.\n\n    oob_score : bool (default=False)\n        Whether to use out-of-bag samples to estimate\n        the generalization accuracy.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : DecisionTreeClassifier\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : array of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_decision_function_ : array of shape (n_samples, n_classes)\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN. This attribute exists\n        only when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n    >>> clf.fit(X, y)\n    RandomForestClassifier(max_depth=2, random_state=0)\n    >>> print(clf.feature_importances_)\n    [0.14205973 0.76664038 0.0282433  0.06305659]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    See Also\n    --------\n    DecisionTreeClassifier, ExtraTreesClassifier\n    ",
        "klass": "sklearn.ensemble.RandomForestClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble._forest.ForestRegressor"
        ],
        "class_docstring": "\n    A random forest regressor.\n\n    A random forest is a meta estimator that fits a number of classifying\n    decision trees on various sub-samples of the dataset and uses averaging\n    to improve the predictive accuracy and control over-fitting.\n    The sub-sample size is always the same as the original\n    input sample size but the samples are drawn with replacement if\n    `bootstrap=True` (default).\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : integer, optional (default=10)\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : string, optional (default=\"mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion, and \"mae\" for the mean\n        absolute error.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    bootstrap : boolean, optional (default=True)\n        Whether bootstrap samples are used when building trees. If False, the\n        whole datset is used to build each tree.\n\n    oob_score : bool, optional (default=False)\n        whether to use out-of-bag samples to estimate\n        the R^2 on unseen data.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Controls both the randomness of the bootstrapping of the samples used\n        when building trees (if ``bootstrap=True``) and the sampling of the\n        features to consider when looking for the best split at each node\n        (if ``max_features < n_features``).\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    max_samples : int or float, default=None\n        If bootstrap is True, the number of samples to draw from X\n        to train each base estimator.\n\n        - If None (default), then draw `X.shape[0]` samples.\n        - If int, then draw `max_samples` samples.\n        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n          `max_samples` should be in the interval `(0, 1)`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    base_estimator_ : DecisionTreeRegressor\n        The child estimator template used to create the collection of fitted\n        sub-estimators.\n\n    estimators_ : list of DecisionTreeRegressor\n        The collection of fitted sub-estimators.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n        This attribute exists only when ``oob_score`` is True.\n\n    oob_prediction_ : ndarray of shape (n_samples,)\n        Prediction computed with out-of-bag estimate on the training set.\n        This attribute exists only when ``oob_score`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=4, n_informative=2,\n    ...                        random_state=0, shuffle=False)\n    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n    >>> regr.fit(X, y)\n    RandomForestRegressor(max_depth=2, random_state=0)\n    >>> print(regr.feature_importances_)\n    [0.18146984 0.81473937 0.00145312 0.00233767]\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-8.32987858]\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    The default value ``max_features=\"auto\"`` uses ``n_features``\n    rather than ``n_features / 3``. The latter was originally suggested in\n    [1], whereas the former was more recently justified empirically in [2].\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n           trees\", Machine Learning, 63(1), 3-42, 2006.\n\n    See Also\n    --------\n    DecisionTreeRegressor, ExtraTreesRegressor\n    ",
        "klass": "sklearn.ensemble.RandomForestRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.ensemble._voting._BaseVoting"
        ],
        "class_docstring": "Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'``\n        using ``set_params``.\n\n        .. deprecated:: 0.22\n           Using ``None`` to drop an estimator is deprecated in 0.22 and\n           support will be dropped in 0.24. Use the string ``'drop'`` instead.\n\n    voting : str, {'hard', 'soft'} (default='hard')\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like, shape (n_classifiers,), optional (default=`None`)\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    flatten_transform : bool, optional (default=True)\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classifiers * n_classes). If\n        flatten_transform=False, it returns\n        (n_classifiers, n_samples, n_classes).\n\n    Attributes\n    ----------\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : Bunch object, a dictionary with attribute access\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    classes_ : array-like, shape (n_predictions,)\n        The classes labels.\n\n    See Also\n    --------\n    VotingRegressor: Prediction voting regressor.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n    >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> eclf1 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    >>> eclf1 = eclf1.fit(X, y)\n    >>> print(eclf1.predict(X))\n    [1 1 1 2 2 2]\n    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n    ...                eclf1.named_estimators_['lr'].predict(X))\n    True\n    >>> eclf2 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...         voting='soft')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> print(eclf2.predict(X))\n    [1 1 1 2 2 2]\n    >>> eclf3 = VotingClassifier(estimators=[\n    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...        voting='soft', weights=[2,1,1],\n    ...        flatten_transform=True)\n    >>> eclf3 = eclf3.fit(X, y)\n    >>> print(eclf3.predict(X))\n    [1 1 1 2 2 2]\n    >>> print(eclf3.transform(X).shape)\n    (6, 6)\n    ",
        "klass": "sklearn.ensemble.VotingClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the ``register_parallel_backend`` function.\n\n    By default the following backends are available:\n\n    - 'loky': single-host, process-based parallelism (used by default),\n    - 'threading': single-host, thread-based parallelism,\n    - 'multiprocessing': legacy single-host, process-based parallelism.\n\n    'loky' is recommended to run functions that manipulate Python objects.\n    'threading' is a low-overhead alternative that is most efficient for\n    functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n    CPU-bound code in a few calls to native code that explicitly releases the\n    GIL.\n\n    In addition, if the `dask` and `distributed` Python packages are installed,\n    it is possible to use the 'dask' backend for better scheduling of nested\n    parallel calls without over-subscription and potentially distribute\n    parallel calls over a networked cluster of several hosts.\n\n    Alternatively the backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the ``Parallel`` class constructor. It is particularly useful when calling\n    into library code that uses joblib internally but does not expose the\n    backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    Joblib also tries to limit the oversubscription by limiting the number of\n    threads usable in some third-party library threadpools like OpenBLAS, MKL\n    or OpenMP. The default limit in each worker is set to\n    ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n    overwritten with the ``inner_max_num_threads`` argument which will be used\n    to set this limit in the child processes.\n\n    .. versionadded:: 0.10\n\n    ",
        "klass": "sklearn.externals.joblib.parallel_backend",
        "module": "joblib"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Transforms lists of feature-value mappings to vectors.\n\n    This transformer turns lists of mappings (dict-like objects) of feature\n    names to feature values into Numpy arrays or scipy.sparse matrices for use\n    with scikit-learn estimators.\n\n    When feature values are strings, this transformer will do a binary one-hot\n    (aka one-of-K) coding: one boolean-valued feature is constructed for each\n    of the possible string values that the feature can take on. For instance,\n    a feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\n    features in the output, one signifying \"f=ham\", the other \"f=spam\".\n\n    However, note that this transformer will only do a binary one-hot encoding\n    when feature values are of type string. If categorical features are\n    represented as numeric values such as int, the DictVectorizer can be\n    followed by :class:`sklearn.preprocessing.OneHotEncoder` to complete\n    binary one-hot encoding.\n\n    Features that do not occur in a sample (mapping) will have a zero value\n    in the resulting array/matrix.\n\n    Read more in the :ref:`User Guide <dict_feature_extraction>`.\n\n    Parameters\n    ----------\n    dtype : callable, optional\n        The type of feature values. Passed to Numpy array/scipy.sparse matrix\n        constructors as the dtype argument.\n    separator : string, optional\n        Separator string used when constructing new features for one-hot\n        coding.\n    sparse : boolean, optional.\n        Whether transform should produce scipy.sparse matrices.\n        True by default.\n    sort : boolean, optional.\n        Whether ``feature_names_`` and ``vocabulary_`` should be\n        sorted when fitting. True by default.\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A dictionary mapping feature names to feature indices.\n\n    feature_names_ : list\n        A list of length n_features containing the feature names (e.g., \"f=ham\"\n        and \"f=spam\").\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import DictVectorizer\n    >>> v = DictVectorizer(sparse=False)\n    >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n    >>> X = v.fit_transform(D)\n    >>> X\n    array([[2., 0., 1.],\n           [0., 1., 3.]])\n    >>> v.inverse_transform(X) ==         [{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]\n    True\n    >>> v.transform({'foo': 4, 'unseen_feature': 3})\n    array([[0., 0., 4.]])\n\n    See also\n    --------\n    FeatureHasher : performs vectorization using only a hash function.\n    sklearn.preprocessing.OrdinalEncoder : handles nominal/categorical\n      features encoded as columns of arbitrary data types.\n    ",
        "klass": "sklearn.feature_extraction.DictVectorizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Implements feature hashing, aka the hashing trick.\n\n    This class turns sequences of symbolic feature names (strings) into\n    scipy.sparse matrices, using a hash function to compute the matrix column\n    corresponding to a name. The hash function employed is the signed 32-bit\n    version of Murmurhash3.\n\n    Feature names of type byte string are used as-is. Unicode strings are\n    converted to UTF-8 first, but no Unicode normalization is done.\n    Feature values must be (finite) numbers.\n\n    This class is a low-memory alternative to DictVectorizer and\n    CountVectorizer, intended for large-scale (online) learning and situations\n    where memory is tight, e.g. when running prediction code on embedded\n    devices.\n\n    Read more in the :ref:`User Guide <feature_hashing>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_features : integer, optional\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n    input_type : string, optional, default \"dict\"\n        Either \"dict\" (the default) to accept dictionaries over\n        (feature_name, value); \"pair\" to accept pairs of (feature_name, value);\n        or \"string\" to accept single strings.\n        feature_name should be a string, while value should be a number.\n        In the case of \"string\", a value of 1 is implied.\n        The feature_name is hashed to find the appropriate column for the\n        feature. The value's sign might be flipped in the output (but see\n        non_negative, below).\n    dtype : numpy type, optional, default np.float64\n        The type of feature values. Passed to scipy.sparse matrix constructors\n        as the dtype argument. Do not set this to bool, np.boolean or any\n        unsigned integer type.\n    alternate_sign : boolean, optional, default True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction import FeatureHasher\n    >>> h = FeatureHasher(n_features=10)\n    >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n    >>> f = h.transform(D)\n    >>> f.toarray()\n    array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n           [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\n\n    See also\n    --------\n    DictVectorizer : vectorizes string-valued features using a hash table.\n    sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features.\n    ",
        "klass": "sklearn.feature_extraction.FeatureHasher",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Extracts patches from a collection of images\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    Parameters\n    ----------\n    patch_size : tuple of ints (patch_height, patch_width)\n        the dimensions of one patch\n\n    max_patches : integer or float, optional default is None\n        The maximum number of patches per image to extract. If max_patches is a\n        float in (0, 1), it is taken to mean a proportion of the total number\n        of patches.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Determines the random number generator used for random sampling when\n        `max_patches` is not None. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_sample_images\n    >>> from sklearn.feature_extraction import image\n    >>> # Use the array data from the second image in this dataset:\n    >>> X = load_sample_images().images[1]\n    >>> print('Image shape: {}'.format(X.shape))\n    Image shape: (427, 640, 3)\n    >>> pe = image.PatchExtractor(patch_size=(2, 2))\n    >>> pe_fit = pe.fit(X)\n    >>> pe_trans = pe.transform(X)\n    >>> print('Patches shape: {}'.format(pe_trans.shape))\n    Patches shape: (545706, 2, 2)\n    ",
        "klass": "sklearn.feature_extraction.image.PatchExtractor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.feature_extraction.text._VectorizerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Convert a collection of text documents to a matrix of token occurrences\n\n    It turns a collection of text documents into a scipy.sparse matrix holding\n    token occurrence counts (or binary occurrence information), possibly\n    normalized as token frequencies if norm='l1' or projected on the euclidean\n    unit sphere if norm='l2'.\n\n    This text vectorizer implementation uses the hashing trick to find the\n    token string name to feature integer index mapping.\n\n    This strategy has several advantages:\n\n    - it is very low memory scalable to large datasets as there is no need to\n      store a vocabulary dictionary in memory\n\n    - it is fast to pickle and un-pickle as it holds no state besides the\n      constructor parameters\n\n    - it can be used in a streaming (partial fit) or parallel pipeline as there\n      is no state computed during fit.\n\n    There are also a couple of cons (vs using a CountVectorizer with an\n    in-memory vocabulary):\n\n    - there is no way to compute the inverse transform (from feature indices to\n      string feature names) which can be a problem when trying to introspect\n      which features are most important to a model.\n\n    - there can be collisions: distinct tokens can be mapped to the same\n      feature index. However in practice this is rarely an issue if n_features\n      is large enough (e.g. 2 ** 18 for text classification problems).\n\n    - no IDF weighting as this would render the transformer stateful.\n\n    The hash function employed is the signed 32-bit version of Murmurhash3.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n\n    input : string {'filename', 'file', 'content'}\n        If 'filename', the sequence passed as an argument to fit is\n        expected to be a list of filenames that need reading to fetch\n        the raw content to analyze.\n\n        If 'file', the sequence items must have a 'read' method (file-like\n        object) that is called to fetch the bytes in memory.\n\n        Otherwise the input is expected to be a sequence of items that\n        can be of type string or byte.\n\n    encoding : string, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode', None}\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : boolean, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable or None (default)\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable or None (default)\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    stop_words : string {'english'}, list, or None (default)\n        If 'english', a built-in stop word list for English is used.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n    token_pattern : string\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    analyzer : string, {'word', 'char', 'char_wb'} or callable\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n\n        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n        first read from the file and then passed to the given callable\n        analyzer.\n\n    n_features : integer, default=(2 ** 20)\n        The number of features (columns) in the output matrices. Small numbers\n        of features are likely to cause hash collisions, but large numbers\n        will cause larger coefficient dimensions in linear learners.\n\n    binary : boolean, default=False.\n        If True, all non zero counts are set to 1. This is useful for discrete\n        probabilistic models that model binary events rather than integer\n        counts.\n\n    norm : 'l1', 'l2' or None, optional\n        Norm used to normalize term vectors. None for no normalization.\n\n    alternate_sign : boolean, optional, default True\n        When True, an alternating sign is added to the features as to\n        approximately conserve the inner product in the hashed space even for\n        small n_features. This approach is similar to sparse random projection.\n\n        .. versionadded:: 0.19\n\n    dtype : type, optional\n        Type of the matrix returned by fit_transform() or transform().\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import HashingVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = HashingVectorizer(n_features=2**4)\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(X.shape)\n    (4, 16)\n\n    See Also\n    --------\n    CountVectorizer, TfidfVectorizer\n\n    ",
        "klass": "sklearn.feature_extraction.text.HashingVectorizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Transform a count matrix to a normalized tf or tf-idf representation\n\n    Tf means term-frequency while tf-idf means term-frequency times inverse\n    document-frequency. This is a common term weighting scheme in information\n    retrieval, that has also found good use in document classification.\n\n    The goal of using tf-idf instead of the raw frequencies of occurrence of a\n    token in a given document is to scale down the impact of tokens that occur\n    very frequently in a given corpus and that are hence empirically less\n    informative than features that occur in a small fraction of the training\n    corpus.\n\n    The formula that is used to compute the tf-idf for a term t of a document d\n    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n    n is the total number of documents in the document set and df(t) is the\n    document frequency of t; the document frequency is the number of documents\n    in the document set that contain the term t. The effect of adding \"1\" to\n    the idf in the equation above is that terms with zero idf, i.e., terms\n    that occur in all documents in a training set, will not be entirely\n    ignored.\n    (Note that the idf formula above differs from the standard textbook\n    notation that defines the idf as\n    idf(t) = log [ n / (df(t) + 1) ]).\n\n    If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n    numerator and denominator of the idf as if an extra document was seen\n    containing every term in the collection exactly once, which prevents\n    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n\n    Furthermore, the formulas used to compute tf and idf depend\n    on parameter settings that correspond to the SMART notation used in IR\n    as follows:\n\n    Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n    ``sublinear_tf=True``.\n    Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n    Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n    when ``norm=None``.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    norm : 'l1', 'l2' or None, optional (default='l2')\n        Each output row will have unit norm, either:\n        * 'l2': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * 'l1': Sum of absolute values of vector elements is 1.\n        See :func:`preprocessing.normalize`\n\n    use_idf : boolean (default=True)\n        Enable inverse-document-frequency reweighting.\n\n    smooth_idf : boolean (default=True)\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : boolean (default=False)\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    idf_ : array, shape (n_features)\n        The inverse document frequency (IDF) vector; only defined\n        if  ``use_idf`` is True.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import TfidfTransformer\n    >>> from sklearn.feature_extraction.text import CountVectorizer\n    >>> from sklearn.pipeline import Pipeline\n    >>> import numpy as np\n    >>> corpus = ['this is the first document',\n    ...           'this document is the second document',\n    ...           'and this is the third one',\n    ...           'is this the first document']\n    >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n    ...               'and', 'one']\n    >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n    ...                  ('tfid', TfidfTransformer())]).fit(corpus)\n    >>> pipe['count'].transform(corpus).toarray()\n    array([[1, 1, 1, 1, 0, 1, 0, 0],\n           [1, 2, 0, 1, 1, 1, 0, 0],\n           [1, 0, 0, 1, 0, 1, 1, 1],\n           [1, 1, 1, 1, 0, 1, 0, 0]])\n    >>> pipe['tfid'].idf_\n    array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n           1.        , 1.91629073, 1.91629073])\n    >>> pipe.transform(corpus).shape\n    (4, 8)\n\n    References\n    ----------\n\n    .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n                   Information Retrieval. Addison Wesley, pp. 68-74.\n\n    .. [MRS2008] C.D. Manning, P. Raghavan and H. Sch\u00fctze  (2008).\n                   Introduction to Information Retrieval. Cambridge University\n                   Press, pp. 118-120.\n    ",
        "klass": "sklearn.feature_extraction.text.TfidfTransformer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_extraction.text.CountVectorizer"
        ],
        "class_docstring": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\n    Equivalent to :class:`CountVectorizer` followed by\n    :class:`TfidfTransformer`.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : str {'filename', 'file', 'content'}\n        If 'filename', the sequence passed as an argument to fit is\n        expected to be a list of filenames that need reading to fetch\n        the raw content to analyze.\n\n        If 'file', the sequence items must have a 'read' method (file-like\n        object) that is called to fetch the bytes in memory.\n\n        Otherwise the input is expected to be a sequence of items that\n        can be of type string or byte.\n\n    encoding : str, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode', None} (default=None)\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool (default=True)\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable or None (default=None)\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer is not callable``.\n\n    tokenizer : callable or None (default=None)\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    analyzer : str, {'word', 'char', 'char_wb'} or callable\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n\n        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n        first read from the file and then passed to the given callable\n        analyzer.\n\n    stop_words : str {'english'}, list, or None (default=None)\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. 'english' is currently the only supported string\n        value.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    token_pattern : str\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer is not callable``.\n\n    max_df : float in range [0.0, 1.0] or int (default=1.0)\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float in range [0.0, 1.0] or int (default=1)\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int or None (default=None)\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, optional (default=None)\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents.\n\n    binary : bool (default=False)\n        If True, all non-zero term counts are set to 1. This does not mean\n        outputs will have only 0/1 values, only that the tf term in tf-idf\n        is binary. (Set idf and normalization to False to get 0/1 outputs).\n\n    dtype : type, optional (default=float64)\n        Type of the matrix returned by fit_transform() or transform().\n\n    norm : 'l1', 'l2' or None, optional (default='l2')\n        Each output row will have unit norm, either:\n        * 'l2': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * 'l1': Sum of absolute values of vector elements is 1.\n        See :func:`preprocessing.normalize`.\n\n    use_idf : bool (default=True)\n        Enable inverse-document-frequency reweighting.\n\n    smooth_idf : bool (default=True)\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : bool (default=False)\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    fixed_vocabulary_: bool\n        True if a fixed vocabulary of term to indices mapping\n        is provided by the user\n\n    idf_ : array, shape (n_features)\n        The inverse document frequency (IDF) vector; only defined\n        if ``use_idf`` is True.\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    See Also\n    --------\n    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\n    TfidfTransformer : Performs the TF-IDF transformation from a provided\n        matrix of counts.\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = TfidfVectorizer()\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> print(vectorizer.get_feature_names())\n    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n    >>> print(X.shape)\n    (4, 9)\n    ",
        "klass": "sklearn.feature_extraction.text.TfidfVectorizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._base.SelectorMixin",
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Feature ranking with recursive feature elimination.\n\n    Given an external estimator that assigns weights to features (e.g., the\n    coefficients of a linear model), the goal of recursive feature elimination\n    (RFE) is to select features by recursively considering smaller and smaller\n    sets of features. First, the estimator is trained on the initial set of\n    features and the importance of each feature is obtained either through a\n    ``coef_`` attribute or through a ``feature_importances_`` attribute.\n    Then, the least important features are pruned from current set of features.\n    That procedure is recursively repeated on the pruned set until the desired\n    number of features to select is eventually reached.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : object\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance either through a ``coef_``\n        attribute or through a ``feature_importances_`` attribute.\n\n    n_features_to_select : int or None (default=None)\n        The number of features to select. If `None`, half of the features\n        are selected.\n\n    step : int or float, optional (default=1)\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n\n    verbose : int, (default=0)\n        Controls verbosity of output.\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of selected features.\n\n    support_ : array of shape [n_features]\n        The mask of selected features.\n\n    ranking_ : array of shape [n_features]\n        The feature ranking, such that ``ranking_[i]`` corresponds to the\n        ranking position of the i-th feature. Selected (i.e., estimated\n        best) features are assigned rank 1.\n\n    estimator_ : object\n        The external estimator fit on the reduced dataset.\n\n    Examples\n    --------\n    The following example shows how to retrieve the 5 most informative\n    features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFE\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFE(estimator, 5, step=1)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    See also\n    --------\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    ",
        "klass": "sklearn.feature_selection.RFE",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._rfe.RFE"
        ],
        "class_docstring": "Feature ranking with recursive feature elimination and cross-validated\n    selection of the best number of features.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : object\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance either through a ``coef_``\n        attribute or through a ``feature_importances_`` attribute.\n\n    step : int or float, optional (default=1)\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n        Note that the last iteration may remove fewer than ``step`` features in\n        order to reach ``min_features_to_select``.\n\n    min_features_to_select : int, (default=1)\n        The minimum number of features to be selected. This number of features\n        will always be scored, even if the difference between the original\n        feature count and ``min_features_to_select`` isn't divisible by\n        ``step``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used. If the\n        estimator is a classifier or if ``y`` is neither binary nor multiclass,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value of None changed from 3-fold to 5-fold.\n\n    scoring : string, callable or None, optional, (default=None)\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    verbose : int, (default=0)\n        Controls verbosity of output.\n\n    n_jobs : int or None, optional (default=None)\n        Number of cores to run in parallel while fitting across folds.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of selected features with cross-validation.\n\n    support_ : array of shape [n_features]\n        The mask of selected features.\n\n    ranking_ : array of shape [n_features]\n        The feature ranking, such that `ranking_[i]`\n        corresponds to the ranking\n        position of the i-th feature.\n        Selected (i.e., estimated best)\n        features are assigned rank 1.\n\n    grid_scores_ : array of shape [n_subsets_of_features]\n        The cross-validation scores such that\n        ``grid_scores_[i]`` corresponds to\n        the CV score of the i-th subset of features.\n\n    estimator_ : object\n        The external estimator fit on the reduced dataset.\n\n    Notes\n    -----\n    The size of ``grid_scores_`` is equal to\n    ``ceil((n_features - min_features_to_select) / step) + 1``,\n    where step is the number of features removed at each iteration.\n\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    Examples\n    --------\n    The following example shows how to retrieve the a-priori not known 5\n    informative features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFECV\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFECV(estimator, step=1, cv=5)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    See also\n    --------\n    RFE : Recursive feature elimination\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    ",
        "klass": "sklearn.feature_selection.RFECV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.feature_selection._base.SelectorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Meta-transformer for selecting features based on importance weights.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimator : object\n        The base estimator from which the transformer is built.\n        This can be both a fitted (if ``prefit`` is set to True)\n        or a non-fitted estimator. The estimator must have either a\n        ``feature_importances_`` or ``coef_`` attribute after fitting.\n\n    threshold : string, float, optional default None\n        The threshold value to use for feature selection. Features whose\n        importance is greater or equal are kept while the others are\n        discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n        the median (resp. the mean) of the feature importances. A scaling\n        factor (e.g., \"1.25*mean\") may also be used. If None and if the\n        estimator has a parameter penalty set to l1, either explicitly\n        or implicitly (e.g, Lasso), the threshold used is 1e-5.\n        Otherwise, \"mean\" is used by default.\n\n    prefit : bool, default False\n        Whether a prefit model is expected to be passed into the constructor\n        directly or not. If True, ``transform`` must be called directly\n        and SelectFromModel cannot be used with ``cross_val_score``,\n        ``GridSearchCV`` and similar utilities that clone the estimator.\n        Otherwise train the model using ``fit`` and then ``transform`` to do\n        feature selection.\n\n    norm_order : non-zero int, inf, -inf, default 1\n        Order of the norm used to filter the vectors of coefficients below\n        ``threshold`` in the case where the ``coef_`` attribute of the\n        estimator is of dimension 2.\n\n    max_features : int or None, optional\n        The maximum number of features selected scoring above ``threshold``.\n        To disable ``threshold`` and only select based on ``max_features``,\n        set ``threshold=-np.inf``.\n\n        .. versionadded:: 0.20\n\n    Attributes\n    ----------\n    estimator_ : an estimator\n        The base estimator from which the transformer is built.\n        This is stored only when a non-fitted estimator is passed to the\n        ``SelectFromModel``, i.e when prefit is False.\n\n    threshold_ : float\n        The threshold value used for feature selection.\n\n    Notes\n    -----\n    Allows NaN/Inf in the input if the underlying estimator does as well.\n\n    Examples\n    --------\n    >>> from sklearn.feature_selection import SelectFromModel\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X = [[ 0.87, -1.34,  0.31 ],\n    ...      [-2.79, -0.02, -0.85 ],\n    ...      [-1.34, -0.48, -2.55 ],\n    ...      [ 1.92,  1.48,  0.65 ]]\n    >>> y = [0, 1, 0, 1]\n    >>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n    >>> selector.estimator_.coef_\n    array([[-0.3252302 ,  0.83462377,  0.49750423]])\n    >>> selector.threshold_\n    0.55245...\n    >>> selector.get_support()\n    array([False,  True, False])\n    >>> selector.transform(X)\n    array([[-1.34],\n           [-0.02],\n           [-0.48],\n           [ 1.48]])\n    ",
        "klass": "sklearn.feature_selection.SelectFromModel",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._univariate_selection._BaseFilter"
        ],
        "class_docstring": "Filter: Select the p-values corresponding to Family-wise error rate\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues).\n        Default is f_classif (see below \"See also\"). The default function only\n        works with classification tasks.\n\n    alpha : float, optional\n        The highest uncorrected p-value for features to keep.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.feature_selection import SelectFwe, chi2\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> X.shape\n    (569, 30)\n    >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n    >>> X_new.shape\n    (569, 15)\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores.\n\n    See also\n    --------\n    f_classif: ANOVA F-value between label/feature for classification tasks.\n    chi2: Chi-squared stats of non-negative features for classification tasks.\n    f_regression: F-value between label/feature for regression tasks.\n    SelectPercentile: Select features based on percentile of the highest scores.\n    SelectKBest: Select features based on the k highest scores.\n    SelectFpr: Select features based on a false positive rate test.\n    SelectFdr: Select features based on an estimated false discovery rate.\n    GenericUnivariateSelect: Univariate feature selector with configurable mode.\n    ",
        "klass": "sklearn.feature_selection.SelectFwe",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._univariate_selection._BaseFilter"
        ],
        "class_docstring": "Select features according to the k highest scores.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues) or a single array with scores.\n        Default is f_classif (see below \"See also\"). The default function only\n        works with classification tasks.\n\n    k : int or \"all\", optional, default=10\n        Number of top features to select.\n        The \"all\" option bypasses selection, for use in a parameter search.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned only scores.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.feature_selection import SelectKBest, chi2\n    >>> X, y = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n    >>> X_new.shape\n    (1797, 20)\n\n    Notes\n    -----\n    Ties between features with equal scores will be broken in an unspecified\n    way.\n\n    See also\n    --------\n    f_classif: ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif: Mutual information for a discrete target.\n    chi2: Chi-squared stats of non-negative features for classification tasks.\n    f_regression: F-value between label/feature for regression tasks.\n    mutual_info_regression: Mutual information for a continuous target.\n    SelectPercentile: Select features based on percentile of the highest scores.\n    SelectFpr: Select features based on a false positive rate test.\n    SelectFdr: Select features based on an estimated false discovery rate.\n    SelectFwe: Select features based on family-wise error rate.\n    GenericUnivariateSelect: Univariate feature selector with configurable mode.\n    ",
        "klass": "sklearn.feature_selection.SelectKBest",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._univariate_selection._BaseFilter"
        ],
        "class_docstring": "Select features according to a percentile of the highest scores.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    score_func : callable\n        Function taking two arrays X and y, and returning a pair of arrays\n        (scores, pvalues) or a single array with scores.\n        Default is f_classif (see below \"See also\"). The default function only\n        works with classification tasks.\n\n    percentile : int, optional, default=10\n        Percent of features to keep.\n\n    Attributes\n    ----------\n    scores_ : array-like of shape (n_features,)\n        Scores of features.\n\n    pvalues_ : array-like of shape (n_features,)\n        p-values of feature scores, None if `score_func` returned only scores.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.feature_selection import SelectPercentile, chi2\n    >>> X, y = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n    >>> X_new.shape\n    (1797, 7)\n\n    Notes\n    -----\n    Ties between features with equal scores will be broken in an unspecified\n    way.\n\n    See also\n    --------\n    f_classif: ANOVA F-value between label/feature for classification tasks.\n    mutual_info_classif: Mutual information for a discrete target.\n    chi2: Chi-squared stats of non-negative features for classification tasks.\n    f_regression: F-value between label/feature for regression tasks.\n    mutual_info_regression: Mutual information for a continuous target.\n    SelectKBest: Select features based on the k highest scores.\n    SelectFpr: Select features based on a false positive rate test.\n    SelectFdr: Select features based on an estimated false discovery rate.\n    SelectFwe: Select features based on family-wise error rate.\n    GenericUnivariateSelect: Univariate feature selector with configurable mode.\n    ",
        "klass": "sklearn.feature_selection.SelectPercentile",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection._base.SelectorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Feature selector that removes all low-variance features.\n\n    This feature selection algorithm looks only at the features (X), not the\n    desired outputs (y), and can thus be used for unsupervised learning.\n\n    Read more in the :ref:`User Guide <variance_threshold>`.\n\n    Parameters\n    ----------\n    threshold : float, optional\n        Features with a training-set variance lower than this threshold will\n        be removed. The default is to keep all features with non-zero variance,\n        i.e. remove the features that have the same value in all samples.\n\n    Attributes\n    ----------\n    variances_ : array, shape (n_features,)\n        Variances of individual features.\n\n    Notes\n    -----\n    Allows NaN in the input.\n\n    Examples\n    --------\n    The following dataset has integer features, two of which are the same\n    in every sample. These are removed with the default setting for threshold::\n\n        >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n        >>> selector = VarianceThreshold()\n        >>> selector.fit_transform(X)\n        array([[2, 0],\n               [1, 4],\n               [1, 1]])\n    ",
        "klass": "sklearn.feature_selection.VarianceThreshold",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Gaussian process classification (GPC) based on Laplace approximation.\n\n    The implementation is based on Algorithm 3.1, 3.2, and 5.1 of\n    Gaussian Processes for Machine Learning (GPML) by Rasmussen and\n    Williams.\n\n    Internally, the Laplace approximation is used for approximating the\n    non-Gaussian posterior by a Gaussian.\n\n    Currently, the implementation is restricted to using the logistic link\n    function. For multi-class classification, several binary one-versus rest\n    classifiers are fitted. Note that this class thus does not implement\n    a true multi-class Laplace approximation.\n\n    Parameters\n    ----------\n    kernel : kernel object\n        The kernel specifying the covariance function of the GP. If None is\n        passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n        the kernel's hyperparameters are optimized during fitting.\n\n    optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n        Can either be one of the internally supported optimizers for optimizing\n        the kernel's parameters, specified by a string, or an externally\n        defined optimizer passed as a callable. If a callable is passed, it\n        must have the  signature::\n\n            def optimizer(obj_func, initial_theta, bounds):\n                # * 'obj_func' is the objective function to be maximized, which\n                #   takes the hyperparameters theta as parameter and an\n                #   optional flag eval_gradient, which determines if the\n                #   gradient is returned additionally to the function value\n                # * 'initial_theta': the initial value for theta, which can be\n                #   used by local optimizers\n                # * 'bounds': the bounds on the values of theta\n                ....\n                # Returned are the best found hyperparameters theta and\n                # the corresponding value of the target function.\n                return theta_opt, func_min\n\n        Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize\n        is used. If None is passed, the kernel's parameters are kept fixed.\n        Available internal optimizers are::\n\n            'fmin_l_bfgs_b'\n\n    n_restarts_optimizer : int, optional (default: 0)\n        The number of restarts of the optimizer for finding the kernel's\n        parameters which maximize the log-marginal likelihood. The first run\n        of the optimizer is performed from the kernel's initial parameters,\n        the remaining ones (if any) from thetas sampled log-uniform randomly\n        from the space of allowed theta-values. If greater than 0, all bounds\n        must be finite. Note that n_restarts_optimizer=0 implies that one\n        run is performed.\n\n    max_iter_predict : int, optional (default: 100)\n        The maximum number of iterations in Newton's method for approximating\n        the posterior during predict. Smaller values will reduce computation\n        time at the cost of worse results.\n\n    warm_start : bool, optional (default: False)\n        If warm-starts are enabled, the solution of the last Newton iteration\n        on the Laplace approximation of the posterior mode is used as\n        initialization for the next call of _posterior_mode(). This can speed\n        up convergence when _posterior_mode is called several times on similar\n        problems as in hyperparameter optimization. See :term:`the Glossary\n        <warm_start>`.\n\n    copy_X_train : bool, optional (default: True)\n        If True, a persistent copy of the training data is stored in the\n        object. Otherwise, just a reference to the training data is stored,\n        which might cause predictions to change if the data is modified\n        externally.\n\n    random_state : int, RandomState instance or None, optional (default: None)\n        The generator used to initialize the centers.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    multi_class : string, default : \"one_vs_rest\"\n        Specifies how multi-class classification problems are handled.\n        Supported are \"one_vs_rest\" and \"one_vs_one\". In \"one_vs_rest\",\n        one binary Gaussian process classifier is fitted for each class, which\n        is trained to separate this class from the rest. In \"one_vs_one\", one\n        binary Gaussian process classifier is fitted for each pair of classes,\n        which is trained to separate these two classes. The predictions of\n        these binary predictors are combined into multi-class predictions.\n        Note that \"one_vs_one\" does not support predicting probability\n        estimates.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    kernel_ : kernel object\n        The kernel used for prediction. In case of binary classification,\n        the structure of the kernel is the same as the one passed as parameter\n        but with optimized hyperparameters. In case of multi-class\n        classification, a CompoundKernel is returned which consists of the\n        different kernels used in the one-versus-rest classifiers.\n\n    log_marginal_likelihood_value_ : float\n        The log-marginal-likelihood of ``self.kernel_.theta``\n\n    classes_ : array-like of shape (n_classes,)\n        Unique class labels.\n\n    n_classes_ : int\n        The number of classes in the training data\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.gaussian_process import GaussianProcessClassifier\n    >>> from sklearn.gaussian_process.kernels import RBF\n    >>> X, y = load_iris(return_X_y=True)\n    >>> kernel = 1.0 * RBF(1.0)\n    >>> gpc = GaussianProcessClassifier(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpc.score(X, y)\n    0.9866...\n    >>> gpc.predict_proba(X[:2,:])\n    array([[0.83548752, 0.03228706, 0.13222543],\n           [0.79064206, 0.06525643, 0.14410151]])\n\n    .. versionadded:: 0.18\n    ",
        "klass": "sklearn.gaussian_process.GaussianProcessClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Gaussian process regression (GPR).\n\n    The implementation is based on Algorithm 2.1 of Gaussian Processes\n    for Machine Learning (GPML) by Rasmussen and Williams.\n\n    In addition to standard scikit-learn estimator API,\n    GaussianProcessRegressor:\n\n       * allows prediction without prior fitting (based on the GP prior)\n       * provides an additional method sample_y(X), which evaluates samples\n         drawn from the GPR (prior or posterior) at given inputs\n       * exposes a method log_marginal_likelihood(theta), which can be used\n         externally for other ways of selecting hyperparameters, e.g., via\n         Markov chain Monte Carlo.\n\n    Read more in the :ref:`User Guide <gaussian_process>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    kernel : kernel object\n        The kernel specifying the covariance function of the GP. If None is\n        passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that\n        the kernel's hyperparameters are optimized during fitting.\n\n    alpha : float or array-like, optional (default: 1e-10)\n        Value added to the diagonal of the kernel matrix during fitting.\n        Larger values correspond to increased noise level in the observations.\n        This can also prevent a potential numerical issue during fitting, by\n        ensuring that the calculated values form a positive definite matrix.\n        If an array is passed, it must have the same number of entries as the\n        data used for fitting and is used as datapoint-dependent noise level.\n        Note that this is equivalent to adding a WhiteKernel with c=alpha.\n        Allowing to specify the noise level directly as a parameter is mainly\n        for convenience and for consistency with Ridge.\n\n    optimizer : string or callable, optional (default: \"fmin_l_bfgs_b\")\n        Can either be one of the internally supported optimizers for optimizing\n        the kernel's parameters, specified by a string, or an externally\n        defined optimizer passed as a callable. If a callable is passed, it\n        must have the signature::\n\n            def optimizer(obj_func, initial_theta, bounds):\n                # * 'obj_func' is the objective function to be minimized, which\n                #   takes the hyperparameters theta as parameter and an\n                #   optional flag eval_gradient, which determines if the\n                #   gradient is returned additionally to the function value\n                # * 'initial_theta': the initial value for theta, which can be\n                #   used by local optimizers\n                # * 'bounds': the bounds on the values of theta\n                ....\n                # Returned are the best found hyperparameters theta and\n                # the corresponding value of the target function.\n                return theta_opt, func_min\n\n        Per default, the 'L-BGFS-B' algorithm from scipy.optimize.minimize\n        is used. If None is passed, the kernel's parameters are kept fixed.\n        Available internal optimizers are::\n\n            'fmin_l_bfgs_b'\n\n    n_restarts_optimizer : int, optional (default: 0)\n        The number of restarts of the optimizer for finding the kernel's\n        parameters which maximize the log-marginal likelihood. The first run\n        of the optimizer is performed from the kernel's initial parameters,\n        the remaining ones (if any) from thetas sampled log-uniform randomly\n        from the space of allowed theta-values. If greater than 0, all bounds\n        must be finite. Note that n_restarts_optimizer == 0 implies that one\n        run is performed.\n\n    normalize_y : boolean, optional (default: False)\n        Whether the target values y are normalized, i.e., the mean of the\n        observed target values become zero. This parameter should be set to\n        True if the target values' mean is expected to differ considerable from\n        zero. When enabled, the normalization effectively modifies the GP's\n        prior based on the data, which contradicts the likelihood principle;\n        normalization is thus disabled per default.\n\n    copy_X_train : bool, optional (default: True)\n        If True, a persistent copy of the training data is stored in the\n        object. Otherwise, just a reference to the training data is stored,\n        which might cause predictions to change if the data is modified\n        externally.\n\n    random_state : int, RandomState instance or None, optional (default: None)\n        The generator used to initialize the centers. If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    X_train_ : sequence of length n_samples\n        Feature vectors or other representations of training data (also\n        required for prediction). Could either be array-like with shape =\n        (n_samples, n_features) or a list of objects.\n\n    y_train_ : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values in training data (also required for prediction)\n\n    kernel_ : kernel object\n        The kernel used for prediction. The structure of the kernel is the\n        same as the one passed as parameter but with optimized hyperparameters\n\n    L_ : array-like of shape (n_samples, n_samples)\n        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``\n\n    alpha_ : array-like of shape (n_samples,)\n        Dual coefficients of training data points in kernel space\n\n    log_marginal_likelihood_value_ : float\n        The log-marginal-likelihood of ``self.kernel_.theta``\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman2\n    >>> from sklearn.gaussian_process import GaussianProcessRegressor\n    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n    >>> kernel = DotProduct() + WhiteKernel()\n    >>> gpr = GaussianProcessRegressor(kernel=kernel,\n    ...         random_state=0).fit(X, y)\n    >>> gpr.score(X, y)\n    0.3680...\n    >>> gpr.predict(X[:2,:], return_std=True)\n    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))\n\n    ",
        "klass": "sklearn.gaussian_process.GaussianProcessRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.impute._base._BaseImputer"
        ],
        "class_docstring": "Imputation transformer for completing missing values.\n\n    Read more in the :ref:`User Guide <impute>`.\n\n    Parameters\n    ----------\n    missing_values : number, string, np.nan (default) or None\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed.\n\n    strategy : string, optional (default=\"mean\")\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          each column. Can only be used with numeric data.\n        - If \"median\", then replace missing values using the median along\n          each column. Can only be used with numeric data.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along each column. Can be used with strings or numeric data.\n        - If \"constant\", then replace missing values with fill_value. Can be\n          used with strings or numeric data.\n\n        .. versionadded:: 0.20\n           strategy=\"constant\" for fixed value imputation.\n\n    fill_value : string or numerical value, optional (default=None)\n        When strategy == \"constant\", fill_value is used to replace all\n        occurrences of missing_values.\n        If left to the default, fill_value will be 0 when imputing numerical\n        data and \"missing_value\" for strings or object data types.\n\n    verbose : integer, optional (default=0)\n        Controls the verbosity of the imputer.\n\n    copy : boolean, optional (default=True)\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If X is not an array of floating values;\n        - If X is encoded as a CSR matrix;\n        - If add_indicator=True.\n\n    add_indicator : boolean, optional (default=False)\n        If True, a :class:`MissingIndicator` transform will stack onto output\n        of the imputer's transform. This allows a predictive estimator\n        to account for missingness despite imputation. If a feature has no\n        missing values at fit/train time, the feature won't appear on\n        the missing indicator even if there are missing values at\n        transform/test time.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature.\n        Computing statistics can result in `np.nan` values.\n        During :meth:`transform`, features corresponding to `np.nan`\n        statistics will be discarded.\n\n    indicator_ : :class:`sklearn.impute.MissingIndicator`\n        Indicator used to add binary indicators for missing values.\n        ``None`` if add_indicator is False.\n\n    See also\n    --------\n    IterativeImputer : Multivariate imputation of missing values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.impute import SimpleImputer\n    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n    SimpleImputer()\n    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n    >>> print(imp_mean.transform(X))\n    [[ 7.   2.   3. ]\n     [ 4.   3.5  6. ]\n     [10.   3.5  9. ]]\n\n    Notes\n    -----\n    Columns which only contained missing values at :meth:`fit` are discarded\n    upon :meth:`transform` if strategy is not \"constant\".\n\n    ",
        "klass": "sklearn.impute.SimpleImputer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.utils.metaestimators._BaseComposition"
        ],
        "class_docstring": "\n    Pipeline of transforms with a final estimator.\n\n    Sequentially apply a list of transforms and a final estimator.\n    Intermediate steps of the pipeline must be 'transforms', that is, they\n    must implement fit and transform methods.\n    The final estimator only needs to implement fit.\n    The transformers in the pipeline can be cached using ``memory`` argument.\n\n    The purpose of the pipeline is to assemble several steps that can be\n    cross-validated together while setting different parameters.\n    For this, it enables setting parameters of the various steps using their\n    names and the parameter name separated by a '__', as in the example below.\n    A step's estimator may be replaced entirely by setting the parameter\n    with its name to another estimator, or a transformer removed by setting\n    it to 'passthrough' or ``None``.\n\n    Read more in the :ref:`User Guide <pipeline>`.\n\n    Parameters\n    ----------\n    steps : list\n        List of (name, transform) tuples (implementing fit/transform) that are\n        chained, in the order in which they are chained, with the last object\n        an estimator.\n\n    memory : None, str or object with the joblib.Memory interface, optional\n        Used to cache the fitted transformers of the pipeline. By default,\n        no caching is performed. If a string is given, it is the path to\n        the caching directory. Enabling caching triggers a clone of\n        the transformers before fitting. Therefore, the transformer\n        instance given to the pipeline cannot be inspected\n        directly. Use the attribute ``named_steps`` or ``steps`` to\n        inspect estimators within the pipeline. Caching the\n        transformers is advantageous when fitting is time consuming.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each step will be printed as it\n        is completed.\n\n    Attributes\n    ----------\n    named_steps : bunch object, a dictionary with attribute access\n        Read-only attribute to access any step parameter by user given name.\n        Keys are step names and values are steps parameters.\n\n    See Also\n    --------\n    sklearn.pipeline.make_pipeline : Convenience function for simplified\n        pipeline construction.\n\n    Examples\n    --------\n    >>> from sklearn import svm\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.feature_selection import SelectKBest\n    >>> from sklearn.feature_selection import f_regression\n    >>> from sklearn.pipeline import Pipeline\n    >>> # generate some data to play with\n    >>> X, y = make_classification(\n    ...     n_informative=5, n_redundant=0, random_state=42)\n    >>> # ANOVA SVM-C\n    >>> anova_filter = SelectKBest(f_regression, k=5)\n    >>> clf = svm.SVC(kernel='linear')\n    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n    >>> # You can set the parameters using the names issued\n    >>> # For instance, fit using a k of 10 in the SelectKBest\n    >>> # and a parameter 'C' of the svm\n    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n    Pipeline(steps=[('anova', SelectKBest(...)), ('svc', SVC(...))])\n    >>> prediction = anova_svm.predict(X)\n    >>> anova_svm.score(X, y)\n    0.83\n    >>> # getting the selected features chosen by anova_filter\n    >>> anova_svm['anova'].get_support()\n    array([False, False,  True,  True, False, False,  True,  True, False,\n           True, False,  True,  True, False,  True, False,  True,  True,\n           False, False])\n    >>> # Another way to get selected features chosen by anova_filter\n    >>> anova_svm.named_steps.anova.get_support()\n    array([False, False,  True,  True, False, False,  True,  True, False,\n           True, False,  True,  True, False,  True, False,  True,  True,\n           False, False])\n    >>> # Indexing can also be used to extract a sub-pipeline.\n    >>> sub_pipeline = anova_svm[:1]\n    >>> sub_pipeline\n    Pipeline(steps=[('anova', SelectKBest(...))])\n    >>> coef = anova_svm[-1].coef_\n    >>> anova_svm['svc'] is anova_svm[-1]\n    True\n    >>> coef.shape\n    (1, 10)\n    >>> sub_pipeline.inverse_transform(coef).shape\n    (1, 20)\n    ",
        "klass": "sklearn.pipeline.Pipeline",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Approximate feature map for additive chi2 kernel.\n\n    Uses sampling the fourier transform of the kernel characteristic\n    at regular intervals.\n\n    Since the kernel that is to be approximated is additive, the components of\n    the input vectors can be treated separately.  Each entry in the original\n    space is transformed into 2*sample_steps+1 features, where sample_steps is\n    a parameter of the method. Typical values of sample_steps include 1, 2 and\n    3.\n\n    Optimal choices for the sampling interval for certain data ranges can be\n    computed (see the reference). The default values should be reasonable.\n\n    Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.\n\n    Parameters\n    ----------\n    sample_steps : int, optional\n        Gives the number of (complex) sampling points.\n    sample_interval : float, optional\n        Sampling interval. Must be specified when sample_steps not in {1,2,3}.\n\n    Attributes\n    ----------\n    sample_interval_ : float\n        Stored sampling interval. Specified as a parameter if sample_steps not\n        in {1,2,3}.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> from sklearn.kernel_approximation import AdditiveChi2Sampler\n    >>> X, y = load_digits(return_X_y=True)\n    >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)\n    >>> X_transformed = chi2sampler.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)\n    >>> clf.fit(X_transformed, y)\n    SGDClassifier(max_iter=5, random_state=0)\n    >>> clf.score(X_transformed, y)\n    0.9499...\n\n    Notes\n    -----\n    This estimator approximates a slightly different version of the additive\n    chi squared kernel then ``metric.additive_chi2`` computes.\n\n    See also\n    --------\n    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of\n        the chi squared kernel.\n\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n\n    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi\n        squared kernel.\n\n    References\n    ----------\n    See `\"Efficient additive kernels via explicit feature maps\"\n    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_\n    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,\n    2011\n    ",
        "klass": "sklearn.kernel_approximation.AdditiveChi2Sampler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Approximate a kernel map using a subset of the training data.\n\n    Constructs an approximate feature map for an arbitrary kernel\n    using a subset of the data as basis.\n\n    Read more in the :ref:`User Guide <nystroem_kernel_approx>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    kernel : string or callable, default=\"rbf\"\n        Kernel map to be approximated. A callable should accept two arguments\n        and the keyword arguments passed to this object as kernel_params, and\n        should return a floating point number.\n\n    gamma : float, default=None\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n        and sigmoid kernels. Interpretation of the default value is left to\n        the kernel; see the documentation for sklearn.metrics.pairwise.\n        Ignored by other kernels.\n\n    coef0 : float, default=None\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    degree : float, default=None\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    kernel_params : mapping of string to any, optional\n        Additional parameters (keyword arguments) for kernel function passed\n        as callable object.\n\n    n_components : int\n        Number of features to construct.\n        How many data points will be used to construct the mapping.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Subset of training points used to construct the feature map.\n\n    component_indices_ : array, shape (n_components)\n        Indices of ``components_`` in the training set.\n\n    normalization_ : array, shape (n_components, n_components)\n        Normalization matrix needed for embedding.\n        Square root of the kernel matrix on ``components_``.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, svm\n    >>> from sklearn.kernel_approximation import Nystroem\n    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n    >>> data = X / 16.\n    >>> clf = svm.LinearSVC()\n    >>> feature_map_nystroem = Nystroem(gamma=.2,\n    ...                                 random_state=1,\n    ...                                 n_components=300)\n    >>> data_transformed = feature_map_nystroem.fit_transform(data)\n    >>> clf.fit(data_transformed, y)\n    LinearSVC()\n    >>> clf.score(data_transformed, y)\n    0.9987...\n\n    References\n    ----------\n    * Williams, C.K.I. and Seeger, M.\n      \"Using the Nystroem method to speed up kernel machines\",\n      Advances in neural information processing systems 2001\n\n    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n      \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n      Comparison\",\n      Advances in Neural Information Processing Systems 2012\n\n\n    See also\n    --------\n    RBFSampler : An approximation to the RBF kernel using random Fourier\n                 features.\n\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n    ",
        "klass": "sklearn.kernel_approximation.Nystroem",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Approximates feature map of an RBF kernel by Monte Carlo approximation\n    of its Fourier transform.\n\n    It implements a variant of Random Kitchen Sinks.[1]\n\n    Read more in the :ref:`User Guide <rbf_kernel_approx>`.\n\n    Parameters\n    ----------\n    gamma : float\n        Parameter of RBF kernel: exp(-gamma * x^2)\n\n    n_components : int\n        Number of Monte Carlo samples per original feature.\n        Equals the dimensionality of the computed feature space.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0\n\n    Notes\n    -----\n    See \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\n    Benjamin Recht.\n\n    [1] \"Weighted Sums of Random Kitchen Sinks: Replacing\n    minimization with randomization in learning\" by A. Rahimi and\n    Benjamin Recht.\n    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n    ",
        "klass": "sklearn.kernel_approximation.RBFSampler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Approximates feature map of the \"skewed chi-squared\" kernel by Monte\n    Carlo approximation of its Fourier transform.\n\n    Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.\n\n    Parameters\n    ----------\n    skewedness : float\n        \"skewedness\" parameter of the kernel. Needs to be cross-validated.\n\n    n_components : int\n        number of Monte Carlo samples per original feature.\n        Equals the dimensionality of the computed feature space.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import SkewedChi2Sampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> chi2_feature = SkewedChi2Sampler(skewedness=.01,\n    ...                                  n_components=10,\n    ...                                  random_state=0)\n    >>> X_features = chi2_feature.fit_transform(X, y)\n    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=10)\n    >>> clf.score(X_features, y)\n    1.0\n\n    References\n    ----------\n    See \"Random Fourier Approximations for Skewed Multiplicative Histogram\n    Kernels\" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.\n\n    See also\n    --------\n    AdditiveChi2Sampler : A different approach for approximating an additive\n        variant of the chi squared kernel.\n\n    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.\n    ",
        "klass": "sklearn.kernel_approximation.SkewedChi2Sampler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Kernel ridge regression.\n\n    Kernel ridge regression (KRR) combines ridge regression (linear least\n    squares with l2-norm regularization) with the kernel trick. It thus\n    learns a linear function in the space induced by the respective kernel and\n    the data. For non-linear kernels, this corresponds to a non-linear\n    function in the original space.\n\n    The form of the model learned by KRR is identical to support vector\n    regression (SVR). However, different loss functions are used: KRR uses\n    squared error loss while support vector regression uses epsilon-insensitive\n    loss, both combined with l2 regularization. In contrast to SVR, fitting a\n    KRR model can be done in closed-form and is typically faster for\n    medium-sized datasets. On the other hand, the learned model is non-sparse\n    and thus slower than SVR, which learns a sparse model for epsilon > 0, at\n    prediction-time.\n\n    This estimator has built-in support for multi-variate regression\n    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n\n    Read more in the :ref:`User Guide <kernel_ridge>`.\n\n    Parameters\n    ----------\n    alpha : {float, array-like}, shape = [n_targets]\n        Small positive values of alpha improve the conditioning of the problem\n        and reduce the variance of the estimates.  Alpha corresponds to\n        ``(2*C)^-1`` in other linear models such as LogisticRegression or\n        LinearSVC. If an array is passed, penalties are assumed to be specific\n        to the targets. Hence they must correspond in number.\n\n    kernel : string or callable, default=\"linear\"\n        Kernel mapping used internally. A callable should accept two arguments\n        and the keyword arguments passed to this object as kernel_params, and\n        should return a floating point number. Set to \"precomputed\" in\n        order to pass a precomputed kernel matrix to the estimator\n        methods instead of samples.\n\n    gamma : float, default=None\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n        and sigmoid kernels. Interpretation of the default value is left to\n        the kernel; see the documentation for sklearn.metrics.pairwise.\n        Ignored by other kernels.\n\n    degree : float, default=3\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : mapping of string to any, optional\n        Additional parameters (keyword arguments) for kernel function passed\n        as callable object.\n\n    Attributes\n    ----------\n    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]\n        Representation of weight vector(s) in kernel space\n\n    X_fit_ : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training data, which is also required for prediction. If\n        kernel == \"precomputed\" this is instead the precomputed\n        training matrix, shape = [n_samples, n_samples].\n\n    References\n    ----------\n    * Kevin P. Murphy\n      \"Machine Learning: A Probabilistic Perspective\", The MIT Press\n      chapter 14.4.3, pp. 492-493\n\n    See also\n    --------\n    sklearn.linear_model.Ridge:\n        Linear ridge regression.\n    sklearn.svm.SVR:\n        Support Vector Regression implemented using libsvm.\n\n    Examples\n    --------\n    >>> from sklearn.kernel_ridge import KernelRidge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = KernelRidge(alpha=1.0)\n    >>> clf.fit(X, y)\n    KernelRidge(alpha=1.0)\n    ",
        "klass": "sklearn.kernel_ridge.KernelRidge",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Bayesian ARD regression.\n\n    Fit the weights of a regression model, using an ARD prior. The weights of\n    the regression model are assumed to be in Gaussian distributions.\n    Also estimate the parameters lambda (precisions of the distributions of the\n    weights) and alpha (precision of the distribution of the noise).\n    The estimation is done by an iterative procedures (Evidence Maximization)\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    n_iter : int, default=300\n        Maximum number of iterations.\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    compute_score : bool, default=False\n        If True, compute the objective function at each step of the model.\n\n    threshold_lambda : float, default=10 000\n        threshold for removing (pruning) weights with high precision from\n        the computation.\n\n    fit_intercept : bool, default=True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    alpha_ : float\n       estimated precision of the noise.\n\n    lambda_ : array-like of shape (n_features,)\n       estimated precisions of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        estimated variance-covariance matrix of the weights\n\n    scores_ : float\n        if computed, value of the objective function (to be maximized)\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n    ",
        "klass": "sklearn.linear_model.ARDRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Bayesian ridge regression.\n\n    Fit a Bayesian ridge model. See the Notes section for details on this\n    implementation and the optimization of the regularization parameters\n    lambda (precision of the weights) and alpha (precision of the noise).\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    n_iter : int, default=300\n        Maximum number of iterations. Should be greater than or equal to 1.\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    alpha_init : float, default=None\n        Initial value for alpha (precision of the noise).\n        If not set, alpha_init is 1/Var(y).\n\n            .. versionadded:: 0.22\n\n    lambda_init : float, default=None\n        Initial value for lambda (precision of the weights).\n        If not set, lambda_init is 1.\n\n            .. versionadded:: 0.22\n\n    compute_score : bool, default=False\n        If True, compute the log marginal likelihood at each iteration of the\n        optimization.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model.\n        The intercept is not treated as a probabilistic parameter\n        and thus has no associated variance. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n       Estimated precision of the noise.\n\n    lambda_ : float\n       Estimated precision of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        Estimated variance-covariance matrix of the weights\n\n    scores_ : array-like of shape (n_iter_+1,)\n        If computed_score is True, value of the log marginal likelihood (to be\n        maximized) at each iteration of the optimization. The array starts\n        with the value of the log marginal likelihood obtained for the initial\n        values of alpha and lambda and ends with the value obtained for the\n        estimated alpha and lambda.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n    ",
        "klass": "sklearn.linear_model.BayesianRidge",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Linear regression with combined L1 and L2 priors as regularizer.\n\n    Minimizes the objective function::\n\n            1 / (2 * n_samples) * ||y - Xw||^2_2\n            + alpha * l1_ratio * ||w||_1\n            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n            a * L1 + b * L2\n\n    where::\n\n            alpha = a + b and l1_ratio = a / (a + b)\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package while\n    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n    = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n    unless you supply your own sequence of alpha.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Constant that multiplies the penalty terms. Defaults to 1.0.\n        See the notes for the exact mathematical meaning of this\n        parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n        solved by the :class:`LinearRegression` object. For numerical\n        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n        Given this, you should use the :class:`LinearRegression` object.\n\n    l1_ratio : float\n        The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n        is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n        combination of L1 and L2.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If ``False``, the\n        data is assumed to be already centered.\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. The Gram matrix can also be passed as argument.\n        For sparse input this option is always ``True`` to preserve sparsity.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, optional\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n        ``sparse_coef_`` is a readonly property derived from ``coef_``\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    n_iter_ : array-like, shape (n_targets,)\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNet\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNet(random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNet(random_state=0)\n    >>> print(regr.coef_)\n    [18.83816048 64.55968825]\n    >>> print(regr.intercept_)\n    1.451...\n    >>> print(regr.predict([[0, 0]]))\n    [1.451...]\n\n\n    Notes\n    -----\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    See also\n    --------\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    SGDRegressor: implements elastic net regression with incremental training.\n    SGDClassifier: implements logistic regression with elastic net penalty\n        (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n    ",
        "klass": "sklearn.linear_model.ElasticNet",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._coordinate_descent.LinearModelCV"
        ],
        "class_docstring": "Elastic Net model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    l1_ratio : float or array of floats, optional\n        float between 0 and 1 passed to ElasticNet (scaling between\n        l1 and l2 penalties). For ``l1_ratio = 0``\n        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n        This parameter can be a list, in which case the different\n        values are tested by cross-validation and the one giving the best\n        prediction score is used. Note that a good choice of list of\n        values for l1_ratio is often to put more values close to 1\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n        .9, .95, .99, 1]``\n\n    eps : float, optional\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path, used for each l1_ratio.\n\n    alphas : numpy array, optional\n        List of alphas where to compute the models.\n        If None alphas are set automatically\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation\n\n    l1_ratio_ : float\n        The compromise between l1 and l2 penalization chosen by\n        cross validation\n\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        Parameter vector (w in the cost function formula),\n\n    intercept_ : float | array, shape (n_targets, n_features)\n        Independent term in the decision function.\n\n    mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)\n        Mean square error for the test set on each fold, varying l1_ratio and\n        alpha.\n\n    alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n        The grid of alphas used for fitting, for each l1_ratio.\n\n    n_iter_ : int\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNetCV\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\n    >>> regr.fit(X, y)\n    ElasticNetCV(cv=5, random_state=0)\n    >>> print(regr.alpha_)\n    0.199...\n    >>> print(regr.intercept_)\n    0.398...\n    >>> print(regr.predict([[0, 0]]))\n    [0.398...]\n\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package\n    while alpha corresponds to the lambda parameter in glmnet.\n    More specifically, the optimization objective is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\n    for::\n\n        alpha = a + b and l1_ratio = a / (a + b).\n\n    See also\n    --------\n    enet_path\n    ElasticNet\n\n    ",
        "klass": "sklearn.linear_model.ElasticNetCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._base.LinearModel",
            "sklearn.base.RegressorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Linear regression model that is robust to outliers.\n\n    The Huber Regressor optimizes the squared loss for the samples where\n    ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n    where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n    to be optimized. The parameter sigma makes sure that if y is scaled up\n    or down by a certain factor, one does not need to rescale epsilon to\n    achieve the same robustness. Note that this does not take into account\n    the fact that the different features of X may be of different scales.\n\n    This makes sure that the loss function is not heavily influenced by the\n    outliers while not completely ignoring their effect.\n\n    Read more in the :ref:`User Guide <huber_regression>`\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    epsilon : float, greater than 1.0, default 1.35\n        The parameter epsilon controls the number of samples that should be\n        classified as outliers. The smaller the epsilon, the more robust it is\n        to outliers.\n\n    max_iter : int, default 100\n        Maximum number of iterations that\n        ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n\n    alpha : float, default 0.0001\n        Regularization parameter.\n\n    warm_start : bool, default False\n        This is useful if the stored attributes of a previously used model\n        has to be reused. If set to False, then the coefficients will\n        be rewritten for every call to fit.\n        See :term:`the Glossary <warm_start>`.\n\n    fit_intercept : bool, default True\n        Whether or not to fit the intercept. This can be set to False\n        if the data is already centered around the origin.\n\n    tol : float, default 1e-5\n        The iteration will stop when\n        ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n        where pg_i is the i-th component of the projected gradient.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Features got by optimizing the Huber loss.\n\n    intercept_ : float\n        Bias.\n\n    scale_ : float\n        The value by which ``|y - X'w - c|`` is scaled down.\n\n    n_iter_ : int\n        Number of iterations that\n        ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    outliers_ : array, shape (n_samples,)\n        A boolean mask which is set to True where the samples are identified\n        as outliers.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n    >>> from sklearn.datasets import make_regression\n    >>> rng = np.random.RandomState(0)\n    >>> X, y, coef = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n    >>> X[:4] = rng.uniform(10, 20, (4, 2))\n    >>> y[:4] = rng.uniform(10, 20, 4)\n    >>> huber = HuberRegressor().fit(X, y)\n    >>> huber.score(X, y)\n    -7.284608623514573\n    >>> huber.predict(X[:1,])\n    array([806.7200...])\n    >>> linear = LinearRegression().fit(X, y)\n    >>> print(\"True coefficients:\", coef)\n    True coefficients: [20.4923...  34.1698...]\n    >>> print(\"Huber coefficients:\", huber.coef_)\n    Huber coefficients: [17.7906... 31.0106...]\n    >>> print(\"Linear Regression coefficients:\", linear.coef_)\n    Linear Regression coefficients: [-1.9221...  7.0226...]\n\n    References\n    ----------\n    .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n           Concomitant scale estimates, pg 172\n    .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n           https://statweb.stanford.edu/~owen/reports/hhu.pdf\n    ",
        "klass": "sklearn.linear_model.HuberRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._coordinate_descent.ElasticNet"
        ],
        "class_docstring": "Linear Model trained with L1 prior as regularizer (aka the Lasso)\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Technically the Lasso model is optimizing the same objective function as\n    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Constant that multiplies the L1 term. Defaults to 1.0.\n        ``alpha = 0`` is equivalent to an ordinary least square, solved\n        by the :class:`LinearRegression` object. For numerical\n        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n        Given this, you should use the :class:`LinearRegression` object.\n\n    fit_intercept : boolean, optional, default True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | array-like, default=False\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument. For sparse input\n        this option is always ``True`` to preserve sparsity.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, optional\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n        ``sparse_coef_`` is a readonly property derived from ``coef_``\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    n_iter_ : int | array-like, shape (n_targets,)\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15...\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    LassoLars\n    LassoCV\n    LassoLarsCV\n    sklearn.decomposition.sparse_encode\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n    ",
        "klass": "sklearn.linear_model.Lasso",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._coordinate_descent.LinearModelCV"
        ],
        "class_docstring": "Lasso linear model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The best model is selected by cross-validation.\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    eps : float, optional\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path\n\n    alphas : numpy array, optional\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically\n\n    fit_intercept : boolean, default True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, optional\n        If positive, restrict regression coefficients to be positive\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation\n\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    mse_path_ : array, shape (n_alphas, n_folds)\n        mean square error for the test set on each fold, varying alpha\n\n    alphas_ : numpy array, shape (n_alphas,)\n        The grid of alphas used for fitting\n\n    dual_gap_ : ndarray, shape ()\n        The dual gap at the end of the optimization for the optimal alpha\n        (``alpha_``).\n\n    n_iter_ : int\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.predict(X[:1,])\n    array([-78.4951...])\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    LassoLars\n    Lasso\n    LassoLarsCV\n    ",
        "klass": "sklearn.linear_model.LassoCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._least_angle.Lars"
        ],
        "class_docstring": "Lasso model fit with Least Angle Regression a.k.a. Lars\n\n    It is a Linear Model trained with an L1 prior as regularizer.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Constant that multiplies the penalty term. Defaults to 1.0.\n        ``alpha = 0`` is equivalent to an ordinary least square, solved\n        by :class:`LinearRegression`. For numerical reasons, using\n        ``alpha = 0`` with the LassoLars object is not advised and you\n        should prefer the LinearRegression object.\n\n    fit_intercept : bool, default=True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount\n\n    normalize : bool, default=True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : bool, 'auto' or array-like, default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n        By default, ``np.finfo(np.float).eps`` is used.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    fit_path : bool, default=True\n        If ``True`` the full path is stored in the ``coef_path_`` attribute.\n        If you compute the solution for a large problem or many targets,\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\n        with a small alpha.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients will not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n\n    Attributes\n    ----------\n    alphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\n        Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\n\n    active_ : list, length = n_alphas | list of n_targets such lists\n        Indices of active variables at the end of the path.\n\n    coef_path_ : array-like of shape (n_features, n_alphas + 1) or list\n        If a list is passed it's expected to be one of n_targets such arrays.\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``.\n\n    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float or array-like of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int.\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLars(alpha=0.01)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n    LassoLars(alpha=0.01)\n    >>> print(reg.coef_)\n    [ 0.         -0.963257...]\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    Lasso\n    LassoCV\n    LassoLarsCV\n    LassoLarsIC\n    sklearn.decomposition.sparse_encode\n\n    ",
        "klass": "sklearn.linear_model.LassoLars",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._least_angle.LarsCV"
        ],
        "class_docstring": "Cross-validated Lasso, using the LARS algorithm.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    normalize : bool, default=True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : bool or 'auto' , default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    max_n_alphas : int, default=1000\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation\n\n    n_jobs : int or None, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. By default, ``np.finfo(np.float).eps`` is used.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsCV only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    coef_path_ : array-like of shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array-like of shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array-like of shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9992...\n    >>> reg.alpha_\n    0.0484...\n    >>> reg.predict(X[:1,])\n    array([-77.8723...])\n\n    Notes\n    -----\n\n    The object solves the same problem as the LassoCV object. However,\n    unlike the LassoCV, it find the relevant alphas values by itself.\n    In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the LassoCV if only a small number of\n    features are selected compared to the total number, for instance if\n    there are very few samples compared to the number of features.\n\n    See also\n    --------\n    lars_path, LassoLars, LarsCV, LassoCV\n    ",
        "klass": "sklearn.linear_model.LassoLarsCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._least_angle.LassoLars"
        ],
        "class_docstring": "Lasso model fit with Lars using BIC or AIC for model selection\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    AIC is the Akaike information criterion and BIC is the Bayes\n    Information criterion. Such criteria are useful to select the value\n    of the regularization parameter by making a trade-off between the\n    goodness of fit and the complexity of the model. A good model should\n    explain well the data while being simple.\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    criterion : {'bic' , 'aic'}, default='aic'\n        The type of criterion to use.\n\n    fit_intercept : bool, default=True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount\n\n    normalize : bool, default=True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : bool, 'auto' or array-like, default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform. Can be used for\n        early stopping.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n        By default, ``np.finfo(np.float).eps`` is used\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsIC only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    alpha_ : float\n        the alpha parameter chosen by the information criterion\n\n    n_iter_ : int\n        number of iterations run by lars_path to find the grid of\n        alphas.\n\n    criterion_ : array-like of shape (n_alphas,)\n        The value of the information criteria ('aic', 'bic') across all\n        alphas. The alpha which has the smallest information criterion is\n        chosen. This value is larger by a factor of ``n_samples`` compared to\n        Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLarsIC(criterion='bic')\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n    LassoLarsIC(criterion='bic')\n    >>> print(reg.coef_)\n    [ 0.  -1.11...]\n\n    Notes\n    -----\n    The estimation of the number of degrees of freedom is given by:\n\n    \"On the degrees of freedom of the lasso\"\n    Hui Zou, Trevor Hastie, and Robert Tibshirani\n    Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n\n    https://en.wikipedia.org/wiki/Akaike_information_criterion\n    https://en.wikipedia.org/wiki/Bayesian_information_criterion\n\n    See also\n    --------\n    lars_path, LassoLars, LassoLarsCV\n    ",
        "klass": "sklearn.linear_model.LassoLarsIC",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "\n    Ordinary least squares Linear Regression.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n    to minimize the residual sum of squares between the observed targets in\n    the dataset, and the targets predicted by the linear approximation.\n\n    Parameters\n    ----------\n    fit_intercept : bool, optional, default True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : bool, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n        an estimator with ``normalize=False``.\n\n    copy_X : bool, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This will only provide\n        speedup for n_targets > 1 and sufficient large problems.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.\n        If multiple targets are passed during the fit (y 2D), this\n        is a 2D array of shape (n_targets, n_features), while if only\n        one target is passed, this is a 1D array of length n_features.\n\n    rank_ : int\n        Rank of matrix `X`. Only available when `X` is dense.\n\n    singular_ : array of shape (min(X, y),)\n        Singular values of `X`. Only available when `X` is dense.\n\n    intercept_ : float or array of shape of (n_targets,)\n        Independent term in the linear model. Set to 0.0 if\n        `fit_intercept = False`.\n\n    See Also\n    --------\n    sklearn.linear_model.Ridge : Ridge regression addresses some of the\n        problems of Ordinary Least Squares by imposing a penalty on the\n        size of the coefficients with l2 regularization.\n    sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n        sparse coefficients with l1 regularization.\n    sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n        model trained with both l1 and l2 -norm regularization of the\n        coefficients.\n\n    Notes\n    -----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_\n    3.0000...\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])\n    ",
        "klass": "sklearn.linear_model.LinearRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.linear_model._base.LinearClassifierMixin",
            "sklearn.linear_model._base.SparseCoefMixin"
        ],
        "class_docstring": "\n    Logistic Regression (aka logit, MaxEnt) classifier.\n\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n    scheme if the 'multi_class' option is set to 'ovr', and uses the\n    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n    'sag', 'saga' and 'newton-cg' solvers.)\n\n    This class implements regularized logistic regression using the\n    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n    that regularization is applied by default**. It can handle both dense\n    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n    floats for optimal performance; any other input format will be converted\n    (and copied).\n\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n    with primal formulation, or no regularization. The 'liblinear' solver\n    supports both L1 and L2 regularization, with a dual formulation only for\n    the L2 penalty. The Elastic-Net regularization is only supported by the\n    'saga' solver.\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    penalty : str, 'l1', 'l2', 'elasticnet' or 'none', optional (default='l2')\n        Used to specify the norm used in the penalization. The 'newton-cg',\n        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n        only supported by the 'saga' solver. If 'none' (not supported by the\n        liblinear solver), no regularization is applied.\n\n        .. versionadded:: 0.19\n           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n    dual : bool, optional (default=False)\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    tol : float, optional (default=1e-4)\n        Tolerance for stopping criteria.\n\n    C : float, optional (default=1.0)\n        Inverse of regularization strength; must be a positive float.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, optional (default=True)\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    intercept_scaling : float, optional (default=1)\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    class_weight : dict or 'balanced', optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           *class_weight='balanced'*\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag' or\n        'liblinear'.\n\n    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              optional (default='lbfgs')\n\n        Algorithm to use in the optimization problem.\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n          'saga' are faster for large ones.\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n          schemes.\n        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n        - 'liblinear' and 'saga' also handle L1 penalty\n        - 'saga' also supports 'elasticnet' penalty\n        - 'liblinear' does not support setting ``penalty='none'``\n\n        Note that 'sag' and 'saga' fast convergence is only guaranteed on\n        features with approximately the same scale. You can\n        preprocess the data with a scaler from sklearn.preprocessing.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.22\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n\n    max_iter : int, optional (default=100)\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : {'ovr', 'multinomial', 'auto'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    verbose : int, optional (default=0)\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, optional (default=False)\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    l1_ratio : float or None, optional (default=None)\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n        used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n        combination of L1 and L2.\n\n    Attributes\n    ----------\n\n    classes_ : array, shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : array, shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : array, shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_iter_ : array, shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n        SAGA: A Fast Incremental Gradient Method With Support\n        for Non-Strongly Convex Composite Objectives\n        https://arxiv.org/abs/1407.0202\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n    ",
        "klass": "sklearn.linear_model.LogisticRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._logistic.LogisticRegression",
            "sklearn.base.BaseEstimator",
            "sklearn.linear_model._base.LinearClassifierMixin"
        ],
        "class_docstring": "Logistic Regression CV (aka logit, MaxEnt) classifier.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    This class implements logistic regression using liblinear, newton-cg, sag\n    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n    regularization with primal formulation. The liblinear solver supports both\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n    Elastic-Net penalty is only supported by the saga solver.\n\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n    is selected by the cross-validator\n    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    Cs : list of floats or int, optional (default=10)\n        Each of the values in Cs describes the inverse of regularization\n        strength. If Cs is as an int, then a grid of Cs values are chosen\n        in a logarithmic scale between 1e-4 and 1e4.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, optional (default=True)\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    cv : int or cross-validation generator, optional (default=None)\n        The default cross-validation generator used is Stratified K-Folds.\n        If an integer is provided, then it is the number of folds used.\n        See the module :mod:`sklearn.model_selection` module for the\n        list of possible cross-validation objects.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    dual : bool, optional (default=False)\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : str, 'l1', 'l2', or 'elasticnet', optional (default='l2')\n        Used to specify the norm used in the penalization. The 'newton-cg',\n        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n        only supported by the 'saga' solver.\n\n    scoring : string, callable, or None, optional (default=None)\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. For a list of scoring functions\n        that can be used, look at :mod:`sklearn.metrics`. The\n        default scoring option used is 'accuracy'.\n\n    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              optional (default='lbfgs')\n\n        Algorithm to use in the optimization problem.\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n          'saga' are faster for large ones.\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n          schemes.\n        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n          'liblinear' and 'saga' handle L1 penalty.\n        - 'liblinear' might be slower in LogisticRegressionCV because it does\n          not handle warm-starting.\n\n        Note that 'sag' and 'saga' fast convergence is only guaranteed on\n        features with approximately the same scale. You can preprocess the data\n        with a scaler from sklearn.preprocessing.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    tol : float, optional (default=1e-4)\n        Tolerance for stopping criteria.\n\n    max_iter : int, optional (default=100)\n        Maximum number of iterations of the optimization algorithm.\n\n    class_weight : dict or 'balanced', optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           class_weight == 'balanced'\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPU cores used during the cross-validation loop.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, optional (default=0)\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n        positive number for verbosity.\n\n    refit : bool, optional (default=True)\n        If set to True, the scores are averaged across all folds, and the\n        coefs and the C that corresponds to the best score is taken, and a\n        final refit is done using these parameters.\n        Otherwise the coefs, intercepts and C that correspond to the\n        best scores across folds are averaged.\n\n    intercept_scaling : float, optional (default=1)\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : {'ovr', 'multinomial', 'auto'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when `solver='sag'` or `solver='liblinear'`.\n        Note that this only applies to the solver and not the cross-validation\n        generator.\n\n    l1_ratios : list of float or None, optional (default=None)\n        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n        using ``penalty='l2'``, while 1 is equivalent to using\n        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n        of L1 and L2.\n\n    Attributes\n    ----------\n    classes_ : array, shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : array, shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem\n        is binary.\n\n    intercept_ : array, shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape(1,) when the problem is binary.\n\n    Cs_ : array, shape (n_cs)\n        Array of C i.e. inverse of regularization parameter values used\n        for cross-validation.\n\n    l1_ratios_ : array, shape (n_l1_ratios)\n        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\n    coefs_paths_ : array, shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n        dict with classes as the keys, and the path of coefficients obtained\n        during cross-validating across each fold and then across each Cs\n        after doing an OvR for the corresponding class as values.\n        If the 'multi_class' option is set to 'multinomial', then\n        the coefs_paths are the coefficients corresponding to each class.\n        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n\n    scores_ : dict\n        dict with classes as the keys, and the values as the\n        grid of scores obtained during cross-validating each fold, after doing\n        an OvR for the corresponding class. If the 'multi_class' option\n        given is 'multinomial' then the same scores are repeated across\n        all classes, since this is the multinomial class. Each dict value\n        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n        ``penalty='elasticnet'``.\n\n    C_ : array, shape (n_classes,) or (n_classes - 1,)\n        Array of C that maps to the best scores across every class. If refit is\n        set to False, then for each class, the best C is the average of the\n        C's that correspond to the best scores for each fold.\n        `C_` is of shape(n_classes,) when the problem is binary.\n\n    l1_ratio_ : array, shape (n_classes,) or (n_classes - 1,)\n        Array of l1_ratio that maps to the best scores across every class. If\n        refit is set to False, then for each class, the best l1_ratio is the\n        average of the l1_ratio's that correspond to the best scores for each\n        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n\n    n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n        Actual number of iterations for all classes, folds and Cs.\n        In the binary or multinomial cases, the first dimension is equal to 1.\n        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegressionCV\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :]).shape\n    (2, 3)\n    >>> clf.score(X, y)\n    0.98...\n\n    See also\n    --------\n    LogisticRegression\n\n    ",
        "klass": "sklearn.linear_model.LogisticRegressionCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._coordinate_descent.MultiTaskElasticNet"
        ],
        "class_docstring": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <multi_task_lasso>`.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Constant that multiplies the L1/L2 term. Defaults to 1.0\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, optional\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_tasks, n_features)\n        Parameter vector (W in the cost function formula).\n        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n\n    intercept_ : array, shape (n_tasks,)\n        independent term in decision function.\n\n    n_iter_ : int\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n    MultiTaskLasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [[0.89393398 0.        ]\n     [0.89393398 0.        ]]\n    >>> print(clf.intercept_)\n    [0.10606602 0.10606602]\n\n    See also\n    --------\n    MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation\n    Lasso\n    MultiTaskElasticNet\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n    ",
        "klass": "sklearn.linear_model.MultiTaskLasso",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Orthogonal Matching Pursuit model (OMP)\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    n_nonzero_coefs : int, optional\n        Desired number of non-zero entries in the solution. If None (by\n        default) this value is set to 10% of n_features.\n\n    tol : float, optional\n        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\n    fit_intercept : boolean, optional\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : {True, False, 'auto'}, default 'auto'\n        Whether to use a precomputed Gram and Xy matrix to speed up\n        calculations. Improves performance when :term:`n_targets` or\n        :term:`n_samples` is very large. Note that if you already have such\n        matrices, you can pass them directly to the fit method.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        parameter vector (w in the formula)\n\n    intercept_ : float or array, shape (n_targets,)\n        independent term in decision function.\n\n    n_iter_ : int or array-like\n        Number of active features across every target.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n    >>> reg.score(X, y)\n    0.9991...\n    >>> reg.predict(X[:1,])\n    array([-78.3854...])\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    See also\n    --------\n    orthogonal_mp\n    orthogonal_mp_gram\n    lars_path\n    Lars\n    LassoLars\n    decomposition.sparse_encode\n    OrthogonalMatchingPursuitCV\n    ",
        "klass": "sklearn.linear_model.OrthogonalMatchingPursuit",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._stochastic_gradient.BaseSGDClassifier"
        ],
        "class_docstring": "Passive Aggressive Classifier\n\n    Read more in the :ref:`User Guide <passive_aggressive>`.\n\n    Parameters\n    ----------\n\n    C : float\n        Maximum step size (regularization). Defaults to 1.0.\n\n    fit_intercept : bool, default=False\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation.\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : integer, optional\n        The verbosity level\n\n    loss : string, optional\n        The loss function to be used:\n        hinge: equivalent to PA-I in the reference paper.\n        squared_hinge: equivalent to PA-II in the reference paper.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    warm_start : bool, optional\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n\n    class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        .. versionadded:: 0.17\n           parameter *class_weight* to automatically weight samples.\n\n    average : bool or int, optional\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So average=10 will begin averaging after seeing 10 samples.\n\n        .. versionadded:: 0.19\n           parameter *average* to use weights averaging in SGD\n\n    Attributes\n    ----------\n    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n        Weights assigned to the features.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples)``.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import PassiveAggressiveClassifier\n    >>> from sklearn.datasets import make_classification\n\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n    ... tol=1e-3)\n    >>> clf.fit(X, y)\n    PassiveAggressiveClassifier(random_state=0)\n    >>> print(clf.coef_)\n    [[0.26642044 0.45070924 0.67251877 0.64185414]]\n    >>> print(clf.intercept_)\n    [1.84127814]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n\n    See also\n    --------\n\n    SGDClassifier\n    Perceptron\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n\n    ",
        "klass": "sklearn.linear_model.PassiveAggressiveClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._stochastic_gradient.BaseSGDRegressor"
        ],
        "class_docstring": "Passive Aggressive Regressor\n\n    Read more in the :ref:`User Guide <passive_aggressive>`.\n\n    Parameters\n    ----------\n\n    C : float\n        Maximum step size (regularization). Defaults to 1.0.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation.\n        score is not improving. If set to True, it will automatically set aside\n        a fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : integer, optional\n        The verbosity level\n\n    loss : string, optional\n        The loss function to be used:\n        epsilon_insensitive: equivalent to PA-I in the reference paper.\n        squared_epsilon_insensitive: equivalent to PA-II in the reference\n        paper.\n\n    epsilon : float\n        If the difference between the current prediction and the correct label\n        is below this threshold, the model is not updated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    warm_start : bool, optional\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n\n    average : bool or int, optional\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So average=10 will begin averaging after seeing 10 samples.\n\n        .. versionadded:: 0.19\n           parameter *average* to use weights averaging in SGD\n\n    Attributes\n    ----------\n    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n        Weights assigned to the features.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples)``.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import PassiveAggressiveRegressor\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=4, random_state=0)\n    >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n    ... tol=1e-3)\n    >>> regr.fit(X, y)\n    PassiveAggressiveRegressor(max_iter=100, random_state=0)\n    >>> print(regr.coef_)\n    [20.48736655 34.18818427 67.59122734 87.94731329]\n    >>> print(regr.intercept_)\n    [-0.02306214]\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-0.02306214]\n\n    See also\n    --------\n\n    SGDRegressor\n\n    References\n    ----------\n    Online Passive-Aggressive Algorithms\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n\n    ",
        "klass": "sklearn.linear_model.PassiveAggressiveRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._stochastic_gradient.BaseSGDClassifier"
        ],
        "class_docstring": "Perceptron\n\n    Read more in the :ref:`User Guide <perceptron>`.\n\n    Parameters\n    ----------\n\n    penalty : None, 'l2' or 'l1' or 'elasticnet'\n        The penalty (aka regularization term) to be used. Defaults to None.\n\n    alpha : float\n        Constant that multiplies the regularization term if regularization is\n        used. Defaults to 0.0001\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > previous_loss - tol).\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, default=True\n        Whether or not the training data should be shuffled after each epoch.\n\n    verbose : integer, default=0\n        The verbosity level\n\n    eta0 : double\n        Constant by which the updates are multiplied. Defaults to 1.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation.\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution. See\n        :term:`the Glossary <warm_start>`.\n\n    Attributes\n    ----------\n    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n        Weights assigned to the features.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples)``.\n\n    Notes\n    -----\n\n    ``Perceptron`` is a classification algorithm which shares the same\n    underlying implementation with ``SGDClassifier``. In fact,\n    ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n    eta0=1, learning_rate=\"constant\", penalty=None)`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.linear_model import Perceptron\n    >>> X, y = load_digits(return_X_y=True)\n    >>> clf = Perceptron(tol=1e-3, random_state=0)\n    >>> clf.fit(X, y)\n    Perceptron()\n    >>> clf.score(X, y)\n    0.939...\n\n    See also\n    --------\n\n    SGDClassifier\n\n    References\n    ----------\n\n    https://en.wikipedia.org/wiki/Perceptron and references therein.\n    ",
        "klass": "sklearn.linear_model.Perceptron",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "RANSAC (RANdom SAmple Consensus) algorithm.\n\n    RANSAC is an iterative algorithm for the robust estimation of parameters\n    from a subset of inliers from the complete data set.\n\n    Read more in the :ref:`User Guide <ransac_regression>`.\n\n    Parameters\n    ----------\n    base_estimator : object, optional\n        Base estimator object which implements the following methods:\n\n         * `fit(X, y)`: Fit model to given training data and target values.\n         * `score(X, y)`: Returns the mean accuracy on the given test data,\n           which is used for the stop criterion defined by `stop_score`.\n           Additionally, the score is used to decide which of two equally\n           large consensus sets is chosen as the better one.\n         * `predict(X)`: Returns predicted values using the linear model,\n           which is used to compute residual error using loss function.\n\n        If `base_estimator` is None, then\n        ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\n        target values of dtype float.\n\n        Note that the current implementation only supports regression\n        estimators.\n\n    min_samples : int (>= 1) or float ([0, 1]), optional\n        Minimum number of samples chosen randomly from original data. Treated\n        as an absolute number of samples for `min_samples >= 1`, treated as a\n        relative number `ceil(min_samples * X.shape[0]`) for\n        `min_samples < 1`. This is typically chosen as the minimal number of\n        samples necessary to estimate the given `base_estimator`. By default a\n        ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n        `min_samples` is chosen as ``X.shape[1] + 1``.\n\n    residual_threshold : float, optional\n        Maximum residual for a data sample to be classified as an inlier.\n        By default the threshold is chosen as the MAD (median absolute\n        deviation) of the target values `y`.\n\n    is_data_valid : callable, optional\n        This function is called with the randomly selected data before the\n        model is fitted to it: `is_data_valid(X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n\n    is_model_valid : callable, optional\n        This function is called with the estimated model and the randomly\n        selected data: `is_model_valid(model, X, y)`. If its return value is\n        False the current randomly chosen sub-sample is skipped.\n        Rejecting samples with this function is computationally costlier than\n        with `is_data_valid`. `is_model_valid` should therefore only be used if\n        the estimated model is needed for making the rejection decision.\n\n    max_trials : int, optional\n        Maximum number of iterations for random sample selection.\n\n    max_skips : int, optional\n        Maximum number of iterations that can be skipped due to finding zero\n        inliers or invalid data defined by ``is_data_valid`` or invalid models\n        defined by ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    stop_n_inliers : int, optional\n        Stop iteration if at least this number of inliers are found.\n\n    stop_score : float, optional\n        Stop iteration if score is greater equal than this threshold.\n\n    stop_probability : float in range [0, 1], optional\n        RANSAC iteration stops if at least one outlier-free set of the training\n        data is sampled in RANSAC. This requires to generate at least N\n        samples (iterations)::\n\n            N >= log(1 - probability) / log(1 - e**m)\n\n        where the probability (confidence) is typically set to high value such\n        as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n        the total number of samples.\n\n    loss : string, callable, optional, default \"absolute_loss\"\n        String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n        find the absolute loss and squared loss per sample\n        respectively.\n\n        If ``loss`` is a callable, then it should be a function that takes\n        two arrays as inputs, the true and predicted value and returns a 1-D\n        array with the i-th value of the array corresponding to the loss\n        on ``X[i]``.\n\n        If the loss on a sample is greater than the ``residual_threshold``,\n        then this sample is classified as an outlier.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The generator used to initialize the centers.  If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    estimator_ : object\n        Best fitted model (copy of the `base_estimator` object).\n\n    n_trials_ : int\n        Number of random selection trials until one of the stop criteria is\n        met. It is always ``<= max_trials``.\n\n    inlier_mask_ : bool array of shape [n_samples]\n        Boolean mask of inliers classified as ``True``.\n\n    n_skips_no_inliers_ : int\n        Number of iterations skipped due to finding zero inliers.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_data_ : int\n        Number of iterations skipped due to invalid data defined by\n        ``is_data_valid``.\n\n        .. versionadded:: 0.19\n\n    n_skips_invalid_model_ : int\n        Number of iterations skipped due to an invalid model defined by\n        ``is_model_valid``.\n\n        .. versionadded:: 0.19\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import RANSACRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9885...\n    >>> reg.predict(X[:1,])\n    array([-31.9417...])\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/RANSAC\n    .. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\n    .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n    ",
        "klass": "sklearn.linear_model.RANSACRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._ridge._BaseRidge"
        ],
        "class_docstring": "Linear least squares with l2 regularization.\n\n    Minimizes the objective function::\n\n    ||y - Xw||^2_2 + alpha * ||w||^2_2\n\n    This model solves a regression model where the loss function is\n    the linear least squares function and regularization is given by\n    the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n    This estimator has built-in support for multi-variate regression\n    (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : {float, array-like of shape (n_targets,)}, default=1.0\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC. If an array is passed, penalties are\n        assumed to be specific to the targets. Hence they must correspond in\n        number.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        Maximum number of iterations for conjugate gradient solver.\n        For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n        by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n\n    tol : float, default=1e-3\n        Precision of the solution.\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. More stable for singular matrices than\n          'cholesky'.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its improved, unbiased version named SAGA. Both methods also use an\n          iterative procedure, and are often faster than other solvers when\n          both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n        All last five solvers support both dense and sparse data. However, only\n        'sparse_cg' supports sparse input when `fit_intercept` is True.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag'.\n\n        .. versionadded:: 0.17\n           *random_state* to support Stochastic Average Gradient.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features,) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float or array of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : None or array of shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n        .. versionadded:: 0.17\n\n    See also\n    --------\n    RidgeClassifier : Ridge classifier\n    RidgeCV : Ridge regression with built-in cross validation\n    :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n        combines ridge regression with the kernel trick\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import Ridge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = Ridge(alpha=1.0)\n    >>> clf.fit(X, y)\n    Ridge()\n    ",
        "klass": "sklearn.linear_model.Ridge",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._base.LinearClassifierMixin",
            "sklearn.linear_model._ridge._BaseRidge"
        ],
        "class_docstring": "Classifier using Ridge regression.\n\n    This classifier first converts the target values into ``{-1, 1}`` and\n    then treats the problem as a regression task (multi-output regression in\n    the multiclass case).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : float, default=1.0\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set to false, no\n        intercept will be used in calculations (e.g. data is expected to be\n        already centered).\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        Maximum number of iterations for conjugate gradient solver.\n        The default value is determined by scipy.sparse.linalg.\n\n    tol : float, default=1e-3\n        Precision of the solution.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. More stable for singular matrices than\n          'cholesky'.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its unbiased and more flexible version named SAGA. Both methods\n          use an iterative procedure, and are often faster than other solvers\n          when both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n          .. versionadded:: 0.17\n             Stochastic Average Gradient descent solver.\n          .. versionadded:: 0.19\n           SAGA solver.\n\n    random_state : int, RandomState instance or None, default=None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag'.\n\n    Attributes\n    ----------\n    coef_ : array of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float or array of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : None or array of shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifier\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifier().fit(X, y)\n    >>> clf.score(X, y)\n    0.9595...\n    ",
        "klass": "sklearn.linear_model.RidgeClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._stochastic_gradient.BaseSGDClassifier"
        ],
        "class_docstring": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n\n    This estimator implements regularized linear models with stochastic\n    gradient descent (SGD) learning: the gradient of the loss is estimated\n    each sample at a time and the model is updated along the way with a\n    decreasing strength schedule (aka learning rate). SGD allows minibatch\n    (online/out-of-core) learning, see the partial_fit method.\n    For best results using the default learning rate schedule, the data should\n    have zero mean and unit variance.\n\n    This implementation works with data represented as dense or sparse arrays\n    of floating point values for the features. The model it fits can be\n    controlled with the loss parameter; by default, it fits a linear support\n    vector machine (SVM).\n\n    The regularizer is a penalty added to the loss function that shrinks model\n    parameters towards the zero vector using either the squared euclidean norm\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n    parameter update crosses the 0.0 value because of the regularizer, the\n    update is truncated to 0.0 to allow for learning sparse models and achieve\n    online feature selection.\n\n    Read more in the :ref:`User Guide <sgd>`.\n\n    Parameters\n    ----------\n    loss : str, default: 'hinge'\n        The loss function to be used. Defaults to 'hinge', which gives a\n        linear SVM.\n\n        The possible options are 'hinge', 'log', 'modified_huber',\n        'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n\n        The 'log' loss gives logistic regression, a probabilistic classifier.\n        'modified_huber' is another smooth loss that brings tolerance to\n        outliers as well as probability estimates.\n        'squared_hinge' is like hinge but is quadratically penalized.\n        'perceptron' is the linear loss used by the perceptron algorithm.\n        The other losses are designed for regression but can be useful in\n        classification as well; see SGDRegressor for a description.\n\n    penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\n        which is the standard regularizer for linear SVM models. 'l1' and\n        'elasticnet' might bring sparsity to the model (feature selection)\n        not achievable with 'l2'.\n\n    alpha : float\n        Constant that multiplies the regularization term. Defaults to 0.0001.\n        Also used to compute learning_rate when set to 'optimal'.\n\n    l1_ratio : float\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n        Defaults to 0.15.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, optional\n        Whether or not the training data should be shuffled after each epoch.\n        Defaults to True.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    epsilon : float, default=0.1\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    learning_rate : str, optional\n        The learning rate schedule:\n\n        'constant':\n            eta = eta0\n        'optimal': [default]\n            eta = 1.0 / (alpha * (t + t0))\n            where t0 is chosen by a heuristic proposed by Leon Bottou.\n        'invscaling':\n            eta = eta0 / pow(t, power_t)\n        'adaptive':\n            eta = eta0, as long as the training keeps decreasing.\n            Each time n_iter_no_change consecutive epochs fail to decrease the\n            training loss by tol or fail to increase validation score by tol if\n            early_stopping is True, the current learning rate is divided by 5.\n\n    eta0 : double\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n        the default schedule 'optimal'.\n\n    power_t : double\n        The exponent for inverse scaling learning rate [default 0.5].\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit`` will result in increasing the\n        existing counter.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So ``average=10`` will begin averaging after seeing 10\n        samples.\n\n    Attributes\n    ----------\n    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)\n        Weights assigned to the features.\n\n    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    loss_function_ : concrete ``LossFunction``\n\n    classes_ : array of shape (n_classes,)\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples)``.\n\n    See Also\n    --------\n    sklearn.svm.LinearSVC: Linear support vector classification.\n    LogisticRegression: Logistic regression.\n    Perceptron: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n        ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n        penalty=None)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> Y = np.array([1, 1, 2, 2])\n    >>> clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n    >>> clf.fit(X, Y)\n    SGDClassifier()\n\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    ",
        "klass": "sklearn.linear_model.SGDClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model._stochastic_gradient.BaseSGDRegressor"
        ],
        "class_docstring": "Linear model fitted by minimizing a regularized empirical loss with SGD\n\n    SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n    estimated each sample at a time and the model is updated along the way with\n    a decreasing strength schedule (aka learning rate).\n\n    The regularizer is a penalty added to the loss function that shrinks model\n    parameters towards the zero vector using either the squared euclidean norm\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n    parameter update crosses the 0.0 value because of the regularizer, the\n    update is truncated to 0.0 to allow for learning sparse models and achieve\n    online feature selection.\n\n    This implementation works with data represented as dense numpy arrays of\n    floating point values for the features.\n\n    Read more in the :ref:`User Guide <sgd>`.\n\n    Parameters\n    ----------\n    loss : str, default: 'squared_loss'\n        The loss function to be used. The possible values are 'squared_loss',\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n\n        The 'squared_loss' refers to the ordinary least squares fit.\n        'huber' modifies 'squared_loss' to focus less on getting outliers\n        correct by switching from squared to linear loss past a distance of\n        epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n        linear past that; this is the loss function used in SVR.\n        'squared_epsilon_insensitive' is the same but becomes squared loss past\n        a tolerance of epsilon.\n\n    penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\n        which is the standard regularizer for linear SVM models. 'l1' and\n        'elasticnet' might bring sparsity to the model (feature selection)\n        not achievable with 'l2'.\n\n    alpha : float\n        Constant that multiplies the regularization term. Defaults to 0.0001\n        Also used to compute learning_rate when set to 'optimal'.\n\n    l1_ratio : float\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n        Defaults to 0.15.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        :meth:`partial_fit` method.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, optional\n        Whether or not the training data should be shuffled after each epoch.\n        Defaults to True.\n\n    verbose : integer, default=0\n        The verbosity level.\n\n    epsilon : float, default=0.1\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    learning_rate : string, optional\n        The learning rate schedule:\n\n        'constant':\n            eta = eta0\n        'optimal':\n            eta = 1.0 / (alpha * (t + t0))\n            where t0 is chosen by a heuristic proposed by Leon Bottou.\n        'invscaling': [default]\n            eta = eta0 / pow(t, power_t)\n        'adaptive':\n            eta = eta0, as long as the training keeps decreasing.\n            Each time n_iter_no_change consecutive epochs fail to decrease the\n            training loss by tol or fail to increase validation score by tol if\n            early_stopping is True, the current learning rate is divided by 5.\n\n    eta0 : double\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.01.\n\n    power_t : double\n        The exponent for inverse scaling learning rate [default 0.25].\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit``  will result in increasing the\n        existing counter.\n\n    average : bool or int, default=False\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So ``average=10`` will begin averaging after seeing 10\n        samples.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Weights assigned to the features.\n\n    intercept_ : array, shape (1,)\n        The intercept term.\n\n    average_coef_ : array, shape (n_features,)\n        Averaged weights assigned to the features.\n\n    average_intercept_ : array, shape (1,)\n        The averaged intercept term.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    t_ : int\n        Number of weight updates performed during training.\n        Same as ``(n_iter_ * n_samples)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n    >>> clf.fit(X, y)\n    SGDRegressor()\n\n    See also\n    --------\n    Ridge, ElasticNet, Lasso, sklearn.svm.SVR\n\n    ",
        "klass": "sklearn.linear_model.SGDRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Theil-Sen Estimator: robust multivariate regression model.\n\n    The algorithm calculates least square solutions on subsets with size\n    n_subsamples of the samples in X. Any value of n_subsamples between the\n    number of features and samples leads to an estimator with a compromise\n    between robustness and efficiency. Since the number of least square\n    solutions is \"n_samples choose n_subsamples\", it can be extremely large\n    and can therefore be limited with max_subpopulation. If this limit is\n    reached, the subsets are chosen randomly. In a final step, the spatial\n    median (or L1 median) is calculated of all least square solutions.\n\n    Read more in the :ref:`User Guide <theil_sen_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : boolean, optional, default True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_subpopulation : int, optional, default 1e4\n        Instead of computing with a set of cardinality 'n choose k', where n is\n        the number of samples and k is the number of subsamples (at least\n        number of features), consider only a stochastic subpopulation of a\n        given maximal size if 'n choose k' is larger than max_subpopulation.\n        For other than small problem sizes this parameter will determine\n        memory usage and runtime if n_subsamples is not changed.\n\n    n_subsamples : int, optional, default None\n        Number of samples to calculate the parameters. This is at least the\n        number of features (plus 1 if fit_intercept=True) and the number of\n        samples as a maximum. A lower number leads to a higher breakdown\n        point and a low efficiency while a high number leads to a low\n        breakdown point and a high efficiency. If None, take the\n        minimum number of subsamples leading to maximal robustness.\n        If n_subsamples is set to n_samples, Theil-Sen is identical to least\n        squares.\n\n    max_iter : int, optional, default 300\n        Maximum number of iterations for the calculation of spatial median.\n\n    tol : float, optional, default 1.e-3\n        Tolerance when calculating spatial median.\n\n    random_state : int, RandomState instance or None, optional, default None\n        A random number generator instance to define the state of the random\n        permutations generator.  If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : boolean, optional, default False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : array, shape = (n_features)\n        Coefficients of the regression model (median of distribution).\n\n    intercept_ : float\n        Estimated intercept of regression model.\n\n    breakdown_ : float\n        Approximated breakdown point.\n\n    n_iter_ : int\n        Number of iterations needed for the spatial median.\n\n    n_subpopulation_ : int\n        Number of combinations taken into account from 'n choose k', where n is\n        the number of samples and k is the number of subsamples.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import TheilSenRegressor\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(\n    ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n    >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n    >>> reg.score(X, y)\n    0.9884...\n    >>> reg.predict(X[:1,])\n    array([-31.5871...])\n\n    References\n    ----------\n    - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n      Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n      http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n    ",
        "klass": "sklearn.linear_model.TheilSenRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Isomap Embedding\n\n    Non-linear dimensionality reduction through Isometric Mapping\n\n    Read more in the :ref:`User Guide <isomap>`.\n\n    Parameters\n    ----------\n    n_neighbors : integer\n        number of neighbors to consider for each point.\n\n    n_components : integer\n        number of coordinates for the manifold\n\n    eigen_solver : ['auto'|'arpack'|'dense']\n        'auto' : Attempt to choose the most efficient solver\n        for the given problem.\n\n        'arpack' : Use Arnoldi decomposition to find the eigenvalues\n        and eigenvectors.\n\n        'dense' : Use a direct solver (i.e. LAPACK)\n        for the eigenvalue decomposition.\n\n    tol : float\n        Convergence tolerance passed to arpack or lobpcg.\n        not used if eigen_solver == 'dense'.\n\n    max_iter : integer\n        Maximum number of iterations for the arpack solver.\n        not used if eigen_solver == 'dense'.\n\n    path_method : string ['auto'|'FW'|'D']\n        Method to use in finding shortest path.\n\n        'auto' : attempt to choose the best algorithm automatically.\n\n        'FW' : Floyd-Warshall algorithm.\n\n        'D' : Dijkstra's algorithm.\n\n    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']\n        Algorithm to use for nearest neighbors search,\n        passed to neighbors.NearestNeighbors instance.\n\n    n_jobs : int or None, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    metric : string, or callable, default=\"minkowski\"\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`Glossary <sparse graph>`.\n\n        .. versionadded:: 0.22\n\n    p : int, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n        .. versionadded:: 0.22\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape (n_samples, n_components)\n        Stores the embedding vectors.\n\n    kernel_pca_ : object\n        :class:`~sklearn.decomposition.KernelPCA` object used to implement the\n        embedding.\n\n    nbrs_ : sklearn.neighbors.NearestNeighbors instance\n        Stores nearest neighbors instance, including BallTree or KDtree\n        if applicable.\n\n    dist_matrix_ : array-like, shape (n_samples, n_samples)\n        Stores the geodesic distance matrix of training data.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import Isomap\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = Isomap(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    .. [1] Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. A global geometric\n           framework for nonlinear dimensionality reduction. Science 290 (5500)\n    ",
        "klass": "sklearn.manifold.Isomap",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base._UnstableArchMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Locally Linear Embedding\n\n    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n\n    Parameters\n    ----------\n    n_neighbors : integer\n        number of neighbors to consider for each point.\n\n    n_components : integer\n        number of coordinates for the manifold\n\n    reg : float\n        regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n\n    eigen_solver : string, {'auto', 'arpack', 'dense'}\n        auto : algorithm will attempt to choose the best method for input data\n\n        arpack : use arnoldi iteration in shift-invert mode.\n                    For this method, M may be a dense matrix, sparse matrix,\n                    or general linear operator.\n                    Warning: ARPACK can be unstable for some problems.  It is\n                    best to try several random seeds in order to check results.\n\n        dense  : use standard dense matrix operations for the eigenvalue\n                    decomposition.  For this method, M must be an array\n                    or matrix type.  This method should be avoided for\n                    large problems.\n\n    tol : float, optional\n        Tolerance for 'arpack' method\n        Not used if eigen_solver=='dense'.\n\n    max_iter : integer\n        maximum number of iterations for the arpack solver.\n        Not used if eigen_solver=='dense'.\n\n    method : string ('standard', 'hessian', 'modified' or 'ltsa')\n        standard : use the standard locally linear embedding algorithm.  see\n                   reference [1]\n        hessian  : use the Hessian eigenmap method. This method requires\n                   ``n_neighbors > n_components * (1 + (n_components + 1) / 2``\n                   see reference [2]\n        modified : use the modified locally linear embedding algorithm.\n                   see reference [3]\n        ltsa     : use local tangent space alignment algorithm\n                   see reference [4]\n\n    hessian_tol : float, optional\n        Tolerance for Hessian eigenmapping method.\n        Only used if ``method == 'hessian'``\n\n    modified_tol : float, optional\n        Tolerance for modified LLE method.\n        Only used if ``method == 'modified'``\n\n    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']\n        algorithm to use for nearest neighbors search,\n        passed to neighbors.NearestNeighbors instance\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape [n_samples, n_components]\n        Stores the embedding vectors\n\n    reconstruction_error_ : float\n        Reconstruction error associated with `embedding_`\n\n    nbrs_ : NearestNeighbors object\n        Stores nearest neighbors instance, including BallTree or KDtree\n        if applicable.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import LocallyLinearEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = LocallyLinearEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).\n    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n        linear embedding techniques for high-dimensional data.\n        Proc Natl Acad Sci U S A.  100:5591 (2003).\n    .. [3] Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n        Embedding Using Multiple Weights.\n        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n        dimensionality reduction via tangent space alignment.\n        Journal of Shanghai Univ.  8:406 (2004)\n    ",
        "klass": "sklearn.manifold.LocallyLinearEmbedding",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Multidimensional scaling\n\n    Read more in the :ref:`User Guide <multidimensional_scaling>`.\n\n    Parameters\n    ----------\n    n_components : int, optional, default: 2\n        Number of dimensions in which to immerse the dissimilarities.\n\n    metric : boolean, optional, default: True\n        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.\n\n    n_init : int, optional, default: 4\n        Number of times the SMACOF algorithm will be run with different\n        initializations. The final results will be the best output of the runs,\n        determined by the run with the smallest final stress.\n\n    max_iter : int, optional, default: 300\n        Maximum number of iterations of the SMACOF algorithm for a single run.\n\n    verbose : int, optional, default: 0\n        Level of verbosity.\n\n    eps : float, optional, default: 1e-3\n        Relative tolerance with respect to stress at which to declare\n        convergence.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. If multiple\n        initializations are used (``n_init``), each run of the algorithm is\n        computed in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        The generator used to initialize the centers.  If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    dissimilarity : 'euclidean' | 'precomputed', optional, default: 'euclidean'\n        Dissimilarity measure to use:\n\n        - 'euclidean':\n            Pairwise Euclidean distances between points in the dataset.\n\n        - 'precomputed':\n            Pre-computed dissimilarities are passed directly to ``fit`` and\n            ``fit_transform``.\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape (n_samples, n_components)\n        Stores the position of the dataset in the embedding space.\n\n    stress_ : float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points).\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import MDS\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = MDS(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n    \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\n    Groenen P. Springer Series in Statistics (1997)\n\n    \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\n    Psychometrika, 29 (1964)\n\n    \"Multidimensional scaling by optimizing goodness of fit to a nonmetric\n    hypothesis\" Kruskal, J. Psychometrika, 29, (1964)\n\n    ",
        "klass": "sklearn.manifold.MDS",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Spectral embedding for non-linear dimensionality reduction.\n\n    Forms an affinity matrix given by the specified function and\n    applies spectral decomposition to the corresponding graph laplacian.\n    The resulting transformation is given by the value of the\n    eigenvectors for each data point.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    ----------\n    n_components : integer, default: 2\n        The dimension of the projected subspace.\n\n    affinity : string or callable, default : \"nearest_neighbors\"\n        How to construct the affinity matrix.\n         - 'nearest_neighbors' : construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf' : construct the affinity matrix by computing a radial basis\n           function (RBF) kernel.\n         - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n         - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n           of precomputed nearest neighbors, and constructs the affinity matrix\n           by selecting the ``n_neighbors`` nearest neighbors.\n         - callable : use passed in function as affinity\n           the function takes in data matrix (n_samples, n_features)\n           and return affinity matrix (n_samples, n_samples).\n\n    gamma : float, optional, default : 1/n_features\n        Kernel coefficient for rbf kernel.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors.  If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``solver`` ==\n        'amg'.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems.\n\n    n_neighbors : int, default : max(n_samples/10 , 1)\n        Number of nearest neighbors for nearest_neighbors graph building.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n\n    embedding_ : array, shape = (n_samples, n_components)\n        Spectral embedding of the training matrix.\n\n    affinity_matrix_ : array, shape = (n_samples, n_samples)\n        Affinity_matrix constructed from samples or precomputed.\n\n    n_neighbors_ : int\n        Number of nearest neighbors effectively used.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import SpectralEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = SpectralEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - On Spectral Clustering: Analysis and an algorithm, 2001\n      Andrew Y. Ng, Michael I. Jordan, Yair Weiss\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n    ",
        "klass": "sklearn.manifold.SpectralEmbedding",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "t-distributed Stochastic Neighbor Embedding.\n\n    t-SNE [1] is a tool to visualize high-dimensional data. It converts\n    similarities between data points to joint probabilities and tries\n    to minimize the Kullback-Leibler divergence between the joint\n    probabilities of the low-dimensional embedding and the\n    high-dimensional data. t-SNE has a cost function that is not convex,\n    i.e. with different initializations we can get different results.\n\n    It is highly recommended to use another dimensionality reduction\n    method (e.g. PCA for dense data or TruncatedSVD for sparse data)\n    to reduce the number of dimensions to a reasonable amount (e.g. 50)\n    if the number of features is very high. This will suppress some\n    noise and speed up the computation of pairwise distances between\n    samples. For more tips see Laurens van der Maaten's FAQ [2].\n\n    Read more in the :ref:`User Guide <t_sne>`.\n\n    Parameters\n    ----------\n    n_components : int, optional (default: 2)\n        Dimension of the embedded space.\n\n    perplexity : float, optional (default: 30)\n        The perplexity is related to the number of nearest neighbors that\n        is used in other manifold learning algorithms. Larger datasets\n        usually require a larger perplexity. Consider selecting a value\n        between 5 and 50. Different values can result in significanlty\n        different results.\n\n    early_exaggeration : float, optional (default: 12.0)\n        Controls how tight natural clusters in the original space are in\n        the embedded space and how much space will be between them. For\n        larger values, the space between natural clusters will be larger\n        in the embedded space. Again, the choice of this parameter is not\n        very critical. If the cost function increases during initial\n        optimization, the early exaggeration factor or the learning rate\n        might be too high.\n\n    learning_rate : float, optional (default: 200.0)\n        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If\n        the learning rate is too high, the data may look like a 'ball' with any\n        point approximately equidistant from its nearest neighbours. If the\n        learning rate is too low, most points may look compressed in a dense\n        cloud with few outliers. If the cost function gets stuck in a bad local\n        minimum increasing the learning rate may help.\n\n    n_iter : int, optional (default: 1000)\n        Maximum number of iterations for the optimization. Should be at\n        least 250.\n\n    n_iter_without_progress : int, optional (default: 300)\n        Maximum number of iterations without progress before we abort the\n        optimization, used after 250 initial iterations with early\n        exaggeration. Note that progress is only checked every 50 iterations so\n        this value is rounded to the next multiple of 50.\n\n        .. versionadded:: 0.17\n           parameter *n_iter_without_progress* to control stopping criteria.\n\n    min_grad_norm : float, optional (default: 1e-7)\n        If the gradient norm is below this threshold, the optimization will\n        be stopped.\n\n    metric : string or callable, optional\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string, it must be one of the options\n        allowed by scipy.spatial.distance.pdist for its metric parameter, or\n        a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n        If metric is \"precomputed\", X is assumed to be a distance matrix.\n        Alternatively, if metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays from X as input and return a value indicating\n        the distance between them. The default is \"euclidean\" which is\n        interpreted as squared euclidean distance.\n\n    init : string or numpy array, optional (default: \"random\")\n        Initialization of embedding. Possible options are 'random', 'pca',\n        and a numpy array of shape (n_samples, n_components).\n        PCA initialization cannot be used with precomputed distances and is\n        usually more globally stable than random initialization.\n\n    verbose : int, optional (default: 0)\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, optional (default: None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.  Note that different initializations might result in\n        different local minima of the cost function.\n\n    method : string (default: 'barnes_hut')\n        By default the gradient calculation algorithm uses Barnes-Hut\n        approximation running in O(NlogN) time. method='exact'\n        will run on the slower, but exact, algorithm in O(N^2) time. The\n        exact algorithm should be used when nearest-neighbor errors need\n        to be better than 3%. However, the exact method cannot scale to\n        millions of examples.\n\n        .. versionadded:: 0.17\n           Approximate optimization *method* via the Barnes-Hut.\n\n    angle : float (default: 0.5)\n        Only used if method='barnes_hut'\n        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.\n        'angle' is the angular size (referred to as theta in [3]) of a distant\n        node as measured from a point. If this size is below 'angle' then it is\n        used as a summary node of all points contained within it.\n        This method is not very sensitive to changes in this parameter\n        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing\n        computation time and angle greater 0.8 has quickly increasing error.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search. This parameter\n        has no impact when ``metric=\"precomputed\"`` or\n        (``metric=\"euclidean\"`` and ``method=\"exact\"``).\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape (n_samples, n_components)\n        Stores the embedding vectors.\n\n    kl_divergence_ : float\n        Kullback-Leibler divergence after optimization.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from sklearn.manifold import TSNE\n    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    >>> X_embedded = TSNE(n_components=2).fit_transform(X)\n    >>> X_embedded.shape\n    (4, 2)\n\n    References\n    ----------\n\n    [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data\n        Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.\n\n    [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding\n        https://lvdmaaten.github.io/tsne/\n\n    [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.\n        Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n        https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf\n    ",
        "klass": "sklearn.manifold.TSNE",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.mixture._base.BaseMixture"
        ],
        "class_docstring": "Variational Bayesian estimation of a Gaussian mixture.\n\n    This class allows to infer an approximate posterior distribution over the\n    parameters of a Gaussian mixture distribution. The effective number of\n    components can be inferred from the data.\n\n    This class implements two types of prior for the weights distribution: a\n    finite mixture model with Dirichlet distribution and an infinite mixture\n    model with the Dirichlet Process. In practice Dirichlet Process inference\n    algorithm is approximated and uses a truncated distribution with a fixed\n    maximum number of components (called the Stick-breaking representation).\n    The number of components actually used almost always depends on the data.\n\n    .. versionadded:: 0.18\n\n    Read more in the :ref:`User Guide <bgmm>`.\n\n    Parameters\n    ----------\n    n_components : int, defaults to 1.\n        The number of mixture components. Depending on the data and the value\n        of the `weight_concentration_prior` the model can decide to not use\n        all the components by setting some component `weights_` to values very\n        close to zero. The number of effective components is therefore smaller\n        than n_components.\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'\n        String describing the type of covariance parameters to use.\n        Must be one of::\n\n            'full' (each component has its own general covariance matrix),\n            'tied' (all components share the same general covariance matrix),\n            'diag' (each component has its own diagonal covariance matrix),\n            'spherical' (each component has its own single variance).\n\n    tol : float, defaults to 1e-3.\n        The convergence threshold. EM iterations will stop when the\n        lower bound average gain on the likelihood (of the training data with\n        respect to the model) is below this threshold.\n\n    reg_covar : float, defaults to 1e-6.\n        Non-negative regularization added to the diagonal of covariance.\n        Allows to assure that the covariance matrices are all positive.\n\n    max_iter : int, defaults to 100.\n        The number of EM iterations to perform.\n\n    n_init : int, defaults to 1.\n        The number of initializations to perform. The result with the highest\n        lower bound value on the likelihood is kept.\n\n    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.\n        The method used to initialize the weights, the means and the\n        covariances.\n        Must be one of::\n\n            'kmeans' : responsibilities are initialized using kmeans.\n            'random' : responsibilities are initialized randomly.\n\n    weight_concentration_prior_type : str, defaults to 'dirichlet_process'.\n        String describing the type of the weight concentration prior.\n        Must be one of::\n\n            'dirichlet_process' (using the Stick-breaking representation),\n            'dirichlet_distribution' (can favor more uniform weights).\n\n    weight_concentration_prior : float | None, optional.\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). This is commonly called gamma in the\n        literature. The higher concentration puts more mass in\n        the center and will lead to more components being active, while a lower\n        concentration parameter will lead to more mass at the edge of the\n        mixture weights simplex. The value of the parameter must be greater\n        than 0. If it is None, it's set to ``1. / n_components``.\n\n    mean_precision_prior : float | None, optional.\n        The precision prior on the mean distribution (Gaussian).\n        Controls the extent of where means can be placed. Larger\n        values concentrate the cluster means around `mean_prior`.\n        The value of the parameter must be greater than 0.\n        If it is None, it is set to 1.\n\n    mean_prior : array-like, shape (n_features,), optional\n        The prior on the mean distribution (Gaussian).\n        If it is None, it is set to the mean of X.\n\n    degrees_of_freedom_prior : float | None, optional.\n        The prior of the number of degrees of freedom on the covariance\n        distributions (Wishart). If it is None, it's set to `n_features`.\n\n    covariance_prior : float or array-like, optional\n        The prior on the covariance distribution (Wishart).\n        If it is None, the emiprical covariance prior is initialized using the\n        covariance of X. The shape depends on `covariance_type`::\n\n                (n_features, n_features) if 'full',\n                (n_features, n_features) if 'tied',\n                (n_features)             if 'diag',\n                float                    if 'spherical'\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, default to False.\n        If 'warm_start' is True, the solution of the last fitting is used as\n        initialization for the next call of fit(). This can speed up\n        convergence when fit is called several times on similar problems.\n        See :term:`the Glossary <warm_start>`.\n\n    verbose : int, default to 0.\n        Enable verbose output. If 1 then it prints the current\n        initialization and each iteration step. If greater than 1 then\n        it prints also the log probability and the time needed\n        for each step.\n\n    verbose_interval : int, default to 10.\n        Number of iteration done before the next print.\n\n    Attributes\n    ----------\n    weights_ : array-like, shape (n_components,)\n        The weights of each mixture components.\n\n    means_ : array-like, shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the likelihood (of the training data with\n        respect to the model) of the best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n\n        The higher concentration puts more mass in\n        the center and will lead to more components being active, while a lower\n        concentration parameter will lead to more mass at the edge of the\n        simplex.\n\n    weight_concentration_ : array-like, shape (n_components,)\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet).\n\n    mean_precision_prior_ : float\n        The precision prior on the mean distribution (Gaussian).\n        Controls the extent of where means can be placed.\n        Larger values concentrate the cluster means around `mean_prior`.\n        If mean_precision_prior is set to None, `mean_precision_prior_` is set\n        to 1.\n\n    mean_precision_ : array-like, shape (n_components,)\n        The precision of each components on the mean distribution (Gaussian).\n\n    mean_prior_ : array-like, shape (n_features,)\n        The prior on the mean distribution (Gaussian).\n\n    degrees_of_freedom_prior_ : float\n        The prior of the number of degrees of freedom on the covariance\n        distributions (Wishart).\n\n    degrees_of_freedom_ : array-like, shape (n_components,)\n        The number of degrees of freedom of each components in the model.\n\n    covariance_prior_ : float or array-like\n        The prior on the covariance distribution (Wishart).\n        The shape depends on `covariance_type`::\n\n            (n_features, n_features) if 'full',\n            (n_features, n_features) if 'tied',\n            (n_features)             if 'diag',\n            float                    if 'spherical'\n\n    See Also\n    --------\n    GaussianMixture : Finite Gaussian mixture fit with EM.\n\n    References\n    ----------\n\n    .. [1] `Bishop, Christopher M. (2006). \"Pattern recognition and machine\n       learning\". Vol. 4 No. 4. New York: Springer.\n       <https://www.springer.com/kr/book/9780387310732>`_\n\n    .. [2] `Hagai Attias. (2000). \"A Variational Bayesian Framework for\n       Graphical Models\". In Advances in Neural Information Processing\n       Systems 12.\n       <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&rep=rep1&type=pdf>`_\n\n    .. [3] `Blei, David M. and Michael I. Jordan. (2006). \"Variational\n       inference for Dirichlet process mixtures\". Bayesian analysis 1.1\n       <https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>`_\n    ",
        "klass": "sklearn.mixture.BayesianGaussianMixture",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.mixture._base.BaseMixture"
        ],
        "class_docstring": "Gaussian Mixture.\n\n    Representation of a Gaussian mixture model probability distribution.\n    This class allows to estimate the parameters of a Gaussian mixture\n    distribution.\n\n    Read more in the :ref:`User Guide <gmm>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    n_components : int, defaults to 1.\n        The number of mixture components.\n\n    covariance_type : {'full' (default), 'tied', 'diag', 'spherical'}\n        String describing the type of covariance parameters to use.\n        Must be one of:\n\n        'full'\n            each component has its own general covariance matrix\n        'tied'\n            all components share the same general covariance matrix\n        'diag'\n            each component has its own diagonal covariance matrix\n        'spherical'\n            each component has its own single variance\n\n    tol : float, defaults to 1e-3.\n        The convergence threshold. EM iterations will stop when the\n        lower bound average gain is below this threshold.\n\n    reg_covar : float, defaults to 1e-6.\n        Non-negative regularization added to the diagonal of covariance.\n        Allows to assure that the covariance matrices are all positive.\n\n    max_iter : int, defaults to 100.\n        The number of EM iterations to perform.\n\n    n_init : int, defaults to 1.\n        The number of initializations to perform. The best results are kept.\n\n    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.\n        The method used to initialize the weights, the means and the\n        precisions.\n        Must be one of::\n\n            'kmeans' : responsibilities are initialized using kmeans.\n            'random' : responsibilities are initialized randomly.\n\n    weights_init : array-like, shape (n_components, ), optional\n        The user-provided initial weights, defaults to None.\n        If it None, weights are initialized using the `init_params` method.\n\n    means_init : array-like, shape (n_components, n_features), optional\n        The user-provided initial means, defaults to None,\n        If it None, means are initialized using the `init_params` method.\n\n    precisions_init : array-like, optional.\n        The user-provided initial precisions (inverse of the covariance\n        matrices), defaults to None.\n        If it None, precisions are initialized using the 'init_params' method.\n        The shape depends on 'covariance_type'::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    warm_start : bool, default to False.\n        If 'warm_start' is True, the solution of the last fitting is used as\n        initialization for the next call of fit(). This can speed up\n        convergence when fit is called several times on similar problems.\n        In that case, 'n_init' is ignored and only a single initialization\n        occurs upon the first call.\n        See :term:`the Glossary <warm_start>`.\n\n    verbose : int, default to 0.\n        Enable verbose output. If 1 then it prints the current\n        initialization and each iteration step. If greater than 1 then\n        it prints also the log probability and the time needed\n        for each step.\n\n    verbose_interval : int, default to 10.\n        Number of iteration done before the next print.\n\n    Attributes\n    ----------\n    weights_ : array-like, shape (n_components,)\n        The weights of each mixture components.\n\n    means_ : array-like, shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of EM to reach the convergence.\n\n    lower_bound_ : float\n        Lower bound value on the log-likelihood (of the training data with\n        respect to the model) of the best fit of EM.\n\n    See Also\n    --------\n    BayesianGaussianMixture : Gaussian mixture model fit with a variational\n        inference.\n    ",
        "klass": "sklearn.mixture.GaussianMixture",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.model_selection._search.BaseSearchCV"
        ],
        "class_docstring": "Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"predict\", \"predict_proba\", \"decision_function\",\n    \"transform\" and \"inverse_transform\" if they are implemented in the\n    estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (string) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    iid : boolean, default=False\n        If True, return the average score across folds, weighted by the number\n        of samples in each test set. In this case, the data is assumed to be\n        identically distributed across the folds, and the loss minimized is\n        the total loss per sample, and not the mean loss across the folds.\n\n        .. deprecated:: 0.22\n            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    refit : boolean, string, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a string denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_parameters_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    verbose : integer\n        Controls the verbosity: the higher, the more messages.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is ``np.nan``.\n\n    return_train_score : boolean, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    See Also\n    ---------\n    :class:`ParameterGrid`:\n        generates all the combinations of a hyperparameter grid.\n\n    :func:`sklearn.model_selection.train_test_split`:\n        utility function to split the data into a development set usable\n        for fitting a GridSearchCV instance and an evaluation set for\n        its final evaluation.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    ",
        "klass": "sklearn.model_selection.GridSearchCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.model_selection._search.BaseSearchCV"
        ],
        "class_docstring": "Randomized search on hyper parameters.\n\n    RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"predict\", \"predict_proba\", \"decision_function\",\n    \"transform\" and \"inverse_transform\" if they are implemented in the\n    estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated search over parameter settings.\n\n    In contrast to GridSearchCV, not all parameter values are tried out, but\n    rather a fixed number of parameter settings is sampled from the specified\n    distributions. The number of parameter settings that are tried is\n    given by n_iter.\n\n    If all parameters are presented as a list,\n    sampling without replacement is performed. If at least one parameter\n    is given as a distribution, sampling with replacement is used.\n    It is highly recommended to use continuous distributions for continuous\n    parameters.\n\n    Read more in the :ref:`User Guide <randomized_parameter_search>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : estimator object.\n        A object of that type is instantiated for each grid point.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_distributions : dict or list of dicts\n        Dictionary with parameters names (string) as keys and distributions\n        or lists of parameters to try. Distributions must provide a ``rvs``\n        method for sampling (such as those from scipy.stats.distributions).\n        If a list is given, it is sampled uniformly.\n        If a list of dicts is given, first a dict is sampled uniformly, and\n        then a parameter is sampled using that dict as above.\n\n    n_iter : int, default=10\n        Number of parameter settings that are sampled. n_iter trades\n        off runtime vs quality of the solution.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    iid : boolean, default=False\n        If True, return the average score across folds, weighted by the number\n        of samples in each test set. In this case, the data is assumed to be\n        identically distributed across the folds, and the loss minimized is\n        the total loss per sample, and not the mean loss across the folds.\n\n        .. deprecated:: 0.22\n            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    refit : boolean, string, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a string denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given the ``cv_results``. In that\n        case, the ``best_estimator_`` and ``best_parameters_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``RandomizedSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    verbose : integer\n        Controls the verbosity: the higher, the more messages.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        Pseudo random number generator state used for random uniform sampling\n        from lists of possible values instead of scipy.stats distributions.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is ``np.nan``.\n\n    return_train_score : boolean, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +--------------+-------------+-------------------+---+---------------+\n        | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n        +==============+=============+===================+===+===============+\n        |    'rbf'     |     0.1     |       0.80        |...|       2       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.2     |       0.90        |...|       1       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.3     |       0.70        |...|       1       |\n        +--------------+-------------+-------------------+---+---------------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n                                          mask = False),\n            'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n            'split0_test_score'  : [0.80, 0.90, 0.70],\n            'split1_test_score'  : [0.82, 0.50, 0.70],\n            'mean_test_score'    : [0.81, 0.70, 0.70],\n            'std_test_score'     : [0.01, 0.20, 0.00],\n            'rank_test_score'    : [3, 1, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70],\n            'split1_train_score' : [0.82, 0.55, 0.70],\n            'mean_train_score'   : [0.81, 0.74, 0.70],\n            'std_train_score'    : [0.01, 0.19, 0.00],\n            'mean_fit_time'      : [0.73, 0.63, 0.43],\n            'std_fit_time'       : [0.01, 0.02, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00],\n            'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        For multi-metric evaluation, this attribute is present only if\n        ``refit`` is specified.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the held-out\n    data, according to the scoring parameter.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    parameter setting(and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    See Also\n    --------\n    :class:`GridSearchCV`:\n        Does exhaustive search over a grid of parameters.\n\n    :class:`ParameterSampler`:\n        A generator over parameter settings, constructed from\n        param_distributions.\n\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.model_selection import RandomizedSearchCV\n    >>> from scipy.stats import uniform\n    >>> iris = load_iris()\n    >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n    ...                               random_state=0)\n    >>> distributions = dict(C=uniform(loc=0, scale=4),\n    ...                      penalty=['l2', 'l1'])\n    >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n    >>> search = clf.fit(iris.data, iris.target)\n    >>> search.best_params_\n    {'C': 2..., 'penalty': 'l1'}\n    ",
        "klass": "sklearn.model_selection.RandomizedSearchCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.model_selection._split._BaseKFold"
        ],
        "class_docstring": "Stratified K-Folds cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a variation of KFold that returns\n    stratified folds. The folds are made by preserving the percentage of\n    samples for each class.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    shuffle : boolean, optional\n        Whether to shuffle each class's samples before splitting into batches.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Only used when ``shuffle`` is True. This should be left\n        to None if ``shuffle`` is False.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> skf = StratifiedKFold(n_splits=2)\n    >>> skf.get_n_splits(X, y)\n    2\n    >>> print(skf)\n    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for train_index, test_index in skf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    The implementation is designed to:\n\n    * Generate test sets such that all contain the same distribution of\n      classes, or as close as possible.\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Preserve order dependencies in the dataset ordering, when\n      ``shuffle=False``: all samples from class k in some test set were\n      contiguous in y, or separated in y by samples from classes other than k.\n    * Generate test sets where the smallest and largest differ by at most one\n      sample.\n\n    .. versionchanged:: 0.22\n        The previous implementation did not follow the last constraint.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    ",
        "klass": "sklearn.model_selection.StratifiedKFold",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "One-vs-one multiclass strategy\n\n    This strategy consists in fitting one classifier per class pair.\n    At prediction time, the class which received the most votes is selected.\n    Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,\n    this method is usually slower than one-vs-the-rest, due to its\n    O(n_classes^2) complexity. However, this method may be advantageous for\n    algorithms such as kernel algorithms which don't scale well with\n    `n_samples`. This is because each individual learning problem only involves\n    a small subset of the data whereas, with one-vs-the-rest, the complete\n    dataset is used `n_classes` times.\n\n    Read more in the :ref:`User Guide <ovo_classification>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An estimator object implementing :term:`fit` and one of\n        :term:`decision_function` or :term:`predict_proba`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    estimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators\n        Estimators used for predictions.\n\n    classes_ : numpy array of shape [n_classes]\n        Array containing labels.\n\n    n_classes_ : int\n        Number of classes\n\n    pairwise_indices_ : list, length = ``len(estimators_)``, or ``None``\n        Indices of samples used when training the estimators.\n        ``None`` when ``estimator`` does not have ``_pairwise`` attribute.\n    ",
        "klass": "sklearn.multiclass.OneVsOneClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MultiOutputMixin",
            "sklearn.base.ClassifierMixin",
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "One-vs-the-rest (OvR) multiclass/multilabel strategy\n\n    Also known as one-vs-all, this strategy consists in fitting one classifier\n    per class. For each classifier, the class is fitted against all the other\n    classes. In addition to its computational efficiency (only `n_classes`\n    classifiers are needed), one advantage of this approach is its\n    interpretability. Since each class is represented by one and one classifier\n    only, it is possible to gain knowledge about the class by inspecting its\n    corresponding classifier. This is the most commonly used strategy for\n    multiclass classification and is a fair default choice.\n\n    This strategy can also be used for multilabel learning, where a classifier\n    is used to predict multiple labels for instance, by fitting on a 2-d matrix\n    in which cell [i, j] is 1 if sample i has label j and 0 otherwise.\n\n    In the multilabel learning literature, OvR is also known as the binary\n    relevance method.\n\n    Read more in the :ref:`User Guide <ovr_classification>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An estimator object implementing :term:`fit` and one of\n        :term:`decision_function` or :term:`predict_proba`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    estimators_ : list of `n_classes` estimators\n        Estimators used for predictions.\n\n    classes_ : array, shape = [`n_classes`]\n        Class labels.\n\n    n_classes_ : int\n        Number of classes.\n\n    label_binarizer_ : LabelBinarizer object\n        Object used to transform multiclass labels to binary labels and\n        vice-versa.\n\n    multilabel_ : boolean\n        Whether a OneVsRestClassifier is a multilabel classifier.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.multiclass import OneVsRestClassifier\n    >>> from sklearn.svm import SVC\n    >>> X = np.array([\n    ...     [10, 10],\n    ...     [8, 10],\n    ...     [-5, 5.5],\n    ...     [-5.4, 5.5],\n    ...     [-20, -20],\n    ...     [-15, -20]\n    ... ])\n    >>> y = np.array([0, 0, 1, 1, 2, 2])\n    >>> clf = OneVsRestClassifier(SVC()).fit(X, y)\n    >>> clf.predict([[-19, -20], [9, 9], [-5, 5]])\n    array([2, 0, 1])\n\n    ",
        "klass": "sklearn.multiclass.OneVsRestClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "(Error-Correcting) Output-Code multiclass strategy\n\n    Output-code based strategies consist in representing each class with a\n    binary code (an array of 0s and 1s). At fitting time, one binary\n    classifier per bit in the code book is fitted.  At prediction time, the\n    classifiers are used to project new points in the class space and the class\n    closest to the points is chosen. The main advantage of these strategies is\n    that the number of classifiers used can be controlled by the user, either\n    for compressing the model (0 < code_size < 1) or for making the model more\n    robust to errors (code_size > 1). See the documentation for more details.\n\n    Read more in the :ref:`User Guide <ecoc>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An estimator object implementing :term:`fit` and one of\n        :term:`decision_function` or :term:`predict_proba`.\n\n    code_size : float\n        Percentage of the number of classes to be used to create the code book.\n        A number between 0 and 1 will require fewer classifiers than\n        one-vs-the-rest. A number greater than 1 will require more classifiers\n        than one-vs-the-rest.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        The generator used to initialize the codebook.  If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    estimators_ : list of `int(n_classes * code_size)` estimators\n        Estimators used for predictions.\n\n    classes_ : numpy array of shape [n_classes]\n        Array containing labels.\n\n    code_book_ : numpy array of shape [n_classes, code_size]\n        Binary array containing the code of each class.\n\n    Examples\n    --------\n    >>> from sklearn.multiclass import OutputCodeClassifier\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=100, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = OutputCodeClassifier(\n    ...     estimator=RandomForestClassifier(random_state=0),\n    ...     random_state=0).fit(X, y)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n\n    References\n    ----------\n\n    .. [1] \"Solving multiclass learning problems via error-correcting output\n       codes\",\n       Dietterich T., Bakiri G.,\n       Journal of Artificial Intelligence Research 2,\n       1995.\n\n    .. [2] \"The error coding method and PICTs\",\n       James G., Hastie T.,\n       Journal of Computational and Graphical statistics 7,\n       1998.\n\n    .. [3] \"The Elements of Statistical Learning\",\n       Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)\n       2008.\n    ",
        "klass": "sklearn.multiclass.OutputCodeClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.multioutput._MultiOutputEstimator"
        ],
        "class_docstring": "Multi target classification\n\n    This strategy consists of fitting one classifier per target. This is a\n    simple strategy for extending classifiers that do not natively support\n    multi-target classification\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An estimator object implementing :term:`fit`, :term:`score` and\n        :term:`predict_proba`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation.\n        It does each target variable in y in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    estimators_ : list of ``n_output`` estimators\n        Estimators used for predictions.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> from sklearn.multioutput import MultiOutputClassifier\n    >>> from sklearn.neighbors import KNeighborsClassifier\n\n    >>> X, y = make_multilabel_classification(n_classes=3, random_state=0)\n    >>> clf = MultiOutputClassifier(KNeighborsClassifier()).fit(X, y)\n    >>> clf.predict(X[-2:])\n    array([[1, 1, 0], [1, 1, 1]])\n    ",
        "klass": "sklearn.multioutput.MultiOutputClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.multioutput._MultiOutputEstimator"
        ],
        "class_docstring": "Multi target regression\n\n    This strategy consists of fitting one regressor per target. This is a\n    simple strategy for extending regressors that do not natively support\n    multi-target regression.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An estimator object implementing :term:`fit` and :term:`predict`.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for :meth:`fit`.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        When individual estimators are fast to train or predict\n        using `n_jobs>1` can result in slower performance due\n        to the overhead of spawning processes.\n\n    Attributes\n    ----------\n    estimators_ : list of ``n_output`` estimators\n        Estimators used for predictions.\n    ",
        "klass": "sklearn.multioutput.MultiOutputRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.naive_bayes._BaseDiscreteNB"
        ],
        "class_docstring": "Naive Bayes classifier for multivariate Bernoulli models.\n\n    Like MultinomialNB, this classifier is suitable for discrete data. The\n    difference is that while MultinomialNB works with occurrence counts,\n    BernoulliNB is designed for binary/boolean features.\n\n    Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n\n    Parameters\n    ----------\n    alpha : float, optional (default=1.0)\n        Additive (Laplace/Lidstone) smoothing parameter\n        (0 for no smoothing).\n\n    binarize : float or None, optional (default=0.0)\n        Threshold for binarizing (mapping to booleans) of sample features.\n        If None, input is presumed to already consist of binary vectors.\n\n    fit_prior : bool, optional (default=True)\n        Whether to learn class prior probabilities or not.\n        If false, a uniform prior will be used.\n\n    class_prior : array-like, size=[n_classes,], optional (default=None)\n        Prior probabilities of the classes. If specified the priors are not\n        adjusted according to the data.\n\n    Attributes\n    ----------\n    class_log_prior_ : array, shape = [n_classes]\n        Log probability of each class (smoothed).\n\n    feature_log_prob_ : array, shape = [n_classes, n_features]\n        Empirical log probability of features given a class, P(x_i|y).\n\n    class_count_ : array, shape = [n_classes]\n        Number of samples encountered for each class during fitting. This\n        value is weighted by the sample weight when provided.\n\n    classes_ : array, shape (n_classes,)\n        Class labels known to the classifier\n\n    feature_count_ : array, shape = [n_classes, n_features]\n        Number of samples encountered for each (class, feature)\n        during fitting. This value is weighted by the sample weight when\n        provided.\n\n    n_features_ : int\n        Number of features of each sample.\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    See Also\n    ----------\n    MultinomialNB: The multinomial Naive Bayes classifier is         suitable for classification with discrete features.\n\n    References\n    ----------\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n    Information Retrieval. Cambridge University Press, pp. 234-265.\n    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n\n    A. McCallum and K. Nigam (1998). A comparison of event models for naive\n    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n    Text Categorization, pp. 41-48.\n\n    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(1)\n    >>> X = rng.randint(5, size=(6, 100))\n    >>> Y = np.array([1, 2, 3, 4, 4, 5])\n    >>> from sklearn.naive_bayes import BernoulliNB\n    >>> clf = BernoulliNB()\n    >>> clf.fit(X, Y)\n    BernoulliNB()\n    >>> print(clf.predict(X[2:3]))\n    [3]\n    ",
        "klass": "sklearn.naive_bayes.BernoulliNB",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.naive_bayes._BaseNB"
        ],
        "class_docstring": "\n    Gaussian Naive Bayes (GaussianNB)\n\n    Can perform online updates to model parameters via :meth:`partial_fit`.\n    For details on algorithm used to update feature means and variance online,\n    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n\n        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\n    Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n\n    Parameters\n    ----------\n    priors : array-like, shape (n_classes,)\n        Prior probabilities of the classes. If specified the priors are not\n        adjusted according to the data.\n\n    var_smoothing : float, optional (default=1e-9)\n        Portion of the largest variance of all features that is added to\n        variances for calculation stability.\n\n    Attributes\n    ----------\n    class_prior_ : array, shape (n_classes,)\n        probability of each class.\n\n    class_count_ : array, shape (n_classes,)\n        number of training samples observed in each class.\n\n    classes_ : array, shape (n_classes,)\n        class labels known to the classifier\n\n    theta_ : array, shape (n_classes, n_features)\n        mean of each feature per class\n\n    sigma_ : array, shape (n_classes, n_features)\n        variance of each feature per class\n\n    epsilon_ : float\n        absolute additive value to variances\n\n    classes_ : array-like, shape (n_classes,)\n        Unique class labels.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> Y = np.array([1, 1, 1, 2, 2, 2])\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> clf = GaussianNB()\n    >>> clf.fit(X, Y)\n    GaussianNB()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    >>> clf_pf = GaussianNB()\n    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n    GaussianNB()\n    >>> print(clf_pf.predict([[-0.8, -1]]))\n    [1]\n    ",
        "klass": "sklearn.naive_bayes.GaussianNB",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.naive_bayes._BaseDiscreteNB"
        ],
        "class_docstring": "\n    Naive Bayes classifier for multinomial models\n\n    The multinomial Naive Bayes classifier is suitable for classification with\n    discrete features (e.g., word counts for text classification). The\n    multinomial distribution normally requires integer feature counts. However,\n    in practice, fractional counts such as tf-idf may also work.\n\n    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n\n    Parameters\n    ----------\n    alpha : float, optional (default=1.0)\n        Additive (Laplace/Lidstone) smoothing parameter\n        (0 for no smoothing).\n\n    fit_prior : boolean, optional (default=True)\n        Whether to learn class prior probabilities or not.\n        If false, a uniform prior will be used.\n\n    class_prior : array-like, size (n_classes,), optional (default=None)\n        Prior probabilities of the classes. If specified the priors are not\n        adjusted according to the data.\n\n    Attributes\n    ----------\n    class_log_prior_ : array, shape (n_classes, )\n        Smoothed empirical log probability for each class.\n\n    intercept_ : array, shape (n_classes, )\n        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n        as a linear model.\n\n    feature_log_prob_ : array, shape (n_classes, n_features)\n        Empirical log probability of features\n        given a class, ``P(x_i|y)``.\n\n    coef_ : array, shape (n_classes, n_features)\n        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n        as a linear model.\n\n    class_count_ : array, shape (n_classes,)\n        Number of samples encountered for each class during fitting. This\n        value is weighted by the sample weight when provided.\n\n    classes_ : array, shape (n_classes,)\n        Class labels known to the classifier\n\n    feature_count_ : array, shape (n_classes, n_features)\n        Number of samples encountered for each (class, feature)\n        during fitting. This value is weighted by the sample weight when\n        provided.\n\n    n_features_ : int\n        Number of features of each sample.\n\n    classes_ : array-like, shape (n_classes,)\n        Unique class labels.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(1)\n    >>> X = rng.randint(5, size=(6, 100))\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> from sklearn.naive_bayes import MultinomialNB\n    >>> clf = MultinomialNB()\n    >>> clf.fit(X, y)\n    MultinomialNB()\n    >>> print(clf.predict(X[2:3]))\n    [3]\n\n    Notes\n    -----\n    For the rationale behind the names `coef_` and `intercept_`, i.e.\n    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n\n    References\n    ----------\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n    Information Retrieval. Cambridge University Press, pp. 234-265.\n    https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n    ",
        "klass": "sklearn.naive_bayes.MultinomialNB",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors._ball_tree.BinaryTree"
        ],
        "class_docstring": "BallTree for fast generalized N-point problems\n\nBallTree(X, leaf_size=40, metric='minkowski', \\**kwargs)\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive integer (default = 40)\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : string or DistanceMetric object\n    the distance metric to use for the tree.  Default='minkowski'\n    with p=2 (that is, a euclidean metric). See the documentation\n    of the DistanceMetric class for a list of available metrics.\n    ball_tree.valid_metrics gives a list of the metrics which\n    are valid for BallTree.\n\nAdditional keywords are passed to the distance metric class.\nNote: Callable functions in the metric parameter are NOT supported for KDTree\nand Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------\ndata : memory view\n    The training data\n\nExamples\n--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])\n\n",
        "klass": "sklearn.neighbors.BallTree",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors._kd_tree.BinaryTree"
        ],
        "class_docstring": "KDTree for fast generalized N-point problems\n\nKDTree(X, leaf_size=40, metric='minkowski', \\**kwargs)\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive integer (default = 40)\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : string or DistanceMetric object\n    the distance metric to use for the tree.  Default='minkowski'\n    with p=2 (that is, a euclidean metric). See the documentation\n    of the DistanceMetric class for a list of available metrics.\n    kd_tree.valid_metrics gives a list of the metrics which\n    are valid for KDTree.\n\nAdditional keywords are passed to the distance metric class.\nNote: Callable functions in the metric parameter are NOT supported for KDTree\nand Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------\ndata : memory view\n    The training data\n\nExamples\n--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])\n\n",
        "klass": "sklearn.neighbors.KDTree",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors._base.NeighborsBase",
            "sklearn.neighbors._base.KNeighborsMixin",
            "sklearn.neighbors._base.SupervisedIntegerMixin",
            "sklearn.base.ClassifierMixin"
        ],
        "class_docstring": "Classifier implementing the k-nearest neighbors vote.\n\n    Read more in the :ref:`User Guide <classification>`.\n\n    Parameters\n    ----------\n    n_neighbors : int, optional (default = 5)\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    weights : str or callable, optional (default = 'uniform')\n        weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : integer, optional (default = 2)\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : string or callable, default 'minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of the DistanceMetric class for a\n        list of available metrics.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, optional (default = None)\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,)\n        Class labels known to the classifier\n\n    effective_metric_ : string or callble\n        The distance metric used. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    outputs_2d_ : bool\n        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n        otherwise True.\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n    >>> neigh.fit(X, y)\n    KNeighborsClassifier(...)\n    >>> print(neigh.predict([[1.1]]))\n    [0]\n    >>> print(neigh.predict_proba([[0.9]]))\n    [[0.66666667 0.33333333]]\n\n    See also\n    --------\n    RadiusNeighborsClassifier\n    KNeighborsRegressor\n    RadiusNeighborsRegressor\n    NearestNeighbors\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances\n       but different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    ",
        "klass": "sklearn.neighbors.KNeighborsClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors._base.NeighborsBase",
            "sklearn.neighbors._base.KNeighborsMixin",
            "sklearn.neighbors._base.SupervisedFloatMixin",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Regression based on k-nearest neighbors.\n\n    The target is predicted by local interpolation of the targets\n    associated of the nearest neighbors in the training set.\n\n    Read more in the :ref:`User Guide <regression>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    n_neighbors : int, optional (default = 5)\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    weights : str or callable\n        weight function used in prediction.  Possible values:\n\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n\n        Uniform weights are used by default.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    p : integer, optional (default = 2)\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric : string or callable, default 'minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of the DistanceMetric class for a\n        list of available metrics.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, optional (default = None)\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n\n    Attributes\n    ----------\n    effective_metric_ : string or callable\n        The distance metric to use. It will be same as the `metric` parameter\n        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n        'minkowski' and `p` parameter set to 2.\n\n    effective_metric_params_ : dict\n        Additional keyword arguments for the metric function. For most metrics\n        will be same with `metric_params` parameter, but may also contain the\n        `p` parameter value if the `effective_metric_` attribute is set to\n        'minkowski'.\n\n    Examples\n    --------\n    >>> X = [[0], [1], [2], [3]]\n    >>> y = [0, 0, 1, 1]\n    >>> from sklearn.neighbors import KNeighborsRegressor\n    >>> neigh = KNeighborsRegressor(n_neighbors=2)\n    >>> neigh.fit(X, y)\n    KNeighborsRegressor(...)\n    >>> print(neigh.predict([[1.5]]))\n    [0.5]\n\n    See also\n    --------\n    NearestNeighbors\n    RadiusNeighborsRegressor\n    KNeighborsClassifier\n    RadiusNeighborsClassifier\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    .. warning::\n\n       Regarding the Nearest Neighbors algorithms, if it is found that two\n       neighbors, neighbor `k+1` and `k`, have identical distances but\n       different labels, the results will depend on the ordering of the\n       training data.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    ",
        "klass": "sklearn.neighbors.KNeighborsRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Kernel Density Estimation.\n\n    Read more in the :ref:`User Guide <kernel_density>`.\n\n    Parameters\n    ----------\n    bandwidth : float\n        The bandwidth of the kernel.\n\n    algorithm : str\n        The tree algorithm to use.  Valid options are\n        ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.\n\n    kernel : str\n        The kernel to use.  Valid kernels are\n        ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']\n        Default is 'gaussian'.\n\n    metric : str\n        The distance metric to use.  Note that not all metrics are\n        valid with all algorithms.  Refer to the documentation of\n        :class:`BallTree` and :class:`KDTree` for a description of\n        available algorithms.  Note that the normalization of the density\n        output is correct only for the Euclidean distance metric. Default\n        is 'euclidean'.\n\n    atol : float\n        The desired absolute tolerance of the result.  A larger tolerance will\n        generally lead to faster execution. Default is 0.\n\n    rtol : float\n        The desired relative tolerance of the result.  A larger tolerance will\n        generally lead to faster execution.  Default is 1E-8.\n\n    breadth_first : bool\n        If true (default), use a breadth-first approach to the problem.\n        Otherwise use a depth-first approach.\n\n    leaf_size : int\n        Specify the leaf size of the underlying tree.  See :class:`BallTree`\n        or :class:`KDTree` for details.  Default is 40.\n\n    metric_params : dict\n        Additional parameters to be passed to the tree for use with the\n        metric.  For more information, see the documentation of\n        :class:`BallTree` or :class:`KDTree`.\n\n    See Also\n    --------\n    sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n        problems.\n    sklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n        problems.\n\n    Examples\n    --------\n    Compute a gaussian kernel density estimate with a fixed bandwidth.\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n    >>> log_density = kde.score_samples(X[:3])\n    >>> log_density\n    array([-1.52955942, -1.51462041, -1.60244657])\n    ",
        "klass": "sklearn.neighbors.KernelDensity",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Nearest centroid classifier.\n\n    Each class is represented by its centroid, with test samples classified to\n    the class with the nearest centroid.\n\n    Read more in the :ref:`User Guide <nearest_centroid_classifier>`.\n\n    Parameters\n    ----------\n    metric : string, or callable\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        The centroids for the samples corresponding to each class is the point\n        from which the sum of the distances (according to the metric) of all\n        samples that belong to that particular class are minimized.\n        If the \"manhattan\" metric is provided, this centroid is the median and\n        for all other metrics, the centroid is now set to be the mean.\n\n    shrink_threshold : float, optional (default = None)\n        Threshold for shrinking centroids to remove features.\n\n    Attributes\n    ----------\n    centroids_ : array-like of shape (n_classes, n_features)\n        Centroid of each class.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors import NearestCentroid\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = NearestCentroid()\n    >>> clf.fit(X, y)\n    NearestCentroid()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier\n\n    Notes\n    -----\n    When used for text classification with tf-idf vectors, this classifier is\n    also known as the Rocchio classifier.\n\n    References\n    ----------\n    Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n    multiple cancer types by shrunken centroids of gene expression. Proceedings\n    of the National Academy of Sciences of the United States of America,\n    99(10), 6567-6572. The National Academy of Sciences.\n\n    ",
        "klass": "sklearn.neighbors.NearestCentroid",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors._base.NeighborsBase",
            "sklearn.neighbors._base.KNeighborsMixin",
            "sklearn.neighbors._base.RadiusNeighborsMixin",
            "sklearn.neighbors._base.UnsupervisedMixin"
        ],
        "class_docstring": "Unsupervised learner for implementing neighbor searches.\n\n    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    n_neighbors : int, optional (default = 5)\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    radius : float, optional (default = 1.0)\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : string or callable, default 'minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of the DistanceMetric class for a\n        list of available metrics.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    p : integer, optional (default = 2)\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, optional (default = None)\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    effective_metric_ : string\n        Metric used to compute distances to neighbors.\n\n    effective_metric_params_ : dict\n        Parameters for the metric used to compute distances to neighbors.\n\n    Examples\n    --------\n      >>> import numpy as np\n      >>> from sklearn.neighbors import NearestNeighbors\n      >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n\n      >>> neigh = NearestNeighbors(2, 0.4)\n      >>> neigh.fit(samples)\n      NearestNeighbors(...)\n\n      >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n      array([[2, 0]]...)\n\n      >>> nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)\n      >>> np.asarray(nbrs[0][0])\n      array(2)\n\n    See also\n    --------\n    KNeighborsClassifier\n    RadiusNeighborsClassifier\n    KNeighborsRegressor\n    RadiusNeighborsRegressor\n    BallTree\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    ",
        "klass": "sklearn.neighbors.NearestNeighbors",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Bernoulli Restricted Boltzmann Machine (RBM).\n\n    A Restricted Boltzmann Machine with binary visible units and\n    binary hidden units. Parameters are estimated using Stochastic Maximum\n    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n    [2].\n\n    The time complexity of this implementation is ``O(d ** 2)`` assuming\n    d ~ n_features ~ n_components.\n\n    Read more in the :ref:`User Guide <rbm>`.\n\n    Parameters\n    ----------\n    n_components : int, optional\n        Number of binary hidden units.\n\n    learning_rate : float, optional\n        The learning rate for weight updates. It is *highly* recommended\n        to tune this hyper-parameter. Reasonable values are in the\n        10**[0., -3.] range.\n\n    batch_size : int, optional\n        Number of examples per minibatch.\n\n    n_iter : int, optional\n        Number of iterations/sweeps over the training dataset to perform\n        during training.\n\n    verbose : int, optional\n        The verbosity level. The default, zero, means silent mode.\n\n    random_state : integer or RandomState, optional\n        A random number generator instance to define the state of the\n        random permutations generator. If an integer is given, it fixes the\n        seed. Defaults to the global numpy random number generator.\n\n    Attributes\n    ----------\n    intercept_hidden_ : array-like, shape (n_components,)\n        Biases of the hidden units.\n\n    intercept_visible_ : array-like, shape (n_features,)\n        Biases of the visible units.\n\n    components_ : array-like, shape (n_components, n_features)\n        Weight matrix, where n_features in the number of\n        visible units and n_components is the number of hidden units.\n\n    h_samples_ : array-like, shape (batch_size, n_components)\n        Hidden Activation sampled from the model distribution,\n        where batch_size in the number of examples per minibatch and\n        n_components is the number of hidden units.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from sklearn.neural_network import BernoulliRBM\n    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    >>> model = BernoulliRBM(n_components=2)\n    >>> model.fit(X)\n    BernoulliRBM(n_components=2)\n\n    References\n    ----------\n\n    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n        deep belief nets. Neural Computation 18, pp 1527-1554.\n        https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n\n    [2] Tieleman, T. Training Restricted Boltzmann Machines using\n        Approximations to the Likelihood Gradient. International Conference\n        on Machine Learning (ICML) 2008\n    ",
        "klass": "sklearn.neural_network.BernoulliRBM",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron"
        ],
        "class_docstring": "Multi-layer Perceptron classifier.\n\n    This model optimizes the log-loss function using LBFGS or stochastic\n    gradient descent.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n        The ith element represents the number of neurons in the ith\n        hidden layer.\n\n    activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n        Activation function for the hidden layer.\n\n        - 'identity', no-op activation, useful to implement linear bottleneck,\n          returns f(x) = x\n\n        - 'logistic', the logistic sigmoid function,\n          returns f(x) = 1 / (1 + exp(-x)).\n\n        - 'tanh', the hyperbolic tan function,\n          returns f(x) = tanh(x).\n\n        - 'relu', the rectified linear unit function,\n          returns f(x) = max(0, x)\n\n    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n        The solver for weight optimization.\n\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n        - 'sgd' refers to stochastic gradient descent.\n\n        - 'adam' refers to a stochastic gradient-based optimizer proposed\n          by Kingma, Diederik, and Jimmy Ba\n\n        Note: The default solver 'adam' works pretty well on relatively\n        large datasets (with thousands of training samples or more) in terms of\n        both training time and validation score.\n        For small datasets, however, 'lbfgs' can converge faster and perform\n        better.\n\n    alpha : float, optional, default 0.0001\n        L2 penalty (regularization term) parameter.\n\n    batch_size : int, optional, default 'auto'\n        Size of minibatches for stochastic optimizers.\n        If the solver is 'lbfgs', the classifier will not use minibatch.\n        When set to \"auto\", `batch_size=min(200, n_samples)`\n\n    learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n        Learning rate schedule for weight updates.\n\n        - 'constant' is a constant learning rate given by\n          'learning_rate_init'.\n\n        - 'invscaling' gradually decreases the learning rate at each\n          time step 't' using an inverse scaling exponent of 'power_t'.\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n        - 'adaptive' keeps the learning rate constant to\n          'learning_rate_init' as long as training loss keeps decreasing.\n          Each time two consecutive epochs fail to decrease training loss by at\n          least tol, or fail to increase validation score by at least tol if\n          'early_stopping' is on, the current learning rate is divided by 5.\n\n        Only used when ``solver='sgd'``.\n\n    learning_rate_init : double, optional, default 0.001\n        The initial learning rate used. It controls the step-size\n        in updating the weights. Only used when solver='sgd' or 'adam'.\n\n    power_t : double, optional, default 0.5\n        The exponent for inverse scaling learning rate.\n        It is used in updating effective learning rate when the learning_rate\n        is set to 'invscaling'. Only used when solver='sgd'.\n\n    max_iter : int, optional, default 200\n        Maximum number of iterations. The solver iterates until convergence\n        (determined by 'tol') or this number of iterations. For stochastic\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\n        (how many times each data point will be used), not the number of\n        gradient steps.\n\n    shuffle : bool, optional, default True\n        Whether to shuffle samples in each iteration. Only used when\n        solver='sgd' or 'adam'.\n\n    random_state : int, RandomState instance or None, optional, default None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    tol : float, optional, default 1e-4\n        Tolerance for the optimization. When the loss or score is not improving\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n        unless ``learning_rate`` is set to 'adaptive', convergence is\n        considered to be reached and training stops.\n\n    verbose : bool, optional, default False\n        Whether to print progress messages to stdout.\n\n    warm_start : bool, optional, default False\n        When set to True, reuse the solution of the previous\n        call to fit as initialization, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    momentum : float, default 0.9\n        Momentum for gradient descent update. Should be between 0 and 1. Only\n        used when solver='sgd'.\n\n    nesterovs_momentum : boolean, default True\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\n        momentum > 0.\n\n    early_stopping : bool, default False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to true, it will automatically set\n        aside 10% of training data as validation and terminate training when\n        validation score is not improving by at least tol for\n        ``n_iter_no_change`` consecutive epochs. The split is stratified,\n        except in a multilabel setting.\n        Only effective when solver='sgd' or 'adam'\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True\n\n    beta_1 : float, optional, default 0.9\n        Exponential decay rate for estimates of first moment vector in adam,\n        should be in [0, 1). Only used when solver='adam'\n\n    beta_2 : float, optional, default 0.999\n        Exponential decay rate for estimates of second moment vector in adam,\n        should be in [0, 1). Only used when solver='adam'\n\n    epsilon : float, optional, default 1e-8\n        Value for numerical stability in adam. Only used when solver='adam'\n\n    n_iter_no_change : int, optional, default 10\n        Maximum number of epochs to not meet ``tol`` improvement.\n        Only effective when solver='sgd' or 'adam'\n\n        .. versionadded:: 0.20\n\n    max_fun : int, optional, default 15000\n        Only used when solver='lbfgs'. Maximum number of loss function calls.\n        The solver iterates until convergence (determined by 'tol'), number\n        of iterations reaches max_iter, or this number of loss function calls.\n        Note that number of loss function calls will be greater than or equal\n        to the number of iterations for the `MLPClassifier`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    classes_ : array or list of array of shape (n_classes,)\n        Class labels for each output.\n\n    loss_ : float\n        The current loss computed with the loss function.\n\n    coefs_ : list, length n_layers - 1\n        The ith element in the list represents the weight matrix corresponding\n        to layer i.\n\n    intercepts_ : list, length n_layers - 1\n        The ith element in the list represents the bias vector corresponding to\n        layer i + 1.\n\n    n_iter_ : int,\n        The number of iterations the solver has ran.\n\n    n_layers_ : int\n        Number of layers.\n\n    n_outputs_ : int\n        Number of outputs.\n\n    out_activation_ : string\n        Name of the output activation function.\n\n    Notes\n    -----\n    MLPClassifier trains iteratively since at each time step\n    the partial derivatives of the loss function with respect to the model\n    parameters are computed to update the parameters.\n\n    It can also have a regularization term added to the loss function\n    that shrinks model parameters to prevent overfitting.\n\n    This implementation works with data represented as dense numpy arrays or\n    sparse scipy arrays of floating point values.\n\n    References\n    ----------\n    Hinton, Geoffrey E.\n        \"Connectionist learning procedures.\" Artificial intelligence 40.1\n        (1989): 185-234.\n\n    Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n        training deep feedforward neural networks.\" International Conference\n        on Artificial Intelligence and Statistics. 2010.\n\n    He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n        performance on imagenet classification.\" arXiv preprint\n        arXiv:1502.01852 (2015).\n\n    Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n        optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n    ",
        "klass": "sklearn.neural_network.MLPClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron"
        ],
        "class_docstring": "Multi-layer Perceptron regressor.\n\n    This model optimizes the squared-loss using LBFGS or stochastic gradient\n    descent.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n        The ith element represents the number of neurons in the ith\n        hidden layer.\n\n    activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n        Activation function for the hidden layer.\n\n        - 'identity', no-op activation, useful to implement linear bottleneck,\n          returns f(x) = x\n\n        - 'logistic', the logistic sigmoid function,\n          returns f(x) = 1 / (1 + exp(-x)).\n\n        - 'tanh', the hyperbolic tan function,\n          returns f(x) = tanh(x).\n\n        - 'relu', the rectified linear unit function,\n          returns f(x) = max(0, x)\n\n    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n        The solver for weight optimization.\n\n        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n\n        - 'sgd' refers to stochastic gradient descent.\n\n        - 'adam' refers to a stochastic gradient-based optimizer proposed by\n          Kingma, Diederik, and Jimmy Ba\n\n        Note: The default solver 'adam' works pretty well on relatively\n        large datasets (with thousands of training samples or more) in terms of\n        both training time and validation score.\n        For small datasets, however, 'lbfgs' can converge faster and perform\n        better.\n\n    alpha : float, optional, default 0.0001\n        L2 penalty (regularization term) parameter.\n\n    batch_size : int, optional, default 'auto'\n        Size of minibatches for stochastic optimizers.\n        If the solver is 'lbfgs', the classifier will not use minibatch.\n        When set to \"auto\", `batch_size=min(200, n_samples)`\n\n    learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n        Learning rate schedule for weight updates.\n\n        - 'constant' is a constant learning rate given by\n          'learning_rate_init'.\n\n        - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n          at each time step 't' using an inverse scaling exponent of 'power_t'.\n          effective_learning_rate = learning_rate_init / pow(t, power_t)\n\n        - 'adaptive' keeps the learning rate constant to\n          'learning_rate_init' as long as training loss keeps decreasing.\n          Each time two consecutive epochs fail to decrease training loss by at\n          least tol, or fail to increase validation score by at least tol if\n          'early_stopping' is on, the current learning rate is divided by 5.\n\n        Only used when solver='sgd'.\n\n    learning_rate_init : double, optional, default 0.001\n        The initial learning rate used. It controls the step-size\n        in updating the weights. Only used when solver='sgd' or 'adam'.\n\n    power_t : double, optional, default 0.5\n        The exponent for inverse scaling learning rate.\n        It is used in updating effective learning rate when the learning_rate\n        is set to 'invscaling'. Only used when solver='sgd'.\n\n    max_iter : int, optional, default 200\n        Maximum number of iterations. The solver iterates until convergence\n        (determined by 'tol') or this number of iterations. For stochastic\n        solvers ('sgd', 'adam'), note that this determines the number of epochs\n        (how many times each data point will be used), not the number of\n        gradient steps.\n\n    shuffle : bool, optional, default True\n        Whether to shuffle samples in each iteration. Only used when\n        solver='sgd' or 'adam'.\n\n    random_state : int, RandomState instance or None, optional, default None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    tol : float, optional, default 1e-4\n        Tolerance for the optimization. When the loss or score is not improving\n        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n        unless ``learning_rate`` is set to 'adaptive', convergence is\n        considered to be reached and training stops.\n\n    verbose : bool, optional, default False\n        Whether to print progress messages to stdout.\n\n    warm_start : bool, optional, default False\n        When set to True, reuse the solution of the previous\n        call to fit as initialization, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    momentum : float, default 0.9\n        Momentum for gradient descent update.  Should be between 0 and 1. Only\n        used when solver='sgd'.\n\n    nesterovs_momentum : boolean, default True\n        Whether to use Nesterov's momentum. Only used when solver='sgd' and\n        momentum > 0.\n\n    early_stopping : bool, default False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to true, it will automatically set\n        aside 10% of training data as validation and terminate training when\n        validation score is not improving by at least ``tol`` for\n        ``n_iter_no_change`` consecutive epochs.\n        Only effective when solver='sgd' or 'adam'\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True\n\n    beta_1 : float, optional, default 0.9\n        Exponential decay rate for estimates of first moment vector in adam,\n        should be in [0, 1). Only used when solver='adam'\n\n    beta_2 : float, optional, default 0.999\n        Exponential decay rate for estimates of second moment vector in adam,\n        should be in [0, 1). Only used when solver='adam'\n\n    epsilon : float, optional, default 1e-8\n        Value for numerical stability in adam. Only used when solver='adam'\n\n    n_iter_no_change : int, optional, default 10\n        Maximum number of epochs to not meet ``tol`` improvement.\n        Only effective when solver='sgd' or 'adam'\n\n        .. versionadded:: 0.20\n\n    max_fun : int, optional, default 15000\n        Only used when solver='lbfgs'. Maximum number of function calls.\n        The solver iterates until convergence (determined by 'tol'), number\n        of iterations reaches max_iter, or this number of function calls.\n        Note that number of function calls will be greater than or equal to\n        the number of iterations for the MLPRegressor.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    loss_ : float\n        The current loss computed with the loss function.\n\n    coefs_ : list, length n_layers - 1\n        The ith element in the list represents the weight matrix corresponding\n        to layer i.\n\n    intercepts_ : list, length n_layers - 1\n        The ith element in the list represents the bias vector corresponding to\n        layer i + 1.\n\n    n_iter_ : int,\n        The number of iterations the solver has ran.\n\n    n_layers_ : int\n        Number of layers.\n\n    n_outputs_ : int\n        Number of outputs.\n\n    out_activation_ : string\n        Name of the output activation function.\n\n    Notes\n    -----\n    MLPRegressor trains iteratively since at each time step\n    the partial derivatives of the loss function with respect to the model\n    parameters are computed to update the parameters.\n\n    It can also have a regularization term added to the loss function\n    that shrinks model parameters to prevent overfitting.\n\n    This implementation works with data represented as dense and sparse numpy\n    arrays of floating point values.\n\n    References\n    ----------\n    Hinton, Geoffrey E.\n        \"Connectionist learning procedures.\" Artificial intelligence 40.1\n        (1989): 185-234.\n\n    Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n        training deep feedforward neural networks.\" International Conference\n        on Artificial Intelligence and Statistics. 2010.\n\n    He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n        performance on imagenet classification.\" arXiv preprint\n        arXiv:1502.01852 (2015).\n\n    Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n        optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n    ",
        "klass": "sklearn.neural_network.MLPRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.utils.metaestimators._BaseComposition"
        ],
        "class_docstring": "Concatenates results of multiple transformer objects.\n\n    This estimator applies a list of transformer objects in parallel to the\n    input data, then concatenates the results. This is useful to combine\n    several feature extraction mechanisms into a single transformer.\n\n    Parameters of the transformers may be set using its name and the parameter\n    name separated by a '__'. A transformer may be replaced entirely by\n    setting the parameter with its name to another transformer,\n    or removed by setting to 'drop'.\n\n    Read more in the :ref:`User Guide <feature_union>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    transformer_list : list of (string, transformer) tuples\n        List of transformer objects to be applied to the data. The first\n        half of each tuple is the name of the transformer.\n\n        .. versionchanged:: 0.22\n           Deprecated `None` as a transformer in favor of 'drop'.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    transformer_weights : dict, optional\n        Multiplicative weights for features per transformer.\n        Keys are transformer names, values the weights.\n\n    verbose : boolean, optional(default=False)\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    See Also\n    --------\n    sklearn.pipeline.make_union : Convenience function for simplified\n        feature union construction.\n\n    Examples\n    --------\n    >>> from sklearn.pipeline import FeatureUnion\n    >>> from sklearn.decomposition import PCA, TruncatedSVD\n    >>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n    ...                       (\"svd\", TruncatedSVD(n_components=2))])\n    >>> X = [[0., 1., 3], [2., 2., 5]]\n    >>> union.fit_transform(X)\n    array([[ 1.5       ,  3.0...,  0.8...],\n           [-1.5       ,  5.7..., -0.4...]])\n    ",
        "klass": "sklearn.pipeline.FeatureUnion",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Binarize data (set feature values to 0 or 1) according to a threshold\n\n    Values greater than the threshold map to 1, while values less than\n    or equal to the threshold map to 0. With the default threshold of 0,\n    only positive values map to 1.\n\n    Binarization is a common operation on text count data where the\n    analyst can decide to only consider the presence or absence of a\n    feature rather than a quantified number of occurrences for instance.\n\n    It can also be used as a pre-processing step for estimators that\n    consider boolean random variables (e.g. modelled using the Bernoulli\n    distribution in a Bayesian setting).\n\n    Read more in the :ref:`User Guide <preprocessing_binarization>`.\n\n    Parameters\n    ----------\n    threshold : float, optional (0.0 by default)\n        Feature values below or equal to this are replaced by 0, above it by 1.\n        Threshold may not be less than 0 for operations on sparse matrices.\n\n    copy : boolean, optional, default True\n        set to False to perform inplace binarization and avoid a copy (if\n        the input is already a numpy array or a scipy.sparse CSR matrix).\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import Binarizer\n    >>> X = [[ 1., -1.,  2.],\n    ...      [ 2.,  0.,  0.],\n    ...      [ 0.,  1., -1.]]\n    >>> transformer = Binarizer().fit(X)  # fit does nothing.\n    >>> transformer\n    Binarizer()\n    >>> transformer.transform(X)\n    array([[1., 0., 1.],\n           [1., 0., 0.],\n           [0., 1., 0.]])\n\n    Notes\n    -----\n    If the input is a sparse matrix, only the non-zero values are subject\n    to update by the Binarizer class.\n\n    This estimator is stateless (besides constructor parameters), the\n    fit method does nothing but is useful when used in a pipeline.\n\n    See also\n    --------\n    binarize: Equivalent function without the estimator API.\n    ",
        "klass": "sklearn.preprocessing.Binarizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Constructs a transformer from an arbitrary callable.\n\n    A FunctionTransformer forwards its X (and optionally y) arguments to a\n    user-defined function or function object and returns the result of this\n    function. This is useful for stateless transformations such as taking the\n    log of frequencies, doing custom scaling, etc.\n\n    Note: If a lambda is used as the function, then the resulting\n    transformer will not be pickleable.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <function_transformer>`.\n\n    Parameters\n    ----------\n    func : callable, optional default=None\n        The callable to use for the transformation. This will be passed\n        the same arguments as transform, with args and kwargs forwarded.\n        If func is None, then func will be the identity function.\n\n    inverse_func : callable, optional default=None\n        The callable to use for the inverse transformation. This will be\n        passed the same arguments as inverse transform, with args and\n        kwargs forwarded. If inverse_func is None, then inverse_func\n        will be the identity function.\n\n    validate : bool, optional default=False\n        Indicate that the input X array should be checked before calling\n        ``func``. The possibilities are:\n\n        - If False, there is no input validation.\n        - If True, then X will be converted to a 2-dimensional NumPy array or\n          sparse matrix. If the conversion is not possible an exception is\n          raised.\n\n        .. versionchanged:: 0.22\n           The default of ``validate`` changed from True to False.\n\n    accept_sparse : boolean, optional\n        Indicate that func accepts a sparse matrix as input. If validate is\n        False, this has no effect. Otherwise, if accept_sparse is false,\n        sparse matrix inputs will cause an exception to be raised.\n\n    check_inverse : bool, default=True\n       Whether to check that or ``func`` followed by ``inverse_func`` leads to\n       the original inputs. It can be used for a sanity check, raising a\n       warning when the condition is not fulfilled.\n\n       .. versionadded:: 0.20\n\n    kw_args : dict, optional\n        Dictionary of additional keyword arguments to pass to func.\n\n    inv_kw_args : dict, optional\n        Dictionary of additional keyword arguments to pass to inverse_func.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import FunctionTransformer\n    >>> transformer = FunctionTransformer(np.log1p)\n    >>> X = np.array([[0, 1], [2, 3]])\n    >>> transformer.transform(X)\n    array([[0.       , 0.6931...],\n           [1.0986..., 1.3862...]])\n    ",
        "klass": "sklearn.preprocessing.FunctionTransformer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Center a kernel matrix\n\n    Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a\n    function mapping x to a Hilbert space. KernelCenterer centers (i.e.,\n    normalize to have zero mean) the data without explicitly computing phi(x).\n    It is equivalent to centering phi(x) with\n    sklearn.preprocessing.StandardScaler(with_std=False).\n\n    Read more in the :ref:`User Guide <kernel_centering>`.\n\n    Attributes\n    ----------\n    K_fit_rows_ : array, shape (n_samples,)\n        Average of each column of kernel matrix\n\n    K_fit_all_ : float\n        Average of kernel matrix\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import KernelCenterer\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> X = [[ 1., -2.,  2.],\n    ...      [ -2.,  1.,  3.],\n    ...      [ 4.,  1., -2.]]\n    >>> K = pairwise_kernels(X, metric='linear')\n    >>> K\n    array([[  9.,   2.,  -2.],\n           [  2.,  14., -13.],\n           [ -2., -13.,  21.]])\n    >>> transformer = KernelCenterer().fit(K)\n    >>> transformer\n    KernelCenterer()\n    >>> transformer.transform(K)\n    array([[  5.,   0.,  -5.],\n           [  0.,  14., -14.],\n           [ -5., -14.,  19.]])\n    ",
        "klass": "sklearn.preprocessing.KernelCenterer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Binarize labels in a one-vs-all fashion\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    At learning time, this simply consists in learning one regressor\n    or binary classifier per class. In doing so, one needs to convert\n    multi-class labels to binary labels (belong or does not belong\n    to the class). LabelBinarizer makes this process easy with the\n    transform method.\n\n    At prediction time, one assigns the class for which the corresponding\n    model gave the greatest confidence. LabelBinarizer makes this easy\n    with the inverse_transform method.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    Parameters\n    ----------\n\n    neg_label : int (default: 0)\n        Value with which negative labels must be encoded.\n\n    pos_label : int (default: 1)\n        Value with which positive labels must be encoded.\n\n    sparse_output : boolean (default: False)\n        True if the returned array from transform is desired to be in sparse\n        CSR format.\n\n    Attributes\n    ----------\n\n    classes_ : array of shape [n_class]\n        Holds the label for each class.\n\n    y_type_ : str,\n        Represents the type of the target data as evaluated by\n        utils.multiclass.type_of_target. Possible type are 'continuous',\n        'continuous-multioutput', 'binary', 'multiclass',\n        'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n\n    sparse_input_ : boolean,\n        True if the input data to transform is given as a sparse matrix, False\n        otherwise.\n\n    Examples\n    --------\n    >>> from sklearn import preprocessing\n    >>> lb = preprocessing.LabelBinarizer()\n    >>> lb.fit([1, 2, 6, 4, 2])\n    LabelBinarizer()\n    >>> lb.classes_\n    array([1, 2, 4, 6])\n    >>> lb.transform([1, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    Binary targets transform to a column vector\n\n    >>> lb = preprocessing.LabelBinarizer()\n    >>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])\n\n    Passing a 2D matrix for multilabel classification\n\n    >>> import numpy as np\n    >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\n    LabelBinarizer()\n    >>> lb.classes_\n    array([0, 1, 2])\n    >>> lb.transform([0, 1, 2, 1])\n    array([[1, 0, 0],\n           [0, 1, 0],\n           [0, 0, 1],\n           [0, 1, 0]])\n\n    See also\n    --------\n    label_binarize : function to perform the transform operation of\n        LabelBinarizer with fixed classes.\n    sklearn.preprocessing.OneHotEncoder : encode categorical features\n        using a one-hot aka one-of-K scheme.\n    ",
        "klass": "sklearn.preprocessing.LabelBinarizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Encode target labels with value between 0 and n_classes-1.\n\n    This transformer should be used to encode target values, *i.e.* `y`, and\n    not the input `X`.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    .. versionadded:: 0.12\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_class,)\n        Holds the label for each class.\n\n    Examples\n    --------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn import preprocessing\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6])\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    ['amsterdam', 'paris', 'tokyo']\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    ['tokyo', 'tokyo', 'paris']\n\n    See also\n    --------\n    sklearn.preprocessing.OrdinalEncoder : Encode categorical features\n        using an ordinal encoding scheme.\n\n    sklearn.preprocessing.OneHotEncoder : Encode categorical features\n        as a one-hot numeric array.\n    ",
        "klass": "sklearn.preprocessing.LabelEncoder",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Scale each feature by its maximum absolute value.\n\n    This estimator scales and translates each feature individually such\n    that the maximal absolute value of each feature in the\n    training set will be 1.0. It does not shift/center the data, and\n    thus does not destroy any sparsity.\n\n    This scaler can also be applied to sparse CSR or CSC matrices.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    copy : boolean, optional, default is True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    Attributes\n    ----------\n    scale_ : ndarray, shape (n_features,)\n        Per feature relative scaling of the data.\n\n        .. versionadded:: 0.17\n           *scale_* attribute.\n\n    max_abs_ : ndarray, shape (n_features,)\n        Per feature maximum absolute value.\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator. Will be reset on\n        new calls to fit, but increments across ``partial_fit`` calls.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import MaxAbsScaler\n    >>> X = [[ 1., -1.,  2.],\n    ...      [ 2.,  0.,  0.],\n    ...      [ 0.,  1., -1.]]\n    >>> transformer = MaxAbsScaler().fit(X)\n    >>> transformer\n    MaxAbsScaler()\n    >>> transformer.transform(X)\n    array([[ 0.5, -1. ,  1. ],\n           [ 1. ,  0. ,  0. ],\n           [ 0. ,  1. , -0.5]])\n\n    See also\n    --------\n    maxabs_scale: Equivalent function without the estimator API.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",
        "klass": "sklearn.preprocessing.MaxAbsScaler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Transform features by scaling each feature to a given range.\n\n    This estimator scales and translates each feature individually such\n    that it is in the given range on the training set, e.g. between\n    zero and one.\n\n    The transformation is given by::\n\n        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n        X_scaled = X_std * (max - min) + min\n\n    where min, max = feature_range.\n\n    The transformation is calculated as::\n\n        X_scaled = scale * X + min - X.min(axis=0) * scale\n        where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n\n    This transformation is often used as an alternative to zero mean,\n    unit variance scaling.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    feature_range : tuple (min, max), default=(0, 1)\n        Desired range of transformed data.\n\n    copy : bool, default=True\n        Set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array).\n\n    Attributes\n    ----------\n    min_ : ndarray of shape (n_features,)\n        Per feature adjustment for minimum. Equivalent to\n        ``min - X.min(axis=0) * self.scale_``\n\n    scale_ : ndarray of shape (n_features,)\n        Per feature relative scaling of the data. Equivalent to\n        ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n\n        .. versionadded:: 0.17\n           *scale_* attribute.\n\n    data_min_ : ndarray of shape (n_features,)\n        Per feature minimum seen in the data\n\n        .. versionadded:: 0.17\n           *data_min_*\n\n    data_max_ : ndarray of shape (n_features,)\n        Per feature maximum seen in the data\n\n        .. versionadded:: 0.17\n           *data_max_*\n\n    data_range_ : ndarray of shape (n_features,)\n        Per feature range ``(data_max_ - data_min_)`` seen in the data\n\n        .. versionadded:: 0.17\n           *data_range_*\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator.\n        It will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import MinMaxScaler\n    >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n    >>> scaler = MinMaxScaler()\n    >>> print(scaler.fit(data))\n    MinMaxScaler()\n    >>> print(scaler.data_max_)\n    [ 1. 18.]\n    >>> print(scaler.transform(data))\n    [[0.   0.  ]\n     [0.25 0.25]\n     [0.5  0.5 ]\n     [1.   1.  ]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[1.5 0. ]]\n\n    See also\n    --------\n    minmax_scale: Equivalent function without the estimator API.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",
        "klass": "sklearn.preprocessing.MinMaxScaler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Transform between iterable of iterables and a multilabel format\n\n    Although a list of sets or tuples is a very intuitive format for multilabel\n    data, it is unwieldy to process. This transformer converts between this\n    intuitive format and the supported multilabel format: a (samples x classes)\n    binary matrix indicating the presence of a class label.\n\n    Parameters\n    ----------\n    classes : array-like of shape [n_classes] (optional)\n        Indicates an ordering for the class labels.\n        All entries should be unique (cannot contain duplicate classes).\n\n    sparse_output : boolean (default: False),\n        Set to true if output binary array is desired in CSR sparse format\n\n    Attributes\n    ----------\n    classes_ : array of labels\n        A copy of the `classes` parameter where provided,\n        or otherwise, the sorted set of classes found when fitting.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import MultiLabelBinarizer\n    >>> mlb = MultiLabelBinarizer()\n    >>> mlb.fit_transform([(1, 2), (3,)])\n    array([[1, 1, 0],\n           [0, 0, 1]])\n    >>> mlb.classes_\n    array([1, 2, 3])\n\n    >>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\n    array([[0, 1, 1],\n           [1, 0, 0]])\n    >>> list(mlb.classes_)\n    ['comedy', 'sci-fi', 'thriller']\n\n    A common mistake is to pass in a list, which leads to the following issue:\n\n    >>> mlb = MultiLabelBinarizer()\n    >>> mlb.fit(['sci-fi', 'thriller', 'comedy'])\n    MultiLabelBinarizer()\n    >>> mlb.classes_\n    array(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',\n        'y'], dtype=object)\n\n    To correct this, the list of labels should be passed in as:\n\n    >>> mlb = MultiLabelBinarizer()\n    >>> mlb.fit([['sci-fi', 'thriller', 'comedy']])\n    MultiLabelBinarizer()\n    >>> mlb.classes_\n    array(['comedy', 'sci-fi', 'thriller'], dtype=object)\n\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : encode categorical features\n        using a one-hot aka one-of-K scheme.\n    ",
        "klass": "sklearn.preprocessing.MultiLabelBinarizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Normalize samples individually to unit norm.\n\n    Each sample (i.e. each row of the data matrix) with at least one\n    non zero component is rescaled independently of other samples so\n    that its norm (l1 or l2) equals one.\n\n    This transformer is able to work both with dense numpy arrays and\n    scipy.sparse matrix (use CSR format if you want to avoid the burden of\n    a copy / conversion).\n\n    Scaling inputs to unit norms is a common operation for text\n    classification or clustering for instance. For instance the dot\n    product of two l2-normalized TF-IDF vectors is the cosine similarity\n    of the vectors and is the base similarity metric for the Vector\n    Space Model commonly used by the Information Retrieval community.\n\n    Read more in the :ref:`User Guide <preprocessing_normalization>`.\n\n    Parameters\n    ----------\n    norm : 'l1', 'l2', or 'max', optional ('l2' by default)\n        The norm to use to normalize each non zero sample.\n\n    copy : boolean, optional, default True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSR matrix).\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import Normalizer\n    >>> X = [[4, 1, 2, 2],\n    ...      [1, 3, 9, 3],\n    ...      [5, 7, 5, 1]]\n    >>> transformer = Normalizer().fit(X)  # fit does nothing.\n    >>> transformer\n    Normalizer()\n    >>> transformer.transform(X)\n    array([[0.8, 0.2, 0.4, 0.4],\n           [0.1, 0.3, 0.9, 0.3],\n           [0.5, 0.7, 0.5, 0.1]])\n\n    Notes\n    -----\n    This estimator is stateless (besides constructor parameters), the\n    fit method does nothing but is useful when used in a pipeline.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n\n    See also\n    --------\n    normalize: Equivalent function without the estimator API.\n    ",
        "klass": "sklearn.preprocessing.Normalizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.preprocessing._encoders._BaseEncoder"
        ],
        "class_docstring": "\n    Encode categorical features as a one-hot numeric array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n    encoding scheme. This creates a binary column for each category and\n    returns a sparse matrix or dense array (depending on the ``sparse``\n    parameter)\n\n    By default, the encoder derives the categories based on the unique values\n    in each feature. Alternatively, you can also specify the `categories`\n    manually.\n\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n\n    Note: a one-hot encoding of y labels should use a LabelBinarizer\n    instead.\n\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n    .. versionchanged:: 0.20\n\n    Parameters\n    ----------\n    categories : 'auto' or a list of array-like, default='auto'\n        Categories (unique values) per feature:\n\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories should not mix strings and numeric\n          values within a single feature, and should be sorted in case of\n          numeric values.\n\n        The used categories can be found in the ``categories_`` attribute.\n\n    drop : 'first' or a array-like of shape (n_features,), default=None\n        Specifies a methodology to use to drop one of the categories per\n        feature. This is useful in situations where perfectly collinear\n        features cause problems, such as when feeding the resulting data\n        into a neural network or an unregularized regression.\n\n        - None : retain all features (the default).\n        - 'first' : drop the first category in each feature. If only one\n          category is present, the feature will be dropped entirely.\n        - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n          should be dropped.\n\n    sparse : bool, default=True\n        Will return sparse matrix if set True else will return an array.\n\n    dtype : number type, default=np.float\n        Desired dtype of output.\n\n    handle_unknown : {'error', 'ignore'}, default='error'\n        Whether to raise an error or ignore if an unknown categorical feature\n        is present during transform (default is to raise). When this parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros. In the inverse transform, an unknown category\n        will be denoted as None.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order of the features in X and corresponding with the output\n        of ``transform``). This includes the category specified in ``drop``\n        (if any).\n\n    drop_idx_ : array of shape (n_features,)\n        ``drop_idx_[i]`` is\u00a0the index in ``categories_[i]`` of the category to\n        be dropped for each feature. None if all the transformed features will\n        be retained.\n\n    See Also\n    --------\n    sklearn.preprocessing.OrdinalEncoder : Performs an ordinal (integer)\n      encoding of the categorical features.\n    sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n      encoding of dictionary items or strings.\n    sklearn.preprocessing.LabelBinarizer : Binarizes labels in a one-vs-all\n      fashion.\n    sklearn.preprocessing.MultiLabelBinarizer : Transforms between iterable of\n      iterables and a multilabel format, e.g. a (samples x classes) binary\n      matrix indicating the presence of a class label.\n\n    Examples\n    --------\n    Given a dataset with two features, we let the encoder find the unique\n    values per feature and transform the data to a binary one-hot encoding.\n\n    >>> from sklearn.preprocessing import OneHotEncoder\n    >>> enc = OneHotEncoder(handle_unknown='ignore')\n    >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    >>> enc.fit(X)\n    OneHotEncoder(handle_unknown='ignore')\n    >>> enc.categories_\n    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n    >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n    array([[1., 0., 1., 0., 0.],\n           [0., 1., 0., 0., 0.]])\n    >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n    array([['Male', 1],\n           [None, 2]], dtype=object)\n    >>> enc.get_feature_names(['gender', 'group'])\n    array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n      dtype=object)\n    >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n    >>> drop_enc.categories_\n    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n    >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n    array([[0., 0., 0.],\n           [1., 1., 0.]])\n    ",
        "klass": "sklearn.preprocessing.OneHotEncoder",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Generate polynomial and interaction features.\n\n    Generate a new feature matrix consisting of all polynomial combinations\n    of the features with degree less than or equal to the specified degree.\n    For example, if an input sample is two dimensional and of the form\n    [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n\n    Parameters\n    ----------\n    degree : integer\n        The degree of the polynomial features. Default = 2.\n\n    interaction_only : boolean, default = False\n        If true, only interaction features are produced: features that are\n        products of at most ``degree`` *distinct* input features (so not\n        ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n\n    include_bias : boolean\n        If True (default), then include a bias column, the feature in which\n        all polynomial powers are zero (i.e. a column of ones - acts as an\n        intercept term in a linear model).\n\n    order : str in {'C', 'F'}, default 'C'\n        Order of output array in the dense case. 'F' order is faster to\n        compute, but may slow down subsequent estimators.\n\n        .. versionadded:: 0.21\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> X = np.arange(6).reshape(3, 2)\n    >>> X\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    >>> poly = PolynomialFeatures(2)\n    >>> poly.fit_transform(X)\n    array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n           [ 1.,  2.,  3.,  4.,  6.,  9.],\n           [ 1.,  4.,  5., 16., 20., 25.]])\n    >>> poly = PolynomialFeatures(interaction_only=True)\n    >>> poly.fit_transform(X)\n    array([[ 1.,  0.,  1.,  0.],\n           [ 1.,  2.,  3.,  6.],\n           [ 1.,  4.,  5., 20.]])\n\n    Attributes\n    ----------\n    powers_ : array, shape (n_output_features, n_input_features)\n        powers_[i, j] is the exponent of the jth input in the ith output.\n\n    n_input_features_ : int\n        The total number of input features.\n\n    n_output_features_ : int\n        The total number of polynomial output features. The number of output\n        features is computed by iterating over all suitably sized combinations\n        of input features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    polynomially in the number of features of the input array, and\n    exponentially in the degree. High degrees can cause overfitting.\n\n    See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n    <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n    ",
        "klass": "sklearn.preprocessing.PolynomialFeatures",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. Therefore, for a given feature, this transformation tends\n    to spread out the most frequent values. It also reduces the impact of\n    (marginal) outliers: this is therefore a robust preprocessing scheme.\n\n    The transformation is applied on each feature independently. First an\n    estimate of the cumulative distribution function of a feature is\n    used to map the original values to a uniform distribution. The obtained\n    values are then mapped to the desired output distribution using the\n    associated quantile function. Features values of new/unseen data that fall\n    below or above the fitted range will be mapped to the bounds of the output\n    distribution. Note that this transform is non-linear. It may distort linear\n    correlations between variables measured at the same scale but renders\n    variables measured at different scales more directly comparable.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    n_quantiles : int, optional (default=1000 or n_samples)\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n\n    output_distribution : str, optional (default='uniform')\n        Marginal distribution for the transformed data. The choices are\n        'uniform' (default) or 'normal'.\n\n    ignore_implicit_zeros : bool, optional (default=False)\n        Only applies to sparse matrices. If True, the sparse entries of the\n        matrix are discarded to compute the quantile statistics. If False,\n        these entries are treated as zeros.\n\n    subsample : int, optional (default=1e5)\n        Maximum number of samples used to estimate the quantiles for\n        computational efficiency. Note that the subsampling procedure may\n        differ for value-identical sparse and dense matrices.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by np.random. Note that this is used by subsampling and smoothing\n        noise.\n\n    copy : boolean, optional, (default=True)\n        Set to False to perform inplace transformation and avoid a copy (if the\n        input is already a numpy array).\n\n    Attributes\n    ----------\n    n_quantiles_ : integer\n        The actual number of quantiles used to discretize the cumulative\n        distribution function.\n\n    quantiles_ : ndarray, shape (n_quantiles, n_features)\n        The values corresponding the quantiles of reference.\n\n    references_ : ndarray, shape(n_quantiles, )\n        Quantiles of references.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import QuantileTransformer\n    >>> rng = np.random.RandomState(0)\n    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n    >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n    >>> qt.fit_transform(X)\n    array([...])\n\n    See also\n    --------\n    quantile_transform : Equivalent function without the estimator API.\n    PowerTransformer : Perform mapping to a normal distribution using a power\n        transform.\n    StandardScaler : Perform standardization that is faster, but less robust\n        to outliers.\n    RobustScaler : Perform robust standardization that removes the influence\n        of outliers but does not put outliers and inliers on the same scale.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",
        "klass": "sklearn.preprocessing.QuantileTransformer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Scale features using statistics that are robust to outliers.\n\n    This Scaler removes the median and scales the data according to\n    the quantile range (defaults to IQR: Interquartile Range).\n    The IQR is the range between the 1st quartile (25th quantile)\n    and the 3rd quartile (75th quantile).\n\n    Centering and scaling happen independently on each feature by\n    computing the relevant statistics on the samples in the training\n    set. Median and interquartile range are then stored to be used on\n    later data using the ``transform`` method.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators. Typically this is done by removing the mean\n    and scaling to unit variance. However, outliers can often influence the\n    sample mean / variance in a negative way. In such cases, the median and\n    the interquartile range often give better results.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    with_centering : boolean, True by default\n        If True, center the data before scaling.\n        This will cause ``transform`` to raise an exception when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_scaling : boolean, True by default\n        If True, scale the data to interquartile range.\n\n    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n        Quantile range used to calculate ``scale_``.\n\n        .. versionadded:: 0.18\n\n    copy : boolean, optional, default is True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    Attributes\n    ----------\n    center_ : array of floats\n        The median value for each feature in the training set.\n\n    scale_ : array of floats\n        The (scaled) interquartile range for each feature in the training set.\n\n        .. versionadded:: 0.17\n           *scale_* attribute.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import RobustScaler\n    >>> X = [[ 1., -2.,  2.],\n    ...      [ -2.,  1.,  3.],\n    ...      [ 4.,  1., -2.]]\n    >>> transformer = RobustScaler().fit(X)\n    >>> transformer\n    RobustScaler()\n    >>> transformer.transform(X)\n    array([[ 0. , -2. ,  0. ],\n           [-1. ,  0. ,  0.4],\n           [ 1. ,  0. , -1.6]])\n\n    See also\n    --------\n    robust_scale: Equivalent function without the estimator API.\n\n    :class:`sklearn.decomposition.PCA`\n        Further removes the linear correlation across features with\n        'whiten=True'.\n\n    Notes\n    -----\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    https://en.wikipedia.org/wiki/Median\n    https://en.wikipedia.org/wiki/Interquartile_range\n    ",
        "klass": "sklearn.preprocessing.RobustScaler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.TransformerMixin",
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Standardize features by removing the mean and scaling to unit variance\n\n    The standard score of a sample `x` is calculated as:\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using\n    :meth:`transform`.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    that others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : boolean, optional, default True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    with_mean : boolean, True by default\n        If True, center the data before scaling.\n        This does not work (and will raise an exception) when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_std : boolean, True by default\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    Attributes\n    ----------\n    scale_ : ndarray or None, shape (n_features,)\n        Per feature relative scaling of the data. This is calculated using\n        `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n\n        .. versionadded:: 0.17\n           *scale_*\n\n    mean_ : ndarray or None, shape (n_features,)\n        The mean value for each feature in the training set.\n        Equal to ``None`` when ``with_mean=False``.\n\n    var_ : ndarray or None, shape (n_features,)\n        The variance for each feature in the training set. Used to compute\n        `scale_`. Equal to ``None`` when ``with_std=False``.\n\n    n_samples_seen_ : int or array, shape (n_features,)\n        The number of samples processed by the estimator for each feature.\n        If there are not missing samples, the ``n_samples_seen`` will be an\n        integer, otherwise it will be an array.\n        Will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]\n\n    See also\n    --------\n    scale: Equivalent function without the estimator API.\n\n    :class:`sklearn.decomposition.PCA`\n        Further removes the linear correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",
        "klass": "sklearn.preprocessing.StandardScaler",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.random_projection.BaseRandomProjection"
        ],
        "class_docstring": "Reduce dimensionality through Gaussian random projection\n\n    The components of the random matrix are drawn from N(0, 1 / n_components).\n\n    Read more in the :ref:`User Guide <gaussian_random_matrix>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int or 'auto', optional (default = 'auto')\n        Dimensionality of the target projection space.\n\n        n_components can be automatically adjusted according to the\n        number of samples in the dataset and the bound given by the\n        Johnson-Lindenstrauss lemma. In that case the quality of the\n        embedding is controlled by the ``eps`` parameter.\n\n        It should be noted that Johnson-Lindenstrauss lemma can yield\n        very conservative estimated of the required number of components\n        as it makes no assumption on the structure of the dataset.\n\n    eps : strictly positive float, optional (default=0.1)\n        Parameter to control the quality of the embedding according to\n        the Johnson-Lindenstrauss lemma when n_components is set to\n        'auto'.\n\n        Smaller values lead to better embedding and higher number of\n        dimensions (n_components) in the target projection space.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Control the pseudo random number generator used to generate the matrix\n        at fit time.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    n_components_ : int\n        Concrete number of components computed when n_components=\"auto\".\n\n    components_ : numpy array of shape [n_components, n_features]\n        Random matrix used for the projection.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.random_projection import GaussianRandomProjection\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.rand(100, 10000)\n    >>> transformer = GaussianRandomProjection(random_state=rng)\n    >>> X_new = transformer.fit_transform(X)\n    >>> X_new.shape\n    (100, 3947)\n\n    See Also\n    --------\n    SparseRandomProjection\n\n    ",
        "klass": "sklearn.random_projection.GaussianRandomProjection",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.random_projection.BaseRandomProjection"
        ],
        "class_docstring": "Reduce dimensionality through sparse random projection\n\n    Sparse random matrix is an alternative to dense random\n    projection matrix that guarantees similar embedding quality while being\n    much more memory efficient and allowing faster computation of the\n    projected data.\n\n    If we note `s = 1 / density` the components of the random matrix are\n    drawn from:\n\n      - -sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n      -  0                              with probability 1 - 1 / s\n      - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n\n    Read more in the :ref:`User Guide <sparse_random_matrix>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int or 'auto', optional (default = 'auto')\n        Dimensionality of the target projection space.\n\n        n_components can be automatically adjusted according to the\n        number of samples in the dataset and the bound given by the\n        Johnson-Lindenstrauss lemma. In that case the quality of the\n        embedding is controlled by the ``eps`` parameter.\n\n        It should be noted that Johnson-Lindenstrauss lemma can yield\n        very conservative estimated of the required number of components\n        as it makes no assumption on the structure of the dataset.\n\n    density : float in range ]0, 1], optional (default='auto')\n        Ratio of non-zero component in the random projection matrix.\n\n        If density = 'auto', the value is set to the minimum density\n        as recommended by Ping Li et al.: 1 / sqrt(n_features).\n\n        Use density = 1 / 3.0 if you want to reproduce the results from\n        Achlioptas, 2001.\n\n    eps : strictly positive float, optional, (default=0.1)\n        Parameter to control the quality of the embedding according to\n        the Johnson-Lindenstrauss lemma when n_components is set to\n        'auto'.\n\n        Smaller values lead to better embedding and higher number of\n        dimensions (n_components) in the target projection space.\n\n    dense_output : boolean, optional (default=False)\n        If True, ensure that the output of the random projection is a\n        dense numpy array even if the input and random projection matrix\n        are both sparse. In practice, if the number of components is\n        small the number of zero components in the projected data will\n        be very small and it will be more CPU and memory efficient to\n        use a dense representation.\n\n        If False, the projected data uses a sparse representation if\n        the input is sparse.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Control the pseudo random number generator used to generate the matrix\n        at fit time.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    n_components_ : int\n        Concrete number of components computed when n_components=\"auto\".\n\n    components_ : CSR matrix with shape [n_components, n_features]\n        Random matrix used for the projection.\n\n    density_ : float in range 0.0 - 1.0\n        Concrete density computed from when density = \"auto\".\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.random_projection import SparseRandomProjection\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.rand(100, 10000)\n    >>> transformer = SparseRandomProjection(random_state=rng)\n    >>> X_new = transformer.fit_transform(X)\n    >>> X_new.shape\n    (100, 3947)\n    >>> # very few components are non-zero\n    >>> np.mean(transformer.components_ != 0)\n    0.0100...\n\n    See Also\n    --------\n    GaussianRandomProjection\n\n    References\n    ----------\n\n    .. [1] Ping Li, T. Hastie and K. W. Church, 2006,\n           \"Very Sparse Random Projections\".\n           https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf\n\n    .. [2] D. Achlioptas, 2001, \"Database-friendly random projections\",\n           https://users.soe.ucsc.edu/~optas/papers/jl.pdf\n\n    ",
        "klass": "sklearn.random_projection.SparseRandomProjection",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.semi_supervised._label_propagation.BaseLabelPropagation"
        ],
        "class_docstring": "Label Propagation classifier\n\n    Read more in the :ref:`User Guide <label_propagation>`.\n\n    Parameters\n    ----------\n    kernel : {'knn', 'rbf', callable}\n        String identifier for kernel function to use or the kernel function\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\n        passed should take two inputs, each of shape [n_samples, n_features],\n        and return a [n_samples, n_samples] shaped weight matrix.\n\n    gamma : float\n        Parameter for rbf kernel\n\n    n_neighbors : integer > 0\n        Parameter for knn kernel\n\n    max_iter : integer\n        Change maximum number of iterations allowed\n\n    tol : float\n        Convergence tolerance: threshold to consider the system at steady\n        state\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    X_ : array, shape = [n_samples, n_features]\n        Input array.\n\n    classes_ : array, shape = [n_classes]\n        The distinct labels used in classifying instances.\n\n    label_distributions_ : array, shape = [n_samples, n_classes]\n        Categorical distribution for each item.\n\n    transduction_ : array, shape = [n_samples]\n        Label assigned to each item via the transduction.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets\n    >>> from sklearn.semi_supervised import LabelPropagation\n    >>> label_prop_model = LabelPropagation()\n    >>> iris = datasets.load_iris()\n    >>> rng = np.random.RandomState(42)\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\n    >>> labels = np.copy(iris.target)\n    >>> labels[random_unlabeled_points] = -1\n    >>> label_prop_model.fit(iris.data, labels)\n    LabelPropagation(...)\n\n    References\n    ----------\n    Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data\n    with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon\n    University, 2002 http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\n\n    See Also\n    --------\n    LabelSpreading : Alternate label propagation strategy more robust to noise\n    ",
        "klass": "sklearn.semi_supervised.LabelPropagation",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.linear_model._base.LinearClassifierMixin",
            "sklearn.linear_model._base.SparseCoefMixin"
        ],
        "class_docstring": "Linear Support Vector Classification.\n\n    Similar to SVC with parameter kernel='linear', but implemented in terms of\n    liblinear rather than libsvm, so it has more flexibility in the choice of\n    penalties and loss functions and should scale better to large numbers of\n    samples.\n\n    This class supports both dense and sparse input and the multiclass support\n    is handled according to a one-vs-the-rest scheme.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    penalty : str, 'l1' or 'l2' (default='l2')\n        Specifies the norm used in the penalization. The 'l2'\n        penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n        vectors that are sparse.\n\n    loss : str, 'hinge' or 'squared_hinge' (default='squared_hinge')\n        Specifies the loss function. 'hinge' is the standard SVM loss\n        (used e.g. by the SVC class) while 'squared_hinge' is the\n        square of the hinge loss.\n\n    dual : bool, (default=True)\n        Select the algorithm to either solve the dual or primal\n        optimization problem. Prefer dual=False when n_samples > n_features.\n\n    tol : float, optional (default=1e-4)\n        Tolerance for stopping criteria.\n\n    C : float, optional (default=1.0)\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive.\n\n    multi_class : str, 'ovr' or 'crammer_singer' (default='ovr')\n        Determines the multi-class strategy if `y` contains more than\n        two classes.\n        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n        ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n        While `crammer_singer` is interesting from a theoretical perspective\n        as it is consistent, it is seldom used in practice as it rarely leads\n        to better accuracy and is more expensive to compute.\n        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n        will be ignored.\n\n    fit_intercept : bool, optional (default=True)\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be already centered).\n\n    intercept_scaling : float, optional (default=1)\n        When self.fit_intercept is True, instance vector x becomes\n        ``[x, self.intercept_scaling]``,\n        i.e. a \"synthetic\" feature with constant value equals to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes intercept_scaling * synthetic feature weight\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    class_weight : {dict, 'balanced'}, optional\n        Set the parameter C of class i to ``class_weight[i]*C`` for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : int, (default=0)\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in liblinear that, if enabled, may not work\n        properly in a multithreaded context.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data for the dual coordinate descent (if ``dual=True``). When\n        ``dual=False`` the underlying implementation of :class:`LinearSVC`\n        is not random and ``random_state`` has no effect on the results. If\n        int, random_state is the seed used by the random number generator; If\n        RandomState instance, random_state is the random number generator; If\n        None, the random number generator is the RandomState instance used by\n        `np.random`.\n\n    max_iter : int, (default=1000)\n        The maximum number of iterations to be run.\n\n    Attributes\n    ----------\n    coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        ``coef_`` is a readonly property derived from ``raw_coef_`` that\n        follows the internal memory layout of liblinear.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    n_iter_ : int\n        Maximum number of iterations run across all classes.\n\n    See Also\n    --------\n    SVC\n        Implementation of Support Vector Machine classifier using libsvm:\n        the kernel can be non-linear but its SMO algorithm does not\n        scale to large number of samples as LinearSVC does.\n\n        Furthermore SVC multi-class mode is implemented using one\n        vs one scheme while LinearSVC uses one vs the rest. It is\n        possible to implement one vs the rest with SVC by using the\n        :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n\n        Finally SVC can fit dense data without memory copy if the input\n        is C-contiguous. Sparse data will still incur memory copy though.\n\n    sklearn.linear_model.SGDClassifier\n        SGDClassifier can optimize the same cost function as LinearSVC\n        by adjusting the penalty and loss parameters. In addition it requires\n        less memory, allows incremental (online) learning, and implements\n        various loss functions and regularization regimes.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller ``tol`` parameter.\n\n    The underlying implementation, liblinear, uses a sparse internal\n    representation for the data that will incur a memory copy.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n    `LIBLINEAR: A Library for Large Linear Classification\n    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n\n    Examples\n    --------\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_features=4, random_state=0)\n    >>> clf = LinearSVC(random_state=0, tol=1e-5)\n    >>> clf.fit(X, y)\n    LinearSVC(random_state=0, tol=1e-05)\n    >>> print(clf.coef_)\n    [[0.085... 0.394... 0.498... 0.375...]]\n    >>> print(clf.intercept_)\n    [0.284...]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n    ",
        "klass": "sklearn.svm.LinearSVC",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.linear_model._base.LinearModel"
        ],
        "class_docstring": "Linear Support Vector Regression.\n\n    Similar to SVR with parameter kernel='linear', but implemented in terms of\n    liblinear rather than libsvm, so it has more flexibility in the choice of\n    penalties and loss functions and should scale better to large numbers of\n    samples.\n\n    This class supports both dense and sparse input.\n\n    Read more in the :ref:`User Guide <svm_regression>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    epsilon : float, optional (default=0.0)\n        Epsilon parameter in the epsilon-insensitive loss function. Note\n        that the value of this parameter depends on the scale of the target\n        variable y. If unsure, set ``epsilon=0``.\n\n    tol : float, optional (default=1e-4)\n        Tolerance for stopping criteria.\n\n    C : float, optional (default=1.0)\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive.\n\n    loss : string, optional (default='epsilon_insensitive')\n        Specifies the loss function. The epsilon-insensitive loss\n        (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n        loss ('squared_epsilon_insensitive') is the L2 loss.\n\n    fit_intercept : boolean, optional (default=True)\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be already centered).\n\n    intercept_scaling : float, optional (default=1)\n        When self.fit_intercept is True, instance vector x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equals to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes intercept_scaling * synthetic feature weight\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    dual : bool, (default=True)\n        Select the algorithm to either solve the dual or primal\n        optimization problem. Prefer dual=False when n_samples > n_features.\n\n    verbose : int, (default=0)\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in liblinear that, if enabled, may not work\n        properly in a multithreaded context.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    max_iter : int, (default=1000)\n        The maximum number of iterations to be run.\n\n    Attributes\n    ----------\n    coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `raw_coef_` that\n        follows the internal memory layout of liblinear.\n\n    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n        Constants in decision function.\n\n    n_iter_ : int\n        Maximum number of iterations run across all classes.\n\n    Examples\n    --------\n    >>> from sklearn.svm import LinearSVR\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_features=4, random_state=0)\n    >>> regr = LinearSVR(random_state=0, tol=1e-5)\n    >>> regr.fit(X, y)\n    LinearSVR(random_state=0, tol=1e-05)\n    >>> print(regr.coef_)\n    [16.35... 26.91... 42.30... 60.47...]\n    >>> print(regr.intercept_)\n    [-4.29...]\n    >>> print(regr.predict([[0, 0, 0, 0]]))\n    [-4.29...]\n\n    See also\n    --------\n    LinearSVC\n        Implementation of Support Vector Machine classifier using the\n        same library as this class (liblinear).\n\n    SVR\n        Implementation of Support Vector Machine regression using libsvm:\n        the kernel can be non-linear but its SMO algorithm does not\n        scale to large number of samples as LinearSVC does.\n\n    sklearn.linear_model.SGDRegressor\n        SGDRegressor can optimize the same cost function as LinearSVR\n        by adjusting the penalty and loss parameters. In addition it requires\n        less memory, allows incremental (online) learning, and implements\n        various loss functions and regularization regimes.\n    ",
        "klass": "sklearn.svm.LinearSVR",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.svm._base.BaseSVC"
        ],
        "class_docstring": "Nu-Support Vector Classification.\n\n    Similar to SVC but uses a parameter to control the number of support\n    vectors.\n\n    The implementation is based on libsvm.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    nu : float, optional (default=0.5)\n        An upper bound on the fraction of training errors and a lower\n        bound of the fraction of support vectors. Should be in the\n        interval (0, 1].\n\n    kernel : string, optional (default='rbf')\n         Specifies the kernel type to be used in the algorithm.\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n         a callable.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, optional (default='scale')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n\n    probability : boolean, optional (default=False)\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : {dict, 'balanced'}, optional\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one. The \"balanced\" mode uses the values of y to automatically\n        adjust weights inversely proportional to class frequencies as\n        ``n_samples / (n_classes * np.bincount(y))``\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : 'ovo', 'ovr', default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2).\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, optional (default=False)\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator used when shuffling\n        the data for probability estimates. If int, random_state is the seed\n        used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random\n        number generator is the RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    support_ : array-like of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : array-like of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : array-like, dtype=int32, shape = [n_class]\n        Number of support vectors for each class.\n\n    dual_coef_ : array, shape = [n_class-1, n_SV]\n        Coefficients of the support vector in the decision function.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the section about multi-class classification in\n        the SVM section of the User Guide for details.\n\n    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    intercept_ : ndarray of shape (n_class * (n_class-1) / 2,)\n        Constants in decision function.\n\n    classes_ : array of shape (n_classes,)\n        The unique classes labels.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 if the algorithm did not converge.\n\n    probA_ : ndarray, shape of (n_class * (n_class-1) / 2,)\n    probB_ : ndarray of shape (n_class * (n_class-1) / 2,)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    class_weight_ : ndarray of shape (n_class,)\n        Multipliers of parameter C of each class.\n        Computed based on the ``class_weight`` parameter.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import NuSVC\n    >>> clf = NuSVC()\n    >>> clf.fit(X, y)\n    NuSVC()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    SVC\n        Support Vector Machine for classification using libsvm.\n\n    LinearSVC\n        Scalable linear Support Vector Machine for classification using\n        liblinear.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n        machines and comparison to regularizedlikelihood methods.\"\n        <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n    ",
        "klass": "sklearn.svm.NuSVC",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.svm._base.BaseLibSVM"
        ],
        "class_docstring": "Nu Support Vector Regression.\n\n    Similar to NuSVC, for regression, uses a parameter nu to control\n    the number of support vectors. However, unlike NuSVC, where nu\n    replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n\n    The implementation is based on libsvm.\n\n    Read more in the :ref:`User Guide <svm_regression>`.\n\n    Parameters\n    ----------\n    nu : float, optional\n        An upper bound on the fraction of training errors and a lower bound of\n        the fraction of support vectors. Should be in the interval (0, 1].  By\n        default 0.5 will be taken.\n\n    C : float, optional (default=1.0)\n        Penalty parameter C of the error term.\n\n    kernel : string, optional (default='rbf')\n         Specifies the kernel type to be used in the algorithm.\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n         a callable.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, optional (default='scale')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    Attributes\n    ----------\n    support_ : array-like of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : array-like of shape (n_SV, n_features)\n        Support vectors.\n\n    dual_coef_ : array, shape = [1, n_SV]\n        Coefficients of the support vector in the decision function.\n\n    coef_ : array, shape = [1, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    intercept_ : array, shape = [1]\n        Constants in decision function.\n\n    Examples\n    --------\n    >>> from sklearn.svm import NuSVR\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> np.random.seed(0)\n    >>> y = np.random.randn(n_samples)\n    >>> X = np.random.randn(n_samples, n_features)\n    >>> clf = NuSVR(C=1.0, nu=0.1)\n    >>> clf.fit(X, y)\n    NuSVR(nu=0.1)\n\n    See also\n    --------\n    NuSVC\n        Support Vector Machine for classification implemented with libsvm\n        with a parameter to control the number of support vectors.\n\n    SVR\n        epsilon Support Vector Machine for regression implemented with libsvm.\n\n    Notes\n    -----\n    **References:**\n    `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\n    ",
        "klass": "sklearn.svm.NuSVR",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.OutlierMixin",
            "sklearn.svm._base.BaseLibSVM"
        ],
        "class_docstring": "Unsupervised Outlier Detection.\n\n    Estimate the support of a high-dimensional distribution.\n\n    The implementation is based on libsvm.\n\n    Read more in the :ref:`User Guide <outlier_detection>`.\n\n    Parameters\n    ----------\n    kernel : string, optional (default='rbf')\n         Specifies the kernel type to be used in the algorithm.\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n         a callable.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, optional (default='scale')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    tol : float, optional\n        Tolerance for stopping criterion.\n\n    nu : float, optional\n        An upper bound on the fraction of training\n        errors and a lower bound of the fraction of support\n        vectors. Should be in the interval (0, 1]. By default 0.5\n        will be taken.\n\n    shrinking : boolean, optional\n        Whether to use the shrinking heuristic.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    Attributes\n    ----------\n    support_ : array-like of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : array-like of shape (n_SV, n_features)\n        Support vectors.\n\n    dual_coef_ : array, shape = [1, n_SV]\n        Coefficients of the support vectors in the decision function.\n\n    coef_ : array, shape = [1, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`\n\n    intercept_ : array, shape = [1,]\n        Constant in the decision function.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: decision_function = score_samples - `offset_`.\n        The offset is the opposite of `intercept_` and is provided for\n        consistency with other outlier detection algorithms.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    Examples\n    --------\n    >>> from sklearn.svm import OneClassSVM\n    >>> X = [[0], [0.44], [0.45], [0.46], [1]]\n    >>> clf = OneClassSVM(gamma='auto').fit(X)\n    >>> clf.predict(X)\n    array([-1,  1,  1,  1, -1])\n    >>> clf.score_samples(X)  # doctest: +ELLIPSIS\n    array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])\n    ",
        "klass": "sklearn.svm.OneClassSVM",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.svm._base.BaseSVC"
        ],
        "class_docstring": "C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples. For large datasets\n    consider using :class:`sklearn.svm.LinearSVC` or\n    :class:`sklearn.linear_model.SGDClassifier` instead, possibly after a\n    :class:`sklearn.kernel_approximation.Nystroem` transformer.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    C : float, optional (default=1.0)\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive. The penalty\n        is a squared l2 penalty.\n\n    kernel : string, optional (default='rbf')\n        Specifies the kernel type to be used in the algorithm.\n        It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n        a callable.\n        If none is given, 'rbf' will be used. If a callable is given it is\n        used to pre-compute the kernel matrix from data matrices; that matrix\n        should be an array of shape ``(n_samples, n_samples)``.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, optional (default='scale')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n\n    probability : boolean, optional (default=False)\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : {dict, 'balanced'}, optional\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : 'ovo', 'ovr', default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n        ('ovo') is always used as multi-class strategy.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, optional (default=False)\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator used when shuffling\n        the data for probability estimates. If int, random_state is the\n        seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random\n        number generator is the RandomState instance used by `np.random`.\n\n    Attributes\n    ----------\n    support_ : array-like of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : array-like of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : array-like, dtype=int32, shape = [n_class]\n        Number of support vectors for each class.\n\n    dual_coef_ : array, shape = [n_class-1, n_SV]\n        Coefficients of the support vector in the decision function.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the section about multi-class classification in the\n        SVM section of the User Guide for details.\n\n    coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    intercept_ : ndarray of shape (n_class * (n_class-1) / 2,)\n        Constants in decision function.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    classes_ : array of shape (n_classes,)\n        The classes labels.\n\n    probA_ : array, shape = [n_class * (n_class-1) / 2]\n    probB_ : array, shape = [n_class * (n_class-1) / 2]\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    class_weight_ : ndarray of shape (n_class,)\n        Multipliers of parameter C for each class.\n        Computed based on the ``class_weight`` parameter.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import SVC\n    >>> clf = SVC(gamma='auto')\n    >>> clf.fit(X, y)\n    SVC(gamma='auto')\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    SVR\n        Support Vector Machine for Regression implemented using libsvm.\n\n    LinearSVC\n        Scalable Linear Support Vector Machine for classification\n        implemented using liblinear. Check the See also section of\n        LinearSVC for more comparison element.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n        machines and comparison to regularizedlikelihood methods.\"\n        <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n    ",
        "klass": "sklearn.svm.SVC",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.svm._base.BaseLibSVM"
        ],
        "class_docstring": "Epsilon-Support Vector Regression.\n\n    The free parameters in the model are C and epsilon.\n\n    The implementation is based on libsvm. The fit time complexity\n    is more than quadratic with the number of samples which makes it hard\n    to scale to datasets with more than a couple of 10000 samples. For large\n    datasets consider using :class:`sklearn.svm.LinearSVR` or\n    :class:`sklearn.linear_model.SGDRegressor` instead, possibly after a\n    :class:`sklearn.kernel_approximation.Nystroem` transformer.\n\n    Read more in the :ref:`User Guide <svm_regression>`.\n\n    Parameters\n    ----------\n    kernel : string, optional (default='rbf')\n         Specifies the kernel type to be used in the algorithm.\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n         a callable.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, optional (default='scale')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n\n    C : float, optional (default=1.0)\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive.\n        The penalty is a squared l2 penalty.\n\n    epsilon : float, optional (default=0.1)\n         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n         within which no penalty is associated in the training loss function\n         with points predicted within a distance epsilon from the actual\n         value.\n\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    Attributes\n    ----------\n    support_ : array-like of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : array-like of shape (n_SV, n_features)\n        Support vectors.\n\n    dual_coef_ : array, shape = [1, n_SV]\n        Coefficients of the support vector in the decision function.\n\n    coef_ : array, shape = [1, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    intercept_ : array, shape = [1]\n        Constants in decision function.\n\n    Examples\n    --------\n    >>> from sklearn.svm import SVR\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = SVR(C=1.0, epsilon=0.2)\n    >>> clf.fit(X, y)\n    SVR(epsilon=0.2)\n\n    See also\n    --------\n    NuSVR\n        Support Vector Machine for regression implemented using libsvm\n        using a parameter to control the number of support vectors.\n\n    LinearSVR\n        Scalable Linear Support Vector Machine for regression\n        implemented using liblinear.\n\n    Notes\n    -----\n    **References:**\n    `LIBSVM: A Library for Support Vector Machines\n    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`__\n    ",
        "klass": "sklearn.svm.SVR",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.ClassifierMixin",
            "sklearn.tree._es.BaseDecisionTree"
        ],
        "class_docstring": "A decision tree classifier.\n\n    Read more in the :ref:`User Guide <tree>`.\n\n    Parameters\n    ----------\n    criterion : str, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n    splitter : str, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, str or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n            - If int, then consider `max_features` features at each split.\n            - If float, then `max_features` is a fraction and\n              `int(max_features * n_features)` features are considered at each\n              split.\n            - If \"auto\", then `max_features=sqrt(n_features)`.\n            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n            - If \"log2\", then `max_features=log2(n_features)`.\n            - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, default=1e-7\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    presort : deprecated, default='deprecated'\n        This parameter is deprecated and will be removed in v0.24.\n\n        .. deprecated:: 0.22\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_classes,) or a list of such arrays\n        The classes labels (single output problem),\n        or a list of arrays of class labels (multi-output problem).\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances. The higher, the more important the\n        feature. The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance [4]_.\n\n    max_features_ : int,\n        The inferred value of max_features.\n\n    n_classes_ : int or list\n        The number of classes (for single output problems),\n        or a list containing the number of classes for each\n        output (for multi-output problems).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    tree_ : Tree object\n        The underlying Tree object. Please refer to\n        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n        for basic usage of these attributes.\n\n    See Also\n    --------\n    DecisionTreeRegressor : A decision tree regressor.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n           Learning\", Springer, 2009.\n\n    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> clf = DecisionTreeClassifier(random_state=0)\n    >>> iris = load_iris()\n    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n    ...                             # doctest: +SKIP\n    ...\n    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n    ",
        "klass": "sklearn.tree.DecisionTreeClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.RegressorMixin",
            "sklearn.tree._es.BaseDecisionTree"
        ],
        "class_docstring": "A decision tree regressor.\n\n    Read more in the :ref:`User Guide <tree>`.\n\n    Parameters\n    ----------\n    criterion : str, optional (default=\"mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion and minimizes the L2 loss\n        using the mean of each terminal node, \"friedman_mse\", which uses mean\n        squared error with Friedman's improvement score for potential splits,\n        and \"mae\" for the mean absolute error, which minimizes the L1 loss\n        using the median of each terminal node.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    splitter : str, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, str or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    presort : deprecated, default='deprecated'\n        This parameter is deprecated and will be removed in v0.24.\n\n        .. deprecated:: 0.22\n\n    ccp_alpha : non-negative float, optional (default=0.0)\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n        :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    feature_importances_ : ndarray of shape (n_features,)\n        The feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the\n        (normalized) total reduction of the criterion brought\n        by that feature. It is also known as the Gini importance [4]_.\n\n    max_features_ : int,\n        The inferred value of max_features.\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    tree_ : Tree object\n        The underlying Tree object. Please refer to\n        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n        for basic usage of these attributes.\n\n    See Also\n    --------\n    DecisionTreeClassifier : A decision tree classifier.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n           Learning\", Springer, 2009.\n\n    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_boston\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.tree import DecisionTreeRegressor\n    >>> X, y = load_boston(return_X_y=True)\n    >>> regressor = DecisionTreeRegressor(random_state=0)\n    >>> cross_val_score(regressor, X, y, cv=10)\n    ...                    # doctest: +SKIP\n    ...\n    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\n            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\n    ",
        "klass": "sklearn.tree.DecisionTreeRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Writes reports for a GoDepth1Letters object.",
        "klass": "goatools.gosubdag.rpt.wr_xlsx.GoDepth1LettersWr",
        "module": "goatools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Runs Fisher's exact test, as well as multiple corrections\n    ",
        "klass": "goatools.go_enrichment.GOEnrichmentStudy",
        "module": "goatools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a client connection that connects to Discord.\n    This class is used to interact with the Discord WebSocket and API.\n\n    A number of options can be passed to the :class:`Client`.\n\n    Parameters\n    -----------\n    max_messages: Optional[:class:`int`]\n        The maximum number of messages to store in the internal message cache.\n        This defaults to 5000. Passing in ``None`` or a value less than 100\n        will use the default instead of the passed in value.\n    loop: Optional[:class:`asyncio.AbstractEventLoop`]\n        The :class:`asyncio.AbstractEventLoop` to use for asynchronous operations.\n        Defaults to ``None``, in which case the default event loop is used via\n        :func:`asyncio.get_event_loop()`.\n    connector: :class:`aiohttp.BaseConnector`\n        The connector to use for connection pooling.\n    proxy: Optional[:class:`str`]\n        Proxy URL.\n    proxy_auth: Optional[:class:`aiohttp.BasicAuth`]\n        An object that represents proxy HTTP Basic Authorization.\n    shard_id: Optional[:class:`int`]\n        Integer starting at 0 and less than :attr:`.shard_count`.\n    shard_count: Optional[:class:`int`]\n        The total number of shards.\n    fetch_offline_members: :class:`bool`\n        Indicates if :func:`.on_ready` should be delayed to fetch all offline\n        members from the guilds the bot belongs to. If this is ``False``\\, then\n        no offline members are received and :meth:`request_offline_members`\n        must be used to fetch the offline members of the guild.\n    status: Optional[:class:`.Status`]\n        A status to start your presence with upon logging on to Discord.\n    activity: Optional[Union[:class:`.Activity`, :class:`.Game`, :class:`.Streaming`]]\n        An activity to start your presence with upon logging on to Discord.\n    heartbeat_timeout: :class:`float`\n        The maximum numbers of seconds before timing out and restarting the\n        WebSocket in the case of not receiving a HEARTBEAT_ACK. Useful if\n        processing the initial packets take too long to the point of disconnecting\n        you. The default timeout is 60 seconds.\n\n    Attributes\n    -----------\n    ws\n        The websocket gateway the client is currently connected to. Could be ``None``.\n    loop: :class:`asyncio.AbstractEventLoop`\n        The event loop that the client uses for HTTP requests and websocket operations.\n    ",
        "klass": "discord.Client",
        "module": "discord"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a Discord role colour. This class is similar\n    to an (red, green, blue) :class:`tuple`.\n\n    There is an alias for this called Color.\n\n    .. container:: operations\n\n        .. describe:: x == y\n\n             Checks if two colours are equal.\n\n        .. describe:: x != y\n\n             Checks if two colours are not equal.\n\n        .. describe:: hash(x)\n\n             Return the colour's hash.\n\n        .. describe:: str(x)\n\n             Returns the hex format for the colour.\n\n    Attributes\n    ------------\n    value: :class:`int`\n        The raw integer colour value.\n    ",
        "klass": "discord.Colour",
        "module": "discord"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a Discord embed.\n\n    .. container:: operations\n\n        .. describe:: len(x)\n\n            Returns the total size of the embed.\n            Useful for checking if it's within the 6000 character limit.\n\n    Certain properties return an ``EmbedProxy``, a type\n    that acts similar to a regular :class:`dict` except using dotted access,\n    e.g. ``embed.author.icon_url``. If the attribute\n    is invalid or empty, then a special sentinel value is returned,\n    :attr:`Embed.Empty`.\n\n    For ease of use, all parameters that expect a :class:`str` are implicitly\n    casted to :class:`str` for you.\n\n    Attributes\n    -----------\n    title: :class:`str`\n        The title of the embed.\n        This can be set during initialisation.\n    type: :class:`str`\n        The type of embed. Usually \"rich\".\n        This can be set during initialisation.\n    description: :class:`str`\n        The description of the embed.\n        This can be set during initialisation.\n    url: :class:`str`\n        The URL of the embed.\n        This can be set during initialisation.\n    timestamp: :class:`datetime.datetime`\n        The timestamp of the embed content. This could be a naive or aware datetime.\n    colour: Union[:class:`Colour`, :class:`int`]\n        The colour code of the embed. Aliased to ``color`` as well.\n        This can be set during initialisation.\n    Empty\n        A special sentinel value used by ``EmbedProxy`` and this class\n        to denote that the value or attribute is empty.\n    ",
        "klass": "discord.embeds.Embed",
        "module": "discord"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents a Discord embed.\n\n    .. container:: operations\n\n        .. describe:: len(x)\n\n            Returns the total size of the embed.\n            Useful for checking if it's within the 6000 character limit.\n\n    Certain properties return an ``EmbedProxy``, a type\n    that acts similar to a regular :class:`dict` except using dotted access,\n    e.g. ``embed.author.icon_url``. If the attribute\n    is invalid or empty, then a special sentinel value is returned,\n    :attr:`Embed.Empty`.\n\n    For ease of use, all parameters that expect a :class:`str` are implicitly\n    casted to :class:`str` for you.\n\n    Attributes\n    -----------\n    title: :class:`str`\n        The title of the embed.\n        This can be set during initialisation.\n    type: :class:`str`\n        The type of embed. Usually \"rich\".\n        This can be set during initialisation.\n    description: :class:`str`\n        The description of the embed.\n        This can be set during initialisation.\n    url: :class:`str`\n        The URL of the embed.\n        This can be set during initialisation.\n    timestamp: :class:`datetime.datetime`\n        The timestamp of the embed content. This could be a naive or aware datetime.\n    colour: Union[:class:`Colour`, :class:`int`]\n        The colour code of the embed. Aliased to ``color`` as well.\n        This can be set during initialisation.\n    Empty\n        A special sentinel value used by ``EmbedProxy`` and this class\n        to denote that the value or attribute is empty.\n    ",
        "klass": "discord.Embed",
        "module": "discord"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Wraps up the Discord permission value.\n\n    The properties provided are two way. You can set and retrieve individual\n    bits using the properties as if they were regular bools. This allows\n    you to edit permissions.\n\n    .. container:: operations\n\n        .. describe:: x == y\n\n            Checks if two permissions are equal.\n        .. describe:: x != y\n\n            Checks if two permissions are not equal.\n        .. describe:: x <= y\n\n            Checks if a permission is a subset of another permission.\n        .. describe:: x >= y\n\n            Checks if a permission is a superset of another permission.\n        .. describe:: x < y\n\n             Checks if a permission is a strict subset of another permission.\n        .. describe:: x > y\n\n             Checks if a permission is a strict superset of another permission.\n        .. describe:: hash(x)\n\n               Return the permission's hash.\n        .. describe:: iter(x)\n\n               Returns an iterator of ``(perm, value)`` pairs. This allows it\n               to be, for example, constructed as a dict or a list of pairs.\n\n    Attributes\n    -----------\n    value\n        The raw value. This value is a bit array field of a 53-bit integer\n        representing the currently available permissions. You should query\n        permissions via the properties rather than using this raw value.\n    ",
        "klass": "discord.Permissions",
        "module": "discord"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Convert a string or number to a floating point number, if possible.",
        "klass": "numpy.float",
        "module": "float"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\n\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n>>> int('0b100', base=0)\n4",
        "klass": "numpy.int",
        "module": "int"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager that does no additional processing.\n\n    Used as a stand-in for a normal context manager, when a particular\n    block of code is only sometimes used with a normal context manager:\n\n    cm = optional_cm if condition else nullcontext()\n    with cm:\n        # Perform operation, using optional_cm if condition is True\n    ",
        "klass": "numpy.compat.contextlib_nullcontext",
        "module": "numpy"
    },
    {
        "base_classes": [
            "contextlib.ContextDecorator"
        ],
        "class_docstring": "\n    errstate(**kwargs)\n\n    Context manager for floating-point error handling.\n\n    Using an instance of `errstate` as a context manager allows statements in\n    that context to execute with a known error handling behavior. Upon entering\n    the context the error handling is set with `seterr` and `seterrcall`, and\n    upon exiting it is reset to what it was before.\n\n    ..  versionchanged:: 1.17.0\n        `errstate` is also usable as a function decorator, saving\n        a level of indentation if an entire function is wrapped.\n        See :py:class:`contextlib.ContextDecorator` for more information.\n\n    Parameters\n    ----------\n    kwargs : {divide, over, under, invalid}\n        Keyword arguments. The valid keywords are the possible floating-point\n        exceptions. Each keyword should have a string value that defines the\n        treatment for the particular error. Possible values are\n        {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.\n\n    See Also\n    --------\n    seterr, geterr, seterrcall, geterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> olderr = np.seterr(all='ignore')  # Set error handling to known state.\n\n    >>> np.arange(3) / 0.\n    array([nan, inf, inf])\n    >>> with np.errstate(divide='warn'):\n    ...     np.arange(3) / 0.\n    array([nan, inf, inf])\n\n    >>> np.sqrt(-1)\n    nan\n    >>> with np.errstate(invalid='raise'):\n    ...     np.sqrt(-1)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 2, in <module>\n    FloatingPointError: invalid value encountered in sqrt\n\n    Outside the context the error handling behavior has not changed:\n\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'ignore'), ('invalid', 'ignore'), ('over', 'ignore'), ('under', 'ignore')])\n\n    ",
        "klass": "numpy.core._ufunc_config.errstate",
        "module": "numpy"
    },
    {
        "base_classes": [
            "contextlib.ContextDecorator"
        ],
        "class_docstring": "\n    errstate(**kwargs)\n\n    Context manager for floating-point error handling.\n\n    Using an instance of `errstate` as a context manager allows statements in\n    that context to execute with a known error handling behavior. Upon entering\n    the context the error handling is set with `seterr` and `seterrcall`, and\n    upon exiting it is reset to what it was before.\n\n    ..  versionchanged:: 1.17.0\n        `errstate` is also usable as a function decorator, saving\n        a level of indentation if an entire function is wrapped.\n        See :py:class:`contextlib.ContextDecorator` for more information.\n\n    Parameters\n    ----------\n    kwargs : {divide, over, under, invalid}\n        Keyword arguments. The valid keywords are the possible floating-point\n        exceptions. Each keyword should have a string value that defines the\n        treatment for the particular error. Possible values are\n        {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.\n\n    See Also\n    --------\n    seterr, geterr, seterrcall, geterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> olderr = np.seterr(all='ignore')  # Set error handling to known state.\n\n    >>> np.arange(3) / 0.\n    array([nan, inf, inf])\n    >>> with np.errstate(divide='warn'):\n    ...     np.arange(3) / 0.\n    array([nan, inf, inf])\n\n    >>> np.sqrt(-1)\n    nan\n    >>> with np.errstate(invalid='raise'):\n    ...     np.sqrt(-1)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 2, in <module>\n    FloatingPointError: invalid value encountered in sqrt\n\n    Outside the context the error handling behavior has not changed:\n\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'ignore'), ('invalid', 'ignore'), ('over', 'ignore'), ('under', 'ignore')])\n\n    ",
        "klass": "numpy.core.numeric.errstate",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.DataSource"
        ],
        "class_docstring": "\n    Repository(baseurl, destpath='.')\n\n    A data repository where multiple DataSource's share a base\n    URL/directory.\n\n    `Repository` extends `DataSource` by prepending a base URL (or\n    directory) to all the files it handles. Use `Repository` when you will\n    be working with multiple files from one base URL.  Initialize\n    `Repository` with the base URL, then refer to each file by its filename\n    only.\n\n    Parameters\n    ----------\n    baseurl : str\n        Path to the local directory or remote location that contains the\n        data files.\n    destpath : str or None, optional\n        Path to the directory where the source file gets downloaded to for\n        use.  If `destpath` is None, a temporary directory will be created.\n        The default path is the current directory.\n\n    Examples\n    --------\n    To analyze all files in the repository, do something like this\n    (note: this is not self-contained code)::\n\n        >>> repos = np.lib._datasource.Repository('/home/user/data/dir/')\n        >>> for filename in filelist:\n        ...     fp = repos.open(filename)\n        ...     fp.analyze()\n        ...     fp.close()\n\n    Similarly you could use a URL for a repository::\n\n        >>> repos = np.lib._datasource.Repository('http://www.xyz.edu/data')\n\n    ",
        "klass": "numpy.lib._datasource.Repository",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.ndarray"
        ],
        "class_docstring": "\n    An array class with possibly masked values.\n\n    Masked values of True exclude the corresponding element from any\n    computation.\n\n    Construction::\n\n      x = MaskedArray(data, mask=nomask, dtype=None, copy=False, subok=True,\n                      ndmin=0, fill_value=None, keep_mask=True, hard_mask=None,\n                      shrink=True, order=None)\n\n    Parameters\n    ----------\n    data : array_like\n        Input data.\n    mask : sequence, optional\n        Mask. Must be convertible to an array of booleans with the same\n        shape as `data`. True indicates a masked (i.e. invalid) data.\n    dtype : dtype, optional\n        Data type of the output.\n        If `dtype` is None, the type of the data argument (``data.dtype``)\n        is used. If `dtype` is not None and different from ``data.dtype``,\n        a copy is performed.\n    copy : bool, optional\n        Whether to copy the input data (True), or to use a reference instead.\n        Default is False.\n    subok : bool, optional\n        Whether to return a subclass of `MaskedArray` if possible (True) or a\n        plain `MaskedArray`. Default is True.\n    ndmin : int, optional\n        Minimum number of dimensions. Default is 0.\n    fill_value : scalar, optional\n        Value used to fill in the masked values when necessary.\n        If None, a default based on the data-type is used.\n    keep_mask : bool, optional\n        Whether to combine `mask` with the mask of the input data, if any\n        (True), or to use only `mask` for the output (False). Default is True.\n    hard_mask : bool, optional\n        Whether to use a hard mask or not. With a hard mask, masked values\n        cannot be unmasked. Default is False.\n    shrink : bool, optional\n        Whether to force compression of an empty mask. Default is True.\n    order : {'C', 'F', 'A'}, optional\n        Specify the order of the array.  If order is 'C', then the array\n        will be in C-contiguous order (last-index varies the fastest).\n        If order is 'F', then the returned array will be in\n        Fortran-contiguous order (first-index varies the fastest).\n        If order is 'A' (default), then the returned array may be\n        in any order (either C-, Fortran-contiguous, or even discontiguous),\n        unless a copy is required, in which case it will be C-contiguous.\n\n    ",
        "klass": "numpy.ma.MaskedArray",
        "module": "numpy"
    },
    {
        "base_classes": [
            "numpy.polynomial._polybase.ABCPolyBase"
        ],
        "class_docstring": "A Legendre series class.\n\n    The Legendre class provides the standard Python numerical methods\n    '+', '-', '*', '//', '%', 'divmod', '**', and '()' as well as the\n    attributes and methods listed in the `ABCPolyBase` documentation.\n\n    Parameters\n    ----------\n    coef : array_like\n        Legendre coefficients in order of increasing degree, i.e.,\n        ``(1, 2, 3)`` gives ``1*P_0(x) + 2*P_1(x) + 3*P_2(x)``.\n    domain : (2,) array_like, optional\n        Domain to use. The interval ``[domain[0], domain[1]]`` is mapped\n        to the interval ``[window[0], window[1]]`` by shifting and scaling.\n        The default value is [-1, 1].\n    window : (2,) array_like, optional\n        Window, see `domain` for its use. The default value is [-1, 1].\n\n        .. versionadded:: 1.6.0\n\n    ",
        "klass": "numpy.polynomial.Legendre",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Generator(bit_generator)\n\n    Container for the BitGenerators.\n\n    ``Generator`` exposes a number of methods for generating random\n    numbers drawn from a variety of probability distributions. In addition to\n    the distribution-specific arguments, each method takes a keyword argument\n    `size` that defaults to ``None``. If `size` is ``None``, then a single\n    value is generated and returned. If `size` is an integer, then a 1-D\n    array filled with generated values is returned. If `size` is a tuple,\n    then an array with that shape is filled and returned.\n\n    The function :func:`numpy.random.default_rng` will instantiate\n    a `Generator` with numpy's default `BitGenerator`.\n\n    **No Compatibility Guarantee**\n\n    ``Generator`` does not provide a version compatibility guarantee. In\n    particular, as better algorithms evolve the bit stream may change.\n\n    Parameters\n    ----------\n    bit_generator : BitGenerator\n        BitGenerator to use as the core generator.\n\n    Notes\n    -----\n    The Python stdlib module `random` contains pseudo-random number generator\n    with a number of methods that are similar to the ones available in\n    ``Generator``. It uses Mersenne Twister, and this bit generator can\n    be accessed using ``MT19937``. ``Generator``, besides being\n    NumPy-aware, has the advantage that it provides a much larger number\n    of probability distributions to choose from.\n\n    Examples\n    --------\n    >>> from numpy.random import Generator, PCG64\n    >>> rg = Generator(PCG64())\n    >>> rg.standard_normal()\n    -0.203  # random\n\n    See Also\n    --------\n    default_rng : Recommended constructor for `Generator`.\n    ",
        "klass": "numpy.random.Generator",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    RandomState(seed=None)\n\n    Container for the slow Mersenne Twister pseudo-random number generator.\n    Consider using a different BitGenerator with the Generator container\n    instead.\n\n    `RandomState` and `Generator` expose a number of methods for generating\n    random numbers drawn from a variety of probability distributions. In\n    addition to the distribution-specific arguments, each method takes a\n    keyword argument `size` that defaults to ``None``. If `size` is ``None``,\n    then a single value is generated and returned. If `size` is an integer,\n    then a 1-D array filled with generated values is returned. If `size` is a\n    tuple, then an array with that shape is filled and returned.\n\n    **Compatibility Guarantee**\n\n    A fixed bit generator using a fixed seed and a fixed series of calls to\n    'RandomState' methods using the same parameters will always produce the\n    same results up to roundoff error except when the values were incorrect.\n    `RandomState` is effectively frozen and will only receive updates that\n    are required by changes in the the internals of Numpy. More substantial\n    changes, including algorithmic improvements, are reserved for\n    `Generator`.\n\n    Parameters\n    ----------\n    seed : {None, int, array_like, BitGenerator}, optional\n        Random seed used to initialize the pseudo-random number generator or\n        an instantized BitGenerator.  If an integer or array, used as a seed for\n        the MT19937 BitGenerator. Values can be any integer between 0 and\n        2**32 - 1 inclusive, an array (or other sequence) of such integers,\n        or ``None`` (the default).  If `seed` is ``None``, then the `MT19937`\n        BitGenerator is initialized by reading data from ``/dev/urandom``\n        (or the Windows analogue) if available or seed from the clock\n        otherwise.\n\n    Notes\n    -----\n    The Python stdlib module \"random\" also contains a Mersenne Twister\n    pseudo-random number generator with a number of methods that are similar\n    to the ones available in `RandomState`. `RandomState`, besides being\n    NumPy-aware, has the advantage that it provides a much larger number\n    of probability distributions to choose from.\n\n    See Also\n    --------\n    Generator\n    MT19937\n    :ref:`bit_generator`\n\n    ",
        "klass": "numpy.random.RandomState",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager and decorator doing much the same as\n    ``warnings.catch_warnings``.\n\n    However, it also provides a filter mechanism to work around\n    https://bugs.python.org/issue4180.\n\n    This bug causes Python before 3.4 to not reliably show warnings again\n    after they have been ignored once (even within catch_warnings). It\n    means that no \"ignore\" filter can be used easily, since following\n    tests might need to see the warning. Additionally it allows easier\n    specificity for testing warnings and can be nested.\n\n    Parameters\n    ----------\n    forwarding_rule : str, optional\n        One of \"always\", \"once\", \"module\", or \"location\". Analogous to\n        the usual warnings module filter mode, it is useful to reduce\n        noise mostly on the outmost level. Unsuppressed and unrecorded\n        warnings will be forwarded based on this rule. Defaults to \"always\".\n        \"location\" is equivalent to the warnings \"default\", match by exact\n        location the warning warning originated from.\n\n    Notes\n    -----\n    Filters added inside the context manager will be discarded again\n    when leaving it. Upon entering all filters defined outside a\n    context will be applied automatically.\n\n    When a recording filter is added, matching warnings are stored in the\n    ``log`` attribute as well as in the list returned by ``record``.\n\n    If filters are added and the ``module`` keyword is given, the\n    warning registry of this module will additionally be cleared when\n    applying it, entering the context, or exiting it. This could cause\n    warnings to appear a second time after leaving the context if they\n    were configured to be printed once (default) and were already\n    printed before the context was entered.\n\n    Nesting this context manager will work as expected when the\n    forwarding rule is \"always\" (default). Unfiltered and unrecorded\n    warnings will be passed out and be matched by the outer level.\n    On the outmost level they will be printed (or caught by another\n    warnings context). The forwarding rule argument can modify this\n    behaviour.\n\n    Like ``catch_warnings`` this context manager is not threadsafe.\n\n    Examples\n    --------\n\n    With a context manager::\n\n        with np.testing.suppress_warnings() as sup:\n            sup.filter(DeprecationWarning, \"Some text\")\n            sup.filter(module=np.ma.core)\n            log = sup.record(FutureWarning, \"Does this occur?\")\n            command_giving_warnings()\n            # The FutureWarning was given once, the filtered warnings were\n            # ignored. All other warnings abide outside settings (may be\n            # printed/error)\n            assert_(len(log) == 1)\n            assert_(len(sup.log) == 1)  # also stored in log attribute\n\n    Or as a decorator::\n\n        sup = np.testing.suppress_warnings()\n        sup.filter(module=np.ma.core)  # module must match exactly\n        @sup\n        def some_function():\n            # do something which causes a warning in np.ma.core\n            pass\n    ",
        "klass": "numpy.testing.suppress_warnings",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Returns an epolling object.\n\n  sizehint\n    The expected number of events to be registered.  It must be positive,\n    or -1 to use the default.  It is only used on older systems where\n    epoll_create1() is not available; otherwise it has no effect (though its\n    value is still checked).\n  flags\n    Deprecated and completely ignored.  However, when supplied, its value\n    must be 0 or select.EPOLL_CLOEXEC, otherwise OSError is raised.",
        "klass": "select.epoll",
        "module": "select"
    },
    {
        "base_classes": [
            "django.contrib.sessions.backends.base.SessionBase"
        ],
        "class_docstring": "\n    Implement database session store.\n    ",
        "klass": "django.contrib.sessions.backends.db.SessionStore",
        "module": "django"
    },
    {
        "base_classes": [
            "django.core.files.base.File"
        ],
        "class_docstring": "\n    A File-like object that takes just raw content, rather than an actual file.\n    ",
        "klass": "django.core.files.base.ContentFile",
        "module": "django"
    },
    {
        "base_classes": [
            "django.core.serializers.base.Serializer"
        ],
        "class_docstring": "\n    Serialize a QuerySet to basic Python objects.\n    ",
        "klass": "django.core.serializers.json.Serializer",
        "module": "django"
    },
    {
        "base_classes": [
            "wsgiref.simple_server.WSGIServer"
        ],
        "class_docstring": "BaseHTTPServer that implements the Python WSGI protocol",
        "klass": "django.core.servers.basehttp.WSGIServer",
        "module": "django"
    },
    {
        "base_classes": [
            "django.db.migrations.operations.models.ModelOperation"
        ],
        "class_docstring": "Create a model's table.",
        "klass": "django.db.migrations.CreateModel",
        "module": "django"
    },
    {
        "base_classes": [
            "django.db.migrations.operations.base.Operation"
        ],
        "class_docstring": "\n    Run Python code in a context suitable for doing versioned ORM operations.\n    ",
        "klass": "django.db.migrations.RunPython",
        "module": "django"
    },
    {
        "base_classes": [
            "django.db.migrations.operations.base.Operation"
        ],
        "class_docstring": "\n    Run some raw SQL. A reverse SQL statement may be provided.\n\n    Also accept a list of operations that represent the state change effected\n    by this SQL change, in case it's custom column/table creation/deletion.\n    ",
        "klass": "django.db.migrations.RunSQL",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Base class for all signals\n\n    Internal attributes:\n\n        receivers\n            { receiverkey (id) : weakref(receiver) }\n    ",
        "klass": "django.dispatch.Signal",
        "module": "django"
    },
    {
        "base_classes": [
            "django.forms.fields.ChoiceField"
        ],
        "class_docstring": "A ChoiceField whose choices are a model QuerySet.",
        "klass": "django.forms.ModelChoiceField",
        "module": "django"
    },
    {
        "base_classes": [
            "django.forms.models.ModelChoiceField"
        ],
        "class_docstring": "A MultipleChoiceField whose choices are a model QuerySet.",
        "klass": "django.forms.ModelMultipleChoiceField",
        "module": "django"
    },
    {
        "base_classes": [
            "django.forms.fields.Field"
        ],
        "class_docstring": "\n    Aggregate the logic of multiple Fields.\n\n    Its clean() method takes a \"decompressed\" list of values, which are then\n    cleaned into a single value according to self.fields. Each value in\n    this list is cleaned by the corresponding field -- the first value is\n    cleaned by the first field, the second value is cleaned by the second\n    field, etc. Once all fields are cleaned, the list of clean values is\n    \"compressed\" into a single value.\n\n    Subclasses should not have to implement clean(). Instead, they must\n    implement compress(), which takes a list of valid values and returns a\n    \"compressed\" version of those values -- a single value.\n\n    You'll probably want to use this with MultiWidget.\n    ",
        "klass": "django.forms.MultiValueField",
        "module": "django"
    },
    {
        "base_classes": [
            "django.forms.fields.BooleanField"
        ],
        "class_docstring": "\n    A field whose valid values are None, True, and False. Clean invalid values\n    to None.\n    ",
        "klass": "django.forms.NullBooleanField",
        "module": "django"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "\n    A subclass of dictionary customized to handle multiple values for the\n    same key.\n\n    >>> d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n    >>> d['name']\n    'Simon'\n    >>> d.getlist('name')\n    ['Adrian', 'Simon']\n    >>> d.getlist('doesnotexist')\n    []\n    >>> d.getlist('doesnotexist', ['Adrian', 'Simon'])\n    ['Adrian', 'Simon']\n    >>> d.get('lastname', 'nonexistent')\n    'nonexistent'\n    >>> d.setlist('lastname', ['Holovaty', 'Willison'])\n\n    This class exists to solve the irritating problem raised by cgi.parse_qs,\n    which returns a list for every key, even though most Web forms submit\n    single name-value pairs.\n    ",
        "klass": "django.utils.datastructures.MultiValueDict",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A basic HTTP request.",
        "klass": "django.http.HttpRequest",
        "module": "django"
    },
    {
        "base_classes": [
            "django.http.response.HttpResponseBase"
        ],
        "class_docstring": "\n    An HTTP response class with a string as content.\n\n    This content that can be read, appended to, or replaced.\n    ",
        "klass": "django.http.HttpResponse",
        "module": "django"
    },
    {
        "base_classes": [
            "django.http.response.HttpResponse"
        ],
        "class_docstring": "\n    An HTTP response class that consumes data to be serialized to JSON.\n\n    :param data: Data to be dumped into json. By default only ``dict`` objects\n      are allowed to be passed due to a security flaw before EcmaScript 5. See\n      the ``safe`` parameter for more information.\n    :param encoder: Should be a json encoder class. Defaults to\n      ``django.core.serializers.json.DjangoJSONEncoder``.\n    :param safe: Controls if only ``dict`` objects may be serialized. Defaults\n      to ``True``.\n    :param json_dumps_params: A dictionary of kwargs passed to json.dumps().\n    ",
        "klass": "django.http.JsonResponse",
        "module": "django"
    },
    {
        "base_classes": [
            "django.utils.datastructures.MultiValueDict"
        ],
        "class_docstring": "\n    A specialized MultiValueDict which represents a query string.\n\n    A QueryDict can be used to represent GET or POST data. It subclasses\n    MultiValueDict since keys in such data can be repeated, for instance\n    in the data from a form with a <select multiple> field.\n\n    By default QueryDicts are immutable, though the copy() method\n    will always return a mutable copy.\n\n    Both keys and values set on this class are converted from the given encoding\n    (DEFAULT_CHARSET by default) to unicode.\n    ",
        "klass": "django.http.request.QueryDict",
        "module": "django"
    },
    {
        "base_classes": [
            "django.utils.datastructures.MultiValueDict"
        ],
        "class_docstring": "\n    A specialized MultiValueDict which represents a query string.\n\n    A QueryDict can be used to represent GET or POST data. It subclasses\n    MultiValueDict since keys in such data can be repeated, for instance\n    in the data from a form with a <select multiple> field.\n\n    By default QueryDicts are immutable, though the copy() method\n    will always return a mutable copy.\n\n    Both keys and values set on this class are converted from the given encoding\n    (DEFAULT_CHARSET by default) to str.\n    ",
        "klass": "django.http.QueryDict",
        "module": "django"
    },
    {
        "base_classes": [
            "django.template.context.BaseContext"
        ],
        "class_docstring": "A stack container for variable context",
        "klass": "django.template.Context",
        "module": "django"
    },
    {
        "base_classes": [
            "django.template.context.Context"
        ],
        "class_docstring": "\n    This subclass of template.Context automatically populates itself using\n    the processors defined in the engine's configuration.\n    Additional processors can be specified as a list of callables\n    using the \"processors\" keyword argument.\n    ",
        "klass": "django.template.RequestContext",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A template variable, resolvable against a given context. The variable may\n    be a hard-coded string (if it begins and ends with single or double quote\n    marks)::\n\n        >>> c = {'article': {'section':'News'}}\n        >>> Variable('article.section').resolve(c)\n        'News'\n        >>> Variable('article').resolve(c)\n        {'section': 'News'}\n        >>> class AClass: pass\n        >>> c = AClass()\n        >>> c.article = AClass()\n        >>> c.article.section = 'News'\n\n    (The example assumes VARIABLE_ATTRIBUTE_SEPARATOR is '.')\n    ",
        "klass": "django.template.Variable",
        "module": "django"
    },
    {
        "base_classes": [
            "django.test.client.RequestFactory"
        ],
        "class_docstring": "\n    A class that can act as a client for testing purposes.\n\n    It allows the user to compose GET and POST requests, and\n    obtain the response that the server gave to those requests.\n    The server Response objects are annotated with the details\n    of the contexts and templates that were rendered during the\n    process of serving the request.\n\n    Client objects are stateful - they will retain cookie (and\n    thus session) details for the lifetime of the Client instance.\n\n    This is not intended as a replacement for Twill/Selenium or\n    the like - it is here to allow testing against the\n    contexts and templates produced by a view, rather than the\n    HTML rendered to the end-user.\n    ",
        "klass": "django.test.client.Client",
        "module": "django"
    },
    {
        "base_classes": [
            "django.test.client.RequestFactory"
        ],
        "class_docstring": "\n    A class that can act as a client for testing purposes.\n\n    It allows the user to compose GET and POST requests, and\n    obtain the response that the server gave to those requests.\n    The server Response objects are annotated with the details\n    of the contexts and templates that were rendered during the\n    process of serving the request.\n\n    Client objects are stateful - they will retain cookie (and\n    thus session) details for the lifetime of the Client instance.\n\n    This is not intended as a replacement for Twill/Selenium or\n    the like - it is here to allow testing against the\n    contexts and templates produced by a view, rather than the\n    HTML rendered to the end-user.\n    ",
        "klass": "django.test.Client",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Class that lets you create mock Request objects for use in testing.\n\n    Usage:\n\n    rf = RequestFactory()\n    get_request = rf.get('/hello/')\n    post_request = rf.post('/submit/', {'foo': 'bar'})\n\n    Once you have a request object you can pass it to any view function,\n    just as if that view had been hooked up using a URLconf.\n    ",
        "klass": "django.test.client.RequestFactory",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Class that lets you create mock Request objects for use in testing.\n\n    Usage:\n\n    rf = RequestFactory()\n    get_request = rf.get('/hello/')\n    post_request = rf.post('/submit/', {'foo': 'bar'})\n\n    Once you have a request object you can pass it to any view function,\n    just as if that view had been hooked up using a URLconf.\n    ",
        "klass": "django.test.RequestFactory",
        "module": "django"
    },
    {
        "base_classes": [
            "django.test.testcases.TransactionTestCase"
        ],
        "class_docstring": "\n    Similar to TransactionTestCase, but use `transaction.atomic()` to achieve\n    test isolation.\n\n    In most situations, TestCase should be preferred to TransactionTestCase as\n    it allows faster execution. However, there are some situations where using\n    TransactionTestCase might be necessary (e.g. testing some transactional\n    behavior).\n\n    On database backends with no transaction support, TestCase behaves as\n    TransactionTestCase.\n    ",
        "klass": "django.test.TestCase",
        "module": "django"
    },
    {
        "base_classes": [
            "django.test.utils.TestContextDecorator"
        ],
        "class_docstring": "\n    Acts as either a decorator or a context manager. If it's a decorator it\n    takes a function and returns a wrapped function. If it's a contextmanager\n    it's used with the ``with`` statement. In either event entering/exiting\n    are called before and after, respectively, the function/block is executed.\n    ",
        "klass": "django.test.utils.override_settings",
        "module": "django"
    },
    {
        "base_classes": [
            "django.test.utils.TestContextDecorator"
        ],
        "class_docstring": "\n    Act as either a decorator or a context manager. If it's a decorator, take a\n    function and return a wrapped function. If it's a contextmanager, use it\n    with the ``with`` statement. In either event, entering/exiting are called\n    before and after, respectively, the function/block is executed.\n    ",
        "klass": "django.test.override_settings",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The external API class that encapsulates an archive implementation.\n    ",
        "klass": "django.utils.archive.Archive",
        "module": "django"
    },
    {
        "base_classes": [
            "tuple"
        ],
        "class_docstring": "\n    A tuple-like object that raises useful errors when it is asked to mutate.\n\n    Example::\n\n        >>> a = ImmutableList(range(5), warning=\"You cannot mutate this.\")\n        >>> a[3] = '4'\n        Traceback (most recent call last):\n            ...\n        AttributeError: You cannot mutate this.\n    ",
        "klass": "django.utils.datastructures.ImmutableList",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "date(year, month, day) --> date object",
        "klass": "django.utils.datetime_safe.date",
        "module": "datetime"
    },
    {
        "base_classes": [
            "datetime.date"
        ],
        "class_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "django.utils.datetime_safe.datetime",
        "module": "datetime"
    },
    {
        "base_classes": [
            "datetime.date"
        ],
        "class_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "django.utils.timezone.datetime",
        "module": "datetime"
    },
    {
        "base_classes": [
            "contextlib.ContextDecorator"
        ],
        "class_docstring": "\n    Temporarily set the time zone for the current thread.\n\n    This is a context manager that uses django.utils.timezone.activate()\n    to set the timezone on entry and restores the previously active timezone\n    on exit.\n\n    The ``timezone`` argument must be an instance of a ``tzinfo`` subclass, a\n    time zone name, or ``None``. If it is ``None``, Django enables the default\n    time zone.\n    ",
        "klass": "django.utils.timezone.override",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Difference between two datetime values.\n\ntimedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)\n\nAll arguments are optional and default to 0.\nArguments may be integers or floats, and may be positive or negative.",
        "klass": "django.utils.timezone.timedelta",
        "module": "datetime"
    },
    {
        "base_classes": [
            "django.middleware.cache.UpdateCacheMiddleware",
            "django.middleware.cache.FetchFromCacheMiddleware"
        ],
        "class_docstring": "\n    Cache middleware that provides basic behavior for many simple sites.\n\n    Also used as the hook point for the cache decorator, which is generated\n    using the decorator-from-middleware utility.\n    ",
        "klass": "django.middleware.cache.CacheMiddleware",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class that provides consistent eventlet/threading Event API.\n\n    This wraps the eventlet.event.Event class to have the same API as\n    the standard threading.Event object.\n    ",
        "klass": "oslo_utils.eventletutils.EventletEvent",
        "module": "oslo_utils"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A simple timer/stopwatch helper class.\n\n    Inspired by: apache-commons-lang java stopwatch.\n\n    Not thread-safe (when a single watch is mutated by multiple threads at\n    the same time). Thread-safe when used by a single thread (not shared) or\n    when operations are performed in a thread-safe manner on these objects by\n    wrapping those operations with locks.\n\n    It will use the `monotonic`_ pypi library to find an appropriate\n    monotonically increasing time providing function (which typically varies\n    depending on operating system and python version).\n\n    .. _monotonic: https://pypi.org/project/monotonic/\n\n    .. versionadded:: 1.4\n    ",
        "klass": "oslo_utils.timeutils.StopWatch",
        "module": "oslo_utils"
    },
    {
        "base_classes": [
            "easybuild.framework.easyconfig.format.version.VersionOperator"
        ],
        "class_docstring": "Class which represents a toolchain and versionoperator instance",
        "klass": "easybuild.framework.easyconfig.format.version.ToolchainVersionOperator",
        "module": "easybuild"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    VersionOperator class represents a version expression that includes an operator.\n    ",
        "klass": "easybuild.framework.easyconfig.format.version.VersionOperator",
        "module": "easybuild"
    },
    {
        "base_classes": [
            "nibabel.nifti1.Nifti1Pair",
            "nibabel.filebasedimages.SerializableImage"
        ],
        "class_docstring": " Class for single file NIfTI1 format image\n    ",
        "klass": "nibabel.Nifti1Image",
        "module": "nibabel"
    },
    {
        "base_classes": [
            "nibabel.tmpdirs.TemporaryDirectory"
        ],
        "class_docstring": " Create, return, and change directory to a temporary directory\n\n    Examples\n    --------\n    >>> import os\n    >>> my_cwd = os.getcwd()\n    >>> with InTemporaryDirectory() as tmpdir:\n    ...     _ = open('test.txt', 'wt').write('some text')\n    ...     assert os.path.isfile('test.txt')\n    ...     assert os.path.isfile(os.path.join(tmpdir, 'test.txt'))\n    >>> os.path.exists(tmpdir)\n    False\n    >>> os.getcwd() == my_cwd\n    True\n    ",
        "klass": "nibabel.tmpdirs.InTemporaryDirectory",
        "module": "nibabel"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": " Represents a filesystem path.\n\n    For documentation on individual methods, consult their\n    counterparts in os.path.\n    ",
        "klass": "paver.path.path",
        "module": "paver"
    },
    {
        "base_classes": [
            "pyasn1.type.univ.OctetString"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type Python :class:`str` objects\n    (always empty).\n\n    Keyword Args\n    ------------\n    value: :class:`str` or |ASN.1| object\n        Python empty :class:`str` literal or any object that evaluates to :obj:`False`\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        class Ack(Null):\n            '''\n            ASN.1 specification:\n\n            Ack ::= NULL\n            '''\n        ack = Ack('')\n    ",
        "klass": "pyasn1.type.univ.Null",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.base.SimpleAsn1Type"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type Python :class:`int` objects.\n\n    Keyword Args\n    ------------\n    value: :class:`int`, :class:`str` or |ASN.1| object\n        Python :class:`int` or :class:`str` literal or |ASN.1| class\n        instance. If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    namedValues: :py:class:`~pyasn1.type.namedval.NamedValues`\n        Object representing non-default symbolic aliases for numbers\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        class ErrorCode(Integer):\n            '''\n            ASN.1 specification:\n\n            ErrorCode ::=\n                INTEGER { disk-full(1), no-disk(-1),\n                          disk-not-formatted(2) }\n\n            error ErrorCode ::= disk-full\n            '''\n            namedValues = NamedValues(\n                ('disk-full', 1), ('no-disk', -1),\n                ('disk-not-formatted', 2)\n            )\n\n        error = ErrorCode('disk-full')\n    ",
        "klass": "pyasn1.type.univ.Integer",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.base.SimpleAsn1Type"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type Python :class:`tuple` objects\n    (tuple of non-negative integers).\n\n    Keyword Args\n    ------------\n    value: :class:`tuple`, :class:`str` or |ASN.1| object\n        Python sequence of :class:`int` or :class:`str` literal or |ASN.1| object.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        class ID(ObjectIdentifier):\n            '''\n            ASN.1 specification:\n\n            ID ::= OBJECT IDENTIFIER\n\n            id-edims ID ::= { joint-iso-itu-t mhs-motif(6) edims(7) }\n            id-bp ID ::= { id-edims 11 }\n            '''\n        id_edims = ID('2.6.7')\n        id_bp = id_edims + (11,)\n    ",
        "klass": "pyasn1.type.univ.ObjectIdentifier",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.base.SimpleAsn1Type"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type Python 2 :class:`str` or\n    Python 3 :class:`bytes`. When used in Unicode context, |ASN.1| type\n    assumes \"|encoding|\" serialisation.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        class:`str` (Python 2) or :class:`bytes` (Python 3), alternatively\n        class:`unicode` object (Python 2) or :class:`str` (Python 3)\n        representing character string to be serialised into octets\n        (note `encoding` parameter) or |ASN.1| object.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in text string context.\n\n    binValue: :py:class:`str`\n        Binary string initializer to use instead of the *value*.\n        Example: '10110011'.\n\n    hexValue: :py:class:`str`\n        Hexadecimal string initializer to use instead of the *value*.\n        Example: 'DEADBEEF'.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        class Icon(OctetString):\n            '''\n            ASN.1 specification:\n\n            Icon ::= OCTET STRING\n\n            icon1 Icon ::= '001100010011001000110011'B\n            icon2 Icon ::= '313233'H\n            '''\n        icon1 = Icon.fromBinaryString('001100010011001000110011')\n        icon2 = Icon.fromHexString('313233')\n    ",
        "klass": "pyasn1.type.univ.OctetString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "xlsxwriter.xmlwriter.XMLwriter"
        ],
        "class_docstring": "\n    A class for writing the Excel XLSX Workbook file.\n\n\n    ",
        "klass": "xlsxwriter.workbook.Workbook",
        "module": "xlsxwriter"
    },
    {
        "base_classes": [
            "xlsxwriter.xmlwriter.XMLwriter"
        ],
        "class_docstring": "\n    A class for writing the Excel XLSX Workbook file.\n\n\n    ",
        "klass": "xlsxwriter.Workbook",
        "module": "xlsxwriter"
    },
    {
        "base_classes": [
            "oslo_context.context.RequestContext"
        ],
        "class_docstring": "\n    Stores information about the security context under which the user\n    accesses the system, as well as additional request information.\n    ",
        "klass": "trove.common.context.TroveContext",
        "module": "trove"
    },
    {
        "base_classes": [
            "oslo_messaging.serializer.Serializer"
        ],
        "class_docstring": "The Trove serializer class that handles class inheritance and base\n       serializers.\n    ",
        "klass": "trove.common.rpc.serializer.TroveSerializer",
        "module": "trove"
    },
    {
        "base_classes": [
            "trove.common.strategies.storage.base.Storage"
        ],
        "class_docstring": "Implementation of Storage Strategy for Swift.",
        "klass": "trove.common.strategies.storage.swift.SwiftStorage",
        "module": "trove"
    },
    {
        "base_classes": [
            "trove.common.stream_codecs.StreamCodec"
        ],
        "class_docstring": "Serialize (encode) and deserialize (decode) using the base64 codec.\n    To read binary data from a file and b64encode it, used the decode=False\n    flag on operating_system's read calls.  Use encode=False to decode\n    binary data before writing to a file as well.\n    ",
        "klass": "trove.common.stream_codecs.Base64Codec",
        "module": "trove"
    },
    {
        "base_classes": [
            "psychopy.clock.MonotonicClock"
        ],
        "class_docstring": "A convenient class to keep track of time in your experiments.\n    You can have as many independent clocks as you like (e.g. one\n    to time responses, one to keep track of stimuli...)\n\n    This clock is identical to the :class:`~psychopy.core.MonotonicClock`\n    except that it can also be reset to 0 or another value at any point.\n    ",
        "klass": "psychopy.core.Clock",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "psychopy.clock.Clock"
        ],
        "class_docstring": "Similar to a :class:`~psychopy.core.Clock` except that time counts down\n    from the time of last reset\n\n    Typical usage::\n\n        timer = core.CountdownTimer(5)\n        while timer.getTime() > 0:  # after 5s will become negative\n            # do stuff\n    ",
        "klass": "psychopy.core.CountdownTimer",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A convenient class to keep track of time in your experiments using a\n    sub-millisecond timer.\n\n    Unlike the :class:`~psychopy.core.Clock` this cannot be reset to\n    arbitrary times. For this clock t=0 always represents the time that\n    the clock was created.\n\n    Don't confuse this `class` with `core.monotonicClock` which is an\n    `instance` of it that got created when PsychoPy.core was imported.\n    That clock instance is deliberately designed always to return the\n    time since the start of the study.\n\n    Version Notes: This class was added in PsychoPy 1.77.00\n    ",
        "klass": "psychopy.core.MonotonicClock",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "psychopy.data.base._BaseTrialHandler"
        ],
        "class_docstring": "Class to handle trial sequencing and data storage.\n\n    Calls to .next() will fetch the next trial object given to this handler,\n    according to the method specified (random, sequential, fullRandom).\n    Calls will raise a StopIteration error if trials have finished.\n\n    See demo_trialHandler.py\n\n    The psydat file format is literally just a pickled copy of the\n    TrialHandler object that saved it. You can open it with::\n\n            from psychopy.tools.filetools import fromFile\n            dat = fromFile(path)\n\n    Then you'll find that `dat` has the following attributes that\n    ",
        "klass": "psychopy.data.TrialHandler",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "psychopy.iohub.datastore.pandas.interestperiod.InterestPeriodDefinition"
        ],
        "class_docstring": "EventBasedIP Class.\n\n    trial_ip=BoundingEventsIP(name='trial_ip',\n                              start_source_df=exp_data.MESSAGE,\n                              start_criteria={'text':'TRIAL_START'},\n                              end_source_df=exp_data.MESSAGE,\n                              end_criteria={'text':'TRIAL_END'}\n                              )\n\n    ",
        "klass": "psychopy.iohub.datastore.pandas.interestperiod.EventBasedIP",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "NumPyRingBuffer is a circular buffer implemented using a one dimensional\n    numpy array on the backend. The algorithm used to implement the ring buffer\n    behavior does not require any array copies to occur while the ring buffer\n    is maintained, while at the same time allowing sequential element access\n    into the numpy array using a subset of standard slice notation.\n\n    When the circular buffer is created, a maximum size , or maximum\n    number of elements,  that the buffer can hold *must* be specified. When\n    the buffer becomes full, each element added to the buffer removes the oldest\n    element from the buffer so that max_size is never exceeded.\n\n    Items are added to the ring buffer using the classes append method.\n\n    The current number of elements in the buffer can be retrieved using the\n    getLength() method of the class.\n\n    The isFull() method can be used to determine if\n    the ring buffer has reached its maximum size, at which point each new element\n    added will disregard the oldest element in the array.\n\n    The getElements() method is used to retrieve the actual numpy array containing\n    the elements in the ring buffer. The element in index 0 is the oldest remaining\n    element added to the buffer, and index n (which can be up to max_size-1)\n    is the the most recent element added to the buffer.\n\n    Methods that can be called from a standard numpy array can also be called using the\n    NumPyRingBuffer instance created. However Numpy module level functions will not accept\n    a NumPyRingBuffer as a valid arguement.\n\n    To clear the ring buffer and start with no data in the buffer, without\n    needing to create a new NumPyRingBuffer object, call the clear() method\n    of the class.\n\n    Example::\n\n        ring_buffer=NumPyRingBuffer(10)\n\n        for i in xrange(25):\n            ring_buffer.append(i)\n            print('-------')\n            print('Ring Buffer Stats:')\n            print('\tWindow size: ',len(ring_buffer))\n            print('\tMin Value: ',ring_buffer.min())\n            print('\tMax Value: ',ring_buffer.max())\n            print('\tMean Value: ',ring_buffer.mean())\n            print('\tStandard Deviation: ',ring_buffer.std())\n            print('\tFirst 3 Elements: ',ring_buffer[:3])\n            print('\tLast 3 Elements: ',ring_buffer[-3:])\n\n    ",
        "klass": "psychopy.iohub.util.NumPyRingBuffer",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates a monitor object for storing calibration details.\n    This will be loaded automatically from disk if the\n    monitor name is already defined (see methods).\n\n    Many settings from the stored monitor can easilly be overridden\n    either by adding them as arguments during the initial call.\n\n    **arguments**:\n\n        - ``width, distance, gamma`` are details about the calibration\n        - ``notes`` is a text field to store any useful info\n        - ``useBits`` True, False, None\n        - ``verbose`` True, False, None\n        - ``currentCalib`` is a dictionary object containing various\n            fields for a calibration. Use with caution since the\n            dictionary may not contain all the necessary fields that\n            a monitor object expects to find.\n\n    **eg**:\n\n    ``myMon = Monitor('sony500', distance=114)``\n    Fetches the info on the sony500 and overrides its usual distance\n    to be 114cm for this experiment.\n\n    These can be saved to the monitor file using\n    :func:`~psychopy.monitors.Monitor.save`\n    or not (in which case the changes will be lost)\n    ",
        "klass": "psychopy.monitors.calibTools.Monitor",
        "module": "psychopy"
    },
    {
        "base_classes": [
            "xarray.core.common.AbstractArray",
            "xarray.core.common.DataWithCoords"
        ],
        "class_docstring": "N-dimensional array with labeled coordinates and dimensions.\n\n    DataArray provides a wrapper around numpy ndarrays that uses labeled\n    dimensions and coordinates to support metadata aware operations. The API is\n    similar to that for the pandas Series or DataFrame, but DataArray objects\n    can have any number of dimensions, and their contents have fixed data\n    types.\n\n    Additional features over raw numpy arrays:\n\n    - Apply operations over dimensions by name: ``x.sum('time')``.\n    - Select or assign values by integer location (like numpy): ``x[:10]``\n      or by label (like pandas): ``x.loc['2014-01-01']`` or\n      ``x.sel(time='2014-01-01')``.\n    - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n      dimensions (known in numpy as \"broadcasting\") based on dimension names,\n      regardless of their original order.\n    - Keep track of arbitrary metadata in the form of a Python dictionary:\n      ``x.attrs``\n    - Convert to a pandas Series: ``x.to_series()``.\n\n    Getting items from or doing mathematical operations with a DataArray\n    always returns another DataArray.\n\n    Attributes\n    ----------\n    dims : tuple\n        Dimension names associated with this array.\n    values : numpy.ndarray\n        Access or modify DataArray values as a numpy array.\n    coords : dict-like\n        Dictionary of DataArray objects that label values along each dimension.\n    name : str or None\n        Name of this array.\n    attrs : dict\n        Dictionary for holding arbitrary metadata.\n    ",
        "klass": "xarray.DataArray",
        "module": "xarray"
    },
    {
        "base_classes": [
            "collections.abc.Mapping",
            "typing.Generic",
            "xarray.core.common.ImplementsDatasetReduce",
            "xarray.core.common.DataWithCoords"
        ],
        "class_docstring": "A multi-dimensional, in memory, array database.\n\n    A dataset resembles an in-memory representation of a NetCDF file, and\n    consists of variables, coordinates and attributes which together form a\n    self describing dataset.\n\n    Dataset implements the mapping interface with keys given by variable names\n    and values given by DataArray objects for each variable name.\n\n    One dimensional variables with name equal to their dimension are index\n    coordinates used for label based indexing.\n    ",
        "klass": "xarray.Dataset",
        "module": "xarray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Set options for xarray in a controlled context.\n\n    Currently supported options:\n\n    - ``display_width``: maximum display width for ``repr`` on xarray objects.\n      Default: ``80``.\n    - ``arithmetic_join``: DataArray/Dataset alignment in binary operations.\n      Default: ``'inner'``.\n    - ``file_cache_maxsize``: maximum number of open files to hold in xarray's\n      global least-recently-usage cached. This should be smaller than your\n      system's per-process file descriptor limit, e.g., ``ulimit -n`` on Linux.\n      Default: 128.\n    - ``warn_for_unclosed_files``: whether or not to issue a warning when\n      unclosed files are deallocated (default False). This is mostly useful\n      for debugging.\n    - ``cmap_sequential``: colormap to use for nondivergent data plots.\n      Default: ``viridis``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``cmap_divergent``: colormap to use for divergent data plots.\n      Default: ``RdBu_r``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``keep_attrs``: rule for whether to keep attributes on xarray\n      Datasets/dataarrays after operations. Either ``True`` to always keep\n      attrs, ``False`` to always discard them, or ``'default'`` to use original\n      logic that attrs should only be kept in unambiguous circumstances.\n      Default: ``'default'``.\n    - ``display_style``: display style to use in jupyter for xarray objects.\n      Default: ``'text'``. Other options are ``'html'``.\n\n\n    You can use ``set_options`` either as a context manager:\n\n    >>> ds = xr.Dataset({'x': np.arange(1000)})\n    >>> with xr.set_options(display_width=40):\n    ...     print(ds)\n    <xarray.Dataset>\n    Dimensions:  (x: 1000)\n    Coordinates:\n      * x        (x) int64 0 1 2 3 4 5 6 ...\n    Data variables:\n        *empty*\n\n    Or to set global options:\n\n    >>> xr.set_options(display_width=80)\n    ",
        "klass": "xarray.core.options.set_options",
        "module": "xarray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Set options for xarray in a controlled context.\n\n    Currently supported options:\n\n    - ``display_width``: maximum display width for ``repr`` on xarray objects.\n      Default: ``80``.\n    - ``arithmetic_join``: DataArray/Dataset alignment in binary operations.\n      Default: ``'inner'``.\n    - ``file_cache_maxsize``: maximum number of open files to hold in xarray's\n      global least-recently-usage cached. This should be smaller than your\n      system's per-process file descriptor limit, e.g., ``ulimit -n`` on Linux.\n      Default: 128.\n    - ``warn_for_unclosed_files``: whether or not to issue a warning when\n      unclosed files are deallocated (default False). This is mostly useful\n      for debugging.\n    - ``cmap_sequential``: colormap to use for nondivergent data plots.\n      Default: ``viridis``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``cmap_divergent``: colormap to use for divergent data plots.\n      Default: ``RdBu_r``. If string, must be matplotlib built-in colormap.\n      Can also be a Colormap object (e.g. mpl.cm.magma)\n    - ``keep_attrs``: rule for whether to keep attributes on xarray\n      Datasets/dataarrays after operations. Either ``True`` to always keep\n      attrs, ``False`` to always discard them, or ``'default'`` to use original\n      logic that attrs should only be kept in unambiguous circumstances.\n      Default: ``'default'``.\n    - ``display_style``: display style to use in jupyter for xarray objects.\n      Default: ``'text'``. Other options are ``'html'``.\n\n\n    You can use ``set_options`` either as a context manager:\n\n    >>> ds = xr.Dataset({'x': np.arange(1000)})\n    >>> with xr.set_options(display_width=40):\n    ...     print(ds)\n    <xarray.Dataset>\n    Dimensions:  (x: 1000)\n    Coordinates:\n      * x        (x) int64 0 1 2 3 4 5 6 ...\n    Data variables:\n        *empty*\n\n    Or to set global options:\n\n    >>> xr.set_options(display_width=80)\n    ",
        "klass": "xarray.set_options",
        "module": "xarray"
    },
    {
        "base_classes": [
            "xarray.backends.file_manager.FileManager"
        ],
        "class_docstring": "Wrapper for automatically opening and closing file objects.\n\n    Unlike files, CachingFileManager objects can be safely pickled and passed\n    between processes. They should be explicitly closed to release resources,\n    but a per-process least-recently-used cache for open files ensures that you\n    can safely create arbitrarily large numbers of FileManager objects.\n\n    Don't directly close files acquired from a FileManager. Instead, call\n    FileManager.close(), which ensures that closed files are removed from the\n    cache as well.\n\n    Example usage:\n\n        manager = FileManager(open, 'example.txt', mode='w')\n        f = manager.acquire()\n        f.write(...)\n        manager.close()  # ensures file is closed\n\n    Note that as long as previous files are still cached, acquiring a file\n    multiple times from the same FileManager is essentially free:\n\n        f1 = manager.acquire()\n        f2 = manager.acquire()\n        assert f1 is f2\n\n    ",
        "klass": "xarray.backends.file_manager.CachingFileManager",
        "module": "xarray"
    },
    {
        "base_classes": [
            "xarray.coding.variables.VariableCoder"
        ],
        "class_docstring": "Transforms between arrays containing bytes and character arrays.",
        "klass": "xarray.coding.strings.CharacterArrayCoder",
        "module": "xarray"
    },
    {
        "base_classes": [
            "xarray.coding.variables.VariableCoder"
        ],
        "class_docstring": "Transforms between unicode strings and fixed-width UTF-8 bytes.",
        "klass": "xarray.coding.strings.EncodedStringCoder",
        "module": "xarray"
    },
    {
        "base_classes": [
            "xarray.coding.variables.VariableCoder"
        ],
        "class_docstring": "Mask or unmask fill values according to CF conventions.",
        "klass": "xarray.coding.variables.CFMaskCoder",
        "module": "xarray"
    },
    {
        "base_classes": [
            "xarray.coding.variables.VariableCoder"
        ],
        "class_docstring": "Scale and offset variables according to CF conventions.\n\n    Follows the formula:\n        decode_values = encoded_values * scale_factor + add_offset\n    ",
        "klass": "xarray.coding.variables.CFScaleOffsetCoder",
        "module": "xarray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Encapsulates a DB update\n\n    An instance of this object carries the information necessary to prioritize\n    and process a request to update a DB entry.\n    ",
        "klass": "dragonflow.db.db_common.DbUpdate",
        "module": "dragonflow"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A constant depth radix tree written (originally) for indexing in DbStore\n\n    This implementation stores several (or none) items for each path\n\n    ",
        "klass": "dragonflow.utils.radix_tree.RadixTree",
        "module": "dragonflow"
    },
    {
        "base_classes": [
            "ipywidgets.widgets.domwidget.DOMWidget",
            "ipywidgets.widgets.widget_core.CoreWidget"
        ],
        "class_docstring": " Displays multiple widgets in a group.\n\n    The widgets are laid out horizontally.\n\n    Parameters\n    ----------\n    children: iterable of Widget instances\n        list of widgets to display\n\n    box_style: str\n        one of 'success', 'info', 'warning' or 'danger', or ''.\n        Applies a predefined style to the box. Defaults to '',\n        which applies no pre-defined style.\n\n    Examples\n    --------\n    >>> import ipywidgets as widgets\n    >>> title_widget = widgets.HTML('<em>Box Example</em>')\n    >>> slider = widgets.IntSlider()\n    >>> widgets.Box([title_widget, slider])\n    ",
        "klass": "ipywidgets.Box",
        "module": "ipywidgets"
    },
    {
        "base_classes": [
            "ipywidgets.widgets.domwidget.DOMWidget"
        ],
        "class_docstring": "Widget used as a context manager to display output.\n\n    This widget can capture and display stdout, stderr, and rich output.  To use\n    it, create an instance of it and display it.\n\n    You can then use the widget as a context manager: any output produced while in the\n    context will be captured and displayed in the widget instead of the standard output\n    area.\n\n    You can also use the .capture() method to decorate a function or a method. Any output \n    produced by the function will then go to the output widget. This is useful for\n    debugging widget callbacks, for example.\n\n    Example::\n        import ipywidgets as widgets\n        from IPython.display import display\n        out = widgets.Output()\n        display(out)\n\n        print('prints to output area')\n\n        with out:\n            print('prints to output widget')\n\n        @out.capture()\n        def func():\n            print('prints to output widget')\n    ",
        "klass": "ipywidgets.Output",
        "module": "ipywidgets"
    },
    {
        "base_classes": [
            "ipywidgets.widgets.widget_box.Box"
        ],
        "class_docstring": " Displays multiple widgets vertically using the flexible box model.\n\n    Parameters\n    ----------\n    children: iterable of Widget instances\n        list of widgets to display\n\n    box_style: str\n        one of 'success', 'info', 'warning' or 'danger', or ''.\n        Applies a predefined style to the box. Defaults to '',\n        which applies no pre-defined style.\n\n    Examples\n    --------\n    >>> import ipywidgets as widgets\n    >>> title_widget = widgets.HTML('<em>Vertical Box Example</em>')\n    >>> slider = widgets.IntSlider()\n    >>> widgets.VBox([title_widget, slider])\n    ",
        "klass": "ipywidgets.VBox",
        "module": "ipywidgets"
    },
    {
        "base_classes": [
            "traitlets.config.configurable.LoggingConfigurable"
        ],
        "class_docstring": "General JSON config manager\n    \n    Deals with persisting/storing config in a json file\n    ",
        "klass": "traitlets.config.manager.BaseJSONConfigManager",
        "module": "traitlets"
    },
    {
        "base_classes": [
            "Cryptodome.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER NULL element.",
        "klass": "Cryptodome.Util.asn1.DerNull",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "Cryptodome.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER OBJECT ID.\n\n    An example of encoding is:\n\n    >>> from Cryptodome.Util.asn1 import DerObjectId\n    >>> from binascii import hexlify, unhexlify\n    >>> oid_der = DerObjectId(\"1.2\")\n    >>> oid_der.value += \".840.113549.1.1.1\"\n    >>> print hexlify(oid_der.encode())\n\n    which will show ``06092a864886f70d010101``, the DER encoding for the\n    RSA Object Identifier ``1.2.840.113549.1.1.1``.\n\n    For decoding:\n\n    >>> s = unhexlify(b'06092a864886f70d010101')\n    >>> try:\n    >>>   oid_der = DerObjectId()\n    >>>   oid_der.decode(s)\n    >>>   print oid_der.value\n    >>> except ValueError:\n    >>>   print \"Not a valid DER OBJECT ID\"\n\n    the output will be ``1.2.840.113549.1.1.1``.\n\n    :ivar value: The Object ID (OID), a dot separated list of integers\n    :vartype value: string\n    ",
        "klass": "Cryptodome.Util.asn1.DerObjectId",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "Cryptodome.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER SEQUENCE.\n\n        This object behaves like a dynamic Python sequence.\n\n        Sub-elements that are INTEGERs behave like Python integers.\n\n        Any other sub-element is a binary string encoded as a complete DER\n        sub-element (TLV).\n\n        An example of encoding is:\n\n          >>> from Cryptodome.Util.asn1 import DerSequence, DerInteger\n          >>> from binascii import hexlify, unhexlify\n          >>> obj_der = unhexlify('070102')\n          >>> seq_der = DerSequence([4])\n          >>> seq_der.append(9)\n          >>> seq_der.append(obj_der.encode())\n          >>> print hexlify(seq_der.encode())\n\n        which will show ``3009020104020109070102``, the DER encoding of the\n        sequence containing ``4``, ``9``, and the object with payload ``02``.\n\n        For decoding:\n\n          >>> s = unhexlify(b'3009020104020109070102')\n          >>> try:\n          >>>   seq_der = DerSequence()\n          >>>   seq_der.decode(s)\n          >>>   print len(seq_der)\n          >>>   print seq_der[0]\n          >>>   print seq_der[:]\n          >>> except ValueError:\n          >>>   print \"Not a valid DER SEQUENCE\"\n\n        the output will be::\n\n          3\n          4\n          [4, 9, b'\u0007\u0001\u0002']\n\n        ",
        "klass": "Cryptodome.Util.asn1.DerSequence",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Element of GF(2^128) field",
        "klass": "Cryptodome.Protocol.SecretSharing._Element",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class defining an ECC key.\n    Do not instantiate directly.\n    Use :func:`generate`, :func:`construct` or :func:`import_key` instead.\n\n    :ivar curve: The name of the ECC as defined in :numref:`curve_names`.\n    :vartype curve: string\n\n    :ivar pointQ: an ECC point representating the public component\n    :vartype pointQ: :class:`EccPoint`\n\n    :ivar d: A scalar representating the private component\n    :vartype d: integer\n    ",
        "klass": "Cryptodome.PublicKey.ECC.EccKey",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to abstract a point over an Elliptic Curve.\n\n    The class support special methods for:\n\n    * Adding two points: ``R = S + T``\n    * In-place addition: ``S += T``\n    * Negating a point: ``R = -T``\n    * Comparing two points: ``if S == T: ...``\n    * Multiplying a point by a scalar: ``R = S*k``\n    * In-place multiplication by a scalar: ``T *= k``\n\n    :ivar x: The affine X-coordinate of the ECC point\n    :vartype x: integer\n\n    :ivar y: The affine Y-coordinate of the ECC point\n    :vartype y: integer\n\n    :ivar xy: The tuple with X- and Y- coordinates\n    ",
        "klass": "Cryptodome.PublicKey.ECC.EccPoint",
        "module": "Cryptodome"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The ``UniversalDetector`` class underlies the ``chardet.detect`` function\n    and coordinates all of the different charset probers.\n\n    To get a ``dict`` containing an encoding and its confidence, you can simply\n    run:\n\n    .. code::\n\n            u = UniversalDetector()\n            u.feed(some_bytes)\n            u.close()\n            detected = u.result\n\n    ",
        "klass": "chardet.universaldetector.UniversalDetector",
        "module": "chardet"
    },
    {
        "base_classes": [
            "django.contrib.admin.options.BaseModelAdmin"
        ],
        "class_docstring": "Encapsulates all admin options and functionality for a given model.",
        "klass": "django.contrib.admin.options.ModelAdmin",
        "module": "django"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class implementing dummy implementation of _thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the _thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    ",
        "klass": "_dummy_thread.LockType",
        "module": "_dummy_thread"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Bending magnet source. The computation is reasonably fast and thus a GPU\n    is not required and is not implemented.\n    ",
        "klass": "xrt.backends.raycing.sources.BendingMagnet",
        "module": "xrt"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Undulator source. The computation is volumnous an thus requires a GPU.\n    ",
        "klass": "xrt.backends.raycing.sources.Undulator",
        "module": "xrt"
    },
    {
        "base_classes": [
            "xrt.backends.raycing.sources_synchr.BendingMagnet"
        ],
        "class_docstring": "\n    Wiggler source. The computation is reasonably fast and thus a GPU\n    is not required and is not implemented.\n    ",
        "klass": "xrt.backends.raycing.sources.Wiggler",
        "module": "xrt"
    },
    {
        "base_classes": [
            "matplotlib.artist.Artist"
        ],
        "class_docstring": "\n    The top level container for all the plot elements.\n\n    The Figure instance supports callbacks through a *callbacks* attribute\n    which is a `.CallbackRegistry` instance.  The events you can connect to\n    are 'dpi_changed', and the callback will be called with ``func(fig)`` where\n    fig is the `Figure` instance.\n\n    Attributes\n    ----------\n    patch\n        The `.Rectangle` instance representing the figure background patch.\n\n    suppressComposite\n        For multiple figure images, the figure will make composite images\n        depending on the renderer option_image_nocomposite function.  If\n        *suppressComposite* is a boolean, this will override the renderer.\n    ",
        "klass": "matplotlib.figure.Figure",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Curve is a Chart element representing a line in a 1D coordinate\n    system where the key dimension maps on the line x-coordinate and\n    the first value dimension represents the height of the line along\n    the y-axis.\n    \n\u001b[1;32mParameters of 'Curve'\n=====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Curve'                   String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.Curve",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.element.Element"
        ],
        "class_docstring": "params(datatype=List, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Dataset provides a general baseclass for Element types that\n    contain structured data and supports a range of data formats.\n\n    The Dataset class supports various methods offering a consistent\n    way of working with the stored data regardless of the storage\n    format used. These operations include indexing, selection and\n    various ways of aggregating or collapsing the data with a supplied\n    function.\n    \n\u001b[1;32mParameters of 'Dataset'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \ngroup                     'Dataset'                  String             C RW \nkdims                         []                      List   (0, None)  C RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;31mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;31m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;31m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;31m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;31m          \u001b[0m\n\u001b[1;31m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;34mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;34m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;34m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;31mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m          \u001b[0m\n\u001b[1;31m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.data.Dataset",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.element.Element"
        ],
        "class_docstring": "params(datatype=List, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Dataset provides a general baseclass for Element types that\n    contain structured data and supports a range of data formats.\n\n    The Dataset class supports various methods offering a consistent\n    way of working with the stored data regardless of the storage\n    format used. These operations include indexing, selection and\n    various ways of aggregating or collapsing the data with a supplied\n    function.\n    \n\u001b[1;32mParameters of 'Dataset'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \ngroup                     'Dataset'                  String             C RW \nkdims                         []                      List   (0, None)  C RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;31mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;31m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;31m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;31m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;31m          \u001b[0m\n\u001b[1;31m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;34mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;34m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;34m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;31mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m          \u001b[0m\n\u001b[1;31m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.Dataset",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.spaces.HoloMap"
        ],
        "class_docstring": "params(cache_size=Integer, callback=ClassSelector, streams=List, sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    A DynamicMap is a type of HoloMap where the elements are dynamically\n    generated by a callable. The callable is invoked with values\n    associated with the key dimensions or with values supplied by stream\n    parameters.\n    \n\u001b[1;32mParameters of 'DynamicMap'\n==========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName             Value           Type        Bounds     Mode  \u001b[0m\n\ncache_size        500          Integer                  V RW  \ncallback          None      ClassSelector             C RW AN \ncdims        OrderedDict()       Dict                   V RW  \ngroup         'NdMapping'       String                  C RW  \nkdims              []            List      (0, None)    C RW  \nlabel              ''           String                  C RW  \nsort              True         Boolean       (0, 1)     V RW  \nstreams            []            List      (0, None)    C RW  \nvdims              []            List        (0, 0)     C RW  \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcache_size: The number of entries to cache for fast access. This is an LRU\u001b[0m\n\u001b[1;34m            cache where the least recently used item is overwritten once\u001b[0m\n\u001b[1;34m            the cache is full.\u001b[0m\n\u001b[1;31mcallback:   The callable used to generate the elements. The arguments to the\u001b[0m\n\u001b[1;31m            callable includes any number of declared key dimensions as well\u001b[0m\n\u001b[1;31m            as any number of stream parameters defined on the input streams.\u001b[0m\n\u001b[1;31m            \u001b[0m\n\u001b[1;31m            If the callable is an instance of Callable it will be used\u001b[0m\n\u001b[1;31m            directly, otherwise it will be automatically wrapped in one.\u001b[0m\n\u001b[1;34mcdims:      The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m            pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m            \u001b[0m\n\u001b[1;34m            Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup:      A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:      The key dimensions of a DynamicMap map to the arguments of the\u001b[0m\n\u001b[1;34m            callback. This mapping can be by position or by name.\u001b[0m\n\u001b[1;31mlabel:      Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m            or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m            measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:       Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mstreams:    List of Stream instances to associate with the DynamicMap. The\u001b[0m\n\u001b[1;31m            set of parameter values across these streams will be supplied as\u001b[0m\n\u001b[1;31m            keyword arguments to the callback when the events are received,\u001b[0m\n\u001b[1;31m            updating the streams.\u001b[0m\n\u001b[1;34mvdims:      The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m            describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m            dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m            indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m            \u001b[0m\n\u001b[1;34m            Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.DynamicMap",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.data.Dataset",
            "holoviews.core.element.Element2D"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Graph is high-level Element representing both nodes and edges.\n    A Graph may be defined in an abstract form representing just\n    the abstract edges between nodes and optionally may be made\n    concrete by supplying a Nodes Element defining the concrete\n    positions of each node. If the node positions are supplied\n    the EdgePaths (defining the concrete edges) can be inferred\n    automatically or supplied explicitly.\n\n    The constructor accepts regular columnar data defining the edges\n    or a tuple of the abstract edges and nodes, or a tuple of the\n    abstract edges, nodes, and edgepaths.\n    \n\u001b[1;32mParameters of 'Graph'\n=====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Graph'                   String             C RW \nkdims      [Dimension('start'), Dimension('end')...   List     (2, 2)   V RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.graphs.Graph",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Histogram is a Chart element representing a number of bins in a 1D\n    coordinate system. The key dimension represents the binned values,\n    which may be declared as bin edges or bin centers, while the value\n    dimensions usually defines a count, frequency or density associated\n    with each bin.\n    \n\u001b[1;32mParameters of 'Histogram'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                Value             Type     Bounds   Mode \u001b[0m\n\ncdims           OrderedDict()         Dict              V RW \ndatatype           ['grid']           List   (0, None)  V RW \nextents    (None, None, None, None)  Tuple              V RW \ngroup            'Histogram'         String             C RW \nkdims          [Dimension('x')]       List     (1, 1)   V RW \nlabel                 ''             String             C RW \nvdims      [Dimension('Frequency')]   List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    Dimensions on Element2Ds determine the number of indexable\u001b[0m\n\u001b[1;34m          dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.Histogram",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.ndmapping.UniformNdMapping",
            "holoviews.core.overlay.Overlayable"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    A HoloMap is an n-dimensional mapping of viewable elements or\n    overlays. Each item in a HoloMap has an tuple key defining the\n    values along each of the declared key dimensions, defining the\n    discretely sampled space of values.\n\n    The visual representation of a HoloMap consists of the viewable\n    objects inside the HoloMap which can be explored by varying one\n    or more widgets mapping onto the key dimensions of the HoloMap.\n    \n\u001b[1;32mParameters of 'HoloMap'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName            Value             Type     Bounds   Mode \u001b[0m\n\ncdims       OrderedDict()         Dict              V RW \ngroup        'NdMapping'         String             C RW \nkdims   [Dimension('Default')]    List   (0, None)  C RW \nlabel             ''             String             C RW \nsort             True           Boolean    (0, 1)   V RW \nvdims             []              List     (0, 0)   C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.spaces.HoloMap",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.ndmapping.UniformNdMapping",
            "holoviews.core.overlay.Overlayable"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    A HoloMap is an n-dimensional mapping of viewable elements or\n    overlays. Each item in a HoloMap has an tuple key defining the\n    values along each of the declared key dimensions, defining the\n    discretely sampled space of values.\n\n    The visual representation of a HoloMap consists of the viewable\n    objects inside the HoloMap which can be explored by varying one\n    or more widgets mapping onto the key dimensions of the HoloMap.\n    \n\u001b[1;32mParameters of 'HoloMap'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName            Value             Type     Bounds   Mode \u001b[0m\n\ncdims       OrderedDict()         Dict              V RW \ngroup        'NdMapping'         String             C RW \nkdims   [Dimension('Default')]    List   (0, None)  C RW \nlabel             ''             String             C RW \nsort             True           Boolean    (0, 1)   V RW \nvdims             []              List     (0, 0)   C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.HoloMap",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.ndmapping.MultiDimensionalMapping"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    NdMapping supports the same indexing semantics as\n    MultiDimensionalMapping but also supports slicing semantics.\n\n    Slicing semantics on an NdMapping is dependent on the ordering\n    semantics of the keys. As MultiDimensionalMapping sort the keys, a\n    slice on an NdMapping is effectively a way of filtering out the\n    keys that are outside the slice range.\n    \n\u001b[1;32mParameters of 'NdMapping'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName            Value             Type     Bounds   Mode \u001b[0m\n\ncdims       OrderedDict()         Dict              V RW \ngroup        'NdMapping'         String             C RW \nkdims   [Dimension('Default')]    List   (0, None)  C RW \nlabel             ''             String             C RW \nsort             True           Boolean    (0, 1)   V RW \nvdims             []              List     (0, 0)   C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.ndmapping.NdMapping",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.geom.Geometry"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Points represents a set of coordinates in 2D space, which may\n    optionally be associated with any number of value dimensions.\n    \n\u001b[1;32mParameters of 'Points'\n======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Points'                  String             C RW \nkdims          [Dimension('x'), Dimension('y')]       List     (2, 2)   C RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions of a geometry represent the x- and y-\u001b[0m\n\u001b[1;34m          coordinates in a 2D space.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    Value dimensions can be associated with a geometry.\u001b[0m",
        "klass": "holoviews.Points",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.graphs.Graph"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    A TriMesh represents a mesh of triangles represented as the\n    simplices and nodes. The simplices represent a indices into the\n    nodes array. The mesh therefore follows a datastructure very\n    similar to a graph, with the abstract connectivity between nodes\n    stored on the TriMesh element itself, the node positions stored on\n    a Nodes element and the concrete paths making up each triangle\n    generated when required by accessing the edgepaths.\n\n    Unlike a Graph each simplex is represented as the node indices of\n    the three corners of each triangle.\n    \n\u001b[1;32mParameters of 'TriMesh'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                     'TriMesh'                  String             C RW \nkdims            ['node1', 'node2', 'node3']          List     (3, 3)   V RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    Dimensions declaring the node indices of each triangle.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.graphs.TriMesh",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    dim transform objects are a way to express deferred transforms on\n    Datasets. dim transforms support all mathematical and bitwise\n    operators, NumPy ufuncs and methods, and provide a number of\n    useful methods for normalizing, binning and categorizing data.\n    ",
        "klass": "holoviews.util.transform.dim",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.dimension.LabelledData"
        ],
        "class_docstring": "params(cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Dimensioned is a base class that allows the data contents of a\n    class to be associated with dimensions. The contents associated\n    with dimensions may be partitioned into one of three types\n\n    * key dimensions: These are the dimensions that can be indexed via\n                      the __getitem__ method. Dimension objects\n                      supporting key dimensions must support indexing\n                      over these dimensions and may also support\n                      slicing. This list ordering of dimensions\n                      describes the positional components of each\n                      multi-dimensional indexing operation.\n\n                      For instance, if the key dimension names are\n                      'weight' followed by 'height' for Dimensioned\n                      object 'obj', then obj[80,175] indexes a weight\n                      of 80 and height of 175.\n\n                      Accessed using either kdims.\n\n    * value dimensions: These dimensions correspond to any data held\n                        on the Dimensioned object not in the key\n                        dimensions. Indexing by value dimension is\n                        supported by dimension name (when there are\n                        multiple possible value dimensions); no\n                        slicing semantics is supported and all the\n                        data associated with that dimension will be\n                        returned at once. Note that it is not possible\n                        to mix value dimensions and deep dimensions.\n\n                        Accessed using either vdims.\n\n    * deep dimensions: These are dynamically computed dimensions that\n                       belong to other Dimensioned objects that are\n                       nested in the data. Objects that support this\n                       should enable the _deep_indexable flag. Note\n                       that it is not possible to mix value dimensions\n                       and deep dimensions.\n\n                       Accessed using either ddims.\n\n    Dimensioned class support generalized methods for finding the\n    range and type of values along a particular Dimension. The range\n    method relies on the appropriate implementation of the\n    dimension_values methods on subclasses.\n\n    The index of an arbitrary dimension is its positional index in the\n    list of all dimensions, starting with the key dimensions, followed\n    by the value dimensions and ending with the deep dimensions.\n    \n\u001b[1;32mParameters of 'Dimensioned'\n===========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName        Value       Type     Bounds   Mode \u001b[0m\n\ncdims   OrderedDict()   Dict              V RW \ngroup   'Dimensioned'  String             C RW \nkdims         []        List   (0, None)  C RW \nlabel         ''       String             C RW \nvdims         []        List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.Dimensioned",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.ndmapping.UniformNdMapping"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Grids are distinct from Layouts as they ensure all contained\n    elements to be of the same type. Unlike Layouts, which have\n    integer keys, Grids usually have floating point keys, which\n    correspond to a grid sampling in some two-dimensional space. This\n    two-dimensional space may have to arbitrary dimensions, e.g. for\n    2D parameter spaces.\n    \n\u001b[1;32mParameters of 'GridSpace'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                 Value                  Type   Bounds  Mode \u001b[0m\n\ncdims            OrderedDict()              Dict           V RW \ngroup             'NdMapping'              String          C RW \nkdims   [Dimension('X'), Dimension('Y')]    List   (1, 2)  V RW \nlabel                  ''                  String          C RW \nsort                  True                Boolean  (0, 1)  V RW \nvdims                  []                   List   (0, 0)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.GridSpace",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.overlay.Overlayable",
            "holoviews.core.ndmapping.UniformNdMapping",
            "holoviews.core.overlay.CompositeOverlay"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    An NdOverlay allows a group of NdOverlay to be overlaid together. NdOverlay can\n    be indexed out of an overlay and an overlay is an iterable that iterates\n    over the contained layers.\n    \n\u001b[1;32mParameters of 'NdOverlay'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName            Value             Type     Bounds   Mode \u001b[0m\n\ncdims       OrderedDict()         Dict              V RW \ngroup        'NdMapping'         String             C RW \nkdims   [Dimension('Element')]    List   (0, None)  C RW \nlabel             ''             String             C RW \nsort             True           Boolean    (0, 1)   V RW \nvdims             []              List     (0, 0)   C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: List of dimensions the NdOverlay can be indexed by.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.NdOverlay",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.dimension.Dimensioned"
        ],
        "class_docstring": "params(sort=Boolean, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    An MultiDimensionalMapping is a Dimensioned mapping (like a\n    dictionary or array) that uses fixed-length multidimensional\n    keys. This behaves like a sparse N-dimensional array that does not\n    require a dense sampling over the multidimensional space.\n\n    If the underlying value for each (key, value) pair also supports\n    indexing (such as a dictionary, array, or list), fully qualified\n    (deep) indexing may be used from the top level, with the first N\n    dimensions of the index selecting a particular Dimensioned object\n    and the remaining dimensions indexing into that object.\n\n    For instance, for a MultiDimensionalMapping with dimensions \"Year\"\n    and \"Month\" and underlying values that are 2D floating-point\n    arrays indexed by (r,c), a 2D array may be indexed with x[2000,3]\n    and a single floating-point number may be indexed as\n    x[2000,3,1,9].\n\n    In practice, this class is typically only used as an abstract base\n    class, because the NdMapping subclass extends it with a range of\n    useful slicing methods for selecting subsets of the data. Even so,\n    keeping the slicing support separate from the indexing and data\n    storage methods helps make both classes easier to understand.\n    \n\u001b[1;32mParameters of 'MultiDimensionalMapping'\n=======================================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName              Value              Type     Bounds   Mode \u001b[0m\n\ncdims         OrderedDict()          Dict              V RW \ngroup   'MultiDimensionalMapping'   String             C RW \nkdims     [Dimension('Default')]     List   (0, None)  C RW \nlabel               ''              String             C RW \nsort               True            Boolean    (0, 1)   V RW \nvdims               []               List     (0, 0)   C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims: The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m       pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mgroup: A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims: The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m       used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m       of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m       component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m       \u001b[0m\n\u001b[1;34m       Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel: Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m       or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m       measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34msort:  Whether the items should be sorted in the constructor.\u001b[0m\n\u001b[1;31mvdims: The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;31m       describe the components of the data. If multiple value\u001b[0m\n\u001b[1;31m       dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;31m       indexed by name after the key dimensions.\u001b[0m\n\u001b[1;31m       \u001b[0m\n\u001b[1;31m       Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.core.ndmapping.MultiDimensionalMapping",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Curve"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Area is a Chart element representing the area under a curve or\n    between two curves in a 1D coordinate system. The key dimension\n    represents the location of each coordinate along the x-axis, while\n    the value dimension(s) represent the height of the area or the\n    lower and upper bounds of the area between curves.\n\n    Multiple areas may be stacked by overlaying them an passing them\n    to the stack method.\n    \n\u001b[1;32mParameters of 'Area'\n====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                       'Area'                   String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Area",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Bars is a Chart element representing categorical observations\n    using the height of rectangular bars. The key dimensions represent\n    the categorical groupings of the data, but may also be used to\n    stack the bars, while the first value dimension represents the\n    height of each bar.\n    \n\u001b[1;32mParameters of 'Bars'\n====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                       'Bars'                   String             C RW \nkdims                  [Dimension('x')]               List     (1, 3)   V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Bars",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.stats.StatisticsElement"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Bivariate elements are containers for two dimensional data, which\n    is to be visualized as a kernel density estimate. The data should\n    be supplied in a tabular format of x- and y-columns.\n    \n\u001b[1;32mParameters of 'Bivariate'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                    'Bivariate'                 String             C RW \nkdims          [Dimension('x'), Dimension('y')]       List     (2, 2)   V RW \nlabel                         ''                     String             C RW \nvdims               [Dimension('Density')]            List     (0, 1)   V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.Bivariate",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.data.Dataset",
            "holoviews.core.element.Element2D"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    BoxWhisker represent data as a distributions highlighting the\n    median, mean and various percentiles. It may have a single value\n    dimension and any number of key dimensions declaring the grouping\n    of each violin.\n    \n\u001b[1;32mParameters of 'BoxWhisker'\n==========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                    'BoxWhisker'                String             C RW \nkdims                         []                      List   (0, None)  V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List     (1, 1)   V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.BoxWhisker",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.path.Path"
        ],
        "class_docstring": "params(level=Number, datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    The Contours element is a subtype of a Path which is characterized\n    by the fact that each path geometry may only be associated with\n    scalar values. It supports all the same data formats as a `Path`\n    but does not allow continuously varying values along the path\n    geometry's coordinates. Conceptually Contours therefore represent\n    iso-contours or isoclines, i.e. a function of two variables which\n    describes a curve along which the function has a constant value.\n\n    The canonical representation is a list of dictionaries storing the\n    x- and y-coordinates along with any other (scalar) values:\n\n        [{'x': 1d-array, 'y': 1d-array, 'value': scalar}, ...]\n\n    Alternatively Contours also supports a single columnar\n    data-structure to specify an individual contour:\n\n        {'x': 1d-array, 'y': 1d-array, 'value': scalar, 'continuous': 1d-array}\n\n    Since not all formats allow storing scalar values as actual\n    scalars arrays which are the same length as the coordinates but\n    have only one unique value are also considered scalar. This is\n    strictly enforced, ensuring that each path geometry represents\n    a valid iso-contour.\n\n    The easiest way of accessing the individual geometries is using\n    the `Contours.split` method, which returns each path geometry as a\n    separate entity, while the other methods assume a flattened\n    representation where all paths are separated by NaN values.\n    \n\u001b[1;32mParameters of 'Contours'\n========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                         Type         Bounds     Mode  \u001b[0m\n\ncdims                   OrderedDict()                     Dict                    V RW  \ndatatype   ['multitabular', 'dataframe', 'dictio...  ObjectSelector               V RW  \nextents            (None, None, None, None)              Tuple                    V RW  \ngroup                     'Contours'                     String                   C RW  \nkdims          [Dimension('x'), Dimension('y')]           List         (2, 2)     C RW  \nlabel                         ''                         String                   C RW  \nlevel                        None                        Number                 V RW AN \nvdims                         []                          List       (0, None)    C RW  \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions of a geometry represent the x- and y-\u001b[0m\n\u001b[1;34m          coordinates in a 2D space.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mlevel:    Optional level associated with the set of Contours.\u001b[0m\n\u001b[1;31mvdims:    Contours optionally accept a value dimension, corresponding\u001b[0m\n\u001b[1;31m          to the supplied values.\u001b[0m",
        "klass": "holoviews.element.Contours",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Curve is a Chart element representing a line in a 1D coordinate\n    system where the key dimension maps on the line x-coordinate and\n    the first value dimension represents the height of the line along\n    the y-axis.\n    \n\u001b[1;32mParameters of 'Curve'\n=====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Curve'                   String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Curve",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.stats.StatisticsElement"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Distribution elements provides a representation for a\n    one-dimensional distribution which can be visualized as a kernel\n    density estimate. The data should be supplied in a tabular format\n    and will use the first column.\n    \n\u001b[1;32mParameters of 'Distribution'\n============================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                   'Distribution'               String             C RW \nkdims                [Dimension('Value')]             List     (1, 1)   V RW \nlabel                         ''                     String             C RW \nvdims               [Dimension('Density')]            List     (0, 1)   V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.Distribution",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    ErrorBars is a Chart element representing error bars in a 1D\n    coordinate system where the key dimension corresponds to the\n    location along the x-axis and the value dimensions define the\n    location along the y-axis and the symmetric or assymetric spread.\n    \n\u001b[1;32mParameters of 'ErrorBars'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                    'ErrorBars'                 String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims       [Dimension('y'), Dimension('yerror')]     List   (1, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the quantity measured by the ErrorBars\u001b[0m\n\u001b[1;31m          object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.ErrorBars",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.data.Dataset",
            "holoviews.core.element.Element2D"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    HeatMap represents a 2D grid of categorical coordinates which can\n    be computed from a sparse tabular representation. A HeatMap does\n    not automatically aggregate the supplied values, so if the data\n    contains multiple entries for the same coordinate on the 2D grid\n    it should be aggregated using the aggregate method before display.\n\n    The HeatMap constructor will support any tabular or gridded data\n    format with 2 coordinates and at least one value dimension. A\n    simple example:\n\n        HeatMap([(x1, y1, z1), (x2, y2, z2), ...])\n\n    However any tabular and gridded format, including pandas\n    DataFrames, dictionaries of columns, xarray DataArrays and more\n    are supported if the library is importable.\n    \n\u001b[1;32mParameters of 'HeatMap'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                     'HeatMap'                  String             C RW \nkdims          [Dimension('x'), Dimension('y')]       List     (2, 2)   C RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('z')]               List   (0, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.HeatMap",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.data.Dataset",
            "holoviews.element.raster.Raster",
            "holoviews.core.sheetcoords.SheetCoordinateSystem"
        ],
        "class_docstring": "params(bounds=ClassSelector, rtol=Number, datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Image represents a regularly sampled 2D grid of an underlying\n    continuous space of intensity values, which will be colormapped on\n    plotting. The grid of intensity values may be specified as a NxM\n    sized array of values along with a bounds, but it may also be\n    defined through explicit and regularly spaced x/y-coordinate\n    arrays of shape M and N respectively. The two most basic supported\n    constructors of an Image therefore include:\n\n        Image((X, Y, Z))\n\n    where X is a 1D array of shape M, Y is a 1D array of shape N and\n    Z is a 2D array of shape NxM, or equivalently:\n\n        Image(Z, bounds=(x0, y0, x1, y1))\n\n    where Z is a 2D array of shape NxM defining the intensity values\n    and the bounds define the (left, bottom, top, right) edges of four\n    corners of the grid. Other gridded formats which support declaring\n    of explicit x/y-coordinate arrays such as xarray are also\n    supported.\n\n    Note that the interpretation of the orientation of the array\n    changes depending on whether bounds or explicit coordinates are\n    used.\n    \n\u001b[1;32mParameters of 'Image'\n=====================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                         Type        Bounds     Mode  \u001b[0m\n\nbounds             BoundingBox(radius=0.5)           ClassSelector               V RW  \ncdims                   OrderedDict()                     Dict                   V RW  \ndatatype   ['grid', 'xarray', 'image', 'cube', '...       List      (0, None)    V RW  \nextents            (None, None, None, None)              Tuple                   V RW  \ngroup                      'Image'                       String                  C RW  \nkdims          [Dimension('x'), Dimension('y')]           List        (2, 2)     C RW  \nlabel                         ''                         String                  C RW  \nrtol                         None                        Number                V RW AN \nvdims                  [Dimension('z')]                   List      (1, None)    V RW  \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mbounds:   The bounding region in sheet coordinates containing the data.\u001b[0m\n\u001b[1;31mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;31m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;31m          \u001b[0m\n\u001b[1;31m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;34mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;34m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;34m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;34m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;34m          the data fails to be understood).\u001b[0m\n\u001b[1;31mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;31m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;34mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;31mkdims:    The label of the x- and y-dimension of the Raster in the form\u001b[0m\n\u001b[1;31m          of a string or dimension object.\u001b[0m\n\u001b[1;34mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;34m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;34m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;31mrtol:     The tolerance used to enforce regular sampling for regular, gridded\u001b[0m\n\u001b[1;31m          data where regular sampling is expected. Expressed as the maximal\u001b[0m\n\u001b[1;31m          allowable sampling difference between sample locations.\u001b[0m\n\u001b[1;34mvdims:    The dimension description of the data held in the matrix.\u001b[0m",
        "klass": "holoviews.element.Image",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.path.Contours"
        ],
        "class_docstring": "params(level=Number, datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    The Polygons element represents one or more polygon geometries\n    with associated scalar values. Each polygon geometry may be split\n    into sub-geometries on NaN-values and may be associated with\n    scalar values. In analogy to GEOS geometry types a Polygons\n    element is a collection of Polygon and MultiPolygon\n    geometries. Polygon geometries are defined as a set of coordinates\n    describing the exterior bounding ring and any number of interior\n    holes.\n\n    Like all other elements a Polygons element may be defined through\n    an extensible list of interfaces. Natively HoloViews provides the\n    MultiInterface which allows representing paths as lists of regular\n    columnar data objects including arrays, dataframes and\n    dictionaries of column arrays and scalars.\n\n    The canonical representation is a list of dictionaries storing the\n    x- and y-coordinates, a list-of-lists of arrays representing the\n    holes, along with any other values:\n\n        [{'x': 1d-array, 'y': 1d-array, 'holes': list-of-lists-of-arrays, 'value': scalar}, ...]\n\n    Alternatively Polygons also supports a single columnar\n    data-structure to specify an individual polygon:\n\n        {'x': 1d-array, 'y': 1d-array, 'holes': list-of-lists-of-arrays, 'value': scalar}\n\n    The list-of-lists format of the holes corresponds to the potential\n    for each coordinate array to be split into a multi-geometry\n    through NaN-separators. Each sub-geometry separated by the NaNs\n    therefore has an unambiguous mapping to a list of holes. If a\n    (multi-)polygon has no holes, the 'holes' key may be ommitted.\n\n    Any value dimensions stored on a Polygons geometry must be scalar,\n    just like the Contours element. Since not all formats allow\n    storing scalar values as actual scalars arrays which are the same\n    length as the coordinates but have only one unique value are also\n    considered scalar.\n\n    The easiest way of accessing the individual geometries is using\n    the `Polygons.split` method, which returns each path geometry as a\n    separate entity, while the other methods assume a flattened\n    representation where all paths are separated by NaN values.\n    \n\u001b[1;32mParameters of 'Polygons'\n========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                         Type         Bounds     Mode  \u001b[0m\n\ncdims                   OrderedDict()                     Dict                    V RW  \ndatatype   ['multitabular', 'dataframe', 'dictio...  ObjectSelector               V RW  \nextents            (None, None, None, None)              Tuple                    V RW  \ngroup                     'Polygons'                     String                   C RW  \nkdims          [Dimension('x'), Dimension('y')]           List         (2, 2)     C RW  \nlabel                         ''                         String                   C RW  \nlevel                        None                        Number                 V RW AN \nvdims                         []                          List       (0, None)    V RW  \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions of a geometry represent the x- and y-\u001b[0m\n\u001b[1;34m          coordinates in a 2D space.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mlevel:    Optional level associated with the set of Contours.\u001b[0m\n\u001b[1;31mvdims:    Polygons optionally accept a value dimension, corresponding\u001b[0m\n\u001b[1;31m          to the supplied value.\u001b[0m",
        "klass": "holoviews.element.Polygons",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.data.Dataset",
            "holoviews.core.element.Element2D"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    A QuadMesh represents 2D rectangular grid expressed as x- and\n    y-coordinates defined as 1D or 2D arrays. Unlike the Image type\n    a QuadMesh may be regularly or irregularly spaced and contain\n    either bin edges or bin centers. If bin edges are supplied the\n    shape of the x/y-coordinate arrays should be one greater than the\n    shape of the value array.\n\n    The default interface expects data to be specified in the form:\n\n        QuadMesh((X, Y, Z))\n\n    where X and Y may be 1D or 2D arrays of the shape N(+1) and M(+1)\n    respectively or N(+1)xM(+1) and the Z value array should be of\n    shape NxM. Other gridded formats such as xarray are also supported\n    if installed.\n\n    The grid orientation follows the standard matrix convention: An\n    array Z with shape (nrows, ncolumns) is plotted with the column\n    number as X and the row number as Y.\n    \n\u001b[1;32mParameters of 'QuadMesh'\n========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                     'QuadMesh'                 String             C RW \nkdims          [Dimension('x'), Dimension('y')]       List     (2, 2)   C RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('z')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.QuadMesh",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.element.Element2D"
        ],
        "class_docstring": "params(extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Raster is a basic 2D element type for presenting either numpy or\n    dask arrays as two dimensional raster images.\n\n    Arrays with a shape of (N,M) are valid inputs for Raster whereas\n    subclasses of Raster (e.g. RGB) may also accept 3D arrays\n    containing channel information.\n\n    Raster does not support slicing like the Image or RGB subclasses\n    and the extents are in matrix coordinates if not explicitly\n    specified.\n    \n\u001b[1;32mParameters of 'Raster'\n======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                   Value                 Type     Bounds   Mode \u001b[0m\n\ncdims              OrderedDict()             Dict              V RW \nextents       (None, None, None, None)      Tuple              V RW \ngroup                 'Raster'              String             C RW \nkdims     [Dimension('x'), Dimension('y')]   List     (2, 2)   C RW \nlabel                    ''                 String             C RW \nvdims             [Dimension('z')]           List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:   The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m         pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m         \u001b[0m\n\u001b[1;34m         Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mextents: Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;31m         as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;34mgroup:   A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;31mkdims:   The label of the x- and y-dimension of the Raster in form\u001b[0m\n\u001b[1;31m         of a string or dimension object.\u001b[0m\n\u001b[1;34mlabel:   Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;34m         or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;34m         measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;31mvdims:   The dimension description of the data held in the matrix.\u001b[0m",
        "klass": "holoviews.element.Raster",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Scatter is a Chart element representing a set of points in a 1D\n    coordinate system where the key dimension maps to the points\n    location along the x-axis while the first value dimension\n    represents the location of the point along the y-axis.\n    \n\u001b[1;32mParameters of 'Scatter'\n=======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                     'Scatter'                  String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List   (1, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Scatter",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.core.element.Element3D",
            "holoviews.element.geom.Points"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Scatter3D is a 3D element representing the position of a collection\n    of coordinates in a 3D space. The key dimensions represent the \n    position of each coordinate along the x-, y- and z-axis while the\n    value dimensions can optionally supply additional information.\n    \n\u001b[1;32mParameters of 'Scatter3D'\n=========================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents      (None, None, None, None, None, None)    Tuple              V RW \ngroup                    'Scatter3D'                 String             C RW \nkdims      [Dimension('x'), Dimension('y'), Dime...   List   (0, None)  V RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 3D space\u001b[0m\n\u001b[1;34m          defined as (xmin, ymin, zmin, xmax, ymax, zmax).\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions of a geometry represent the x- and y-\u001b[0m\n\u001b[1;34m          coordinates in a 2D space.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    Scatter3D can have optional value dimensions,\u001b[0m\n\u001b[1;34m          which may be mapped onto color and size.\u001b[0m",
        "klass": "holoviews.element.Scatter3D",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.Chart"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Spikes is a Chart element which represents a number of discrete\n    spikes, events or observations in a 1D coordinate system. The key\n    dimension therefore represents the position of each spike along\n    the x-axis while the first value dimension, if defined, controls\n    the height along the y-axis. It may therefore be used to visualize\n    the distribution of discrete events, representing a rug plot, or\n    to draw the strength some signal.\n    \n\u001b[1;32mParameters of 'Spikes'\n======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Spikes'                  String             C RW \nkdims                  [Dimension('x')]               List     (1, 1)   V RW \nlabel                         ''                     String             C RW \nvdims                         []                      List   (0, None)  V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Spikes",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.chart.ErrorBars"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Spread is a Chart element representing a spread of values or\n    confidence band in a 1D coordinate system. The key dimension(s)\n    corresponds to the location along the x-axis and the value\n    dimensions define the location along the y-axis as well as the\n    symmetric or assymetric spread.\n    \n\u001b[1;32mParameters of 'Spread'\n======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Spread'                  String             C RW \nkdims                  [Dimension('x')]               List     (1, 2)   V RW \nlabel                         ''                     String             C RW \nvdims       [Dimension('y'), Dimension('yerror')]     List   (1, None)  C RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the quantity measured by the ErrorBars\u001b[0m\n\u001b[1;31m          object.\u001b[0m\n\u001b[1;34mkdims:    The key dimension(s) of a Chart represent the independent\u001b[0m\n\u001b[1;34m          variable(s).\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions of the Chart, usually corresponding to a\u001b[0m\n\u001b[1;34m          number of dependent variables.\u001b[0m",
        "klass": "holoviews.element.Spread",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "holoviews.element.stats.BoxWhisker"
        ],
        "class_docstring": "params(datatype=List, extents=Tuple, cdims=Dict, kdims=List, vdims=List, group=String, label=String, name=String)\n\n    Violin elements represent data as 1D distributions visualized\n    as a kernel-density estimate. It may have a single value dimension\n    and any number of key dimensions declaring the grouping of each\n    violin.\n    \n\u001b[1;32mParameters of 'Violin'\n======================\n\u001b[0m\n\u001b[1;31mParameters changed from their default values are marked in red.\u001b[0m\n\u001b[1;36mSoft bound values are marked in cyan.\u001b[0m\nC/V= Constant/Variable, RO/RW = ReadOnly/ReadWrite, AN=Allow None\n\n\u001b[1;34mName                        Value                     Type     Bounds   Mode \u001b[0m\n\ncdims                   OrderedDict()                 Dict              V RW \ndatatype   ['dictionary', 'grid', 'xarray', 'arr...   List   (0, None)  V RW \nextents            (None, None, None, None)          Tuple              V RW \ngroup                      'Violin'                  String             C RW \nkdims                         []                      List   (0, None)  V RW \nlabel                         ''                     String             C RW \nvdims                  [Dimension('y')]               List     (1, 1)   V RW \n\n\u001b[1;32mParameter docstrings:\n=====================\u001b[0m\n\n\u001b[1;34mcdims:    The constant dimensions defined as a dictionary of Dimension:value\u001b[0m\n\u001b[1;34m          pairs providing additional dimension information about the object.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with constant_dimensions.\u001b[0m\n\u001b[1;31mdatatype: A priority list of the data types to be used for storage\u001b[0m\n\u001b[1;31m          on the .data attribute. If the input supplied to the element\u001b[0m\n\u001b[1;31m          constructor cannot be put into the requested format, the next\u001b[0m\n\u001b[1;31m          format listed will be used until a suitable format is found (or\u001b[0m\n\u001b[1;31m          the data fails to be understood).\u001b[0m\n\u001b[1;34mextents:  Allows overriding the extents of the Element in 2D space defined\u001b[0m\n\u001b[1;34m          as four-tuple defining the (left, bottom, right and top) edges.\u001b[0m\n\u001b[1;31mgroup:    A string describing the data wrapped by the object.\u001b[0m\n\u001b[1;34mkdims:    The key dimensions defined as list of dimensions that may be\u001b[0m\n\u001b[1;34m          used in indexing (and potential slicing) semantics. The order\u001b[0m\n\u001b[1;34m          of the dimensions listed here determines the semantics of each\u001b[0m\n\u001b[1;34m          component of a multi-dimensional indexing operation.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with key_dimensions.\u001b[0m\n\u001b[1;31mlabel:    Optional label describing the data, typically reflecting where\u001b[0m\n\u001b[1;31m          or how it was measured. The label should allow a specific\u001b[0m\n\u001b[1;31m          measurement or dataset to be referenced for a given group.\u001b[0m\n\u001b[1;34mvdims:    The value dimensions defined as the list of dimensions used to\u001b[0m\n\u001b[1;34m          describe the components of the data. If multiple value\u001b[0m\n\u001b[1;34m          dimensions are supplied, a particular value dimension may be\u001b[0m\n\u001b[1;34m          indexed by name after the key dimensions.\u001b[0m\n\u001b[1;34m          \u001b[0m\n\u001b[1;34m          Aliased with value_dimensions.\u001b[0m",
        "klass": "holoviews.element.Violin",
        "module": "holoviews"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A compatibility class for writing CSV files.\n\n  This class is to `csv.DictWriter` what `CsvWriter` is `csv.writer`. Consult\n  documentation for `CsvWriter` for more rationale.\n\n  Args:\n    columns: A list of column names to base row writing on.\n    delimiter: A delimiter to separate the values with. Defaults to a comma.\n  ",
        "klass": "grr_response_core.lib.util.compat.csv.DictWriter",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A compatibility class for writing CSV files.\n\n  This class should be used instead of the `csv.writer` that has API differences\n  across Python 2 and Python 3. This class provides unified interface that\n  should work the same way on both versions. Once support for Python 2 is\n  dropped, this class can be removed and code can be refactored to use the\n  native class.\n\n  Args:\n    delimiter: A delimiter to separate the values with. Defaults to a comma.\n  ",
        "klass": "grr_response_core.lib.util.compat.csv.Writer",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "contextlib.AbstractContextManager",
            "typing.Generic"
        ],
        "class_docstring": "A context managers that sequences multiple context managers.\n\n  This is similar to the monadic `sequence` operator: it takes a list of context\n  managers, enters each of them and yields list of values that the managers\n  yield.\n\n  One possible scenario where this class comes in handy is when one needs to\n  open multiple files.\n  ",
        "klass": "grr_response_core.lib.util.context.MultiContext",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "contextlib.AbstractContextManager",
            "typing.Generic"
        ],
        "class_docstring": "A context manager that always yields provided values.\n\n  This class is useful for providing context-like semantics for values that are\n  not context managers themselves because they do not need to manage any\n  resources but are used as context managers.\n\n  This is a backport of the `contextlib.nullcontext` class introduced in Python\n  3.7. Once support for old versions of Python is dropped, all uses of this\n  class should be replaced with the one provided by the standard library.\n  ",
        "klass": "grr_response_core.lib.util.context.NullContext",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates a temporary directory based on the environment configuration.\n\n  The directory will be placed in folder as specified by the `TEST_TMPDIR`\n  environment variable if available or fallback to `Test.tmpdir` of the current\n  configuration if not.\n\n  This object is a context manager and the directory is automatically removed\n  when it goes out of scope.\n\n  Args:\n    suffix: A suffix to end the directory name with.\n    prefix: A prefix to begin the directory name with.\n    remove_non_empty: If set to `True` the directory removal will succeed even\n      if it is not empty.\n\n  Returns:\n    An absolute path to the created directory.\n  ",
        "klass": "grr_response_core.lib.util.temp.AutoTempDirPath",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Creates a temporary file based on the environment configuration.\n\n  If no directory is specified the file will be placed in folder as specified by\n  the `TEST_TMPDIR` environment variable if available or fallback to\n  `Test.tmpdir` of the current configuration if not.\n\n  If directory is specified it must be part of the default test temporary\n  directory.\n\n  This object is a context manager and the associated file is automatically\n  removed when it goes out of scope.\n\n  Args:\n    suffix: A suffix to end the file name with.\n    prefix: A prefix to begin the file name with.\n    dir: A directory to place the file in.\n\n  Returns:\n    An absolute path to the created file.\n\n  Raises:\n    ValueError: If the specified directory is not part of the default test\n        temporary directory.\n  ",
        "klass": "grr_response_core.lib.util.temp.AutoTempFilePath",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for doing simple stubs.",
        "klass": "grr_response_core.lib.utils.MultiStubber",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A context manager for doing simple stubs.",
        "klass": "grr_response_core.lib.utils.Stubber",
        "module": "grr_response_core"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a key (object) in an S3 bucket.\n\n    :ivar bucket: The parent :class:`boto.s3.bucket.Bucket`.\n    :ivar name: The name of this Key object.\n    :ivar metadata: A dictionary containing user metadata that you\n        wish to store with the object or that has been retrieved from\n        an existing object.\n    :ivar cache_control: The value of the `Cache-Control` HTTP header.\n    :ivar content_type: The value of the `Content-Type` HTTP header.\n    :ivar content_encoding: The value of the `Content-Encoding` HTTP header.\n    :ivar content_disposition: The value of the `Content-Disposition` HTTP\n        header.\n    :ivar content_language: The value of the `Content-Language` HTTP header.\n    :ivar etag: The `etag` associated with this object.\n    :ivar last_modified: The string timestamp representing the last\n        time this object was modified in S3.\n    :ivar owner: The ID of the owner of this object.\n    :ivar storage_class: The storage class of the object.  Currently, one of:\n        STANDARD | REDUCED_REDUNDANCY | GLACIER\n    :ivar md5: The MD5 hash of the contents of the object.\n    :ivar size: The size, in bytes, of the object.\n    :ivar version_id: The version ID of this object, if it is a versioned\n        object.\n    :ivar encrypted: Whether the object is encrypted while at rest on\n        the server.\n    ",
        "klass": "boto.s3.key.Key",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    ",
        "klass": "boto.compat.Queue",
        "module": "queue"
    },
    {
        "base_classes": [
            "_io._TextIOBase"
        ],
        "class_docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.",
        "klass": "boto.compat.StringIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Control serialization/deserialization of types.\n\n    This class controls the encoding of python types to the\n    format that is expected by the DynamoDB API, as well as\n    taking DynamoDB types and constructing the appropriate\n    python types.\n\n    If you want to customize this process, you can subclass\n    this class and override the encoding/decoding of\n    specific types.  For example::\n\n        'foo'      (Python type)\n            |\n            v\n        encode('foo')\n            |\n            v\n        _encode_s('foo')\n            |\n            v\n        {'S': 'foo'}  (Encoding sent to/received from DynamoDB)\n            |\n            V\n        decode({'S': 'foo'})\n            |\n            v\n        _decode_s({'S': 'foo'})\n            |\n            v\n        'foo'     (Python type)\n\n    ",
        "klass": "boto.dynamodb.types.Dynamizer",
        "module": "boto"
    },
    {
        "base_classes": [
            "boto.dynamodb.types.NonBooleanDynamizer"
        ],
        "class_docstring": "Use float/int instead of Decimal for numeric types.\n\n    This class is provided for backwards compatibility.  Instead of\n    using Decimals for the 'N', 'NS' types it uses ints/floats.\n\n    This class is deprecated and its usage is not encouraged,\n    as doing so may result in loss of precision.  Use the\n    `Dynamizer` class instead.\n\n    ",
        "klass": "boto.dynamodb.types.LossyFloatDynamizer",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Interacts & models the behavior of a DynamoDB table.\n\n    The ``Table`` object represents a set (or rough categorization) of\n    records within DynamoDB. The important part is that all records within the\n    table, while largely-schema-free, share the same schema & are essentially\n    namespaced for use in your application. For example, you might have a\n    ``users`` table or a ``forums`` table.\n    ",
        "klass": "boto.dynamodb2.table.Table",
        "module": "boto"
    },
    {
        "base_classes": [
            "boto.ec2.ec2object.TaggedEC2Object"
        ],
        "class_docstring": "\n    Represents an instance.\n\n    :ivar id: The unique ID of the Instance.\n    :ivar groups: A list of Group objects representing the security\n                  groups associated with the instance.\n    :ivar public_dns_name: The public dns name of the instance.\n    :ivar private_dns_name: The private dns name of the instance.\n    :ivar state: The string representation of the instance's current state.\n    :ivar state_code: An integer representation of the instance's\n        current state.\n    :ivar previous_state: The string representation of the instance's\n        previous state.\n    :ivar previous_state_code: An integer representation of the\n        instance's current state.\n    :ivar key_name: The name of the SSH key associated with the instance.\n    :ivar instance_type: The type of instance (e.g. m1.small).\n    :ivar launch_time: The time the instance was launched.\n    :ivar image_id: The ID of the AMI used to launch this instance.\n    :ivar placement: The availability zone in which the instance is running.\n    :ivar placement_group: The name of the placement group the instance\n        is in (for cluster compute instances).\n    :ivar placement_tenancy: The tenancy of the instance, if the instance\n        is running within a VPC.  An instance with a tenancy of dedicated\n        runs on a single-tenant hardware.\n    :ivar kernel: The kernel associated with the instance.\n    :ivar ramdisk: The ramdisk associated with the instance.\n    :ivar architecture: The architecture of the image (i386|x86_64).\n    :ivar hypervisor: The hypervisor used.\n    :ivar virtualization_type: The type of virtualization used.\n    :ivar product_codes: A list of product codes associated with this instance.\n    :ivar ami_launch_index: This instances position within it's launch group.\n    :ivar monitored: A boolean indicating whether monitoring is enabled or not.\n    :ivar monitoring_state: A string value that contains the actual value\n        of the monitoring element returned by EC2.\n    :ivar spot_instance_request_id: The ID of the spot instance request\n        if this is a spot instance.\n    :ivar subnet_id: The VPC Subnet ID, if running in VPC.\n    :ivar vpc_id: The VPC ID, if running in VPC.\n    :ivar private_ip_address: The private IP address of the instance.\n    :ivar ip_address: The public IP address of the instance.\n    :ivar platform: Platform of the instance (e.g. Windows)\n    :ivar root_device_name: The name of the root device.\n    :ivar root_device_type: The root device type (ebs|instance-store).\n    :ivar block_device_mapping: The Block Device Mapping for the instance.\n    :ivar state_reason: The reason for the most recent state transition.\n    :ivar interfaces: List of Elastic Network Interfaces associated with\n        this instance.\n    :ivar ebs_optimized: Whether instance is using optimized EBS volumes\n        or not.\n    :ivar instance_profile: A Python dict containing the instance\n        profile id and arn associated with this instance.\n    ",
        "klass": "boto.ec2.instance.Instance",
        "module": "boto"
    },
    {
        "base_classes": [
            "boto.s3.key.Key"
        ],
        "class_docstring": "\n    Represents a key (object) in a GS bucket.\n\n    :ivar bucket: The parent :class:`boto.gs.bucket.Bucket`.\n    :ivar name: The name of this Key object.\n    :ivar metadata: A dictionary containing user metadata that you\n        wish to store with the object or that has been retrieved from\n        an existing object.\n    :ivar cache_control: The value of the `Cache-Control` HTTP header.\n    :ivar content_type: The value of the `Content-Type` HTTP header.\n    :ivar content_encoding: The value of the `Content-Encoding` HTTP header.\n    :ivar content_disposition: The value of the `Content-Disposition` HTTP\n        header.\n    :ivar content_language: The value of the `Content-Language` HTTP header.\n    :ivar etag: The `etag` associated with this object.\n    :ivar last_modified: The string timestamp representing the last\n        time this object was modified in GS.\n    :ivar owner: The ID of the owner of this object.\n    :ivar storage_class: The storage class of the object. Currently, one of:\n        STANDARD | DURABLE_REDUCED_AVAILABILITY.\n    :ivar md5: The MD5 hash of the contents of the object.\n    :ivar size: The size, in bytes, of the object.\n    :ivar generation: The generation number of the object.\n    :ivar metageneration: The generation number of the object metadata.\n    :ivar encrypted: Whether the object is encrypted while at rest on\n        the server.\n    :ivar cloud_hashes: Dictionary of checksums as supplied by the storage\n        provider.\n    ",
        "klass": "boto.gs.key.Key",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents a key (object) in an S3 bucket.\n\n    :ivar bucket: The parent :class:`boto.s3.bucket.Bucket`.\n    :ivar name: The name of this Key object.\n    :ivar metadata: A dictionary containing user metadata that you\n        wish to store with the object or that has been retrieved from\n        an existing object.\n    :ivar cache_control: The value of the `Cache-Control` HTTP header.\n    :ivar content_type: The value of the `Content-Type` HTTP header.\n    :ivar content_encoding: The value of the `Content-Encoding` HTTP header.\n    :ivar content_disposition: The value of the `Content-Disposition` HTTP\n        header.\n    :ivar content_language: The value of the `Content-Language` HTTP header.\n    :ivar etag: The `etag` associated with this object.\n    :ivar last_modified: The string timestamp representing the last\n        time this object was modified in S3.\n    :ivar owner: The ID of the owner of this object.\n    :ivar storage_class: The storage class of the object.  Currently, one of:\n        STANDARD | REDUCED_REDUNDANCY | GLACIER\n    :ivar md5: The MD5 hash of the contents of the object.\n    :ivar size: The size, in bytes, of the object.\n    :ivar version_id: The version ID of this object, if it is a versioned\n        object.\n    :ivar encrypted: Whether the object is encrypted while at rest on\n        the server.\n    ",
        "klass": "boto.s3.connection.Key",
        "module": "boto"
    },
    {
        "base_classes": [
            "boto.swf.layer2.SWFBase"
        ],
        "class_docstring": "A versioned workflow type.",
        "klass": "boto.swf.layer2.WorkflowType",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The core component of Jinja is the `Environment`.  It contains\n    important shared variables like configuration, filters, tests,\n    globals and others.  Instances of this class may be modified if\n    they are not shared and if no template was loaded so far.\n    Modifications on environments after the first template was loaded\n    will lead to surprising effects and undefined behavior.\n\n    Here are the possible initialization parameters:\n\n        `block_start_string`\n            The string marking the beginning of a block.  Defaults to ``'{%'``.\n\n        `block_end_string`\n            The string marking the end of a block.  Defaults to ``'%}'``.\n\n        `variable_start_string`\n            The string marking the beginning of a print statement.\n            Defaults to ``'{{'``.\n\n        `variable_end_string`\n            The string marking the end of a print statement.  Defaults to\n            ``'}}'``.\n\n        `comment_start_string`\n            The string marking the beginning of a comment.  Defaults to ``'{#'``.\n\n        `comment_end_string`\n            The string marking the end of a comment.  Defaults to ``'#}'``.\n\n        `line_statement_prefix`\n            If given and a string, this will be used as prefix for line based\n            statements.  See also :ref:`line-statements`.\n\n        `line_comment_prefix`\n            If given and a string, this will be used as prefix for line based\n            comments.  See also :ref:`line-statements`.\n\n            .. versionadded:: 2.2\n\n        `trim_blocks`\n            If this is set to ``True`` the first newline after a block is\n            removed (block, not variable tag!).  Defaults to `False`.\n\n        `lstrip_blocks`\n            If this is set to ``True`` leading spaces and tabs are stripped\n            from the start of a line to a block.  Defaults to `False`.\n\n        `newline_sequence`\n            The sequence that starts a newline.  Must be one of ``'\\r'``,\n            ``'\\n'`` or ``'\\r\\n'``.  The default is ``'\\n'`` which is a\n            useful default for Linux and OS X systems as well as web\n            applications.\n\n        `keep_trailing_newline`\n            Preserve the trailing newline when rendering templates.\n            The default is ``False``, which causes a single newline,\n            if present, to be stripped from the end of the template.\n\n            .. versionadded:: 2.7\n\n        `extensions`\n            List of Jinja extensions to use.  This can either be import paths\n            as strings or extension classes.  For more information have a\n            look at :ref:`the extensions documentation <jinja-extensions>`.\n\n        `optimized`\n            should the optimizer be enabled?  Default is ``True``.\n\n        `undefined`\n            :class:`Undefined` or a subclass of it that is used to represent\n            undefined values in the template.\n\n        `finalize`\n            A callable that can be used to process the result of a variable\n            expression before it is output.  For example one can convert\n            ``None`` implicitly into an empty string here.\n\n        `autoescape`\n            If set to ``True`` the XML/HTML autoescaping feature is enabled by\n            default.  For more details about autoescaping see\n            :class:`~jinja2.utils.Markup`.  As of Jinja 2.4 this can also\n            be a callable that is passed the template name and has to\n            return ``True`` or ``False`` depending on autoescape should be\n            enabled by default.\n\n            .. versionchanged:: 2.4\n               `autoescape` can now be a function\n\n        `loader`\n            The template loader for this environment.\n\n        `cache_size`\n            The size of the cache.  Per default this is ``400`` which means\n            that if more than 400 templates are loaded the loader will clean\n            out the least recently used template.  If the cache size is set to\n            ``0`` templates are recompiled all the time, if the cache size is\n            ``-1`` the cache will not be cleaned.\n\n            .. versionchanged:: 2.8\n               The cache size was increased to 400 from a low 50.\n\n        `auto_reload`\n            Some loaders load templates from locations where the template\n            sources may change (ie: file system or database).  If\n            ``auto_reload`` is set to ``True`` (default) every time a template is\n            requested the loader checks if the source changed and if yes, it\n            will reload the template.  For higher performance it's possible to\n            disable that.\n\n        `bytecode_cache`\n            If set to a bytecode cache object, this object will provide a\n            cache for the internal Jinja bytecode so that templates don't\n            have to be parsed if they were not changed.\n\n            See :ref:`bytecode-cache` for more information.\n\n        `enable_async`\n            If set to true this enables async template execution which allows\n            you to take advantage of newer Python features.  This requires\n            Python 3.6 or later.\n    ",
        "klass": "jinja2.Environment",
        "module": "jinja2"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "A string that is ready to be safely inserted into an HTML or XML\n    document, either because it was escaped or because it was marked\n    safe.\n\n    Passing an object to the constructor converts it to text and wraps\n    it to mark it safe without escaping. To escape the text, use the\n    :meth:`escape` class method instead.\n\n    >>> Markup('Hello, <em>World</em>!')\n    Markup('Hello, <em>World</em>!')\n    >>> Markup(42)\n    Markup('42')\n    >>> Markup.escape('Hello, <em>World</em>!')\n    Markup('Hello &lt;em&gt;World&lt;/em&gt;!')\n\n    This implements the ``__html__()`` interface that some frameworks\n    use. Passing an object that implements ``__html__()`` will wrap the\n    output of that method, marking it safe.\n\n    >>> class Foo:\n    ...     def __html__(self):\n    ...         return '<a href=\"/foo\">foo</a>'\n    ...\n    >>> Markup(Foo())\n    Markup('<a href=\"/foo\">foo</a>')\n\n    This is a subclass of the text type (``str`` in Python 3,\n    ``unicode`` in Python 2). It has the same methods as that type, but\n    all methods escape their arguments and return a ``Markup`` instance.\n\n    >>> Markup('<em>%s</em>') % 'foo & bar'\n    Markup('<em>foo &amp; bar</em>')\n    >>> Markup('<em>Hello</em> ') + '<foo>'\n    Markup('<em>Hello</em> &lt;foo&gt;')\n    ",
        "klass": "jinja2.Markup",
        "module": "markupsafe"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The central template object.  This class represents a compiled template\n    and is used to evaluate it.\n\n    Normally the template object is generated from an :class:`Environment` but\n    it also has a constructor that makes it possible to create a template\n    instance directly using the constructor.  It takes the same arguments as\n    the environment constructor but it's not possible to specify a loader.\n\n    Every template object has a few methods and members that are guaranteed\n    to exist.  However it's important that a template object should be\n    considered immutable.  Modifications on the object are not supported.\n\n    Template objects created from the constructor rather than an environment\n    do have an `environment` attribute that points to a temporary environment\n    that is probably shared with other templates created with the constructor\n    and compatible settings.\n\n    >>> template = Template('Hello {{ name }}!')\n    >>> template.render(name='John Doe') == u'Hello John Doe!'\n    True\n    >>> stream = template.stream(name='John Doe')\n    >>> next(stream) == u'Hello John Doe!'\n    True\n    >>> next(stream)\n    Traceback (most recent call last):\n        ...\n    StopIteration\n    ",
        "klass": "jinja2.Template",
        "module": "jinja2"
    },
    {
        "base_classes": [
            "jinja2.environment.Environment"
        ],
        "class_docstring": "The sandboxed environment.  It works like the regular environment but\n    tells the compiler to generate sandboxed code.  Additionally subclasses of\n    this environment may override the methods that tell the runtime what\n    attributes or functions are safe to access.\n\n    If the template tries to access insecure code a :exc:`SecurityError` is\n    raised.  However also other exceptions may occur during the rendering so\n    the caller has to ensure that all exceptions are caught.\n    ",
        "klass": "jinja2.sandbox.SandboxedEnvironment",
        "module": "jinja2"
    },
    {
        "base_classes": [
            "docutils.io.Input"
        ],
        "class_docstring": "\n    Input for single, simple file-like objects.\n    ",
        "klass": "docutils.io.FileInput",
        "module": "docutils"
    },
    {
        "base_classes": [
            "docutils.nodes.Special",
            "docutils.nodes.Inline",
            "docutils.nodes.PreBibliographic",
            "docutils.nodes.FixedTextElement"
        ],
        "class_docstring": "\n    Raw data that is to be passed untouched to the Writer.\n    ",
        "klass": "docutils.nodes.raw",
        "module": "docutils"
    },
    {
        "base_classes": [
            "docutils.nodes.Structural",
            "docutils.nodes.Element"
        ],
        "class_docstring": "\n    Topics are terminal, \"leaf\" mini-sections, like block quotes with titles,\n    or textual figures.  A topic is just like a section, except that it has no\n    subsections, and it doesn't have to conform to section placement rules.\n\n    Topics are allowed wherever body elements (list, table, etc.) are allowed,\n    but only at the top level of a section or document.  Topics cannot nest\n    inside topics, sidebars, or body elements; you can't have a topic inside a\n    table, list, block quote, etc.\n    ",
        "klass": "docutils.nodes.topic",
        "module": "docutils"
    },
    {
        "base_classes": [
            "reportlab.graphics.shapes.Shape"
        ],
        "class_docstring": "Groups elements together.  May apply a transform\n    to its contents.  Has a publicly accessible property\n    'contents' which may be used to iterate over contents.\n    In addition, child nodes may be given a name in which\n    case they are subsequently accessible as properties.",
        "klass": "reportlab.graphics.shapes.Group",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.pdfgen.textobject._PDFColorSetter"
        ],
        "class_docstring": "This class is the programmer's interface to the PDF file format.  Methods\n    are (or will be) provided here to do just about everything PDF can do.\n\n    The underlying model to the canvas concept is that of a graphics state machine\n    that at any given point in time has a current font, fill color (for figure\n    interiors), stroke color (for figure borders), line width and geometric transform, among\n    many other characteristics.\n\n    Canvas methods generally either draw something (like canvas.line) using the\n    current state of the canvas or change some component of the canvas\n    state (like canvas.setFont).  The current state can be saved and restored\n    using the saveState/restoreState methods.\n\n    Objects are \"painted\" in the order they are drawn so if, for example\n    two rectangles overlap the last draw will appear \"on top\".  PDF form\n    objects (supported here) are used to draw complex drawings only once,\n    for possible repeated use.\n\n    There are other features of canvas which are not visible when printed,\n    such as outlines and bookmarks which are used for navigating a document\n    in a viewer.\n\n    Here is a very silly example usage which generates a Hello World pdf document.\n\n    Example:: \n    \n       from reportlab.pdfgen import canvas\n       c = canvas.Canvas(\"hello.pdf\")\n       from reportlab.lib.units import inch\n       # move the origin up and to the left\n       c.translate(inch,inch)\n       # define a large font\n       c.setFont(\"Helvetica\", 80)\n       # choose some colors\n       c.setStrokeColorRGB(0.2,0.5,0.3)\n       c.setFillColorRGB(1,0,1)\n       # draw a rectangle\n       c.rect(inch,inch,6*inch,9*inch, fill=1)\n       # make text go straight up\n       c.rotate(90)\n       # change color\n       c.setFillColorRGB(0,0,0.77)\n       # say hello (note after rotate the y coord needs to be negative!)\n       c.drawString(3*inch, -3*inch, \"Hello World\")\n       c.showPage()\n       c.save()\n\n    ",
        "klass": "reportlab.pdfgen.canvas.Canvas",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.graphics.shapes.Group",
            "reportlab.platypus.flowables.Flowable"
        ],
        "class_docstring": "Outermost container; the thing a renderer works on.\n    This has no properties except a height, width and list\n    of contents.",
        "klass": "reportlab.graphics.shapes.Drawing",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.platypus.flowables.Flowable"
        ],
        "class_docstring": " Paragraph(text, style, bulletText=None, caseSensitive=1)\n        text a string of stuff to go into the paragraph.\n        style is a style definition as in reportlab.lib.styles.\n        bulletText is an optional bullet defintion.\n        caseSensitive set this to 0 if you want the markup tags and their attributes to be case-insensitive.\n\n        This class is a flowable that can format a block of text\n        into a paragraph with a given style.\n\n        The paragraph Text can contain XML-like markup including the tags:\n        <b> ... </b> - bold\n        < u [color=\"red\"] [width=\"pts\"] [offset=\"pts\"]> < /u > - underline\n            width and offset can be empty meaning use existing canvas line width\n            or with an f/F suffix regarded as a fraction of the font size\n        < strike > < /strike > - strike through has the same parameters as underline\n        <i> ... </i> - italics\n        <u> ... </u> - underline\n        <strike> ... </strike> - strike through\n        <super> ... </super> - superscript\n        <sub> ... </sub> - subscript\n        <font name=fontfamily/fontname color=colorname size=float>\n        <span name=fontfamily/fontname color=colorname backcolor=colorname size=float style=stylename>\n        <onDraw name=callable label=\"a label\"/>\n        <index [name=\"callablecanvasattribute\"] label=\"a label\"/>\n        <link>link text</link>\n            attributes of links\n                size/fontSize/uwidth/uoffset=num\n                name/face/fontName=name\n                fg/textColor/color/ucolor=color\n                backcolor/backColor/bgcolor=color\n                dest/destination/target/href/link=target\n                underline=bool turn on underline\n        <a>anchor text</a>\n            attributes of anchors\n                size/fontSize/uwidth/uoffset=num\n                fontName=name\n                fg/textColor/color/ucolor=color\n                backcolor/backColor/bgcolor=color\n                href=href\n                underline=\"yes|no\"\n        <a name=\"anchorpoint\"/>\n        <unichar name=\"unicode character name\"/>\n        <unichar value=\"unicode code point\"/>\n        <img src=\"path\" width=\"1in\" height=\"1in\" valign=\"bottom\"/>\n                width=\"w%\" --> fontSize*w/100   idea from Roberto Alsina\n                height=\"h%\" --> linewidth*h/100 <ralsina@netmanagers.com.ar>\n\n        The whole may be surrounded by <para> </para> tags\n\n        The <b> and <i> tags will work for the built-in fonts (Helvetica\n        /Times / Courier).  For other fonts you need to register a family\n        of 4 fonts using reportlab.pdfbase.pdfmetrics.registerFont; then\n        use the addMapping function to tell the library that these 4 fonts\n        form a family e.g.\n        from reportlab.lib.fonts import addMapping\n        addMapping('Vera', 0, 0, 'Vera')    #normal\n        addMapping('Vera', 0, 1, 'Vera-Italic')    #italic\n        addMapping('Vera', 1, 0, 'Vera-Bold')    #bold\n        addMapping('Vera', 1, 1, 'Vera-BoldItalic')    #italic and bold\n\n        It will also be able to handle any MathML specified Greek characters.\n    ",
        "klass": "reportlab.platypus.paragraph.Paragraph",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.platypus.doctemplate.BaseDocTemplate"
        ],
        "class_docstring": "A special case document template that will handle many simple documents.\n       See documentation for BaseDocTemplate.  No pageTemplates are required\n       for this special case.   A page templates are inferred from the\n       margin information and the onFirstPage, onLaterPages arguments to the build method.\n\n       A document which has all pages with the same look except for the first\n       page may can be built using this special approach.\n    ",
        "klass": "reportlab.platypus.doctemplate.SimpleDocTemplate",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.common.Codabar"
        ],
        "class_docstring": "Used in blood banks, photo labs and FedEx labels.\n                    Encodes 0-9, -$:/.+, and four start/stop characters A-D.",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeCodabar",
        "module": "BarcodeCodabar"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.common.Code11"
        ],
        "class_docstring": "Used mostly for labelling telecommunications equipment.\n                    It encodes numeric digits.",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeCode11",
        "module": "BarcodeCode11"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.code128.Code128"
        ],
        "class_docstring": "Code 128 encodes any number of characters in the ASCII character set.",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeCode128",
        "module": "BarcodeCode128"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.code39.Extended39"
        ],
        "class_docstring": "Extended 39 encodes the full ASCII character set by encoding\n                        characters as pairs of Code 39 characters; $, /, % and + are used as\n                        shift characters.",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeExtended39",
        "module": "BarcodeExtended39"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.code93.Extended93"
        ],
        "class_docstring": "This is a compressed form of Code 39, allowing the full ASCII charset",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeExtended93",
        "module": "BarcodeExtended93"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.usps.FIM"
        ],
        "class_docstring": "\n                FIM was developed as part of the POSTNET barcoding system.\n                FIM (Face Identification Marking) is used by the cancelling machines\n                to sort mail according to whether or not they have bar code\n                and their postage requirements. There are four types of FIM\n                called FIM A, FIM B, FIM C, and FIM D.\n\n                The four FIM types have the following meanings:\n                    FIM A- Postage required pre-barcoded\n                    FIM B - Postage pre-paid, no bar code exists\n                    FIM C- Postage prepaid prebarcoded\n                    FIM D- Postage required, no bar code exists",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeFIM",
        "module": "BarcodeFIM"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.common.I2of5"
        ],
        "class_docstring": "Interleaved 2 of 5 is used in distribution and warehouse industries.\n\n    It encodes an even-numbered sequence of numeric digits. There is an optional\n    module 10 check digit; if including this, the total length must be odd so that\n    it becomes even after including the check digit.  Otherwise the length must be\n    even. Since the check digit is optional, our library does not check it.\n    ",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeI2of5",
        "module": "BarcodeI2of5"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.common.MSI"
        ],
        "class_docstring": "MSI is used for inventory control in retail applications.\n\n                There are several methods for calculating check digits so we\n                do not implement one.\n                ",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeMSI",
        "module": "BarcodeMSI"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.code39.Standard39"
        ],
        "class_docstring": "Code39 is widely used in non-retail, especially US defence and health.\n                        Allowed characters are 0-9, A-Z (caps only), space, and -.$/+%*.",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeStandard39",
        "module": "BarcodeStandard39"
    },
    {
        "base_classes": [
            "reportlab.graphics.barcode.widgets._BarcodeWidget",
            "reportlab.graphics.barcode.code93.Standard93"
        ],
        "class_docstring": "This is a compressed form of Code 39",
        "klass": "reportlab.graphics.barcode.widgets.BarcodeStandard93",
        "module": "BarcodeStandard93"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Something to make it easy to number paragraphs, sections,\n    images and anything else.  The features include registering\n    new string formats for sequences, and 'chains' whereby\n    some counters are reset when their parents.\n    It keeps track of a number of\n    'counters', which are created on request:\n    Usage::\n    \n        >>> seq = layout.Sequencer()\n        >>> seq.next('Bullets')\n        1\n        >>> seq.next('Bullets')\n        2\n        >>> seq.next('Bullets')\n        3\n        >>> seq.reset('Bullets')\n        >>> seq.next('Bullets')\n        1\n        >>> seq.next('Figures')\n        1\n        >>>\n    ",
        "klass": "reportlab.lib.sequencer.Sequencer",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a new cell object.\n\n  contents\n    the contents of the cell. If not specified, the cell will be empty,\n    and \n further attempts to access its cell_contents attribute will\n    raise a ValueError.",
        "klass": "cell",
        "module": "cell"
    },
    {
        "base_classes": [
            "reportlab.platypus.flowables.Flowable"
        ],
        "class_docstring": "an image (digital picture).  Formats supported by PIL/Java 1.4 (the Python/Java Imaging Library\n       are supported. Images as flowables may be aligned horizontally in the\n       frame with the hAlign parameter - accepted values are 'CENTER',\n       'LEFT' or 'RIGHT' with 'CENTER' being the default.\n       We allow for two kinds of lazyness to allow for many images in a document\n       which could lead to file handle starvation.\n       lazy=1 don't open image until required.\n       lazy=2 open image when required then shut it.\n    ",
        "klass": "reportlab.platypus.Image",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "reportlab.platypus.flowables.Flowable"
        ],
        "class_docstring": " Paragraph(text, style, bulletText=None, caseSensitive=1)\n        text a string of stuff to go into the paragraph.\n        style is a style definition as in reportlab.lib.styles.\n        bulletText is an optional bullet defintion.\n        caseSensitive set this to 0 if you want the markup tags and their attributes to be case-insensitive.\n\n        This class is a flowable that can format a block of text\n        into a paragraph with a given style.\n\n        The paragraph Text can contain XML-like markup including the tags:\n        <b> ... </b> - bold\n        < u [color=\"red\"] [width=\"pts\"] [offset=\"pts\"]> < /u > - underline\n            width and offset can be empty meaning use existing canvas line width\n            or with an f/F suffix regarded as a fraction of the font size\n        < strike > < /strike > - strike through has the same parameters as underline\n        <i> ... </i> - italics\n        <u> ... </u> - underline\n        <strike> ... </strike> - strike through\n        <super> ... </super> - superscript\n        <sub> ... </sub> - subscript\n        <font name=fontfamily/fontname color=colorname size=float>\n        <span name=fontfamily/fontname color=colorname backcolor=colorname size=float style=stylename>\n        <onDraw name=callable label=\"a label\"/>\n        <index [name=\"callablecanvasattribute\"] label=\"a label\"/>\n        <link>link text</link>\n            attributes of links\n                size/fontSize/uwidth/uoffset=num\n                name/face/fontName=name\n                fg/textColor/color/ucolor=color\n                backcolor/backColor/bgcolor=color\n                dest/destination/target/href/link=target\n                underline=bool turn on underline\n        <a>anchor text</a>\n            attributes of anchors\n                size/fontSize/uwidth/uoffset=num\n                fontName=name\n                fg/textColor/color/ucolor=color\n                backcolor/backColor/bgcolor=color\n                href=href\n                underline=\"yes|no\"\n        <a name=\"anchorpoint\"/>\n        <unichar name=\"unicode character name\"/>\n        <unichar value=\"unicode code point\"/>\n        <img src=\"path\" width=\"1in\" height=\"1in\" valign=\"bottom\"/>\n                width=\"w%\" --> fontSize*w/100   idea from Roberto Alsina\n                height=\"h%\" --> linewidth*h/100 <ralsina@netmanagers.com.ar>\n\n        The whole may be surrounded by <para> </para> tags\n\n        The <b> and <i> tags will work for the built-in fonts (Helvetica\n        /Times / Courier).  For other fonts you need to register a family\n        of 4 fonts using reportlab.pdfbase.pdfmetrics.registerFont; then\n        use the addMapping function to tell the library that these 4 fonts\n        form a family e.g.\n        from reportlab.lib.fonts import addMapping\n        addMapping('Vera', 0, 0, 'Vera')    #normal\n        addMapping('Vera', 0, 1, 'Vera-Italic')    #italic\n        addMapping('Vera', 1, 0, 'Vera-Bold')    #bold\n        addMapping('Vera', 1, 1, 'Vera-BoldItalic')    #italic and bold\n\n        It will also be able to handle any MathML specified Greek characters.\n    ",
        "klass": "reportlab.platypus.Paragraph",
        "module": "reportlab"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\nA netCDF `netCDF4.Dataset` is a collection of dimensions, groups, variables and\nattributes. Together they describe the meaning of data and relations among\ndata fields stored in a netCDF file. See `netCDF4.Dataset.__init__` for more\ndetails.\n\nA list of attribute names corresponding to global netCDF attributes\ndefined for the `netCDF4.Dataset` can be obtained with the\n`netCDF4.Dataset.ncattrs` method.\nThese attributes can be created by assigning to an attribute of the\n`netCDF4.Dataset` instance. A dictionary containing all the netCDF attribute\nname/value pairs is provided by the `__dict__` attribute of a\n`netCDF4.Dataset` instance.\n\nThe following class variables are read-only and should not be\nmodified by the user.\n\n**`dimensions`**: The `dimensions` dictionary maps the names of\ndimensions defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances of the\n`netCDF4.Dimension` class.\n\n**`variables`**: The `variables` dictionary maps the names of variables\ndefined for this `netCDF4.Dataset` or `netCDF4.Group` to instances of the\n`netCDF4.Variable` class.\n\n**`groups`**: The groups dictionary maps the names of groups created for\nthis `netCDF4.Dataset` or `netCDF4.Group` to instances of the `netCDF4.Group` class (the\n`netCDF4.Dataset` class is simply a special case of the `netCDF4.Group` class which\ndescribes the root group in the netCDF4 file).\n\n**`cmptypes`**: The `cmptypes` dictionary maps the names of\ncompound types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances of the\n`netCDF4.CompoundType` class.\n\n**`vltypes`**: The `vltypes` dictionary maps the names of\nvariable-length types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances\nof the `netCDF4.VLType` class.\n\n**`enumtypes`**: The `enumtypes` dictionary maps the names of\nEnum types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances\nof the `netCDF4.EnumType` class.\n\n**`data_model`**: `data_model` describes the netCDF\ndata model version, one of `NETCDF3_CLASSIC`, `NETCDF4`,\n`NETCDF4_CLASSIC`, `NETCDF3_64BIT_OFFSET` or `NETCDF3_64BIT_DATA`.\n\n**`file_format`**: same as `data_model`, retained for backwards compatibility.\n\n**`disk_format`**: `disk_format` describes the underlying\nfile format, one of `NETCDF3`, `HDF5`, `HDF4`,\n`PNETCDF`, `DAP2`, `DAP4` or `UNDEFINED`. Only available if using\nnetcdf C library version >= 4.3.1, otherwise will always return\n`UNDEFINED`.\n\n**`parent`**: `parent` is a reference to the parent\n`netCDF4.Group` instance. `None` for the root group or `netCDF4.Dataset`\ninstance.\n\n**`path`**: `path` shows the location of the `netCDF4.Group` in\nthe `netCDF4.Dataset` in a unix directory format (the names of groups in the\nhierarchy separated by backslashes). A `netCDF4.Dataset` instance is the root\ngroup, so the path is simply `'/'`.\n\n**`keepweakref`**: If `True`, child Dimension and Variables objects only keep weak\nreferences to the parent Dataset or Group.\n\n**`_ncstring_attrs__`**: If `True`, all text attributes will be written as variable-length\nstrings.\n    ",
        "klass": "netCDF4.Dataset",
        "module": "netCDF4"
    },
    {
        "base_classes": [
            "netCDF4._netCDF4.Dataset"
        ],
        "class_docstring": "\nClass for reading multi-file netCDF Datasets, making variables\nspanning multiple files appear as if they were in one file.\nDatasets must be in `NETCDF4_CLASSIC, NETCDF3_CLASSIC, NETCDF3_64BIT_OFFSET\nor NETCDF3_64BIT_DATA` format (`NETCDF4` Datasets won't work).\n\nAdapted from [pycdf](http://pysclint.sourceforge.net/pycdf) by Andre Gosselin.\n\nExample usage (See `netCDF4.MFDataset.__init__` for more details):\n\n    :::python\n    >>> import numpy as np\n    >>> # create a series of netCDF files with a variable sharing\n    >>> # the same unlimited dimension.\n    >>> for nf in range(10):\n    ...     with Dataset(\"mftest%s.nc\" % nf, \"w\", format='NETCDF4_CLASSIC') as f:\n    ...         f.createDimension(\"x\",None)\n    ...         x = f.createVariable(\"x\",\"i\",(\"x\",))\n    ...         x[0:10] = np.arange(nf*10,10*(nf+1))\n    >>> # now read all those files in at once, in one Dataset.\n    >>> f = MFDataset(\"mftest*nc\")\n    >>> print(f.variables[\"x\"][:])\n    [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n     24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n     48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n     72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n     96 97 98 99]\n    ",
        "klass": "netCDF4.MFDataset",
        "module": "netCDF4"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An XML element hierarchy.\n\n    This class also provides support for serialization to and from\n    standard XML.\n\n    *element* is an optional root element node,\n    *file* is an optional file handle or file name of an XML file whose\n    contents will be used to initialize the tree with.\n\n    ",
        "klass": "xml.etree.cElementTree.ElementTree",
        "module": "xml"
    },
    {
        "base_classes": [
            "flask.helpers._PackageBoundObject"
        ],
        "class_docstring": "Represents a blueprint, a collection of routes and other\n    app-related functions that can be registered on a real application\n    later.\n\n    A blueprint is an object that allows defining application functions\n    without requiring an application object ahead of time. It uses the\n    same decorators as :class:`~flask.Flask`, but defers the need for an\n    application by recording them for later registration.\n\n    Decorating a function with a blueprint creates a deferred function\n    that is called with :class:`~flask.blueprints.BlueprintSetupState`\n    when the blueprint is registered on an application.\n\n    See :ref:`blueprints` for more information.\n\n    .. versionchanged:: 1.1.0\n        Blueprints have a ``cli`` group to register nested CLI commands.\n        The ``cli_group`` parameter controls the name of the group under\n        the ``flask`` command.\n\n    .. versionadded:: 0.7\n\n    :param name: The name of the blueprint. Will be prepended to each\n        endpoint name.\n    :param import_name: The name of the blueprint package, usually\n        ``__name__``. This helps locate the ``root_path`` for the\n        blueprint.\n    :param static_folder: A folder with static files that should be\n        served by the blueprint's static route. The path is relative to\n        the blueprint's root path. Blueprint static files are disabled\n        by default.\n    :param static_url_path: The url to serve static files from.\n        Defaults to ``static_folder``. If the blueprint does not have\n        a ``url_prefix``, the app's static route will take precedence,\n        and the blueprint's static files won't be accessible.\n    :param template_folder: A folder with templates that should be added\n        to the app's template search path. The path is relative to the\n        blueprint's root path. Blueprint templates are disabled by\n        default. Blueprint templates have a lower precedence than those\n        in the app's templates folder.\n    :param url_prefix: A path to prepend to all of the blueprint's URLs,\n        to make them distinct from the rest of the app's routes.\n    :param subdomain: A subdomain that blueprint routes will match on by\n        default.\n    :param url_defaults: A dict of default values that blueprint routes\n        will receive by default.\n    :param root_path: By default, the blueprint will automatically this\n        based on ``import_name``. In certain situations this automatic\n        detection can fail, so the path can be specified manually\n        instead.\n    ",
        "klass": "flask.Blueprint",
        "module": "flask"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Works exactly like a dict but provides ways to fill it from files\n    or special dictionaries.  There are two common patterns to populate the\n    config.\n\n    Either you can fill the config from a config file::\n\n        app.config.from_pyfile('yourconfig.cfg')\n\n    Or alternatively you can define the configuration options in the\n    module that calls :meth:`from_object` or provide an import path to\n    a module that should be loaded.  It is also possible to tell it to\n    use the same module and with that provide the configuration values\n    just before the call::\n\n        DEBUG = True\n        SECRET_KEY = 'development key'\n        app.config.from_object(__name__)\n\n    In both cases (loading from any Python file or loading from modules),\n    only uppercase keys are added to the config.  This makes it possible to use\n    lowercase values in the config file for temporary values that are not added\n    to the config or to define the config keys in the same file that implements\n    the application.\n\n    Probably the most interesting way to load configurations is from an\n    environment variable pointing to a file::\n\n        app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n\n    In this case before launching the application you have to set this\n    environment variable to the file you want to use.  On Linux and OS X\n    use the export statement::\n\n        export YOURAPPLICATION_SETTINGS='/path/to/config/file'\n\n    On windows use `set` instead.\n\n    :param root_path: path to which files are read relative from.  When the\n                      config object is created by the application, this is\n                      the application's :attr:`~flask.Flask.root_path`.\n    :param defaults: an optional dictionary of default values\n    ",
        "klass": "flask.config.Config",
        "module": "flask"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Works exactly like a dict but provides ways to fill it from files\n    or special dictionaries.  There are two common patterns to populate the\n    config.\n\n    Either you can fill the config from a config file::\n\n        app.config.from_pyfile('yourconfig.cfg')\n\n    Or alternatively you can define the configuration options in the\n    module that calls :meth:`from_object` or provide an import path to\n    a module that should be loaded.  It is also possible to tell it to\n    use the same module and with that provide the configuration values\n    just before the call::\n\n        DEBUG = True\n        SECRET_KEY = 'development key'\n        app.config.from_object(__name__)\n\n    In both cases (loading from any Python file or loading from modules),\n    only uppercase keys are added to the config.  This makes it possible to use\n    lowercase values in the config file for temporary values that are not added\n    to the config or to define the config keys in the same file that implements\n    the application.\n\n    Probably the most interesting way to load configurations is from an\n    environment variable pointing to a file::\n\n        app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n\n    In this case before launching the application you have to set this\n    environment variable to the file you want to use.  On Linux and OS X\n    use the export statement::\n\n        export YOURAPPLICATION_SETTINGS='/path/to/config/file'\n\n    On windows use `set` instead.\n\n    :param root_path: path to which files are read relative from.  When the\n                      config object is created by the application, this is\n                      the application's :attr:`~flask.Flask.root_path`.\n    :param defaults: an optional dictionary of default values\n    ",
        "klass": "flask.Config",
        "module": "flask"
    },
    {
        "base_classes": [
            "flask.helpers._PackageBoundObject"
        ],
        "class_docstring": "The flask object implements a WSGI application and acts as the central\n    object.  It is passed the name of the module or package of the\n    application.  Once it is created it will act as a central registry for\n    the view functions, the URL rules, template configuration and much more.\n\n    The name of the package is used to resolve resources from inside the\n    package or the folder the module is contained in depending on if the\n    package parameter resolves to an actual python package (a folder with\n    an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n\n    For more information about resource loading, see :func:`open_resource`.\n\n    Usually you create a :class:`Flask` instance in your main module or\n    in the :file:`__init__.py` file of your package like this::\n\n        from flask import Flask\n        app = Flask(__name__)\n\n    .. admonition:: About the First Parameter\n\n        The idea of the first parameter is to give Flask an idea of what\n        belongs to your application.  This name is used to find resources\n        on the filesystem, can be used by extensions to improve debugging\n        information and a lot more.\n\n        So it's important what you provide there.  If you are using a single\n        module, `__name__` is always the correct value.  If you however are\n        using a package, it's usually recommended to hardcode the name of\n        your package there.\n\n        For example if your application is defined in :file:`yourapplication/app.py`\n        you should create it with one of the two versions below::\n\n            app = Flask('yourapplication')\n            app = Flask(__name__.split('.')[0])\n\n        Why is that?  The application will work even with `__name__`, thanks\n        to how resources are looked up.  However it will make debugging more\n        painful.  Certain extensions can make assumptions based on the\n        import name of your application.  For example the Flask-SQLAlchemy\n        extension will look for the code in your application that triggered\n        an SQL query in debug mode.  If the import name is not properly set\n        up, that debugging information is lost.  (For example it would only\n        pick up SQL queries in `yourapplication.app` and not\n        `yourapplication.views.frontend`)\n\n    .. versionadded:: 0.7\n       The `static_url_path`, `static_folder`, and `template_folder`\n       parameters were added.\n\n    .. versionadded:: 0.8\n       The `instance_path` and `instance_relative_config` parameters were\n       added.\n\n    .. versionadded:: 0.11\n       The `root_path` parameter was added.\n\n    .. versionadded:: 1.0\n       The ``host_matching`` and ``static_host`` parameters were added.\n\n    .. versionadded:: 1.0\n       The ``subdomain_matching`` parameter was added. Subdomain\n       matching needs to be enabled manually now. Setting\n       :data:`SERVER_NAME` does not implicitly enable it.\n\n    :param import_name: the name of the application package\n    :param static_url_path: can be used to specify a different path for the\n                            static files on the web.  Defaults to the name\n                            of the `static_folder` folder.\n    :param static_folder: the folder with static files that should be served\n                          at `static_url_path`.  Defaults to the ``'static'``\n                          folder in the root path of the application.\n    :param static_host: the host to use when adding the static route.\n        Defaults to None. Required when using ``host_matching=True``\n        with a ``static_folder`` configured.\n    :param host_matching: set ``url_map.host_matching`` attribute.\n        Defaults to False.\n    :param subdomain_matching: consider the subdomain relative to\n        :data:`SERVER_NAME` when matching routes. Defaults to False.\n    :param template_folder: the folder that contains the templates that should\n                            be used by the application.  Defaults to\n                            ``'templates'`` folder in the root path of the\n                            application.\n    :param instance_path: An alternative instance path for the application.\n                          By default the folder ``'instance'`` next to the\n                          package or module is assumed to be the instance\n                          path.\n    :param instance_relative_config: if set to ``True`` relative filenames\n                                     for loading the config are assumed to\n                                     be relative to the instance path instead\n                                     of the application root.\n    :param root_path: Flask by default will automatically calculate the path\n                      to the root of the application.  In certain situations\n                      this cannot be achieved (for instance if the package\n                      is a Python 3 namespace package) and needs to be\n                      manually defined.\n    ",
        "klass": "flask.Flask",
        "module": "flask"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "A string that is ready to be safely inserted into an HTML or XML\n    document, either because it was escaped or because it was marked\n    safe.\n\n    Passing an object to the constructor converts it to text and wraps\n    it to mark it safe without escaping. To escape the text, use the\n    :meth:`escape` class method instead.\n\n    >>> Markup('Hello, <em>World</em>!')\n    Markup('Hello, <em>World</em>!')\n    >>> Markup(42)\n    Markup('42')\n    >>> Markup.escape('Hello, <em>World</em>!')\n    Markup('Hello &lt;em&gt;World&lt;/em&gt;!')\n\n    This implements the ``__html__()`` interface that some frameworks\n    use. Passing an object that implements ``__html__()`` will wrap the\n    output of that method, marking it safe.\n\n    >>> class Foo:\n    ...     def __html__(self):\n    ...         return '<a href=\"/foo\">foo</a>'\n    ...\n    >>> Markup(Foo())\n    Markup('<a href=\"/foo\">foo</a>')\n\n    This is a subclass of the text type (``str`` in Python 3,\n    ``unicode`` in Python 2). It has the same methods as that type, but\n    all methods escape their arguments and return a ``Markup`` instance.\n\n    >>> Markup('<em>%s</em>') % 'foo & bar'\n    Markup('<em>foo &amp; bar</em>')\n    >>> Markup('<em>Hello</em> ') + '<foo>'\n    Markup('<em>Hello</em> &lt;foo&gt;')\n    ",
        "klass": "flask.Markup",
        "module": "markupsafe"
    },
    {
        "base_classes": [
            "werkzeug.wrappers.response.Response",
            "flask.wrappers.JSONMixin"
        ],
        "class_docstring": "The response object that is used by default in Flask.  Works like the\n    response object from Werkzeug but is set to have an HTML mimetype by\n    default.  Quite often you don't have to create this object yourself because\n    :meth:`~flask.Flask.make_response` will take care of that for you.\n\n    If you want to replace the response object used you can subclass this and\n    set :attr:`~flask.Flask.response_class` to your subclass.\n\n    .. versionchanged:: 1.0\n        JSON support is added to the response, like the request. This is useful\n        when testing to get the test client response data as JSON.\n\n    .. versionchanged:: 1.0\n\n        Added :attr:`max_cookie_size`.\n    ",
        "klass": "flask.Response",
        "module": "flask"
    },
    {
        "base_classes": [
            "itsdangerous.url_safe.URLSafeSerializerMixin",
            "itsdangerous.timed.TimedSerializer"
        ],
        "class_docstring": "Works like :class:`.TimedSerializer` but dumps and loads into a\n    URL safe string consisting of the upper and lowercase character of\n    the alphabet as well as ``'_'``, ``'-'`` and ``'.'``.\n    ",
        "klass": "itsdangerous.url_safe.URLSafeTimedSerializer",
        "module": "itsdangerous"
    },
    {
        "base_classes": [
            "werkzeug.test.Client"
        ],
        "class_docstring": "Works like a regular Werkzeug test client but has some knowledge about\n    how Flask works to defer the cleanup of the request context stack to the\n    end of a ``with`` body when used in a ``with`` statement.  For general\n    information about how to use this class refer to\n    :class:`werkzeug.test.Client`.\n\n    .. versionchanged:: 0.12\n       `app.test_client()` includes preset default environment, which can be\n       set after instantiation of the `app.test_client()` object in\n       `client.environ_base`.\n\n    Basic usage is outlined in the :ref:`testing` chapter.\n    ",
        "klass": "flask.testing.FlaskClient",
        "module": "flask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "CLI Argument management.",
        "klass": "clint.arguments.Args",
        "module": "clint"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "CLI Argument management.",
        "klass": "clint.Args",
        "module": "clint"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Enhanced string for __len__ operations on Colored output.",
        "klass": "clint.textui.colored.ColoredString",
        "module": "clint"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `SurfaceProperty:HeatTransferAlgorithm:SurfaceList`\n        Determines which Heat Balance Algorithm will be used for a list of surfaces\n        Allows selectively overriding the global setting in HeatBalanceAlgorithm\n        CTF (Conduction Transfer Functions),\n        EMPD (Effective Moisture Penetration Depth with Conduction Transfer Functions).\n        Advanced/Research Usage: CondFD (Conduction Finite Difference)\n        Advanced/Research Usage: HAMT (Combined Heat And Moisture Finite Element)\n    ",
        "klass": "pyidf.advanced_construction.SurfacePropertyHeatTransferAlgorithmSurfaceList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `AirLoopHVAC:ReturnPath`\n        A return air path can only contain one AirLoopHVAC:ZoneMixer\n        and one or more AirLoopHVAC:ReturnPlenum objects.\n    ",
        "klass": "pyidf.air_distribution.AirLoopHvacReturnPath",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `AirLoopHVAC:SupplyPlenum`\n        Connects 1 zone inlet air stream, through zone supply plenum, to one or more outlets.\n        Node names cannot be duplicated within a single supply plenum list.\n    ",
        "klass": "pyidf.air_distribution.AirLoopHvacSupplyPlenum",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Controller:MechanicalVentilation`\n        This object is used in conjunction with Controller:OutdoorAir to specify outdoor\n        ventilation air based on outdoor air specified in the DesignSpecification:OutdoorAir object\n        The Controller:OutdoorAir object is associated with a specific air loop, so the\n        outdoor air flow rates specified in Controller:MechanicalVentilation correspond to the zones\n        attached to that specific air loop.\n        Duplicate groups of Zone name, Design Specification Outdoor Air Object Name,\n        and Design Specification Zone Air Distribution Object Name to increase allowable number of entries\n    ",
        "klass": "pyidf.controllers.ControllerMechanicalVentilation",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `DaylightingDevice:Tubular`\n        Defines a tubular daylighting device (TDD) consisting of three components:\n        a dome, a pipe, and a diffuser. The dome and diffuser are defined separately using the\n        FenestrationSurface:Detailed object.\n    ",
        "klass": "pyidf.daylighting.DaylightingDeviceTubular",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `DemandManager:ElectricEquipment`\n        used for demand limiting ElectricEquipment objects.\n    ",
        "klass": "pyidf.demand_limiting_controls.DemandManagerElectricEquipment",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `DemandManager:Lights`\n        used for demand limiting Lights objects.\n    ",
        "klass": "pyidf.demand_limiting_controls.DemandManagerLights",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `DemandManager:Thermostats`\n        used for demand limiting ZoneControl:Thermostat objects.\n    ",
        "klass": "pyidf.demand_limiting_controls.DemandManagerThermostats",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `DemandManager:Ventilation`\n        used for demand limiting Controller:OutdoorAir objects.\n    ",
        "klass": "pyidf.demand_limiting_controls.DemandManagerVentilation",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `LifeCycleCost:UseAdjustment`\n        Used by advanced users to adjust the energy or water use costs for future years. This\n        should not be used for compensating for inflation but should only be used to increase\n        the costs of energy or water based on assumed changes to the actual usage, such as\n        anticipated changes in the future function of the building. The adjustments begin at\n        the start of the service period.\n    ",
        "klass": "pyidf.economics.LifeCycleCostUseAdjustment",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `LifeCycleCost:UsePriceEscalation`\n        Life cycle cost escalation factors. The values for this object may be found in the\n        annual supplement to NIST Handbook 135 in Tables Ca-1 to Ca-5 and are included in an\n        EnergyPlus dataset file.\n    ",
        "klass": "pyidf.economics.LifeCycleCostUsePriceEscalation",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ElectricLoadCenter:Generators`\n        List of electric power generators to include in the simulation including the name and\n        type of each generators along with availability schedule, rated power output,\n        and thermal-to-electrical power ratio.\n    ",
        "klass": "pyidf.electric_load_center.ElectricLoadCenterGenerators",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ElectricLoadCenter:Transformer`\n        a list of meters that can be reported are available after a run on\n        the meter dictionary file (.mdd) if the Output:VariableDictionary has been requested.\n    ",
        "klass": "pyidf.electric_load_center.ElectricLoadCenterTransformer",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `EnergyManagementSystem:GlobalVariable`\n        Declares Erl variable as having global scope\n        No spaces allowed in names used for Erl variables\n    ",
        "klass": "pyidf.energy_management_system.EnergyManagementSystemGlobalVariable",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `EnergyManagementSystem:Program`\n        This input defines an Erl program\n        Each field after the name is a line of EMS Runtime Language\n    ",
        "klass": "pyidf.energy_management_system.EnergyManagementSystemProgram",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `EnergyManagementSystem:ProgramCallingManager`\n        Input EMS program. a program needs a name\n        a description of when it should be called\n        and then lines of program code for EMS Runtime language\n    ",
        "klass": "pyidf.energy_management_system.EnergyManagementSystemProgramCallingManager",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `EnergyManagementSystem:Subroutine`\n        This input defines an Erl program subroutine\n        Each field after the name is a line of EMS Runtime Language\n    ",
        "klass": "pyidf.energy_management_system.EnergyManagementSystemSubroutine",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Represents an EnergyPlus IDF input file.",
        "klass": "pyidf.idf.IDF",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Branch`\n        List components on the branch in simulation and connection order\n        Note: this should NOT include splitters or mixers which define\n        endpoints of branches\n    ",
        "klass": "pyidf.node.Branch",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `NodeList`\n        This object is used in places where lists of nodes may be\n        needed, e.g. ZoneHVAC:EquipmentConnections field Zone Air Inlet Node or NodeList Name\n    ",
        "klass": "pyidf.node.NodeList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `OutdoorAir:NodeList`\n        This object sets the temperature and humidity conditions\n        for an outdoor air node using the weather data values.\n        to vary outdoor air node conditions with height above ground\n        use OutdoorAir:Node instead of this object.\n        This object may be used more than once.\n        The same node name may not appear in both an OutdoorAir:Node object and\n        an OutdoorAir:NodeList object.\n    ",
        "klass": "pyidf.node.OutdoorAirNodeList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `PipingSystem:Underground:Domain`\n        The ground domain object for underground piping system simulation.\n    ",
        "klass": "pyidf.node.PipingSystemUndergroundDomain",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Meter:Custom`\n        Used to allow users to combine specific variables and/or meters into\n        \"custom\" meter configurations. To access these meters by name, one must\n        first run a simulation to generate the RDD/MDD files and names.\n    ",
        "klass": "pyidf.output_reporting.MeterCustom",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Meter:CustomDecrement`\n        Used to allow users to combine specific variables and/or meters into\n        \"custom\" meter configurations. To access these meters by name, one must\n        first run a simulation to generate the RDD/MDD files and names.\n    ",
        "klass": "pyidf.output_reporting.MeterCustomDecrement",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Output:Table:Monthly`\n        Provides a generic method of setting up tables of monthly results. The report\n        has multiple columns that are each defined using a repeated group of fields for any\n        number of columns. A single Output:Table:Monthly object often produces multiple\n        tables in the output. A table is produced for every instance of a particular output\n        variable. For example, a table defined with zone variables will be produced once for\n        every zone.\n    ",
        "klass": "pyidf.output_reporting.OutputTableMonthly",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Parametric:FileNameSuffix`\n        Defines the suffixes to be appended to the idf and output file names for each\n        parametric run. If this object is omitted, the suffix will default to the run number.\n    ",
        "klass": "pyidf.parametrics.ParametricFileNameSuffix",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Parametric:SetValueForRun`\n        Parametric objects allow a set of multiple simulations to be defined in a single idf\n        file. The parametric preprocessor scans the idf for Parametric:* objects then creates\n        and runs multiple idf files, one for each defined simulation.\n        The core parametric object is Parametric:SetValueForRun which defines the name\n        of a parameters and sets the parameter to different values depending on which\n        run is being simulated.\n    ",
        "klass": "pyidf.parametrics.ParametricSetValueForRun",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Table:TwoIndependentVariables`\n        Allows entry of tabular data pairs as alternate input\n        for performance curve objects.\n        Performance curve objects can be created using these inputs.\n        BiQuadratic Table Equation: Output = a + bX + cX**2 + dY + eY**2 + fXY\n        BiQuadratic solution requires a minimum of 6 data pairs\n        QuadraticLinear Table Equation: Output = a + bX + cX**2 + dY + eXY + fX**2Y\n        QuadraticLinear solution requires a minimum of 6 data pairs\n    ",
        "klass": "pyidf.performance_tables.TableTwoIndependentVariables",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Matrix:TwoDimension`\n        matrix data in row-major order\n        list each row keeping the columns in order\n        number of values must equal N1 x N2\n    ",
        "klass": "pyidf.refrigeration.MatrixTwoDimension",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Refrigeration:CaseAndWalkInList`\n        Provides a list of all the refrigerated cases, walk in coolers, or air chillers\n        cooled by a single refrigeration system.  Note that the names of all cases,\n        walk-ins ,air chillers, and CaseAndWalkInLists must be unique.  That is, you cannot\n        give a list the same name as one of list items. This list may contain a combination\n        of case and walk-in names OR a list of air chiller names.  Air chillers\n        may not be included in any list that also includes cases or walk-ins.\n    ",
        "klass": "pyidf.refrigeration.RefrigerationCaseAndWalkInList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Refrigeration:TransferLoadList`\n        A refrigeration system may provide cooling to other, secondary, systems through\n        either a secondary loop or a cascade condenser. If multiple transfer loads are served\n        by a single primary system, use this list to group them together for reference by the\n        primary system (see the field \"Refrigeration Transfer Load or TransferLoad List Name\"\n        in the Refrigeration:System object).\n    ",
        "klass": "pyidf.refrigeration.RefrigerationTransferLoadList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Refrigeration:WalkIn`\n        Works in conjunction with a compressor rack, a refrigeration system, or a\n        refrigeration secondary system to simulate the performance of a walk-in cooler.\n        The walk-in cooler model uses information at rated conditions along with input\n        descriptions for heat transfer surfaces facing multiple zones to determine\n        performance.\n    ",
        "klass": "pyidf.refrigeration.RefrigerationWalkIn",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:RefrigerationChillerSet`\n        Works in conjunction with one or multiple air chillers, compressor racks,\n        refrigeration systems, or refrigeration secondary system objects to simulate the\n        performance of a group of air chillers cooling a single zone. The chiller set\n        model passes information about the zone conditions to determine the performance of\n        individual chiller coils within the set, thus providing the sensible and latent heat\n        exchange with the zone environment.\n    ",
        "klass": "pyidf.refrigeration.ZoneHvacRefrigerationChillerSet",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoomAir:Node:AirflowNetwork:HVACEquipment`\n        define the zone equipment associated with one particular RoomAir:Node\n    ",
        "klass": "pyidf.room_air_models.RoomAirNodeAirflowNetworkHvacequipment",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoomAir:Node:AirflowNetwork:InternalGains`\n        define the internal gains that are associated with one particular RoomAir:Node\n    ",
        "klass": "pyidf.room_air_models.RoomAirNodeAirflowNetworkInternalGains",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoomAirSettings:AirflowNetwork`\n        RoomAir modeling using Airflow pressure network solver\n    ",
        "klass": "pyidf.room_air_models.RoomAirSettingsAirflowNetwork",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoomAir:TemperaturePattern:NondimensionalHeight`\n        Defines a distribution pattern for air temperatures relative to the current mean air\n        temperature as a function of height. The height, referred to as Zeta, is nondimensional\n        by normalizing with the zone ceiling height.\n        Used in combination with RoomAir:TemperaturePattern:UserDefined.\n    ",
        "klass": "pyidf.room_air_models.RoomAirTemperaturePatternNondimensionalHeight",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoomAir:TemperaturePattern:SurfaceMapping`\n        Defines a distribution pattern for the air temperatures adjacent to individual surfaces.\n        This allows controlling the adjacent air temperature on a surface-by-surface basis\n        rather than by height. This allows modeling different adjacent air temperatures on\n        the opposite sides of the zone. Used in combination with\n        RoomAir:TemperaturePattern:UserDefined.\n    ",
        "klass": "pyidf.room_air_models.RoomAirTemperaturePatternSurfaceMapping",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Schedule:Week:Compact`\n        Compact definition for Schedule:Day:List\n    ",
        "klass": "pyidf.schedules.ScheduleWeekCompact",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Schedule:Year`\n        A Schedule:Year contains from 1 to 52 week schedules\n    ",
        "klass": "pyidf.schedules.ScheduleYear",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `SolarCollector:UnglazedTranspired`\n        Unglazed transpired solar collector (UTSC) used to condition outdoor air. This type of\n        collector is generally used to heat air drawn through perforated absorbers and also\n        recover heat conducted out through the underlying surface. This object represents a\n        single collector attached to one or more building or shading surfaces and to one or\n        more outdoor air systems.\n    ",
        "klass": "pyidf.solar_collectors.SolarCollectorUnglazedTranspired",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `SolarCollector:UnglazedTranspired:Multisystem`\n        quad-tuples of inlet, outlet, control, and zone nodes\n        for multiple different outdoor air systems attached to same collector\n    ",
        "klass": "pyidf.solar_collectors.SolarCollectorUnglazedTranspiredMultisystem",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `MaterialProperty:GlazingSpectralData`\n        Name is followed by up to 800 sets of normal-incidence measured values of\n        [wavelength, transmittance, front reflectance, back reflectance] for wavelengths\n        covering the solar spectrum (from about 0.25 to 2.5 microns)\n    ",
        "klass": "pyidf.surface_construction_elements.MaterialPropertyGlazingSpectralData",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `WindowMaterial:GlazingGroup:Thermochromic`\n        thermochromic glass at different temperatures\n    ",
        "klass": "pyidf.surface_construction_elements.WindowMaterialGlazingGroupThermochromic",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": "Corresponds to IDD object `AvailabilityManagerAssignmentList` Defines\n    the applicable managers used for an AirLoopHVAC or PlantLoop.\n\n    The priority of\n    availability managers is based on a set of rules and are specific to the type of loop.\n    The output from each availability manager is an availability status flag:\n    NoAction, ForceOff, CycleOn, or CycleOnZoneFansOnly (used only for air loops).\n\n    ",
        "klass": "pyidf.system_availability_managers.AvailabilityManagerAssignmentList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `BuildingSurface:Detailed`\n        Allows for detailed entry of building heat transfer surfaces.  Does not include subsurfaces such as windows or doors.\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.BuildingSurfaceDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Floor:Detailed`\n        Allows for detailed entry of floor heat transfer surfaces.\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.FloorDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `RoofCeiling:Detailed`\n        Allows for detailed entry of roof/ceiling heat transfer surfaces.\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.RoofCeilingDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Shading:Building:Detailed`\n        used for shading elements such as trees, other buildings, parts of this building not being modeled\n        these items are relative to the current building and would move with relative geometry\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.ShadingBuildingDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Shading:Zone:Detailed`\n        used For fins, overhangs, elements that shade the building, are attached to the building\n        but are not part of the heat transfer calculations\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.ShadingZoneDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `Wall:Detailed`\n        Allows for detailed entry of wall heat transfer surfaces.\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.WallDetailed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": "Corresponds to IDD object `ZoneList` Defines a list of thermal zones\n    which can be referenced as a group.\n\n    The ZoneList name\n    may be used elsewhere in the input to apply a parameter to all zones in the list.\n    ZoneLists can be used effectively with the following objects: People, Lights,\n    ElectricEquipment, GasEquipment, HotWaterEquipment, ZoneInfiltration:DesignFlowRate,\n    ZoneVentilation:DesignFlowRate, Sizing:Zone, ZoneControl:Thermostat, and others.\n\n    ",
        "klass": "pyidf.thermal_zones_and_surfaces.ZoneList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `UnitarySystemPerformance:Multispeed`\n        The UnitarySystemPerformance object is used to specify the air flow ratio at each\n        operating speed. This object is primarily used for multispeed DX and water coils to allow\n        operation at alternate flow rates different from those specified in the coil object.\n    ",
        "klass": "pyidf.unitary_equipment.UnitarySystemPerformanceMultispeed",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": "Corresponds to IDD object `ZoneTerminalUnitList` List of variable\n    refrigerant flow (VRF) terminal units served by a given VRF condensing\n    unit.\n\n    See ZoneHVAC:TerminalUnit:VariableRefrigerantFlow and\n    AirConditioner:VariableRefrigerantFlow.\n\n    ",
        "klass": "pyidf.variable_refrigerant_flow_equipment.ZoneTerminalUnitList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `WaterUse:RainCollector`\n        Used for harvesting rainwater falling on building surfaces. The rainwater is sent to a\n        WaterUse:Storage object. In order to use this object it is necessary to also include\n        a Site:Precipitation object to describe the rates of rainfall.\n    ",
        "klass": "pyidf.water_systems.WaterUseRainCollector",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:EquipmentList`\n        List equipment in simulation order.  Note that an ZoneHVAC:AirDistributionUnit or\n        AirTerminal:SingleDuct:Uncontrolled object must be listed in this statement if there is a forced\n        air system serving the zone from the air loop.\n        Equipment is simulated in the order specified by Zone Equipment Cooling Sequence and\n        Zone Equipment Heating or No-Load Sequence, depending on the thermostat request.\n        For equipment of similar type, assign sequence 1 to the first system intended to\n        serve that type of load.  For situations where one or more equipment types has limited capacity or\n        limited control, order the sequence so that the most controllable piece of equipment runs last.\n        For example, with a dedicated outdoor air system (DOAS), the air terminal for the DOAS\n        should be assigned Heating Sequence = 1 and Cooling Sequence = 1.  Any other equipment should\n        be assigned sequence 2 or higher so that it will see the net load after the DOAS air is added\n        to the zone.\n    ",
        "klass": "pyidf.zone_hvac_equipment_connections.ZoneHvacEquipmentList",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:Baseboard:RadiantConvective:Electric`\n        The number of surfaces can be expanded beyond 100, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacBaseboardRadiantConvectiveElectric",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:Baseboard:RadiantConvective:Steam`\n        The number of surfaces can be expanded beyond 100, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacBaseboardRadiantConvectiveSteam",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:Baseboard:RadiantConvective:Water`\n        The number of surfaces can be expanded beyond 100, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacBaseboardRadiantConvectiveWater",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:HighTemperatureRadiant`\n        The number of surfaces can be expanded beyond 100, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacHighTemperatureRadiant",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:LowTemperatureRadiant:SurfaceGroup`\n        This is used to allow the coordinate control of several radiant system surfaces.\n        Note that the following flow fractions must sum up to 1.0\n        The number of surfaces can be expanded beyond 100, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacLowTemperatureRadiantSurfaceGroup",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "pyidf.helper.DataObject"
        ],
        "class_docstring": " Corresponds to IDD object `ZoneHVAC:VentilatedSlab:SlabGroup`\n        This is used to allow the coordinate control of several ventilated slab system\n        surfaces. Note that the flow fractions must sum up to 1.0.\n        The number of surfaces can be expanded beyond 10, if necessary, by adding more\n        groups to the end of the list\n    ",
        "klass": "pyidf.zone_hvac_radiative.ZoneHvacVentilatedSlabSlabGroup",
        "module": "pyidf"
    },
    {
        "base_classes": [
            "routes.mapper.SubMapperParent"
        ],
        "class_docstring": "Mapper handles URL generation and URL recognition in a web\n    application.\n\n    Mapper is built handling dictionary's. It is assumed that the web\n    application will handle the dictionary returned by URL recognition\n    to dispatch appropriately.\n\n    URL generation is done by passing keyword parameters into the\n    generate function, a URL is then returned.\n\n    ",
        "klass": "routes.mapper.Mapper",
        "module": "routes"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "Raises *exception* in the current greenthread after *timeout* seconds.\n\n    When *exception* is omitted or ``None``, the :class:`Timeout` instance\n    itself is raised. If *seconds* is None, the timer is not scheduled, and is\n    only useful if you're planning to raise it directly.\n\n    Timeout objects are context managers, and so can be used in with statements.\n    When used in a with statement, if *exception* is ``False``, the timeout is\n    still raised, but the context manager suppresses it, so the code outside the\n    with-block won't see it.\n    ",
        "klass": "ryu.lib.hub.Timeout",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "ARP (RFC 826) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    IPv4 addresses are represented as a string like '192.0.2.1'.\n    MAC addresses are represented as a string like '08:60:6e:7f:74:e7'.\n    __init__ takes the corresponding args in this order.\n\n    ============== ===================================== =====================\n    Attribute      Description                           Example\n    ============== ===================================== =====================\n    hwtype         Hardware address.\n    proto          Protocol address.\n    hlen           byte length of each hardware address.\n    plen           byte length of each protocol address.\n    opcode         operation codes.\n    src_mac        Hardware address of sender.           '08:60:6e:7f:74:e7'\n    src_ip         Protocol address of sender.           '192.0.2.1'\n    dst_mac        Hardware address of target.           '00:00:00:00:00:00'\n    dst_ip         Protocol address of target.           '192.0.2.2'\n    ============== ===================================== =====================\n    ",
        "klass": "ryu.lib.packet.arp.arp",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "BFD (RFC 5880) Control packet encoder/decoder class.\n\n    The serialized packet would looks like the ones described\n    in the following sections.\n\n    * RFC 5880 Generic BFD Control Packet Format\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n\n    __init__ takes the corresponding args in this order.\n\n    .. tabularcolumns:: |l|L|\n\n    ============================== ============================================\n    Attribute                      Description\n    ============================== ============================================\n    ver                            The version number of the protocol.\n                                   This class implements protocol version 1.\n    diag                           A diagnostic code specifying the local\n                                   system's reason for the last change in\n                                   session state.\n    state                          The current BFD session state as seen by\n                                   the transmitting system.\n    flags                          Bitmap of the following flags.\n\n                                   | BFD_FLAG_POLL\n                                   | BFD_FLAG_FINAL\n                                   | BFD_FLAG_CTRL_PLANE_INDEP\n                                   | BFD_FLAG_AUTH_PRESENT\n                                   | BFD_FLAG_DEMAND\n                                   | BFD_FLAG_MULTIPOINT\n    detect_mult                    Detection time multiplier.\n    my_discr                       My Discriminator.\n    your_discr                     Your Discriminator.\n    desired_min_tx_interval        Desired Min TX Interval. (in microseconds)\n    required_min_rx_interval       Required Min RX Interval. (in microseconds)\n    required_min_echo_rx_interval  Required Min Echo RX Interval.\n                                   (in microseconds)\n    auth_cls                       (Optional) Authentication Section instance.\n                                   It's defined only when the Authentication\n                                   Present (A) bit is set in flags.\n                                   Assign an instance of the following classes:\n                                   ``SimplePassword``, ``KeyedMD5``,\n                                   ``MeticulousKeyedMD5``, ``KeyedSHA1``, and\n                                   ``MeticulousKeyedSHA1``.\n    length                         (Optional) Length of the BFD Control packet,\n                                   in bytes.\n    ============================== ============================================\n    ",
        "klass": "ryu.lib.packet.bfd.bfd",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPMessage"
        ],
        "class_docstring": "BMP Initiation Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    info                       One or more piece of information encoded as a\n                               TLV\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPInitiation",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPPeerMessage"
        ],
        "class_docstring": "BMP Peer Down Notification Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    reason                     Reason indicates why the session was closed.\n    data                       vary by the reason.\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPPeerDownNotification",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPPeerMessage"
        ],
        "class_docstring": "BMP Peer Up Notification Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    peer_type                  The type of the peer.\n    peer_flags                 Provide more information about the peer.\n    peer_distinguisher         Use for L3VPN router which can have multiple\n                               instance.\n    peer_address               The remote IP address associated with the TCP\n                               session.\n    peer_as                    The Autonomous System number of the peer.\n    peer_bgp_id                The BGP Identifier of the peer\n    timestamp                  The time when the encapsulated routes were\n                               received.\n    local_address              The local IP address associated with the\n                               peering TCP session.\n    local_port                 The local port number associated with the\n                               peering TCP session.\n    remote_port                The remote port number associated with the\n                               peering TCP session.\n    sent_open_message          The full OPEN message transmitted by the\n                               monitored router to its peer.\n    received_open_message      The full OPEN message received by the monitored\n                               router from its peer.\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPPeerUpNotification",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPPeerMessage"
        ],
        "class_docstring": "BMP Route Monitoring Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    peer_type                  The type of the peer.\n    peer_flags                 Provide more information about the peer.\n    peer_distinguisher         Use for L3VPN router which can have multiple\n                               instance.\n    peer_address               The remote IP address associated with the TCP\n                               session.\n    peer_as                    The Autonomous System number of the peer.\n    peer_bgp_id                The BGP Identifier of the peer\n    timestamp                  The time when the encapsulated routes were\n                               received.\n    bgp_update                 BGP Update PDU\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPRouteMonitoring",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPPeerMessage"
        ],
        "class_docstring": "BMP Statistics Report Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    peer_type                  The type of the peer.\n    peer_flags                 Provide more information about the peer.\n    peer_distinguisher         Use for L3VPN router which can have multiple\n                               instance.\n    peer_address               The remote IP address associated with the TCP\n                               session.\n    peer_as                    The Autonomous System number of the peer.\n    peer_bgp_id                The BGP Identifier of the peer\n    timestamp                  The time when the encapsulated routes were\n                               received.\n    stats                      Statistics (one or more stats encoded as a TLV)\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPStatisticsReport",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.bmp.BMPMessage"
        ],
        "class_docstring": "BMP Termination Message\n\n    ========================== ===============================================\n    Attribute                  Description\n    ========================== ===============================================\n    version                    Version. this packet lib defines BMP ver. 3\n    len                        Length field.  Ignored when encoding.\n    type                       Type field.  one of BMP\\_MSG\\_ constants.\n    info                       One or more piece of information encoded as a\n                               TLV\n    ========================== ===============================================\n    ",
        "klass": "ryu.lib.packet.bmp.BMPTermination",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "DHCP (RFC 2131) header encoder/decoder class.\n\n    The serialized packet would looks like the ones described\n    in the following sections.\n\n    * RFC 2131 DHCP packet format\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    .. tabularcolumns:: |l|L|\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    op             Message op code / message type.                   1 = BOOTREQUEST, 2 = BOOTREPLY\n    htype          Hardware address type (e.g.  '1' = 10mb ethernet).\n    hlen           Hardware address length (e.g.  '6' = 10mb ethernet).\n    hops           Client sets to zero, optionally used by relay agent                   when booting via a relay agent.\n    xid            Transaction ID, a random number chosen by the client,                   used by the client and serverto associate messages                   and responses between a client and a server.\n    secs           Filled in by client, seconds elapsed since client                   began address acquisition or renewal process.\n    flags          Flags.\n    ciaddr         Client IP address; only filled in if client is in                   BOUND, RENEW or REBINDING state and can respond                   to ARP requests.\n    yiaddr         'your' (client) IP address.\n    siaddr         IP address of next server to use in bootstrap;                   returned in DHCPOFFER, DHCPACK by server.\n    giaddr         Relay agent IP address, used in booting via a                   relay agent.\n    chaddr         Client hardware address.\n    sname          Optional server host name, null terminated string.\n    boot_file      Boot file name, null terminated string; \"generic\"                   name or null in DHCPDISCOVER, fully qualified                   directory-path name in DHCPOFFER.\n    options        Optional parameters field                   ('DHCP message type' option must be included in                    every DHCP message).\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.dhcp.dhcp",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "Ethernet header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    MAC addresses are represented as a string like '08:60:6e:7f:74:e7'.\n    __init__ takes the corresponding args in this order.\n\n    ============== ==================== =====================\n    Attribute      Description          Example\n    ============== ==================== =====================\n    dst            destination address  'ff:ff:ff:ff:ff:ff'\n    src            source address       '08:60:6e:7f:74:e7'\n    ethertype      ether type           0x0800\n    ============== ==================== =====================\n    ",
        "klass": "ryu.lib.packet.ethernet.ethernet",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "IPv4 (RFC 791) header encoder/decoder class.\n\n    NOTE: When decoding, this implementation tries to decode the upper\n    layer protocol even for a fragmented datagram.  It isn't likely\n    what a user would want.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    IPv4 addresses are represented as a string like '192.0.2.1'.\n    __init__ takes the corresponding args in this order.\n\n    ============== ======================================== ==================\n    Attribute      Description                              Example\n    ============== ======================================== ==================\n    version        Version\n    header_length  IHL\n    tos            Type of Service\n    total_length   Total Length\n                   (0 means automatically-calculate\n                   when encoding)\n    identification Identification\n    flags          Flags\n    offset         Fragment Offset\n    ttl            Time to Live\n    proto          Protocol\n    csum           Header Checksum\n                   (Ignored and automatically-calculated\n                   when encoding)\n    src            Source Address                           '192.0.2.1'\n    dst            Destination Address                      '192.0.2.2'\n    option         A bytearray which contains the entire\n                   Options, or None for  no Options\n    ============== ======================================== ==================\n    ",
        "klass": "ryu.lib.packet.ipv4.ipv4",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "I-TAG (IEEE 802.1ah-2008) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    pcp            Priority Code Point\n    dei            Drop Eligible Indication\n    uca            Use Customer Address\n    sid            Service Instance ID\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.pbb.itag",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "TCP (RFC 793) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    src_port       Source Port\n    dst_port       Destination Port\n    seq            Sequence Number\n    ack            Acknowledgement Number\n    offset         Data Offset                    (0 means automatically-calculate when encoding)\n    bits           Control Bits\n    window_size    Window\n    csum           Checksum                    (0 means automatically-calculate when encoding)\n    urgent         Urgent Pointer\n    option         List of ``TCPOption`` sub-classes or an bytearray\n                   containing options.                    None if no options.\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.tcp.tcp",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "UDP (RFC 768) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    src_port       Source Port\n    dst_port       Destination Port\n    total_length   Length                    (0 means automatically-calculate when encoding)\n    csum           Checksum                    (0 means automatically-calculate when encoding)\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.udp.udp",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.vlan._vlan"
        ],
        "class_docstring": "S-VLAN (IEEE 802.1ad) header encoder/decoder class.\n\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    .. tabularcolumns:: |l|L|\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    pcp            Priority Code Point\n    cfi            Canonical Format Indicator.\n                   In a case to be used as B-TAG,\n                   this field means DEI(Drop Eligible Indication).\n    vid            VLAN Identifier\n    ethertype      EtherType\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.vlan.svlan",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.vlan._vlan"
        ],
        "class_docstring": "VLAN (IEEE 802.1Q) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    pcp            Priority Code Point\n    cfi            Canonical Format Indicator\n    vid            VLAN Identifier\n    ethertype      EtherType\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.vlan.vlan",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.lib.packet.packet_base.PacketBase"
        ],
        "class_docstring": "VXLAN (RFC 7348) header encoder/decoder class.\n\n    An instance has the following attributes at least.\n    Most of them are same to the on-wire counterparts but in host byte order.\n    __init__ takes the corresponding args in this order.\n\n    ============== ====================\n    Attribute      Description\n    ============== ====================\n    vni            VXLAN Network Identifier\n    ============== ====================\n    ",
        "klass": "ryu.lib.packet.vxlan.vxlan",
        "module": "ryu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An endpoint\n    *sock* is a socket-like.  it can be either blocking or non-blocking.\n    ",
        "klass": "ryu.lib.rpc.EndPoint",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.ofproto.ofproto_parser.MsgBase"
        ],
        "class_docstring": "\n    Hello message\n\n    When connection is started, the hello message is exchanged between a\n    switch and a controller.\n\n    This message is handled by the Ryu framework, so the Ryu application\n    do not need to process this typically.\n    ",
        "klass": "ryu.ofproto.ofproto_v1_0_parser.OFPHello",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.ofproto.ofproto_v1_3_parser.generate.locals._NXFlowSpec"
        ],
        "class_docstring": "\n        Add NXAST_REG_LOAD actions\n\n        This class is used by ``NXActionLearn``.\n\n        For the usage of this class, please refer to ``NXActionLearn``.\n\n        ================ ======================================================\n        Attribute        Description\n        ================ ======================================================\n        src              OXM/NXM header and Start bit for source field\n        dst              OXM/NXM header and Start bit for destination field\n        n_bits           The number of bits from the start bit\n        ================ ======================================================\n        ",
        "klass": "ryu.ofproto.ofproto_v1_3_parser.NXFlowSpecLoad",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.ofproto.ofproto_v1_3_parser.generate.locals._NXFlowSpec"
        ],
        "class_docstring": "\n        Specification for adding match criterion\n\n        This class is used by ``NXActionLearn``.\n\n        For the usage of this class, please refer to ``NXActionLearn``.\n\n        ================ ======================================================\n        Attribute        Description\n        ================ ======================================================\n        src              OXM/NXM header and Start bit for source field\n        dst              OXM/NXM header and Start bit for destination field\n        n_bits           The number of bits from the start bit\n        ================ ======================================================\n        ",
        "klass": "ryu.ofproto.ofproto_v1_3_parser.NXFlowSpecMatch",
        "module": "ryu"
    },
    {
        "base_classes": [
            "ryu.ofproto.ofproto_v1_3_parser.generate.locals._NXFlowSpec"
        ],
        "class_docstring": "\n        Add an OFPAT_OUTPUT action\n\n        This class is used by ``NXActionLearn``.\n\n        For the usage of this class, please refer to ``NXActionLearn``.\n\n        ================ ======================================================\n        Attribute        Description\n        ================ ======================================================\n        src              OXM/NXM header and Start bit for source field\n        dst              Must be ''\n        n_bits           The number of bits from the start bit\n        ================ ======================================================\n        ",
        "klass": "ryu.ofproto.ofproto_v1_3_parser.NXFlowSpecOutput",
        "module": "ryu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Adapter to handle Python 2 to 3 conversion when writing to files\n\n    ",
        "klass": "bigml.io.UnicodeWriter",
        "module": "bigml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Retrieves csv info and builds a input data dict\n\n    ",
        "klass": "bigmler.tst_reader.TstReader",
        "module": "bigmler"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Adapter to handle Python 2 to 3 conversion when reading files\n\n    ",
        "klass": "bigml.io.UnicodeReader",
        "module": "bigml"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Atoms object.\n\n    The Atoms object can represent an isolated molecule, or a\n    periodically repeated structure.  It has a unit cell and\n    there may be periodic boundary conditions along any of the three\n    unit cell axes.\n    Information about the atoms (atomic numbers and position) is\n    stored in ndarrays.  Optionally, there can be information about\n    tags, momenta, masses, magnetic moments and charges.\n\n    In order to calculate energies, forces and stresses, a calculator\n    object has to attached to the atoms object.\n\n    Parameters:\n\n    symbols: str (formula) or list of str\n        Can be a string formula, a list of symbols or a list of\n        Atom objects.  Examples: 'H2O', 'COPt12', ['H', 'H', 'O'],\n        [Atom('Ne', (x, y, z)), ...].\n    positions: list of xyz-positions\n        Atomic positions.  Anything that can be converted to an\n        ndarray of shape (n, 3) will do: [(x1,y1,z1), (x2,y2,z2),\n        ...].\n    scaled_positions: list of scaled-positions\n        Like positions, but given in units of the unit cell.\n        Can not be set at the same time as positions.\n    numbers: list of int\n        Atomic numbers (use only one of symbols/numbers).\n    tags: list of int\n        Special purpose tags.\n    momenta: list of xyz-momenta\n        Momenta for all atoms.\n    masses: list of float\n        Atomic masses in atomic units.\n    magmoms: list of float or list of xyz-values\n        Magnetic moments.  Can be either a single value for each atom\n        for collinear calculations or three numbers for each atom for\n        non-collinear calculations.\n    charges: list of float\n        Initial atomic charges.\n    cell: 3x3 matrix or length 3 or 6 vector\n        Unit cell vectors.  Can also be given as just three\n        numbers for orthorhombic cells, or 6 numbers, where\n        first three are lengths of unit cell vectors, and the\n        other three are angles between them (in degrees), in following order:\n        [len(a), len(b), len(c), angle(b,c), angle(a,c), angle(a,b)].\n        First vector will lie in x-direction, second in xy-plane,\n        and the third one in z-positive subspace.\n        Default value: [0, 0, 0].\n    celldisp: Vector\n        Unit cell displacement vector. To visualize a displaced cell\n        around the center of mass of a Systems of atoms. Default value\n        = (0,0,0)\n    pbc: one or three bool\n        Periodic boundary conditions flags.  Examples: True,\n        False, 0, 1, (1, 1, 0), (True, False, False).  Default\n        value: False.\n    constraint: constraint object(s)\n        Used for applying one or more constraints during structure\n        optimization.\n    calculator: calculator object\n        Used to attach a calculator for calculating energies and atomic\n        forces.\n    info: dict of key-value pairs\n        Dictionary of key-value pairs with additional information\n        about the system.  The following keys may be used by ase:\n\n          - spacegroup: Spacegroup instance\n          - unit_cell: 'conventional' | 'primitive' | int | 3 ints\n          - adsorbate_info: Information about special adsorption sites\n\n        Items in the info attribute survives copy and slicing and can\n        be stored in and retrieved from trajectory files given that the\n        key is a string, the value is JSON-compatible and, if the value is a\n        user-defined object, its base class is importable.  One should\n        not make any assumptions about the existence of keys.\n\n    Examples:\n\n    These three are equivalent:\n\n    >>> d = 1.104  # N2 bondlength\n    >>> a = Atoms('N2', [(0, 0, 0), (0, 0, d)])\n    >>> a = Atoms(numbers=[7, 7], positions=[(0, 0, 0), (0, 0, d)])\n    >>> a = Atoms([Atom('N', (0, 0, 0)), Atom('N', (0, 0, d))])\n\n    FCC gold:\n\n    >>> a = 4.05  # Gold lattice constant\n    >>> b = a / 2\n    >>> fcc = Atoms('Au',\n    ...             cell=[(0, b, b), (b, 0, b), (b, b, 0)],\n    ...             pbc=True)\n\n    Hydrogen wire:\n\n    >>> d = 0.9  # H-H distance\n    >>> h = Atoms('H', positions=[(0, 0, 0)],\n    ...           cell=(d, 0, 0),\n    ...           pbc=(1, 0, 0))\n    ",
        "klass": "ase.atoms.Atoms",
        "module": "ase"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Atoms object.\n\n    The Atoms object can represent an isolated molecule, or a\n    periodically repeated structure.  It has a unit cell and\n    there may be periodic boundary conditions along any of the three\n    unit cell axes.\n    Information about the atoms (atomic numbers and position) is\n    stored in ndarrays.  Optionally, there can be information about\n    tags, momenta, masses, magnetic moments and charges.\n\n    In order to calculate energies, forces and stresses, a calculator\n    object has to attached to the atoms object.\n\n    Parameters:\n\n    symbols: str (formula) or list of str\n        Can be a string formula, a list of symbols or a list of\n        Atom objects.  Examples: 'H2O', 'COPt12', ['H', 'H', 'O'],\n        [Atom('Ne', (x, y, z)), ...].\n    positions: list of xyz-positions\n        Atomic positions.  Anything that can be converted to an\n        ndarray of shape (n, 3) will do: [(x1,y1,z1), (x2,y2,z2),\n        ...].\n    scaled_positions: list of scaled-positions\n        Like positions, but given in units of the unit cell.\n        Can not be set at the same time as positions.\n    numbers: list of int\n        Atomic numbers (use only one of symbols/numbers).\n    tags: list of int\n        Special purpose tags.\n    momenta: list of xyz-momenta\n        Momenta for all atoms.\n    masses: list of float\n        Atomic masses in atomic units.\n    magmoms: list of float or list of xyz-values\n        Magnetic moments.  Can be either a single value for each atom\n        for collinear calculations or three numbers for each atom for\n        non-collinear calculations.\n    charges: list of float\n        Initial atomic charges.\n    cell: 3x3 matrix or length 3 or 6 vector\n        Unit cell vectors.  Can also be given as just three\n        numbers for orthorhombic cells, or 6 numbers, where\n        first three are lengths of unit cell vectors, and the\n        other three are angles between them (in degrees), in following order:\n        [len(a), len(b), len(c), angle(b,c), angle(a,c), angle(a,b)].\n        First vector will lie in x-direction, second in xy-plane,\n        and the third one in z-positive subspace.\n        Default value: [0, 0, 0].\n    celldisp: Vector\n        Unit cell displacement vector. To visualize a displaced cell\n        around the center of mass of a Systems of atoms. Default value\n        = (0,0,0)\n    pbc: one or three bool\n        Periodic boundary conditions flags.  Examples: True,\n        False, 0, 1, (1, 1, 0), (True, False, False).  Default\n        value: False.\n    constraint: constraint object(s)\n        Used for applying one or more constraints during structure\n        optimization.\n    calculator: calculator object\n        Used to attach a calculator for calculating energies and atomic\n        forces.\n    info: dict of key-value pairs\n        Dictionary of key-value pairs with additional information\n        about the system.  The following keys may be used by ase:\n\n          - spacegroup: Spacegroup instance\n          - unit_cell: 'conventional' | 'primitive' | int | 3 ints\n          - adsorbate_info: Information about special adsorption sites\n\n        Items in the info attribute survives copy and slicing and can\n        be stored in and retrieved from trajectory files given that the\n        key is a string, the value is JSON-compatible and, if the value is a\n        user-defined object, its base class is importable.  One should\n        not make any assumptions about the existence of keys.\n\n    Examples:\n\n    These three are equivalent:\n\n    >>> d = 1.104  # N2 bondlength\n    >>> a = Atoms('N2', [(0, 0, 0), (0, 0, d)])\n    >>> a = Atoms(numbers=[7, 7], positions=[(0, 0, 0), (0, 0, d)])\n    >>> a = Atoms([Atom('N', (0, 0, 0)), Atom('N', (0, 0, d))])\n\n    FCC gold:\n\n    >>> a = 4.05  # Gold lattice constant\n    >>> b = a / 2\n    >>> fcc = Atoms('Au',\n    ...             cell=[(0, b, b), (b, 0, b), (b, b, 0)],\n    ...             pbc=True)\n\n    Hydrogen wire:\n\n    >>> d = 0.9  # H-H distance\n    >>> h = Atoms('H', positions=[(0, 0, 0)],\n    ...           cell=(d, 0, 0),\n    ...           pbc=(1, 0, 0))\n    ",
        "klass": "ase.Atoms",
        "module": "ase"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for representing VASP charge density",
        "klass": "ase.calculators.vasp.VaspChargeDensity",
        "module": "ase"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Reads/writes Atoms objects into an AMBER-style .nc trajectory file.\n    ",
        "klass": "ase.io.NetCDFTrajectory",
        "module": "ase"
    },
    {
        "base_classes": [
            "ase.md.md.MolecularDynamics"
        ],
        "class_docstring": "Langevin (constant N, V, T) molecular dynamics.\n\n    Usage: Langevin(atoms, dt, temperature, friction)\n\n    atoms\n        The list of atoms.\n\n    dt\n        The time step.\n\n    temperature\n        The desired temperature, in energy units.\n\n    friction\n        A friction coefficient, typically 1e-4 to 1e-2.\n\n    fixcm\n        If True, the position and momentum of the center of mass is\n        kept unperturbed.  Default: True.\n\n    rng\n        Random number generator, by default numpy.random.  Must have a\n        standard_normal method matching the signature of\n        numpy.random.standard_normal.\n\n    The temperature and friction are normally scalars, but in principle one\n    quantity per atom could be specified by giving an array.\n\n    RATTLE constraints can be used with these propagators, see:\n    E. V.-Eijnden, and G. Ciccotti, Chem. Phys. Lett. 429, 310 (2006)\n\n    The propagator is Equation 23 (Eq. 39 if RATTLE constraints are used)\n    of the above reference.  That reference also contains another\n    propagator in Eq. 21/34; but that propagator is not quasi-symplectic\n    and gives a systematic offset in the temperature at large time steps.\n\n    This dynamics accesses the atoms using Cartesian coordinates.",
        "klass": "ase.md.Langevin",
        "module": "ase"
    },
    {
        "base_classes": [
            "ase.optimize.optimize.Optimizer"
        ],
        "class_docstring": "Limited memory BFGS optimizer.\n\n    A limited memory version of the bfgs algorithm. Unlike the bfgs algorithm\n    used in bfgs.py, the inverse of Hessian matrix is updated.  The inverse\n    Hessian is represented only as a diagonal matrix to save memory\n\n    ",
        "klass": "ase.optimize.LBFGS",
        "module": "ase"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for calculating vibrational modes using finite difference.\n\n    The vibrational modes are calculated from a finite difference\n    approximation of the Hessian matrix.\n\n    The *summary()*, *get_energies()* and *get_frequencies()* methods all take\n    an optional *method* keyword.  Use method='Frederiksen' to use the method\n    described in:\n\n      T. Frederiksen, M. Paulsson, M. Brandbyge, A. P. Jauho:\n      \"Inelastic transport theory from first-principles: methodology and\n      applications for nanoscale devices\", Phys. Rev. B 75, 205413 (2007)\n\n    atoms: Atoms object\n        The atoms to work on.\n    indices: list of int\n        List of indices of atoms to vibrate.  Default behavior is\n        to vibrate all atoms.\n    name: str\n        Name to use for files.\n    delta: float\n        Magnitude of displacements.\n    nfree: int\n        Number of displacements per atom and cartesian coordinate, 2 and 4 are\n        supported. Default is 2 which will displace each atom +delta and\n        -delta for each cartesian coordinate.\n\n    Example:\n\n    >>> from ase import Atoms\n    >>> from ase.calculators.emt import EMT\n    >>> from ase.optimize import BFGS\n    >>> from ase.vibrations import Vibrations\n    >>> n2 = Atoms('N2', [(0, 0, 0), (0, 0, 1.1)],\n    ...            calculator=EMT())\n    >>> BFGS(n2).run(fmax=0.01)\n    BFGS:   0  16:01:21        0.440339       3.2518\n    BFGS:   1  16:01:21        0.271928       0.8211\n    BFGS:   2  16:01:21        0.263278       0.1994\n    BFGS:   3  16:01:21        0.262777       0.0088\n    >>> vib = Vibrations(n2)\n    >>> vib.run()\n    Writing vib.eq.pckl\n    Writing vib.0x-.pckl\n    Writing vib.0x+.pckl\n    Writing vib.0y-.pckl\n    Writing vib.0y+.pckl\n    Writing vib.0z-.pckl\n    Writing vib.0z+.pckl\n    Writing vib.1x-.pckl\n    Writing vib.1x+.pckl\n    Writing vib.1y-.pckl\n    Writing vib.1y+.pckl\n    Writing vib.1z-.pckl\n    Writing vib.1z+.pckl\n    >>> vib.summary()\n    ---------------------\n    #    meV     cm^-1\n    ---------------------\n    0    0.0       0.0\n    1    0.0       0.0\n    2    0.0       0.0\n    3    2.5      20.4\n    4    2.5      20.4\n    5  152.6    1230.8\n    ---------------------\n    Zero-point energy: 0.079 eV\n    >>> vib.write_mode(-1)  # write last mode to trajectory file\n\n    ",
        "klass": "ase.vibrations.Vibrations",
        "module": "ase"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    ::\n\n        with raw_mode(stdin):\n            ''' the pseudo-terminal stdin is now used in raw mode '''\n\n    We ignore errors when executing `tcgetattr` fails.\n    ",
        "klass": "prompt_toolkit.input.vt100.raw_mode",
        "module": "prompt_toolkit"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Progress bar context manager.\n\n    Usage ::\n\n        with ProgressBar(...) as pb:\n            for item in pb(data):\n                ...\n\n    :param title: Text to be displayed above the progress bars. This can be a\n        callable or formatted text as well.\n    :param formatters: List of :class:`.Formatter` instances.\n    :param bottom_toolbar: Text to be displayed in the bottom toolbar. This\n        can be a callable or formatted text.\n    :param style: :class:`prompt_toolkit.styles.BaseStyle` instance.\n    :param key_bindings: :class:`.KeyBindings` instance.\n    :param file: The file object used for rendering, by default `sys.stderr` is used.\n\n    :param color_depth: `prompt_toolkit` `ColorDepth` instance.\n    :param output: :class:`~prompt_toolkit.output.Output` instance.\n    :param input: :class:`~prompt_toolkit.input.Input` instance.\n    ",
        "klass": "prompt_toolkit.shortcuts.ProgressBar",
        "module": "prompt_toolkit"
    },
    {
        "base_classes": [
            "pathlib2.PurePath"
        ],
        "class_docstring": "PurePath subclass that can make system calls.\n\n    Path represents a filesystem path but unlike PurePath, also offers\n    methods to do system calls on path objects. Depending on your system,\n    instantiating a Path will return either a PosixPath or a WindowsPath\n    object. You can also instantiate a PosixPath or WindowsPath directly,\n    but cannot instantiate a WindowsPath on a POSIX system or vice versa.\n    ",
        "klass": "pathlib2.Path",
        "module": "pathlib2"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "\n\tThis class represents an action that should be run after a specified amount of time. It is similar to python's\n\town :class:`threading.Timer` class, with the addition of being able to reset the counter to zero.\n\n\tResettableTimers are started, as with threads, by calling their ``start()`` method. The timer can be stopped (in\n\tbetween runs) by calling the :func:`cancel` method. Resetting the counter can be done with the :func:`reset` method.\n\n\tFor example:\n\n\t.. code-block:: python\n\n\t   def hello():\n\t       print(\"Ran hello() at {}\").format(time.time())\n\n\t   t = ResettableTimers(60.0, hello)\n\t   t.start()\n\t   print(\"Started at {}\").format(time.time())\n\t   time.sleep(30)\n\t   t.reset()\n\t   print(\"Reset at {}\").format(time.time())\n\n\tArguments:\n\t    interval (float or callable): The interval before calling ``function``, in seconds. Can also be a callable\n\t        returning the interval to use, in case the interval is not static.\n\t    function (callable): The function to call.\n\t    args (list or tuple): The arguments for the ``function`` call. Defaults to an empty list.\n\t    kwargs (dict): The keyword arguments for the ``function`` call. Defaults to an empty dict.\n\t    on_cancelled (callable): Callback to call when the timer finishes due to being cancelled.\n\t    on_reset (callable): Callback to call when the timer is reset.\n\t",
        "klass": "octoprint.util.ResettableTimer",
        "module": "octoprint"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "\n\tThis class represents an action that should be run repeatedly in an interval. It is similar to python's\n\town :class:`threading.Timer` class, but instead of only running once the ``function`` will be run again and again,\n\tsleeping the stated ``interval`` in between.\n\n\tRepeatedTimers are started, as with threads, by calling their ``start()`` method. The timer can be stopped (in\n\tbetween runs) by calling the :func:`cancel` method. The interval the time waited before execution of a loop may\n\tnot be exactly the same as the interval specified by the user.\n\n\tFor example:\n\n\t.. code-block:: python\n\n\t   def hello():\n\t       print(\"Hello World!\")\n\n\t   t = RepeatedTimer(1.0, hello)\n\t   t.start() # prints \"Hello World!\" every second\n\n\tAnother example with dynamic interval and loop condition:\n\n\t.. code-block:: python\n\n\t   count = 0\n\t   maximum = 5\n\t   factor = 1\n\n\t   def interval():\n\t       global count\n\t       global factor\n\t       return count * factor\n\n\t   def condition():\n\t       global count\n\t       global maximum\n\t       return count <= maximum\n\n\t   def hello():\n\t       print(\"Hello World!\")\n\n\t       global count\n\t       count += 1\n\n\t   t = RepeatedTimer(interval, hello, run_first=True, condition=condition)\n\t   t.start() # prints \"Hello World!\" 5 times, printing the first one\n\t             # directly, then waiting 1, 2, 3, 4s in between (adaptive interval)\n\n\tArguments:\n\t    interval (float or callable): The interval between each ``function`` call, in seconds. Can also be a callable\n\t        returning the interval to use, in case the interval is not static.\n\t    function (callable): The function to call.\n\t    args (list or tuple): The arguments for the ``function`` call. Defaults to an empty list.\n\t    kwargs (dict): The keyword arguments for the ``function`` call. Defaults to an empty dict.\n\t    run_first (boolean): If set to True, the function will be run for the first time *before* the first wait period.\n\t        If set to False (the default), the function will be run for the first time *after* the first wait period.\n\t    condition (callable): Condition that needs to be True for loop to continue. Defaults to ``lambda: True``.\n\t    on_condition_false (callable): Callback to call when the timer finishes due to condition becoming false. Will\n\t        be called before the ``on_finish`` callback.\n\t    on_cancelled (callable): Callback to call when the timer finishes due to being cancelled. Will be called\n\t        before the ``on_finish`` callback.\n\t    on_finish (callable): Callback to call when the timer finishes, either due to being cancelled or since\n\t        the condition became false.\n\t    daemon (bool): daemon flag to set on underlying thread.\n\t",
        "klass": "octoprint.util.RepeatedTimer",
        "module": "octoprint"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The main application class and extensibility interface.\n\n    :ivar srcdir: Directory containing source.\n    :ivar confdir: Directory containing ``conf.py``.\n    :ivar doctreedir: Directory for storing pickled doctrees.\n    :ivar outdir: Directory for storing build documents.\n    ",
        "klass": "sphinx.application.Sphinx",
        "module": "sphinx"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": "\n    Represents a path which behaves like a string.\n    ",
        "klass": "sphinx.testing.path.path",
        "module": "sphinx"
    },
    {
        "base_classes": [
            "argparse.ArgumentParser"
        ],
        "class_docstring": "\n    ArgumentParser extends the one with the same name from the argparse module\n    by adding the common command line arguments found in BACpypes applications.\n\n        --buggers                       list the debugging logger names\n        --debug [DEBUG [DEBUG ...]]     attach a handler to loggers\n        --color                         debug in color\n        --route-aware                   turn on route aware\n    ",
        "klass": "bacpypes.consolelogging.ArgumentParser",
        "module": "bacpypes"
    },
    {
        "base_classes": [
            "bacpypes.consolelogging.ArgumentParser"
        ],
        "class_docstring": "\n    ConfigArgumentParser extends the ArgumentParser with the functionality to\n    read in an INI configuration file.\n\n        --ini INI       provide a separate INI file\n    ",
        "klass": "bacpypes.consolelogging.ConfigArgumentParser",
        "module": "bacpypes"
    },
    {
        "base_classes": [
            "_io._TextIOBase"
        ],
        "class_docstring": "Text I/O implementation using an in-memory buffer.\n\nThe initial_value argument sets the value of object.  The newline\nargument is like the one of TextIOWrapper's constructor.",
        "klass": "nuitka.__past__.StringIO",
        "module": "_io"
    },
    {
        "base_classes": [
            "nuitka.nodes.ExpressionBases.ExpressionChildrenHavingBase"
        ],
        "class_docstring": " Outlined expression code.\n\n        This is for a call to a piece of code to be executed in a specific\n        context. It contains an exclusively owned function body, that has\n        no other references, and can be considered part of the calling\n        context.\n\n        It must return a value, to use as expression value.\n    ",
        "klass": "nuitka.nodes.OutlineNodes.ExpressionOutlineBody",
        "module": "nuitka"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "A dict that can be changed incrementally with 'd.push(k,v)' and have changes rolled back with 'k,v = d.pop()'.",
        "klass": "ufl.utils.stacks.StackDict",
        "module": "ufl"
    },
    {
        "base_classes": [
            "datetime.date"
        ],
        "class_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "future.backports.datetime.datetime",
        "module": "datetime"
    },
    {
        "base_classes": [],
        "class_docstring": "The base class of the class hierarchy.\n\nWhen called, it accepts no arguments and returns a new featureless\ninstance that has no instance attributes and cannot be given any.\n",
        "klass": "future.builtins.object",
        "module": "object"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.",
        "klass": "future.builtins.str",
        "module": "str"
    },
    {
        "base_classes": [
            "http.client.HTTPConnection"
        ],
        "class_docstring": "This class allows communication via SSL.",
        "klass": "future.moves.http.client.HTTPSConnection",
        "module": "http"
    },
    {
        "base_classes": [
            "urllib.request.URLopener"
        ],
        "class_docstring": "Derived class with handlers for errors we can handle (perhaps).",
        "klass": "future.moves.urllib.request.FancyURLopener",
        "module": "urllib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Acts as a context manager. Saves the state of sys.modules and restores it\n    after the 'with' block.\n\n    Use like this:\n\n    >>> from future import standard_library\n    >>> with standard_library.hooks():\n    ...     import http.client\n    >>> import requests\n\n    For this to work, http.client will be scrubbed from sys.modules after the\n    'with' block. That way the modules imported in the 'with' block will\n    continue to be accessible in the current namespace but not from any\n    imported modules (like requests).\n    ",
        "klass": "future.standard_library.hooks",
        "module": "future"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A session stores configuration state and allows you to create service\n    clients and resources.\n\n    :type aws_access_key_id: string\n    :param aws_access_key_id: AWS access key ID\n    :type aws_secret_access_key: string\n    :param aws_secret_access_key: AWS secret access key\n    :type aws_session_token: string\n    :param aws_session_token: AWS temporary session token\n    :type region_name: string\n    :param region_name: Default region when creating new connections\n    :type botocore_session: botocore.session.Session\n    :param botocore_session: Use this Botocore session instead of creating\n                             a new default one.\n    :type profile_name: string\n    :param profile_name: The name of a profile to use. If not given, then\n                         the default profile is used.\n    ",
        "klass": "boto3.session.Session",
        "module": "boto3"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A session stores configuration state and allows you to create service\n    clients and resources.\n\n    :type aws_access_key_id: string\n    :param aws_access_key_id: AWS access key ID\n    :type aws_secret_access_key: string\n    :param aws_secret_access_key: AWS secret access key\n    :type aws_session_token: string\n    :param aws_session_token: AWS temporary session token\n    :type region_name: string\n    :param region_name: Default region when creating new connections\n    :type botocore_session: botocore.session.Session\n    :param botocore_session: Use this Botocore session instead of creating\n                             a new default one.\n    :type profile_name: string\n    :param profile_name: The name of a profile to use. If not given, then\n                         the default profile is used.\n    ",
        "klass": "boto3.Session",
        "module": "boto3"
    },
    {
        "base_classes": [
            "boto3.dynamodb.conditions.AttributeBase"
        ],
        "class_docstring": "Represents an DynamoDB item's attribute.",
        "klass": "boto3.dynamodb.conditions.Attr",
        "module": "boto3"
    },
    {
        "base_classes": [
            "asyncore.dispatcher"
        ],
        "class_docstring": "Accepts messages received from child worker processes.",
        "klass": "taskflow.engines.action_engine.process_executor.Dispatcher",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.engines.worker_based.protocol.Message"
        ],
        "class_docstring": "Represents notify message type.",
        "klass": "taskflow.engines.worker_based.protocol.Notify",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.engines.worker_based.protocol.Message"
        ],
        "class_docstring": "Represents request with execution results.\n\n    Every request is created in the WAITING state and is expired within the\n    given timeout if it does not transition out of the (WAITING, PENDING)\n    states.\n\n    State machine a request goes through as it progresses (or expires)::\n\n        +------------+------------+---------+----------+---------+\n        |   Start    |   Event    |   End   | On Enter | On Exit |\n        +------------+------------+---------+----------+---------+\n        | FAILURE[$] |     .      |    .    |    .     |    .    |\n        |  PENDING   | on_failure | FAILURE |    .     |    .    |\n        |  PENDING   | on_running | RUNNING |    .     |    .    |\n        |  RUNNING   | on_failure | FAILURE |    .     |    .    |\n        |  RUNNING   | on_success | SUCCESS |    .     |    .    |\n        | SUCCESS[$] |     .      |    .    |    .     |    .    |\n        | WAITING[^] | on_failure | FAILURE |    .     |    .    |\n        | WAITING[^] | on_pending | PENDING |    .     |    .    |\n        +------------+------------+---------+----------+---------+\n    ",
        "klass": "taskflow.engines.worker_based.protocol.Request",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.engines.worker_based.protocol.Message"
        ],
        "class_docstring": "Represents response message type.",
        "klass": "taskflow.engines.worker_based.protocol.Response",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.listeners.base.Listener"
        ],
        "class_docstring": "Listener that logs notifications it receives.\n\n    It listens for task and flow notifications and writes those notifications\n    to a provided logger, or logger of its module\n    (``taskflow.listeners.logging``) if none is provided (and no class\n    attribute is overridden). The log level can *slightly* be configured\n    and ``logging.DEBUG`` or ``logging.WARNING`` (unless overridden via a\n    constructor parameter) will be selected automatically based on the\n    execution state and results produced.\n\n    The following flow states cause ``logging.WARNING`` (or provided\n    level) to be used:\n\n    * ``states.FAILURE``\n    * ``states.REVERTED``\n\n    The following task states cause ``logging.WARNING`` (or provided level)\n    to be used:\n\n    * ``states.FAILURE``\n    * ``states.RETRYING``\n    * ``states.REVERTING``\n    * ``states.REVERT_FAILURE``\n\n    When a task produces a :py:class:`~taskflow.types.failure.Failure` object\n    as its result (typically this happens when a task raises an exception) this\n    will **always** switch the logger to use ``logging.WARNING`` (if the\n    failure object contains a ``exc_info`` tuple this will also be logged to\n    provide a meaningful traceback).\n    ",
        "klass": "taskflow.listeners.logging.DynamicLoggingListener",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.listeners.base.DumpingListener"
        ],
        "class_docstring": "Writes the task and flow notifications messages to stdout or stderr.",
        "klass": "taskflow.listeners.printing.PrintingListener",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.listeners.timing.DurationListener"
        ],
        "class_docstring": "Listener that prints the duration as well as recording it.",
        "klass": "taskflow.listeners.timing.PrintingDurationListener",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.flow.Flow"
        ],
        "class_docstring": "Graph flow pattern.\n\n    Contained *flows/tasks* will be executed according to their dependencies\n    which will be resolved by using the *flows/tasks* provides and requires\n    mappings or by following manually created dependency links.\n\n    From dependencies a `directed graph`_ is built. If it has edge ``A -> B``,\n    this means ``B`` depends on ``A`` (and that the execution of ``B`` must\n    wait until ``A`` has finished executing, on reverting this means that the\n    reverting of ``A`` must wait until ``B`` has finished reverting).\n\n    Note: `cyclic`_ dependencies are not allowed.\n\n    .. _directed graph: https://en.wikipedia.org/wiki/Directed_graph\n    .. _cyclic: https://en.wikipedia.org/wiki/Cycle_graph\n    ",
        "klass": "taskflow.patterns.graph_flow.Flow",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.patterns.graph_flow.Flow"
        ],
        "class_docstring": "Graph flow with a target.\n\n    Adds possibility to execute a flow up to certain graph node\n    (task or subflow).\n    ",
        "klass": "taskflow.patterns.graph_flow.TargetedFlow",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.flow.Flow"
        ],
        "class_docstring": "Linear flow pattern.\n\n    A linear (potentially nested) flow of *tasks/flows* that can be\n    applied in order as one unit and rolled back as one unit using\n    the reverse order that the *tasks/flows* have been applied in.\n    ",
        "klass": "taskflow.patterns.linear_flow.Flow",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "taskflow.flow.Flow"
        ],
        "class_docstring": "Unordered flow pattern.\n\n    A unordered (potentially nested) flow of *tasks/flows* that can be\n    executed in any order as one unit and rolled back as one unit.\n    ",
        "klass": "taskflow.patterns.unordered_flow.Flow",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "_io.StringIO"
        ],
        "class_docstring": "String buffer with some small additions.",
        "klass": "taskflow.utils.misc.StringIO",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "oauth2client.client.OAuth2Credentials"
        ],
        "class_docstring": "Application Default Credentials for use in calling Google APIs.\n\n    The Application Default Credentials are being constructed as a function of\n    the environment where the code is being run.\n    More details can be found on this page:\n    https://developers.google.com/accounts/docs/application-default-credentials\n\n    Here is an example of how to use the Application Default Credentials for a\n    service that requires authentication::\n\n        from googleapiclient.discovery import build\n        from oauth2client.client import GoogleCredentials\n\n        credentials = GoogleCredentials.get_application_default()\n        service = build('compute', 'v1', credentials=credentials)\n\n        PROJECT = 'bamboo-machine-422'\n        ZONE = 'us-central1-a'\n        request = service.instances().list(project=PROJECT, zone=ZONE)\n        response = request.execute()\n\n        print(response)\n    ",
        "klass": "oauth2client.client.GoogleCredentials",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "httplib2 Cache implementation which only caches locally.",
        "klass": "oauth2client.transport.MemoryCache",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "oauth2client.client.Credentials"
        ],
        "class_docstring": "Credentials object for OAuth 2.0.\n\n    Credentials can be applied to an httplib2.Http object using the authorize()\n    method, which then adds the OAuth 2.0 access token to each request.\n\n    OAuth2Credentials objects may be safely pickled and unpickled.\n    ",
        "klass": "oauth2client.client.OAuth2Credentials",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "oauth2client.client.Storage"
        ],
        "class_docstring": "Store and retrieve credentials to and from a dictionary-like object.\n\n    Args:\n        dictionary: A dictionary or dictionary-like object.\n        key: A string or other hashable. The credentials will be stored in\n             ``dictionary[key]``.\n        lock: An optional threading.Lock-like object. The lock will be\n              acquired before anything is written or read from the\n              dictionary.\n    ",
        "klass": "oauth2client.contrib.dictionary_storage.DictionaryStorage",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "oauth2client.client.AssertionCredentials"
        ],
        "class_docstring": "Credentials object for Compute Engine Assertion Grants\n\n    This object will allow a Compute Engine instance to identify itself to\n    Google and other OAuth 2.0 servers that can verify assertions. It can be\n    used for the purpose of accessing data stored under an account assigned to\n    the Compute Engine instance itself.\n\n    This credential does not require a flow to instantiate because it\n    represents a two legged flow, and therefore has all of the required\n    information to generate and refresh its own access tokens.\n\n    Note that :attr:`service_account_email` and :attr:`scopes`\n    will both return None until the credentials have been refreshed.\n    To check whether credentials have previously been refreshed use\n    :attr:`invalid`.\n    ",
        "klass": "oauth2client.contrib.gce.AppAssertionCredentials",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "oauth2client.client.Storage"
        ],
        "class_docstring": "Store and retrieve a single credential to and from a file.",
        "klass": "oauth2client.file.Storage",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "rdflib.graph.Graph"
        ],
        "class_docstring": "\n    A ConjunctiveGraph is an (unnamed) aggregation of all the named\n    graphs in a store.\n\n    It has a ``default`` graph, whose name is associated with the\n    graph throughout its life. :meth:`__init__` can take an identifier\n    to use as the name of this default graph or it will assign a\n    BNode.\n\n    All methods that add triples work against this default graph.\n\n    All queries are carried out against the union of all graphs.\n\n    ",
        "klass": "rdflib.graph.ConjunctiveGraph",
        "module": "rdflib"
    },
    {
        "base_classes": [
            "rdflib.graph.Graph"
        ],
        "class_docstring": "\n    A ConjunctiveGraph is an (unnamed) aggregation of all the named\n    graphs in a store.\n\n    It has a ``default`` graph, whose name is associated with the\n    graph throughout its life. :meth:`__init__` can take an identifier\n    to use as the name of this default graph or it will assign a\n    BNode.\n\n    All methods that add triples work against this default graph.\n\n    All queries are carried out against the union of all graphs.\n\n    ",
        "klass": "rdflib.ConjunctiveGraph",
        "module": "rdflib"
    },
    {
        "base_classes": [
            "rdflib.graph.ConjunctiveGraph"
        ],
        "class_docstring": "\n    RDF 1.1 Dataset. Small extension to the Conjunctive Graph:\n    - the primary term is graphs in the datasets and not contexts with quads,\n    so there is a separate method to set/retrieve a graph in a dataset and\n    operate with graphs\n    - graphs cannot be identified with blank nodes\n    - added a method to directly add a single quad\n\n    Examples of usage:\n\n    >>> # Create a new Dataset\n    >>> ds = Dataset()\n    >>> # simple triples goes to default graph\n    >>> ds.add((URIRef('http://example.org/a'),\n    ...    URIRef('http://www.example.org/b'),\n    ...    Literal('foo')))\n    >>>\n    >>> # Create a graph in the dataset, if the graph name has already been\n    >>> # used, the corresponding graph will be returned\n    >>> # (ie, the Dataset keeps track of the constituent graphs)\n    >>> g = ds.graph(URIRef('http://www.example.com/gr'))\n    >>>\n    >>> # add triples to the new graph as usual\n    >>> g.add(\n    ...     (URIRef('http://example.org/x'),\n    ...     URIRef('http://example.org/y'),\n    ...     Literal('bar')) )\n    >>> # alternatively: add a quad to the dataset -> goes to the graph\n    >>> ds.add(\n    ...     (URIRef('http://example.org/x'),\n    ...     URIRef('http://example.org/z'),\n    ...     Literal('foo-bar'),g) )\n    >>>\n    >>> # querying triples return them all regardless of the graph\n    >>> for t in ds.triples((None,None,None)):  # doctest: +SKIP\n    ...     print(t)  # doctest: +NORMALIZE_WHITESPACE\n    (rdflib.term.URIRef('http://example.org/a'),\n     rdflib.term.URIRef('http://www.example.org/b'),\n     rdflib.term.Literal('foo'))\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/z'),\n     rdflib.term.Literal('foo-bar'))\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/y'),\n     rdflib.term.Literal('bar'))\n    >>>\n    >>> # querying quads return quads; the fourth argument can be unrestricted\n    >>> # or restricted to a graph\n    >>> for q in ds.quads((None, None, None, None)):  # doctest: +SKIP\n    ...     print(q)  # doctest: +NORMALIZE_WHITESPACE\n    (rdflib.term.URIRef('http://example.org/a'),\n     rdflib.term.URIRef('http://www.example.org/b'),\n     rdflib.term.Literal('foo'),\n     None)\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/y'),\n     rdflib.term.Literal('bar'),\n     rdflib.term.URIRef('http://www.example.com/gr'))\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/z'),\n     rdflib.term.Literal('foo-bar'),\n     rdflib.term.URIRef('http://www.example.com/gr'))\n    >>>\n    >>> for q in ds.quads((None,None,None,g)):  # doctest: +SKIP\n    ...     print(q)  # doctest: +NORMALIZE_WHITESPACE\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/y'),\n     rdflib.term.Literal('bar'),\n     rdflib.term.URIRef('http://www.example.com/gr'))\n    (rdflib.term.URIRef('http://example.org/x'),\n     rdflib.term.URIRef('http://example.org/z'),\n     rdflib.term.Literal('foo-bar'),\n     rdflib.term.URIRef('http://www.example.com/gr'))\n    >>> # Note that in the call above -\n    >>> # ds.quads((None,None,None,'http://www.example.com/gr'))\n    >>> # would have been accepted, too\n    >>>\n    >>> # graph names in the dataset can be queried:\n    >>> for c in ds.graphs():  # doctest: +SKIP\n    ...     print(c)  # doctest:\n    DEFAULT\n    http://www.example.com/gr\n    >>> # A graph can be created without specifying a name; a skolemized genid\n    >>> # is created on the fly\n    >>> h = ds.graph()\n    >>> for c in ds.graphs():  # doctest: +SKIP\n    ...     print(c)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    DEFAULT\n    http://rdlib.net/.well-known/genid/rdflib/N...\n    http://www.example.com/gr\n    >>> # Note that the Dataset.graphs() call returns names of empty graphs,\n    >>> # too. This can be restricted:\n    >>> for c in ds.graphs(empty=False):  # doctest: +SKIP\n    ...     print(c)  # doctest: +NORMALIZE_WHITESPACE\n    DEFAULT\n    http://www.example.com/gr\n    >>>\n    >>> # a graph can also be removed from a dataset via ds.remove_graph(g)\n\n    .. versionadded:: 4.0\n    ",
        "klass": "rdflib.Dataset",
        "module": "rdflib"
    },
    {
        "base_classes": [
            "rdflib.term.Node"
        ],
        "class_docstring": "An RDF Graph\n\n    The constructor accepts one argument, the 'store'\n    that will be used to store the graph data (see the 'store'\n    package for stores currently shipped with rdflib).\n\n    Stores can be context-aware or unaware.  Unaware stores take up\n    (some) less space but cannot support features that require\n    context, such as true merging/demerging of sub-graphs and\n    provenance.\n\n    The Graph constructor can take an identifier which identifies the Graph\n    by name.  If none is given, the graph is assigned a BNode for its\n    identifier.\n    For more on named graphs, see: http://www.w3.org/2004/03/trix/\n\n    ",
        "klass": "rdflib.Graph",
        "module": "rdflib"
    },
    {
        "base_classes": [
            "pyparsing.Keyword"
        ],
        "class_docstring": "\n    Caseless version of :class:`Keyword`.\n\n    Example::\n\n        OneOrMore(CaselessKeyword(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD']\n\n    (Contrast with example for :class:`CaselessLiteral`.)\n    ",
        "klass": "pyparsing.CaselessKeyword",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Enables adding and parsing of multiple arguments in the context of a\n    single request. Ex::\n\n        from flask_restful import reqparse\n\n        parser = reqparse.RequestParser()\n        parser.add_argument('foo')\n        parser.add_argument('int_bar', type=int)\n        args = parser.parse_args()\n\n    :param bool trim: If enabled, trims whitespace on all arguments in this\n        parser\n    :param bool bundle_errors: If enabled, do not abort when first error occurs,\n        return a dict with the name of the argument and the error message to be\n        bundled and return all validation errors\n    ",
        "klass": "flask_restful.reqparse.RequestParser",
        "module": "flask_restful"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The class that maintains information about network setup\n    of the host. Monitoring netlink events allows it to react\n    immediately. It uses no polling.\n    ",
        "klass": "pyroute2.IPDB",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "pyroute2.iproute.linux.RTNL_API",
            "pyroute2.netlink.rtnl.iprsocket.IPRSocket"
        ],
        "class_docstring": "\n    Regular ordinary utility class, see RTNL API for the list of methods.\n    ",
        "klass": "pyroute2.IPRoute",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "pyroute2.iproute.linux.RTNL_API",
            "pyroute2.remote.RemoteSocket"
        ],
        "class_docstring": "\n    NetNS is the IPRoute API with network namespace support.\n\n    **Why not IPRoute?**\n\n    The task to run netlink commands in some network namespace, being in\n    another network namespace, requires the architecture, that differs\n    too much from a simple Netlink socket.\n\n    NetNS starts a proxy process in a network namespace and uses\n    `multiprocessing` communication channels between the main and the proxy\n    processes to route all `recv()` and `sendto()` requests/responses.\n\n    **Any specific API calls?**\n\n    Nope. `NetNS` supports all the same, that `IPRoute` does, in the same\n    way. It provides full `socket`-compatible API and can be used in\n    poll/select as well.\n\n    The only difference is the `close()` call. In the case of `NetNS` it\n    is **mandatory** to close the socket before exit.\n\n    **NetNS and IPDB**\n\n    It is possible to run IPDB with NetNS::\n\n        from pyroute2 import NetNS\n        from pyroute2 import IPDB\n\n        ip = IPDB(nl=NetNS('somenetns'))\n        ...\n        ip.release()\n\n    Do not forget to call `release()` when the work is done. It will shut\n    down `NetNS` instance as well.\n    ",
        "klass": "pyroute2.NetNS",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "pyroute2.protocols.rawsocket.RawSocket"
        ],
        "class_docstring": "\n    Parameters:\n\n    * ifname -- interface name to work on\n\n    This raw socket binds to an interface and installs BPF filter\n    to get only its UDP port. It can be used in poll/select and\n    provides also the context manager protocol, so can be used in\n    `with` statements.\n\n    It does not provide any DHCP state machine, and does not inspect\n    DHCP packets, it is totally up to you. No default values are\n    provided here, except `xid` -- DHCP transaction ID. If `xid` is\n    not provided, DHCP4Socket generates it for outgoing messages.\n    ",
        "klass": "pyroute2.dhcp.dhcp4socket.DHCP4Socket",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "pyroute2.netlink.nlmsg_atoms"
        ],
        "class_docstring": "\n    Main netlink message class\n    ",
        "klass": "pyroute2.netlink.nlmsg",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "pyroute2.iproute.linux.RTNL_API",
            "pyroute2.netlink.rtnl.iprsocket.IPRSocket"
        ],
        "class_docstring": "\n    Regular ordinary utility class, see RTNL API for the list of methods.\n    ",
        "klass": "pyroute2.iproute.IPRoute",
        "module": "pyroute2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Simple JSON <http://json.org> decoder\n\n    Performs the following translations in decoding by default:\n\n    +---------------+-------------------+\n    | JSON          | Python            |\n    +===============+===================+\n    | object        | dict              |\n    +---------------+-------------------+\n    | array         | list              |\n    +---------------+-------------------+\n    | string        | str, unicode      |\n    +---------------+-------------------+\n    | number (int)  | int, long         |\n    +---------------+-------------------+\n    | number (real) | float             |\n    +---------------+-------------------+\n    | true          | True              |\n    +---------------+-------------------+\n    | false         | False             |\n    +---------------+-------------------+\n    | null          | None              |\n    +---------------+-------------------+\n\n    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as\n    their corresponding ``float`` values, which is outside the JSON spec.\n\n    ",
        "klass": "simplejson.JSONDecoder",
        "module": "simplejson"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Extensible JSON <http://json.org> encoder for Python data structures.\n\n    Supports the following objects and types by default:\n\n    +-------------------+---------------+\n    | Python            | JSON          |\n    +===================+===============+\n    | dict, namedtuple  | object        |\n    +-------------------+---------------+\n    | list, tuple       | array         |\n    +-------------------+---------------+\n    | str, unicode      | string        |\n    +-------------------+---------------+\n    | int, long, float  | number        |\n    +-------------------+---------------+\n    | True              | true          |\n    +-------------------+---------------+\n    | False             | false         |\n    +-------------------+---------------+\n    | None              | null          |\n    +-------------------+---------------+\n\n    To extend this to recognize other objects, subclass and implement a\n    ``.default()`` method with another method that returns a serializable\n    object for ``o`` if possible, otherwise it should call the superclass\n    implementation (to raise ``TypeError``).\n\n    ",
        "klass": "simplejson.encoder.JSONEncoder",
        "module": "simplejson"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Extensible JSON <http://json.org> encoder for Python data structures.\n\n    Supports the following objects and types by default:\n\n    +-------------------+---------------+\n    | Python            | JSON          |\n    +===================+===============+\n    | dict, namedtuple  | object        |\n    +-------------------+---------------+\n    | list, tuple       | array         |\n    +-------------------+---------------+\n    | str, unicode      | string        |\n    +-------------------+---------------+\n    | int, long, float  | number        |\n    +-------------------+---------------+\n    | True              | true          |\n    +-------------------+---------------+\n    | False             | false         |\n    +-------------------+---------------+\n    | None              | null          |\n    +-------------------+---------------+\n\n    To extend this to recognize other objects, subclass and implement a\n    ``.default()`` method with another method that returns a serializable\n    object for ``o`` if possible, otherwise it should call the superclass\n    implementation (to raise ``TypeError``).\n\n    ",
        "klass": "simplejson.JSONEncoder",
        "module": "simplejson"
    },
    {
        "base_classes": [
            "affine.Affine"
        ],
        "class_docstring": "Two dimensional affine transform for 2D linear mapping.\n\n    Parameters\n    ----------\n    a, b, c, d, e, f : float\n        Coefficients of an augmented affine transformation matrix\n\n        | x' |   | a  b  c | | x |\n        | y' | = | d  e  f | | y |\n        | 1  |   | 0  0  1 | | 1 |\n\n        `a`, `b`, and `c` are the elements of the first row of the\n        matrix. `d`, `e`, and `f` are the elements of the second row.\n\n    Attributes\n    ----------\n    a, b, c, d, e, f, g, h, i : float\n        The coefficients of the 3x3 augumented affine transformation\n        matrix\n\n        | x' |   | a  b  c | | x |\n        | y' | = | d  e  f | | y |\n        | 1  |   | g  h  i | | 1 |\n\n        `g`, `h`, and `i` are always 0, 0, and 1.\n\n    The Affine package is derived from Casey Duncan's Planar package.\n    See the copyright statement below.  Parallel lines are preserved by\n    these transforms. Affine transforms can perform any combination of\n    translations, scales/flips, shears, and rotations.  Class methods\n    are provided to conveniently compose transforms from these\n    operations.\n\n    Internally the transform is stored as a 3x3 transformation matrix.\n    The transform may be constructed directly by specifying the first\n    two rows of matrix values as 6 floats. Since the matrix is an affine\n    transform, the last row is always ``(0, 0, 1)``.\n\n    N.B.: multiplication of a transform and an (x, y) vector *always*\n    returns the column vector that is the matrix multiplication product\n    of the transform and (x, y) as a column vector, no matter which is\n    on the left or right side. This is obviously not the case for\n    matrices and vectors in general, but provides a convenience for\n    users of this class.\n\n    ",
        "klass": "affine.Affine",
        "module": "affine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Abstraction for GDAL and AWS configuration\n\n    The GDAL library is stateful: it has a registry of format drivers,\n    an error stack, and dozens of configuration options.\n\n    Rasterio's approach to working with GDAL is to wrap all the state\n    up using a Python context manager (see PEP 343,\n    https://www.python.org/dev/peps/pep-0343/). When the context is\n    entered GDAL drivers are registered, error handlers are\n    configured, and configuration options are set. When the context\n    is exited, drivers are removed from the registry and other\n    configurations are removed.\n\n    Example:\n\n        with rasterio.Env(GDAL_CACHEMAX=512) as env:\n            # All drivers are registered, GDAL's raster block cache\n            # size is set to 512MB.\n            # Commence processing...\n            ...\n            # End of processing.\n\n        # At this point, configuration options are set to their\n        # previous (possible unset) values.\n\n    A boto3 session or boto3 session constructor arguments\n    `aws_access_key_id`, `aws_secret_access_key`, `aws_session_token`\n    may be passed to Env's constructor. In the latter case, a session\n    will be created as soon as needed. AWS credentials are configured\n    for GDAL as needed.\n    ",
        "klass": "rasterio.Env",
        "module": "rasterio"
    },
    {
        "base_classes": [
            "rasterio._io.MemoryFileBase"
        ],
        "class_docstring": "A BytesIO-like object, backed by an in-memory file.\n\n    This allows formatted files to be read and written without I/O.\n\n    A MemoryFile created with initial bytes becomes immutable. A\n    MemoryFile created without initial bytes may be written to using\n    either file-like or dataset interfaces.\n\n    Examples\n    --------\n\n    A GeoTIFF can be loaded in memory and accessed using the GeoTIFF\n    format driver\n\n    >>> with open('tests/data/RGB.byte.tif', 'rb') as f, MemoryFile(f) as memfile:\n    ...     with memfile.open() as src:\n    ...         pprint.pprint(src.profile)\n    ...\n    {'count': 3,\n     'crs': CRS({'init': 'epsg:32618'}),\n     'driver': 'GTiff',\n     'dtype': 'uint8',\n     'height': 718,\n     'interleave': 'pixel',\n     'nodata': 0.0,\n     'tiled': False,\n     'transform': Affine(300.0379266750948, 0.0, 101985.0,\n           0.0, -300.041782729805, 2826915.0),\n     'width': 791}\n\n    ",
        "klass": "rasterio.io.MemoryFile",
        "module": "rasterio"
    },
    {
        "base_classes": [
            "affine.Affine"
        ],
        "class_docstring": "Two dimensional affine transform for 2D linear mapping.\n\n    Parameters\n    ----------\n    a, b, c, d, e, f : float\n        Coefficients of an augmented affine transformation matrix\n\n        | x' |   | a  b  c | | x |\n        | y' | = | d  e  f | | y |\n        | 1  |   | 0  0  1 | | 1 |\n\n        `a`, `b`, and `c` are the elements of the first row of the\n        matrix. `d`, `e`, and `f` are the elements of the second row.\n\n    Attributes\n    ----------\n    a, b, c, d, e, f, g, h, i : float\n        The coefficients of the 3x3 augumented affine transformation\n        matrix\n\n        | x' |   | a  b  c | | x |\n        | y' | = | d  e  f | | y |\n        | 1  |   | g  h  i | | 1 |\n\n        `g`, `h`, and `i` are always 0, 0, and 1.\n\n    The Affine package is derived from Casey Duncan's Planar package.\n    See the copyright statement below.  Parallel lines are preserved by\n    these transforms. Affine transforms can perform any combination of\n    translations, scales/flips, shears, and rotations.  Class methods\n    are provided to conveniently compose transforms from these\n    operations.\n\n    Internally the transform is stored as a 3x3 transformation matrix.\n    The transform may be constructed directly by specifying the first\n    two rows of matrix values as 6 floats. Since the matrix is an affine\n    transform, the last row is always ``(0, 0, 1)``.\n\n    N.B.: multiplication of a transform and an (x, y) vector *always*\n    returns the column vector that is the matrix multiplication product\n    of the transform and (x, y) as a column vector, no matter which is\n    on the left or right side. This is obviously not the case for\n    matrices and vectors in general, but provides a convenience for\n    users of this class.\n\n    ",
        "klass": "rasterio.transform.Affine",
        "module": "affine"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad",
            "collections.abc.Mapping"
        ],
        "class_docstring": "Dictionary encapsulates the mapping between normalized words and their integer ids.\n\n    Notable instance attributes:\n\n    Attributes\n    ----------\n    token2id : dict of (str, int)\n        token -> tokenId.\n    id2token : dict of (int, str)\n        Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n    cfs : dict of (int, int)\n        Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n    dfs : dict of (int, int)\n        Document frequencies: token_id -> how many documents contain this token.\n    num_docs : int\n        Number of documents processed.\n    num_pos : int\n        Total number of corpus positions (number of processed words).\n    num_nnz : int\n        Total number of non-zeroes in the BOW matrix (sum of the number of unique\n        words per document over the entire corpus).\n\n    ",
        "klass": "gensim.corpora.dictionary.Dictionary",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad",
            "collections.abc.Mapping"
        ],
        "class_docstring": "Dictionary encapsulates the mapping between normalized words and their integer ids.\n\n    Notable instance attributes:\n\n    Attributes\n    ----------\n    token2id : dict of (str, int)\n        token -> tokenId.\n    id2token : dict of (int, str)\n        Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n    cfs : dict of (int, int)\n        Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n    dfs : dict of (int, int)\n        Document frequencies: token_id -> how many documents contain this token.\n    num_docs : int\n        Number of documents processed.\n    num_pos : int\n        Total number of corpus positions (number of processed words).\n    num_nnz : int\n        Total number of non-zeroes in the BOW matrix (sum of the number of unique\n        words per document over the entire corpus).\n\n    ",
        "klass": "gensim.corpora.Dictionary",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad",
            "dict"
        ],
        "class_docstring": "Mapping between words and their integer ids, using a hashing function.\n\n    Unlike :class:`~gensim.corpora.dictionary.Dictionary`,\n    building a :class:`~gensim.corpora.hashdictionary.HashDictionary` before using it **isn't a necessary step**.\n\n    You can start converting words to ids immediately, without training on a corpus.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.corpora import HashDictionary\n        >>>\n        >>> dct = HashDictionary(debug=False)  # needs no training corpus!\n        >>>\n        >>> texts = [['human', 'interface', 'computer']]\n        >>> dct.doc2bow(texts[0])\n        [(10608, 1), (12466, 1), (31002, 1)]\n\n    ",
        "klass": "gensim.corpora.hashdictionary.HashDictionary",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC"
        ],
        "class_docstring": "Objects of this class allow for building and maintaining a model for topic coherence.\n\n    Examples\n    ---------\n    One way of using this feature is through providing a trained topic model. A dictionary has to be explicitly provided\n    if the model does not contain a dictionary already\n\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus, common_dictionary\n        >>> from gensim.models.ldamodel import LdaModel\n        >>> from gensim.models.coherencemodel import CoherenceModel\n        >>>\n        >>> model = LdaModel(common_corpus, 5, common_dictionary)\n        >>>\n        >>> cm = CoherenceModel(model=model, corpus=common_corpus, coherence='u_mass')\n        >>> coherence = cm.get_coherence()  # get coherence value\n\n    Another way of using this feature is through providing tokenized topics such as:\n\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus, common_dictionary\n        >>> from gensim.models.coherencemodel import CoherenceModel\n        >>> topics = [\n        ...     ['human', 'computer', 'system', 'interface'],\n        ...     ['graph', 'minors', 'trees', 'eps']\n        ... ]\n        >>>\n        >>> cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n        >>> coherence = cm.get_coherence()  # get coherence value\n\n    ",
        "klass": "gensim.models.coherencemodel.CoherenceModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.base_any2vec.BaseWordEmbeddingsModel"
        ],
        "class_docstring": "Class for training, using and evaluating neural networks described in\n    `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\n\n    Some important internal attributes are the following:\n\n    Attributes\n    ----------\n    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n        This object essentially contains the mapping between words and embeddings. After training, it can be used\n        directly to query those embeddings in various ways. See the module level docstring for examples.\n\n    docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n        This object contains the paragraph vectors learned from the training data. There will be one such vector\n        for each unique document tag supplied during training. They may be individually accessed using the tag\n        as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':\n\n        .. sourcecode:: pycon\n\n            >>> model.docvecs['doc003']\n\n    vocabulary : :class:`~gensim.models.doc2vec.Doc2VecVocab`\n        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n        Besides keeping track of all unique words, this object provides extra functionality, such as\n        sorting words by frequency, or discarding extremely rare words.\n\n    trainables : :class:`~gensim.models.doc2vec.Doc2VecTrainables`\n        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n        The only addition to the underlying NN used in :class:`~gensim.models.word2vec.Word2Vec` is that the input\n        includes not only the word vectors of each word in the context, but also the paragraph vector.\n\n    ",
        "klass": "gensim.models.doc2vec.Doc2Vec",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.base_any2vec.BaseWordEmbeddingsModel"
        ],
        "class_docstring": "Class for training, using and evaluating neural networks described in\n    `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\n\n    Some important internal attributes are the following:\n\n    Attributes\n    ----------\n    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n        This object essentially contains the mapping between words and embeddings. After training, it can be used\n        directly to query those embeddings in various ways. See the module level docstring for examples.\n\n    docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n        This object contains the paragraph vectors learned from the training data. There will be one such vector\n        for each unique document tag supplied during training. They may be individually accessed using the tag\n        as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':\n\n        .. sourcecode:: pycon\n\n            >>> model.docvecs['doc003']\n\n    vocabulary : :class:`~gensim.models.doc2vec.Doc2VecVocab`\n        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n        Besides keeping track of all unique words, this object provides extra functionality, such as\n        sorting words by frequency, or discarding extremely rare words.\n\n    trainables : :class:`~gensim.models.doc2vec.Doc2VecTrainables`\n        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n        The only addition to the underlying NN used in :class:`~gensim.models.word2vec.Word2Vec` is that the input\n        includes not only the word vectors of each word in the context, but also the paragraph vector.\n\n    ",
        "klass": "gensim.models.Doc2Vec",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.base_any2vec.BaseWordEmbeddingsModel"
        ],
        "class_docstring": "Train, use and evaluate word representations learned using the method\n    described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_, aka FastText.\n\n    The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n    :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the original\n    Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n\n    Attributes\n    ----------\n    wv : :class:`~gensim.models.keyedvectors.FastTextKeyedVectors`\n        This object essentially contains the mapping between words and embeddings. These are similar to the embeddings\n        computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also include vectors for n-grams.\n        This allows the model to compute embeddings even for **unseen** words (that do not exist in the vocabulary),\n        as the aggregate of the n-grams included in the word. After training the model, this attribute can be used\n        directly to query those embeddings in various ways. Check the module level docstring for some examples.\n    vocabulary : :class:`~gensim.models.fasttext.FastTextVocab`\n        This object represents the vocabulary of the model.\n        Besides keeping track of all unique words, this object provides extra functionality, such as\n        constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n    trainables : :class:`~gensim.models.fasttext.FastTextTrainables`\n        This object represents the inner shallow neural network used to train the embeddings. This is very\n        similar to the network of the :class:`~gensim.models.word2vec.Word2Vec` model, but it also trains weights\n        for the N-Grams (sequences of more than 1 words). The semantics of the network are almost the same as\n        the one used for the :class:`~gensim.models.word2vec.Word2Vec` model.\n        You can think of it as a NN with a single projection and hidden layer which we train on the corpus.\n        The weights are then used as our embeddings. An important difference however between the two models, is the\n        scoring function used to compute the loss. In the case of FastText, this is modified in word to also account\n        for the internal structure of words, besides their concurrence counts.\n\n    ",
        "klass": "gensim.models.fasttext.FastText",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC",
            "gensim.models.basemodel.BaseTopicModel"
        ],
        "class_docstring": "Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n    `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n\n    Examples\n    -------\n    Initialize a model using a Gensim corpus\n\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus\n        >>>\n        >>> lda = LdaModel(common_corpus, num_topics=10)\n\n    You can then infer topic distributions on new, unseen documents.\n\n    .. sourcecode:: pycon\n\n        >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n        >>> doc_lda = lda[doc_bow]\n\n    The model can be updated (trained) with new documents.\n\n    .. sourcecode:: pycon\n\n        >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n        >>> other_corpus = common_corpus\n        >>>\n        >>> lda.update(other_corpus)\n\n    Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n    :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n\n    ",
        "klass": "gensim.models.ldamodel.LdaModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC",
            "gensim.models.basemodel.BaseTopicModel"
        ],
        "class_docstring": "Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n    `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n\n    Examples\n    -------\n    Initialize a model using a Gensim corpus\n\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus\n        >>>\n        >>> lda = LdaModel(common_corpus, num_topics=10)\n\n    You can then infer topic distributions on new, unseen documents.\n\n    .. sourcecode:: pycon\n\n        >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n        >>> doc_lda = lda[doc_bow]\n\n    The model can be updated (trained) with new documents.\n\n    .. sourcecode:: pycon\n\n        >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n        >>> other_corpus = common_corpus\n        >>>\n        >>> lda.update(other_corpus)\n\n    Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n    :meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n\n    ",
        "klass": "gensim.models.LdaModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.ldamodel.LdaModel"
        ],
        "class_docstring": "An optimized implementation of the LDA algorithm, able to harness the power of multicore CPUs.\n    Follows the similar API as the parent class :class:`~gensim.models.ldamodel.LdaModel`.\n\n    ",
        "klass": "gensim.models.ldamulticore.LdaMulticore",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC"
        ],
        "class_docstring": "Objects of this class realize the transformation between word-document co-occurrence matrix (int)\n    into a locally/globally weighted matrix (positive floats).\n\n    This is done by a log entropy normalization, optionally normalizing the resulting documents to unit length.\n    The following formulas explain how o compute the log entropy weight for term :math:`i` in document :math:`j`:\n\n    .. math::\n\n        local\\_weight_{i,j} = log(frequency_{i,j} + 1)\n\n        P_{i,j} = \\frac{frequency_{i,j}}{\\sum_j frequency_{i,j}}\n\n        global\\_weight_i = 1 + \\frac{\\sum_j P_{i,j} * log(P_{i,j})}{log(number\\_of\\_documents + 1)}\n\n        final\\_weight_{i,j} = local\\_weight_{i,j} * global\\_weight_i\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.models import LogEntropyModel\n        >>> from gensim.test.utils import common_texts\n        >>> from gensim.corpora import Dictionary\n        >>>\n        >>> dct = Dictionary(common_texts)  # fit dictionary\n        >>> corpus = [dct.doc2bow(row) for row in common_texts]  # convert to BoW format\n        >>> model = LogEntropyModel(corpus)  # fit model\n        >>> vector = model[corpus[1]]  # apply model to document\n\n    ",
        "klass": "gensim.models.logentropy_model.LogEntropyModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC",
            "gensim.models.basemodel.BaseTopicModel"
        ],
        "class_docstring": "Model for `Latent Semantic Indexing\n    <https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing>`_.\n\n    The decomposition algorithm is described in `\"Fast and Faster: A Comparison of Two Streamed\n    Matrix Decomposition Algorithms\" <https://nlp.fi.muni.cz/~xrehurek/nips/rehurek_nips.pdf>`_.\n\n    Notes\n    -----\n    * :attr:`gensim.models.lsimodel.LsiModel.projection.u` - left singular vectors,\n    * :attr:`gensim.models.lsimodel.LsiModel.projection.s` - singular values,\n    * ``model[training_corpus]`` - right singular vectors (can be reconstructed if needed).\n\n    See Also\n    --------\n    `FAQ about LSI matrices\n    <https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q4-how-do-you-output-the-u-s-vt-matrices-of-lsi>`_.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n        >>> from gensim.models import LsiModel\n        >>>\n        >>> model = LsiModel(common_corpus[:3], id2word=common_dictionary)  # train model\n        >>> vector = model[common_corpus[4]]  # apply model to BoW document\n        >>> model.add_documents(common_corpus[4:])  # update model with new documents\n        >>> tmp_fname = get_tmpfile(\"lsi.model\")\n        >>> model.save(tmp_fname)  # save model\n        >>> loaded_model = LsiModel.load(tmp_fname)  # load model\n\n    ",
        "klass": "gensim.models.lsimodel.LsiModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC",
            "gensim.models.basemodel.BaseTopicModel"
        ],
        "class_docstring": "Model for `Latent Semantic Indexing\n    <https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing>`_.\n\n    The decomposition algorithm is described in `\"Fast and Faster: A Comparison of Two Streamed\n    Matrix Decomposition Algorithms\" <https://nlp.fi.muni.cz/~xrehurek/nips/rehurek_nips.pdf>`_.\n\n    Notes\n    -----\n    * :attr:`gensim.models.lsimodel.LsiModel.projection.u` - left singular vectors,\n    * :attr:`gensim.models.lsimodel.LsiModel.projection.s` - singular values,\n    * ``model[training_corpus]`` - right singular vectors (can be reconstructed if needed).\n\n    See Also\n    --------\n    `FAQ about LSI matrices\n    <https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q4-how-do-you-output-the-u-s-vt-matrices-of-lsi>`_.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n        >>> from gensim.models import LsiModel\n        >>>\n        >>> model = LsiModel(common_corpus[:3], id2word=common_dictionary)  # train model\n        >>> vector = model[common_corpus[4]]  # apply model to BoW document\n        >>> model.add_documents(common_corpus[4:])  # update model with new documents\n        >>> tmp_fname = get_tmpfile(\"lsi.model\")\n        >>> model.save(tmp_fname)  # save model\n        >>> loaded_model = LsiModel.load(tmp_fname)  # load model\n\n    ",
        "klass": "gensim.models.LsiModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC"
        ],
        "class_docstring": "Objects of this class realize the explicit normalization of vectors (l1 and l2).",
        "klass": "gensim.models.normmodel.NormModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.phrases.SentenceAnalyzer",
            "gensim.models.phrases.PhrasesTransformation"
        ],
        "class_docstring": "Detect phrases based on collocation counts.",
        "klass": "gensim.models.Phrases",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.TransformationABC"
        ],
        "class_docstring": "Objects of this class realize the transformation between word-document co-occurrence matrix (int)\n    into a locally/globally weighted TF-IDF matrix (positive floats).\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> import gensim.downloader as api\n        >>> from gensim.models import TfidfModel\n        >>> from gensim.corpora import Dictionary\n        >>>\n        >>> dataset = api.load(\"text8\")\n        >>> dct = Dictionary(dataset)  # fit dictionary\n        >>> corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n        >>>\n        >>> model = TfidfModel(corpus)  # fit model\n        >>> vector = model[corpus[0]]  # apply model to the first corpus document\n\n    ",
        "klass": "gensim.models.tfidfmodel.TfidfModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad"
        ],
        "class_docstring": "Objects of this class realize the translation matrix which map the source language to the target language.\n    The main methods are:\n\n    We map it to the other language space by computing z = Wx, then return the\n    word whose representation is close to z.\n\n    The details use seen the notebook [3]_\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.models import KeyedVectors\n        >>> from gensim.test.utils import datapath\n        >>> en = datapath(\"EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\")\n        >>> it = datapath(\"IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\")\n        >>> model_en = KeyedVectors.load_word2vec_format(en)\n        >>> model_it = KeyedVectors.load_word2vec_format(it)\n        >>>\n        >>> word_pairs = [\n        ...     (\"one\", \"uno\"), (\"two\", \"due\"), (\"three\", \"tre\"), (\"four\", \"quattro\"), (\"five\", \"cinque\"),\n        ...     (\"seven\", \"sette\"), (\"eight\", \"otto\"),\n        ...     (\"dog\", \"cane\"), (\"pig\", \"maiale\"), (\"fish\", \"cavallo\"), (\"birds\", \"uccelli\"),\n        ...     (\"apple\", \"mela\"), (\"orange\", \"arancione\"), (\"grape\", \"acino\"), (\"banana\", \"banana\")\n        ... ]\n        >>>\n        >>> trans_model = TranslationMatrix(model_en, model_it)\n        >>> trans_model.train(word_pairs)\n        >>> trans_model.translate([\"dog\", \"one\"], topn=3)\n        OrderedDict([('dog', [u'cane', u'gatto', u'cavallo']), ('one', [u'uno', u'due', u'tre'])])\n\n\n    References\n    ----------\n    .. [3] https://github.com/RaRe-Technologies/gensim/blob/3.2.0/docs/notebooks/translation_matrix.ipynb\n\n    ",
        "klass": "gensim.models.translation_matrix.TranslationMatrix",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.base_any2vec.BaseWordEmbeddingsModel"
        ],
        "class_docstring": "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n\n    Once you're finished training a model (=no more updates, only querying)\n    store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n\n    The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n    :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n\n    The trained word vectors can also be stored/loaded from a format compatible with the\n    original word2vec implementation via `self.wv.save_word2vec_format`\n    and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n\n    Some important attributes are the following:\n\n    Attributes\n    ----------\n    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n        This object essentially contains the mapping between words and embeddings. After training, it can be used\n        directly to query those embeddings in various ways. See the module level docstring for examples.\n\n    vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n        Besides keeping track of all unique words, this object provides extra functionality, such as\n        constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n\n    trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n        (which means that the size of the hidden layer is equal to the number of features `self.size`).\n\n    ",
        "klass": "gensim.models.word2vec.Word2Vec",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.models.base_any2vec.BaseWordEmbeddingsModel"
        ],
        "class_docstring": "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n\n    Once you're finished training a model (=no more updates, only querying)\n    store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n\n    The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n    :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n\n    The trained word vectors can also be stored/loaded from a format compatible with the\n    original word2vec implementation via `self.wv.save_word2vec_format`\n    and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n\n    Some important attributes are the following:\n\n    Attributes\n    ----------\n    wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n        This object essentially contains the mapping between words and embeddings. After training, it can be used\n        directly to query those embeddings in various ways. See the module level docstring for examples.\n\n    vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n        This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n        Besides keeping track of all unique words, this object provides extra functionality, such as\n        constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n\n    trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n        This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n        network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n        a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n        (which means that the size of the hidden layer is equal to the number of features `self.size`).\n\n    ",
        "klass": "gensim.models.Word2Vec",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad"
        ],
        "class_docstring": "Python wrapper using `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_.\n\n    Communication between DTM and Python takes place by passing around data files on disk and executing\n    the DTM binary as a subprocess.\n\n    Warnings\n    --------\n    This is **only** python wrapper for `DTM implementation <https://github.com/magsilva/dtm/tree/master/bin>`_,\n    you need to install original implementation first and pass the path to binary to ``dtm_path``.\n\n    ",
        "klass": "gensim.models.wrappers.DtmModel",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad",
            "gensim.models.basemodel.BaseTopicModel"
        ],
        "class_docstring": "Python wrapper for LDA using `MALLET <http://mallet.cs.umass.edu/>`_.\n\n    Communication between MALLET and Python takes place by passing around data files on disk\n    and calling Java with subprocess.call().\n\n    Warnings\n    --------\n    This is **only** python wrapper for `MALLET LDA <http://mallet.cs.umass.edu/>`_,\n    you need to install original implementation first and pass the path to binary to ``mallet_path``.\n\n    ",
        "klass": "gensim.models.wrappers.ldamallet.LdaMallet",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.utils.SaveLoad"
        ],
        "class_docstring": "Python wrapper using `Vowpal Wabbit's online LDA <https://github.com/JohnLangford/vowpal_wabbit/>`_.\n\n    Communication between Vowpal Wabbit and Python takes place by passing around data files\n    on disk and calling the 'vw' binary with the subprocess module.\n\n    Warnings\n    --------\n    This is **only** python wrapper for `Vowpal Wabbit's online LDA <https://github.com/JohnLangford/vowpal_wabbit/>`_,\n    you need to install original implementation first and pass the path to binary to ``vw_path``.\n\n    ",
        "klass": "gensim.models.wrappers.ldavowpalwabbit.LdaVowpalWabbit",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.interfaces.SimilarityABC"
        ],
        "class_docstring": "Compute cosine similarity against a corpus of documents by storing the index matrix in memory.\n\n    Unless the entire matrix fits into main memory, use :class:`~gensim.similarities.docsim.Similarity` instead.\n\n    Examples\n    --------\n    .. sourcecode:: pycon\n\n        >>> from gensim.test.utils import common_corpus, common_dictionary\n        >>> from gensim.similarities import MatrixSimilarity\n        >>>\n        >>> query = [(1, 2), (5, 4)]\n        >>> index = MatrixSimilarity(common_corpus, num_features=len(common_dictionary))\n        >>> sims = index[query]\n\n    ",
        "klass": "gensim.similarities.MatrixSimilarity",
        "module": "gensim"
    },
    {
        "base_classes": [
            "gensim.summarization.graph.IGraph"
        ],
        "class_docstring": "\n    Implementation of an undirected graph, based on IGraph.\n\n    Attributes\n    ----------\n    Graph.DEFAULT_WEIGHT : float\n        Weight set by default.\n\n    ",
        "klass": "gensim.summarization.graph.Graph",
        "module": "gensim"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A connection to the broker.\n\n    Example:\n        >>> Connection('amqp://guest:guest@localhost:5672//')\n        >>> Connection('amqp://foo;amqp://bar',\n        ...            failover_strategy='round-robin')\n        >>> Connection('redis://', transport_options={\n        ...     'visibility_timeout': 3000,\n        ... })\n\n        >>> import ssl\n        >>> Connection('amqp://', login_method='EXTERNAL', ssl={\n        ...    'ca_certs': '/etc/pki/tls/certs/something.crt',\n        ...    'keyfile': '/etc/something/system.key',\n        ...    'certfile': '/etc/something/system.cert',\n        ...    'cert_reqs': ssl.CERT_REQUIRED,\n        ... })\n\n    Note:\n        SSL currently only works with the py-amqp, and qpid\n        transports.  For other transports you can use stunnel.\n\n    Arguments:\n        URL (str, Sequence): Broker URL, or a list of URLs.\n\n    Keyword Arguments:\n        ssl (bool): Use SSL to connect to the server. Default is ``False``.\n            May not be supported by the specified transport.\n        transport (Transport): Default transport if not specified in the URL.\n        connect_timeout (float): Timeout in seconds for connecting to the\n            server. May not be supported by the specified transport.\n        transport_options (Dict): A dict of additional connection arguments to\n            pass to alternate kombu channel implementations.  Consult the\n            transport documentation for available options.\n        heartbeat (float): Heartbeat interval in int/float seconds.\n            Note that if heartbeats are enabled then the\n            :meth:`heartbeat_check` method must be called regularly,\n            around once per second.\n\n    Note:\n        The connection is established lazily when needed. If you need the\n        connection to be established, then force it by calling\n        :meth:`connect`::\n\n            >>> conn = Connection('amqp://')\n            >>> conn.connect()\n\n        and always remember to close the connection::\n\n            >>> conn.release()\n\n    These options have been replaced by the URL argument, but are still\n    supported for backwards compatibility:\n\n    :keyword hostname: Host name/address.\n        NOTE: You cannot specify both the URL argument and use the hostname\n        keyword argument at the same time.\n    :keyword userid: Default user name if not provided in the URL.\n    :keyword password: Default password if not provided in the URL.\n    :keyword virtual_host: Default virtual host if not provided in the URL.\n    :keyword port: Default port if not provided in the URL.\n    ",
        "klass": "kombu.Connection",
        "module": "kombu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Message consumer.\n\n    Arguments:\n        channel (kombu.Connection, ChannelT): see :attr:`channel`.\n        queues (Sequence[kombu.Queue]): see :attr:`queues`.\n        no_ack (bool): see :attr:`no_ack`.\n        auto_declare (bool): see :attr:`auto_declare`\n        callbacks (Sequence[Callable]): see :attr:`callbacks`.\n        on_message (Callable): See :attr:`on_message`\n        on_decode_error (Callable): see :attr:`on_decode_error`.\n        prefetch_count (int): see :attr:`prefetch_count`.\n    ",
        "klass": "kombu.Consumer",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.abstract.MaybeChannelBound"
        ],
        "class_docstring": "An Exchange declaration.\n\n    Arguments:\n        name (str): See :attr:`name`.\n        type (str): See :attr:`type`.\n        channel (kombu.Connection, ChannelT): See :attr:`channel`.\n        durable (bool): See :attr:`durable`.\n        auto_delete (bool): See :attr:`auto_delete`.\n        delivery_mode (enum): See :attr:`delivery_mode`.\n        arguments (Dict): See :attr:`arguments`.\n        no_declare (bool): See :attr:`no_declare`\n\n    Attributes:\n        name (str): Name of the exchange.\n            Default is no name (the default exchange).\n\n        type (str):\n            *This description of AMQP exchange types was shamelessly stolen\n            from the blog post `AMQP in 10 minutes: Part 4`_ by\n            Rajith Attapattu. Reading this article is recommended if you're\n            new to amqp.*\n\n            \"AMQP defines four default exchange types (routing algorithms) that\n            covers most of the common messaging use cases. An AMQP broker can\n            also define additional exchange types, so see your broker\n            manual for more information about available exchange types.\n\n                * `direct` (*default*)\n\n                    Direct match between the routing key in the message,\n                    and the routing criteria used when a queue is bound to\n                    this exchange.\n\n                * `topic`\n\n                    Wildcard match between the routing key and the routing\n                    pattern specified in the exchange/queue binding.\n                    The routing key is treated as zero or more words delimited\n                    by `\".\"` and supports special wildcard characters. `\"*\"`\n                    matches a single word and `\"#\"` matches zero or more words.\n\n                * `fanout`\n\n                    Queues are bound to this exchange with no arguments. Hence\n                    any message sent to this exchange will be forwarded to all\n                    queues bound to this exchange.\n\n                * `headers`\n\n                    Queues are bound to this exchange with a table of arguments\n                    containing headers and values (optional). A special\n                    argument named \"x-match\" determines the matching algorithm,\n                    where `\"all\"` implies an `AND` (all pairs must match) and\n                    `\"any\"` implies `OR` (at least one pair must match).\n\n                    :attr:`arguments` is used to specify the arguments.\n\n\n                .. _`AMQP in 10 minutes: Part 4`:\n                    https://bit.ly/2rcICv5\n\n        channel (ChannelT): The channel the exchange is bound to (if bound).\n\n        durable (bool): Durable exchanges remain active when a server restarts.\n            Non-durable exchanges (transient exchanges) are purged when a\n            server restarts.  Default is :const:`True`.\n\n        auto_delete (bool): If set, the exchange is deleted when all queues\n            have finished using it. Default is :const:`False`.\n\n        delivery_mode (enum): The default delivery mode used for messages.\n            The value is an integer, or alias string.\n\n                * 1 or `\"transient\"`\n\n                    The message is transient. Which means it is stored in\n                    memory only, and is lost if the server dies or restarts.\n\n                * 2 or \"persistent\" (*default*)\n                    The message is persistent. Which means the message is\n                    stored both in-memory, and on disk, and therefore\n                    preserved if the server dies or restarts.\n\n            The default value is 2 (persistent).\n\n        arguments (Dict): Additional arguments to specify when the exchange\n            is declared.\n\n        no_declare (bool): Never declare this exchange\n            (:meth:`declare` does nothing).\n    ",
        "klass": "kombu.Exchange",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.abstract.MaybeChannelBound"
        ],
        "class_docstring": "A Queue declaration.\n\n    Arguments:\n        name (str): See :attr:`name`.\n        exchange (Exchange, str): See :attr:`exchange`.\n        routing_key (str): See :attr:`routing_key`.\n        channel (kombu.Connection, ChannelT): See :attr:`channel`.\n        durable (bool): See :attr:`durable`.\n        exclusive (bool): See :attr:`exclusive`.\n        auto_delete (bool): See :attr:`auto_delete`.\n        queue_arguments (Dict): See :attr:`queue_arguments`.\n        binding_arguments (Dict): See :attr:`binding_arguments`.\n        consumer_arguments (Dict): See :attr:`consumer_arguments`.\n        no_declare (bool): See :attr:`no_declare`.\n        on_declared (Callable): See :attr:`on_declared`.\n        expires (float): See :attr:`expires`.\n        message_ttl (float): See :attr:`message_ttl`.\n        max_length (int): See :attr:`max_length`.\n        max_length_bytes (int): See :attr:`max_length_bytes`.\n        max_priority (int): See :attr:`max_priority`.\n\n    Attributes:\n        name (str): Name of the queue.\n            Default is no name (default queue destination).\n\n        exchange (Exchange): The :class:`Exchange` the queue binds to.\n\n        routing_key (str): The routing key (if any), also called *binding key*.\n\n            The interpretation of the routing key depends on\n            the :attr:`Exchange.type`.\n\n            * direct exchange\n\n                Matches if the routing key property of the message and\n                the :attr:`routing_key` attribute are identical.\n\n            * fanout exchange\n\n                Always matches, even if the binding does not have a key.\n\n            * topic exchange\n\n                Matches the routing key property of the message by a primitive\n                pattern matching scheme. The message routing key then consists\n                of words separated by dots (`\".\"`, like domain names), and\n                two special characters are available; star (`\"*\"`) and hash\n                (`\"#\"`). The star matches any word, and the hash matches\n                zero or more words. For example `\"*.stock.#\"` matches the\n                routing keys `\"usd.stock\"` and `\"eur.stock.db\"` but not\n                `\"stock.nasdaq\"`.\n\n        channel (ChannelT): The channel the Queue is bound to (if bound).\n\n        durable (bool): Durable queues remain active when a server restarts.\n            Non-durable queues (transient queues) are purged if/when\n            a server restarts.\n            Note that durable queues do not necessarily hold persistent\n            messages, although it does not make sense to send\n            persistent messages to a transient queue.\n\n            Default is :const:`True`.\n\n        exclusive (bool): Exclusive queues may only be consumed from by the\n            current connection. Setting the 'exclusive' flag\n            always implies 'auto-delete'.\n\n            Default is :const:`False`.\n\n        auto_delete (bool): If set, the queue is deleted when all consumers\n            have finished using it. Last consumer can be canceled\n            either explicitly or because its channel is closed. If\n            there was no consumer ever on the queue, it won't be\n            deleted.\n\n        expires (float): Set the expiry time (in seconds) for when this\n            queue should expire.\n\n            The expiry time decides how long the queue can stay unused\n            before it's automatically deleted.\n            *Unused* means the queue has no consumers, the queue has not been\n            redeclared, and ``Queue.get`` has not been invoked for a duration\n            of at least the expiration period.\n\n            See https://www.rabbitmq.com/ttl.html#queue-ttl\n\n            **RabbitMQ extension**: Only available when using RabbitMQ.\n\n        message_ttl (float): Message time to live in seconds.\n\n            This setting controls how long messages can stay in the queue\n            unconsumed. If the expiry time passes before a message consumer\n            has received the message, the message is deleted and no consumer\n            will see the message.\n\n            See https://www.rabbitmq.com/ttl.html#per-queue-message-ttl\n\n            **RabbitMQ extension**: Only available when using RabbitMQ.\n\n        max_length (int): Set the maximum number of messages that the\n            queue can hold.\n\n            If the number of messages in the queue size exceeds this limit,\n            new messages will be dropped (or dead-lettered if a dead letter\n            exchange is active).\n\n            See https://www.rabbitmq.com/maxlength.html\n\n            **RabbitMQ extension**: Only available when using RabbitMQ.\n\n        max_length_bytes (int): Set the max size (in bytes) for the total\n            of messages in the queue.\n\n            If the total size of all the messages in the queue exceeds this\n            limit, new messages will be dropped (or dead-lettered if a dead\n            letter exchange is active).\n\n            **RabbitMQ extension**: Only available when using RabbitMQ.\n\n        max_priority (int): Set the highest priority number for this queue.\n\n            For example if the value is 10, then messages can delivered to\n            this queue can have a ``priority`` value between 0 and 10,\n            where 10 is the highest priority.\n\n            RabbitMQ queues without a max priority set will ignore\n            the priority field in the message, so if you want priorities\n            you need to set the max priority field to declare the queue\n            as a priority queue.\n\n            **RabbitMQ extension**: Only available when using RabbitMQ.\n\n        queue_arguments (Dict): Additional arguments used when declaring\n            the queue.  Can be used to to set the arguments value\n            for RabbitMQ/AMQP's ``queue.declare``.\n\n        binding_arguments (Dict): Additional arguments used when binding\n            the queue.  Can be used to to set the arguments value\n            for RabbitMQ/AMQP's ``queue.declare``.\n\n        consumer_arguments (Dict): Additional arguments used when consuming\n            from this queue.  Can be used to to set the arguments value\n            for RabbitMQ/AMQP's ``basic.consume``.\n\n        alias (str): Unused in Kombu, but applications can take advantage\n            of this,  for example to give alternate names to queues with\n            automatically generated queue names.\n\n        on_declared (Callable): Optional callback to be applied when the\n            queue has been declared (the ``queue_declare`` operation is\n            complete).  This must be a function with a signature that\n            accepts at least 3 positional arguments:\n            ``(name, messages, consumers)``.\n\n        no_declare (bool): Never declare this queue, nor related\n            entities (:meth:`declare` does nothing).\n    ",
        "klass": "kombu.Queue",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.abstract.Object"
        ],
        "class_docstring": "Represents a queue or exchange binding.\n\n    Arguments:\n        exchange (Exchange): Exchange to bind to.\n        routing_key (str): Routing key used as binding key.\n        arguments (Dict): Arguments for bind operation.\n        unbind_arguments (Dict): Arguments for unbind operation.\n    ",
        "klass": "kombu.binding",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.asynchronous.aws.connection.AsyncConnection"
        ],
        "class_docstring": "Async AWS Query Connection.",
        "klass": "kombu.asynchronous.aws.connection.AsyncAWSQueryConnection",
        "module": "kombu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Async HTTP Connection.",
        "klass": "kombu.asynchronous.aws.connection.AsyncHTTPSConnection",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.messaging.Consumer"
        ],
        "class_docstring": "Carrot compatible consumer.",
        "klass": "kombu.compat.Consumer",
        "module": "kombu"
    },
    {
        "base_classes": [
            "kombu.messaging.Producer"
        ],
        "class_docstring": "Carrot compatible producer.",
        "klass": "kombu.compat.Publisher",
        "module": "kombu"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    ",
        "klass": "kombu.five.Queue",
        "module": "queue"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for transports.",
        "klass": "kombu.transport.base.Transport",
        "module": "kombu"
    },
    {
        "base_classes": [
            "openfermion.ops._symbolic_operator.SymbolicOperator"
        ],
        "class_docstring": "\n    A sum of terms acting on qubits, e.g., 0.5 * 'X0 X5' + 0.3 * 'Z1 Z2'.\n\n    A term is an operator acting on n qubits and can be represented as:\n\n    coefficient * local_operator[0] x ... x local_operator[n-1]\n\n    where x is the tensor product. A local operator is a Pauli operator\n    ('I', 'X', 'Y', or 'Z') which acts on one qubit. In math notation a term\n    is, for example, 0.5 * 'X0 X5', which means that a Pauli X operator acts\n    on qubit 0 and 5, while the identity operator acts on all other qubits.\n\n    A QubitOperator represents a sum of terms acting on qubits and overloads\n    operations for easy manipulation of these objects by the user.\n\n    Note for a QubitOperator to be a Hamiltonian which is a hermitian\n    operator, the coefficients of all terms must be real.\n\n    .. code-block:: python\n\n        hamiltonian = 0.5 * QubitOperator('X0 X5') + 0.3 * QubitOperator('Z0')\n\n    QubitOperator is a subclass of SymbolicOperator. Importantly, it has\n    attributes set as follows::\n\n        actions = ('X', 'Y', 'Z')\n        action_strings = ('X', 'Y', 'Z')\n        action_before_index = True\n        different_indices_commute = True\n\n    See the documentation of SymbolicOperator for more details.\n\n    Example:\n        .. code-block:: python\n\n            ham = ((QubitOperator('X0 Y3', 0.5)\n                    + 0.6 * QubitOperator('X0 Y3')))\n            # Equivalently\n            ham2 = QubitOperator('X0 Y3', 0.5)\n            ham2 += 0.6 * QubitOperator('X0 Y3')\n\n    Note:\n        Adding QubitOperators is faster using += (as this\n        is done by in-place addition). Specifying the coefficient\n        during initialization is faster than multiplying a QubitOperator\n        with a scalar.\n    ",
        "klass": "openfermion.ops._qubit_operator.QubitOperator",
        "module": "openfermion"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for storing molecule data from a fixed basis set at a fixed\n    geometry that is obtained from classical electronic structure\n    packages. Not every field is filled in every calculation. All data\n    that can (for some instance) exceed 10 MB should be saved\n    separately. Data saved in HDF5 format.\n\n    Attributes:\n        geometry: A list of tuples giving the coordinates of each atom. An\n            example is [('H', (0, 0, 0)), ('H', (0, 0, 0.7414))]. Distances\n            in angstrom. Use atomic symbols to specify atoms.\n        basis: A string giving the basis set. An example is 'cc-pvtz'.\n        charge: An integer giving the total molecular charge. Defaults to 0.\n        multiplicity: An integer giving the spin multiplicity.\n        description: An optional string giving a description. As an example,\n            for dimers a likely description is the bond length (e.g. 0.7414).\n        name: A string giving a characteristic name for the instance.\n        filename: The name of the file where the molecule data is saved.\n        n_atoms: Integer giving the number of atoms in the molecule.\n        n_electrons: Integer giving the number of electrons in the molecule.\n        atoms: List of the atoms in molecule sorted by atomic number.\n        protons: List of atomic charges in molecule sorted by atomic number.\n        hf_energy: Energy from open or closed shell Hartree-Fock.\n        nuclear_repulsion: Energy from nuclei-nuclei interaction.\n        canonical_orbitals: numpy array giving canonical orbital coefficients.\n        n_orbitals: Integer giving total number of spatial orbitals.\n        n_qubits: Integer giving total number of qubits that would be needed.\n        orbital_energies: Numpy array giving the canonical orbital energies.\n        fock_matrix: Numpy array giving the Fock matrix.\n        overlap_integrals: Numpy array of AO overlap integrals\n        one_body_integrals: Numpy array of one-electron integrals\n        two_body_integrals: Numpy array of two-electron integrals\n        mp2_energy: Energy from MP2 perturbation theory.\n        cisd_energy: Energy from configuration interaction singles + doubles.\n        cisd_one_rdm: Numpy array giving 1-RDM from CISD calculation.\n        cisd_two_rdm: Numpy array giving 2-RDM from CISD calculation.\n        fci_energy: Exact energy of molecule within given basis.\n        fci_one_rdm: Numpy array giving 1-RDM from FCI calculation.\n        fci_two_rdm: Numpy array giving 2-RDM from FCI calculation.\n        ccsd_energy: Energy from coupled cluster singles + doubles.\n        ccsd_single_amps: Numpy array holding single amplitudes\n        ccsd_double_amps: Numpy array holding double amplitudes\n        general_calculations: A dictionary storing general calculation results\n            for this system annotated by the key.\n    ",
        "klass": "openfermion.hamiltonians.MolecularData",
        "module": "openfermion"
    },
    {
        "base_classes": [
            "openfermion.ops._symbolic_operator.SymbolicOperator"
        ],
        "class_docstring": "\n    A sum of terms acting on qubits, e.g., 0.5 * 'X0 X5' + 0.3 * 'Z1 Z2'.\n\n    A term is an operator acting on n qubits and can be represented as:\n\n    coefficient * local_operator[0] x ... x local_operator[n-1]\n\n    where x is the tensor product. A local operator is a Pauli operator\n    ('I', 'X', 'Y', or 'Z') which acts on one qubit. In math notation a term\n    is, for example, 0.5 * 'X0 X5', which means that a Pauli X operator acts\n    on qubit 0 and 5, while the identity operator acts on all other qubits.\n\n    A QubitOperator represents a sum of terms acting on qubits and overloads\n    operations for easy manipulation of these objects by the user.\n\n    Note for a QubitOperator to be a Hamiltonian which is a hermitian\n    operator, the coefficients of all terms must be real.\n\n    .. code-block:: python\n\n        hamiltonian = 0.5 * QubitOperator('X0 X5') + 0.3 * QubitOperator('Z0')\n\n    QubitOperator is a subclass of SymbolicOperator. Importantly, it has\n    attributes set as follows::\n\n        actions = ('X', 'Y', 'Z')\n        action_strings = ('X', 'Y', 'Z')\n        action_before_index = True\n        different_indices_commute = True\n\n    See the documentation of SymbolicOperator for more details.\n\n    Example:\n        .. code-block:: python\n\n            ham = ((QubitOperator('X0 Y3', 0.5)\n                    + 0.6 * QubitOperator('X0 Y3')))\n            # Equivalently\n            ham2 = QubitOperator('X0 Y3', 0.5)\n            ham2 += 0.6 * QubitOperator('X0 Y3')\n\n    Note:\n        Adding QubitOperators is faster using += (as this\n        is done by in-place addition). Specifying the coefficient\n        during initialization is faster than multiplying a QubitOperator\n        with a scalar.\n    ",
        "klass": "openfermion.ops.QubitOperator",
        "module": "openfermion"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The Environment provides global singleton instances of various objects.\n\n    FormulaManager and the TypeChecker are among the most commonly used ones.\n\n    Subclasses of Environment should take care of adjusting the list\n    of classes for the different services, by changing the class\n    attributes.\n    ",
        "klass": "pysmt.environment.Environment",
        "module": "pysmt"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Handles and stores (key,value) annotations for formulae",
        "klass": "pysmt.smtlib.annotations.Annotations",
        "module": "pysmt"
    },
    {
        "base_classes": [
            "pysmt.walkers.generic.Walker"
        ],
        "class_docstring": "DagWalker treats the formula as a DAG and performs memoization of the\n    intermediate results.\n\n    This should be used when the result of applying the function to a\n    formula is always the same, independently of where the formula has\n    been found; examples include substitution and solving.\n\n    Due to memoization, a few more things need to be taken into\n    account when using the DagWalker.\n\n    :func _get_key needs to be defined if additional arguments via\n    keywords need to be shared. This function should return the key to\n    be used in memoization. See substituter for an example.\n    ",
        "klass": "pysmt.walkers.DagWalker",
        "module": "pysmt"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.BMPString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.IA5String",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.PrintableString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.TeletexString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.UTF8String",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.char.AbstractCharacterString"
        ],
        "class_docstring": "Creates |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`,\n    its objects are immutable and duck-type Python 2 :class:`str` or Python 3\n    :class:`bytes`. When used in octet-stream context, |ASN.1| type assumes\n    \"|encoding|\" encoding.\n\n    Keyword Args\n    ------------\n    value: :class:`unicode`, :class:`str`, :class:`bytes` or |ASN.1| object\n        :class:`unicode` object (Python 2) or :class:`str` (Python 3),\n        alternatively :class:`str` (Python 2) or :class:`bytes` (Python 3)\n        representing octet-stream of serialised unicode string\n        (note `encoding` parameter) or |ASN.1| class instance.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    encoding: :py:class:`str`\n        Unicode codec ID to encode/decode :class:`unicode` (Python 2) or\n        :class:`str` (Python 3) the payload when |ASN.1| object is used\n        in octet-stream context.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n    ",
        "klass": "pyasn1.type.char.UniversalString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.base.SimpleAsn1Type"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type both Python :class:`tuple` (as a tuple\n    of bits) and :class:`int` objects.\n\n    Keyword Args\n    ------------\n    value: :class:`int`, :class:`str` or |ASN.1| object\n        Python :class:`int` or :class:`str` literal representing binary\n        or hexadecimal number or sequence of integer bits or |ASN.1| object.\n        If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    namedValues: :py:class:`~pyasn1.type.namedval.NamedValues`\n        Object representing non-default symbolic aliases for numbers\n\n    binValue: :py:class:`str`\n        Binary string initializer to use instead of the *value*.\n        Example: '10110011'.\n\n    hexValue: :py:class:`str`\n        Hexadecimal string initializer to use instead of the *value*.\n        Example: 'DEADBEEF'.\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        class Rights(BitString):\n            '''\n            ASN.1 specification:\n\n            Rights ::= BIT STRING { user-read(0), user-write(1),\n                                    group-read(2), group-write(3),\n                                    other-read(4), other-write(5) }\n\n            group1 Rights ::= { group-read, group-write }\n            group2 Rights ::= '0011'B\n            group3 Rights ::= '3'H\n            '''\n            namedValues = NamedValues(\n                ('user-read', 0), ('user-write', 1),\n                ('group-read', 2), ('group-write', 3),\n                ('other-read', 4), ('other-write', 5)\n            )\n\n        group1 = Rights(('group-read', 'group-write'))\n        group2 = Rights('0011')\n        group3 = Rights(0x3)\n    ",
        "klass": "pyasn1.type.univ.BitString",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.univ.Integer"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.SimpleAsn1Type`, its\n    objects are immutable and duck-type Python :class:`int` objects.\n\n    Keyword Args\n    ------------\n    value: :class:`int`, :class:`str` or |ASN.1| object\n        Python :class:`int` or :class:`str` literal or |ASN.1| class\n        instance. If `value` is not given, schema object will be created.\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s).Constraints\n        verification for |ASN.1| type occurs automatically on object\n        instantiation.\n\n    namedValues: :py:class:`~pyasn1.type.namedval.NamedValues`\n        Object representing non-default symbolic aliases for numbers\n\n    Raises\n    ------\n    ~pyasn1.error.ValueConstraintError, ~pyasn1.error.PyAsn1Error\n        On constraint violation or bad initializer.\n\n    Examples\n    --------\n    .. code-block:: python\n\n        class RoundResult(Boolean):\n            '''\n            ASN.1 specification:\n\n            RoundResult ::= BOOLEAN\n\n            ok RoundResult ::= TRUE\n            ko RoundResult ::= FALSE\n            '''\n        ok = RoundResult(True)\n        ko = RoundResult(False)\n    ",
        "klass": "pyasn1.type.univ.Boolean",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.univ.Set"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.ConstructedAsn1Type`,\n    its objects are mutable and duck-type Python :class:`list` objects.\n\n    Keyword Args\n    ------------\n    componentType: :py:class:`~pyasn1.type.namedtype.NamedType`\n        Object holding named ASN.1 types allowed within this collection\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s).  Constraints\n        verification for |ASN.1| type can only occur on explicit\n        `.isInconsistent` call.\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        class Afters(Choice):\n            '''\n            ASN.1 specification:\n\n            Afters ::= CHOICE {\n                cheese  [0] IA5String,\n                dessert [1] IA5String\n            }\n            '''\n            componentType = NamedTypes(\n                NamedType('cheese', IA5String().subtype(\n                    implicitTag=Tag(tagClassContext, tagFormatSimple, 0)\n                ),\n                NamedType('dessert', IA5String().subtype(\n                    implicitTag=Tag(tagClassContext, tagFormatSimple, 1)\n                )\n            )\n\n        afters = Afters()\n        afters['cheese'] = 'Mascarpone'\n    ",
        "klass": "pyasn1.type.univ.Choice",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.univ.SequenceAndSetBase"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.ConstructedAsn1Type`,\n    its objects are mutable and duck-type Python :class:`dict` objects.\n\n    Keyword Args\n    ------------\n    componentType: :py:class:`~pyasn1.type.namedtype.NamedType`\n        Object holding named ASN.1 types allowed within this collection\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s).  Constraints\n        verification for |ASN.1| type can only occur on explicit\n        `.isInconsistent` call.\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        class Description(Sequence):  #  Set is similar\n            '''\n            ASN.1 specification:\n\n            Description ::= SEQUENCE {\n                surname    IA5String,\n                first-name IA5String OPTIONAL,\n                age        INTEGER DEFAULT 40\n            }\n            '''\n            componentType = NamedTypes(\n                NamedType('surname', IA5String()),\n                OptionalNamedType('first-name', IA5String()),\n                DefaultedNamedType('age', Integer(40))\n            )\n\n        descr = Description()\n        descr['surname'] = 'Smith'\n        descr['first-name'] = 'John'\n    ",
        "klass": "pyasn1.type.univ.Sequence",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "pyasn1.type.univ.SequenceOfAndSetOfBase"
        ],
        "class_docstring": "Create |ASN.1| schema or value object.\n\n    |ASN.1| class is based on :class:`~pyasn1.type.base.ConstructedAsn1Type`,\n    its objects are mutable and duck-type Python :class:`list` objects.\n\n    Keyword Args\n    ------------\n    componentType : :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n        A pyasn1 object representing ASN.1 type allowed within |ASN.1| type\n\n    tagSet: :py:class:`~pyasn1.type.tag.TagSet`\n        Object representing non-default ASN.1 tag(s)\n\n    subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n        Object representing non-default ASN.1 subtype constraint(s). Constraints\n        verification for |ASN.1| type can only occur on explicit\n        `.isInconsistent` call.\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        class LotteryDraw(SequenceOf):  #  SetOf is similar\n            '''\n            ASN.1 specification:\n\n            LotteryDraw ::= SEQUENCE OF INTEGER\n            '''\n            componentType = Integer()\n\n        lotteryDraw = LotteryDraw()\n        lotteryDraw.extend([123, 456, 789])\n    ",
        "klass": "pyasn1.type.univ.SequenceOf",
        "module": "pyasn1"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes 'hyper-param=value' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description='Train my model.')\n  parser.add_argument('--hparams', type=str,\n                      help='Comma separated list of \"name=value\" pairs.')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=['relu', 'tanh'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then 'hparams' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> ['relu', 'tanh']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json('{\"learning_rate\": 0.3, \"activations\": \"relu\"}')\n  ```\n  ",
        "klass": "tensor2tensor.utils.hparam.HParams",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "netaddr.ip.BaseIP"
        ],
        "class_docstring": "\n    An individual IPv4 or IPv6 address without a net mask or subnet prefix.\n\n    To support these and other network based operations, see `IPNetwork`.\n\n    ",
        "klass": "netaddr.IPAddress",
        "module": "netaddr"
    },
    {
        "base_classes": [
            "netaddr.ip.BaseIP",
            "netaddr.ip.IPListMixin"
        ],
        "class_docstring": "\n    An IPv4 or IPv6 network or subnet.\n\n    A combination of an IP address and a network mask.\n\n    Accepts CIDR and several related variants :\n\n    a) Standard CIDR::\n\n        x.x.x.x/y -> 192.0.2.0/24\n        x::/y -> fe80::/10\n\n    b) Hybrid CIDR format (netmask address instead of prefix), where 'y'        address represent a valid netmask::\n\n        x.x.x.x/y.y.y.y -> 192.0.2.0/255.255.255.0\n        x::/y:: -> fe80::/ffc0::\n\n    c) ACL hybrid CIDR format (hostmask address instead of prefix like        Cisco's ACL bitmasks), where 'y' address represent a valid netmask::\n\n        x.x.x.x/y.y.y.y -> 192.0.2.0/0.0.0.255\n        x::/y:: -> fe80::/3f:ffff:ffff:ffff:ffff:ffff:ffff:ffff\n\n    d) Abbreviated CIDR format (as of netaddr 0.7.x this requires the        optional constructor argument ``implicit_prefix=True``)::\n\n        x       -> 192\n        x/y     -> 10/8\n        x.x/y   -> 192.168/16\n        x.x.x/y -> 192.168.0/24\n\n        which are equivalent to::\n\n        x.0.0.0/y   -> 192.0.0.0/24\n        x.0.0.0/y   -> 10.0.0.0/8\n        x.x.0.0/y   -> 192.168.0.0/16\n        x.x.x.0/y   -> 192.168.0.0/24\n\n    ",
        "klass": "netaddr.IPNetwork",
        "module": "netaddr"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Represents an unordered collection (set) of unique IP addresses and\n    subnets.\n\n    ",
        "klass": "netaddr.IPSet",
        "module": "netaddr"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "The main class\n    ",
        "klass": "pyquery.PyQuery",
        "module": "pyquery"
    },
    {
        "base_classes": [
            "pandas.core.arrays.base.ExtensionArray",
            "pandas.core.base.PandasObject"
        ],
        "class_docstring": "\n    Represent a categorical variable in classic R / S-plus fashion.\n\n    `Categoricals` can only take on only a limited, and usually fixed, number\n    of possible values (`categories`). In contrast to statistical categorical\n    variables, a `Categorical` might have an order, but numerical operations\n    (additions, divisions, ...) are not possible.\n\n    All values of the `Categorical` are either in `categories` or `np.nan`.\n    Assigning values outside of `categories` will raise a `ValueError`. Order\n    is defined by the order of the `categories`, not lexical order of the\n    values.\n\n    Parameters\n    ----------\n    values : list-like\n        The values of the categorical. If categories are given, values not in\n        categories will be replaced with NaN.\n    categories : Index-like (unique), optional\n        The unique categories for this categorical. If not given, the\n        categories are assumed to be the unique values of `values` (sorted, if\n        possible, otherwise in the order in which they appear).\n    ordered : bool, default False\n        Whether or not this categorical is treated as a ordered categorical.\n        If True, the resulting categorical will be ordered.\n        An ordered categorical respects, when sorted, the order of its\n        `categories` attribute (which in turn is the `categories` argument, if\n        provided).\n    dtype : CategoricalDtype\n        An instance of ``CategoricalDtype`` to use for this categorical\n\n        .. versionadded:: 0.21.0\n\n    Attributes\n    ----------\n    categories : Index\n        The categories of this categorical\n    codes : ndarray\n        The codes (integer positions, which point to the categories) of this\n        categorical, read only.\n    ordered : bool\n        Whether or not this Categorical is ordered.\n    dtype : CategoricalDtype\n        The instance of ``CategoricalDtype`` storing the ``categories``\n        and ``ordered``.\n\n        .. versionadded:: 0.21.0\n\n    Methods\n    -------\n    from_codes\n    __array__\n\n    Raises\n    ------\n    ValueError\n        If the categories do not validate.\n    TypeError\n        If an explicit ``ordered=True`` is given but no `categories` and the\n        `values` are not sortable.\n\n    See Also\n    --------\n    api.types.CategoricalDtype : Type for categorical data.\n    CategoricalIndex : An Index with an underlying ``Categorical``.\n\n    Notes\n    -----\n    See the `user guide\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_\n    for more.\n\n    Examples\n    --------\n    >>> pd.Categorical([1, 2, 3, 1, 2, 3])\n    [1, 2, 3, 1, 2, 3]\n    Categories (3, int64): [1, 2, 3]\n\n    >>> pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'])\n    [a, b, c, a, b, c]\n    Categories (3, object): [a, b, c]\n\n    Ordered `Categoricals` can be sorted according to the custom order\n    of the categories and can have a min and max value.\n\n    >>> c = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'], ordered=True,\n    ...                    categories=['c', 'b', 'a'])\n    >>> c\n    [a, b, c, a, b, c]\n    Categories (3, object): [c < b < a]\n    >>> c.min()\n    'c'\n    ",
        "klass": "pandas.Categorical",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.base.Index",
            "pandas.core.accessor.PandasDelegate"
        ],
        "class_docstring": "\n    Index based on an underlying :class:`Categorical`.\n\n    CategoricalIndex, like Categorical, can only take on a limited,\n    and usually fixed, number of possible values (`categories`). Also,\n    like Categorical, it might have an order, but numerical operations\n    (additions, divisions, ...) are not possible.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n        The values of the categorical. If `categories` are given, values not in\n        `categories` will be replaced with NaN.\n    categories : index-like, optional\n        The categories for the categorical. Items need to be unique.\n        If the categories are not given here (and also not in `dtype`), they\n        will be inferred from the `data`.\n    ordered : bool, optional\n        Whether or not this categorical is treated as an ordered\n        categorical. If not given here or in `dtype`, the resulting\n        categorical will be unordered.\n    dtype : CategoricalDtype or the string \"category\", optional\n        If :class:`CategoricalDtype`, cannot be used together with\n        `categories` or `ordered`.\n\n        .. versionadded:: 0.21.0\n    copy : bool, default False\n        Make a copy of input ndarray.\n    name : object, optional\n        Name to be stored in the index.\n\n    Attributes\n    ----------\n    codes\n    categories\n    ordered\n\n    Methods\n    -------\n    rename_categories\n    reorder_categories\n    add_categories\n    remove_categories\n    remove_unused_categories\n    set_categories\n    as_ordered\n    as_unordered\n    map\n\n    Raises\n    ------\n    ValueError\n        If the categories do not validate.\n    TypeError\n        If an explicit ``ordered=True`` is given but no `categories` and the\n        `values` are not sortable.\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Categorical : A categorical array.\n    CategoricalDtype : Type for categorical data.\n\n    Notes\n    -----\n    See the `user guide\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#categoricalindex>`_\n    for more.\n\n    Examples\n    --------\n    >>> pd.CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'])\n    CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'], categories=['a', 'b', 'c'], ordered=False, dtype='category')  # noqa\n\n    ``CategoricalIndex`` can also be instantiated from a ``Categorical``:\n\n    >>> c = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'])\n    >>> pd.CategoricalIndex(c)\n    CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'], categories=['a', 'b', 'c'], ordered=False, dtype='category')  # noqa\n\n    Ordered ``CategoricalIndex`` can have a min and max value.\n\n    >>> ci = pd.CategoricalIndex(['a','b','c','a','b','c'], ordered=True,\n    ...                          categories=['c', 'b', 'a'])\n    >>> ci\n    CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'], categories=['c', 'b', 'a'], ordered=True, dtype='category')  # noqa\n    >>> ci.min()\n    'c'\n    ",
        "klass": "pandas.CategoricalIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.generic.NDFrame"
        ],
        "class_docstring": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    ",
        "klass": "pandas.DataFrame",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.datetimelike.DatetimeIndexOpsMixin",
            "pandas.core.indexes.numeric.Int64Index",
            "pandas.core.indexes.datetimes.DatetimeDelegateMixin"
        ],
        "class_docstring": "\n    Immutable ndarray of datetime64 data, represented internally as int64, and\n    which can be boxed to Timestamp objects that are subclasses of datetime and\n    carry metadata such as frequency information.\n\n    Parameters\n    ----------\n    data  : array-like (1-dimensional), optional\n        Optional datetime-like data to construct index with\n    copy  : bool\n        Make a copy of input ndarray\n    freq : string or pandas offset object, optional\n        One of pandas date offset strings or corresponding objects. The string\n        'infer' can be passed in order to set the frequency of the index as the\n        inferred frequency upon creation\n\n    start : starting value, datetime-like, optional\n        If data is None, start is used as the start point in generating regular\n        timestamp data.\n\n        .. deprecated:: 0.24.0\n\n    periods  : int, optional, > 0\n        Number of periods to generate, if generating index. Takes precedence\n        over end argument\n\n        .. deprecated:: 0.24.0\n\n    end : end time, datetime-like, optional\n        If periods is none, generated index will extend to first conforming\n        time on or just past end argument\n\n        .. deprecated:: 0.24.0\n\n    closed : string or None, default None\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None)\n\n        .. deprecated:: 0.24. 0\n\n    tz : pytz.timezone or dateutil.tz.tzfile\n    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n        When clocks moved backward due to DST, ambiguous times may arise.\n        For example in Central European Time (UTC+01), when going from 03:00\n        DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC\n        and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter\n        dictates how ambiguous times should be handled.\n\n        - 'infer' will attempt to infer fall dst-transition hours based on\n          order\n        - bool-ndarray where True signifies a DST time, False signifies a\n          non-DST time (note that this flag is only applicable for ambiguous\n          times)\n        - 'NaT' will return NaT where there are ambiguous times\n        - 'raise' will raise an AmbiguousTimeError if there are ambiguous times\n    name : object\n        Name to be stored in the index\n    dayfirst : bool, default False\n        If True, parse dates in `data` with the day first order\n    yearfirst : bool, default False\n        If True parse dates in `data` with the year first order\n\n    Attributes\n    ----------\n    year\n    month\n    day\n    hour\n    minute\n    second\n    microsecond\n    nanosecond\n    date\n    time\n    timetz\n    dayofyear\n    weekofyear\n    week\n    dayofweek\n    weekday\n    quarter\n    tz\n    freq\n    freqstr\n    is_month_start\n    is_month_end\n    is_quarter_start\n    is_quarter_end\n    is_year_start\n    is_year_end\n    is_leap_year\n    inferred_freq\n\n    Methods\n    -------\n    normalize\n    strftime\n    snap\n    tz_convert\n    tz_localize\n    round\n    floor\n    ceil\n    to_period\n    to_perioddelta\n    to_pydatetime\n    to_series\n    to_frame\n    month_name\n    day_name\n    mean\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    TimedeltaIndex : Index of timedelta64 data.\n    PeriodIndex : Index of Period data.\n    to_datetime : Convert argument to datetime.\n    date_range : Create a fixed-frequency DatetimeIndex.\n\n    Notes\n    -----\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n    Creating a DatetimeIndex based on `start`, `periods`, and `end` has\n    been deprecated in favor of :func:`date_range`.\n    ",
        "klass": "pandas.DatetimeIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Class for parsing tabular excel sheets into DataFrame objects.\n    Uses xlrd. See read_excel for more documentation\n\n    Parameters\n    ----------\n    io : string, path object (pathlib.Path or py._path.local.LocalPath),\n        file-like object or xlrd workbook\n        If a string or path object, expected to be a path to xls or xlsx file.\n    engine : string, default None\n        If io is not a buffer or path, this must be set to identify io.\n        Acceptable values are None or ``xlrd``.\n    ",
        "klass": "pandas.ExcelFile",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Class for writing DataFrame objects into excel sheets, default is to use\n    xlwt for xls, openpyxl for xlsx.  See DataFrame.to_excel for typical usage.\n\n    Parameters\n    ----------\n    path : string\n        Path to xls or xlsx file.\n    engine : string (optional)\n        Engine to use for writing. If None, defaults to\n        ``io.excel.<extension>.writer``.  NOTE: can only be passed as a keyword\n        argument.\n    date_format : string, default None\n        Format string for dates written into Excel files (e.g. 'YYYY-MM-DD')\n    datetime_format : string, default None\n        Format string for datetime objects written into Excel files\n        (e.g. 'YYYY-MM-DD HH:MM:SS')\n    mode : {'w', 'a'}, default 'w'\n        File mode to use (write or append).\n\n        .. versionadded:: 0.24.0\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    Notes\n    -----\n    None of the methods and properties are considered public.\n\n    For compatibility with CSV writers, ExcelWriter serializes lists\n    and dicts to strings before writing.\n\n    Examples\n    --------\n    Default usage:\n\n    >>> with ExcelWriter('path_to_file.xlsx') as writer:\n    ...     df.to_excel(writer)\n\n    To write to separate sheets in a single file:\n\n    >>> with ExcelWriter('path_to_file.xlsx') as writer:\n    ...     df1.to_excel(writer, sheet_name='Sheet1')\n    ...     df2.to_excel(writer, sheet_name='Sheet2')\n\n    You can set the date format or datetime format:\n\n    >>> with ExcelWriter('path_to_file.xlsx',\n                          date_format='YYYY-MM-DD',\n                          datetime_format='YYYY-MM-DD HH:MM:SS') as writer:\n    ...     df.to_excel(writer)\n\n    You can also append to an existing Excel file:\n\n    >>> with ExcelWriter('path_to_file.xlsx', mode='a') as writer:\n    ...     df.to_excel(writer, sheet_name='Sheet3')\n    ",
        "klass": "pandas.ExcelWriter",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Dict-like IO interface for storing pandas objects in PyTables\n    either Fixed or Table format.\n\n    Parameters\n    ----------\n    path : string\n        File path to HDF5 file\n    mode : {'a', 'w', 'r', 'r+'}, default 'a'\n\n        ``'r'``\n            Read-only; no data can be modified.\n        ``'w'``\n            Write; a new file is created (an existing file with the same\n            name would be deleted).\n        ``'a'``\n            Append; an existing file is opened for reading and writing,\n            and if the file does not exist it is created.\n        ``'r+'``\n            It is similar to ``'a'``, but the file must already exist.\n    complevel : int, 0-9, default None\n            Specifies a compression level for data.\n            A value of 0 or None disables compression.\n    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n            Specifies the compression library to be used.\n            As of v0.20.2 these additional compressors for Blosc are supported\n            (default if no compressor specified: 'blosc:blosclz'):\n            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n             'blosc:zlib', 'blosc:zstd'}.\n            Specifying a compression library which is not available issues\n            a ValueError.\n    fletcher32 : bool, default False\n            If applying compression use the fletcher32 checksum\n\n    Examples\n    --------\n    >>> bar = pd.DataFrame(np.random.randn(10, 4))\n    >>> store = pd.HDFStore('test.h5')\n    >>> store['foo'] = bar   # write to HDF5\n    >>> bar = store['foo']   # retrieve\n    >>> store.close()\n    ",
        "klass": "pandas.io.pytables.HDFStore",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Dict-like IO interface for storing pandas objects in PyTables\n    either Fixed or Table format.\n\n    Parameters\n    ----------\n    path : string\n        File path to HDF5 file\n    mode : {'a', 'w', 'r', 'r+'}, default 'a'\n\n        ``'r'``\n            Read-only; no data can be modified.\n        ``'w'``\n            Write; a new file is created (an existing file with the same\n            name would be deleted).\n        ``'a'``\n            Append; an existing file is opened for reading and writing,\n            and if the file does not exist it is created.\n        ``'r+'``\n            It is similar to ``'a'``, but the file must already exist.\n    complevel : int, 0-9, default None\n            Specifies a compression level for data.\n            A value of 0 or None disables compression.\n    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n            Specifies the compression library to be used.\n            As of v0.20.2 these additional compressors for Blosc are supported\n            (default if no compressor specified: 'blosc:blosclz'):\n            {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n             'blosc:zlib', 'blosc:zstd'}.\n            Specifying a compression library which is not available issues\n            a ValueError.\n    fletcher32 : bool, default False\n            If applying compression use the fletcher32 checksum\n\n    Examples\n    --------\n    >>> bar = pd.DataFrame(np.random.randn(10, 4))\n    >>> store = pd.HDFStore('test.h5')\n    >>> store['foo'] = bar   # write to HDF5\n    >>> bar = store['foo']   # retrieve\n    >>> store.close()\n    ",
        "klass": "pandas.HDFStore",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.base.IndexOpsMixin",
            "pandas.core.base.PandasObject"
        ],
        "class_docstring": "\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    ",
        "klass": "pandas.Index",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.numeric.IntegerIndex"
        ],
        "class_docstring": "\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects. Int64Index is a special case\n    of `Index` with purely integer labels. \n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: int64)\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects.\n",
        "klass": "pandas.Int64Index",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas._libs.interval.IntervalMixin",
            "pandas.core.indexes.base.Index"
        ],
        "class_docstring": "\nImmutable index of intervals that are closed on the same side.\n\n.. versionadded:: 0.20.0\n\nParameters\n----------\ndata : array-like (1-dimensional)\n    Array-like containing Interval objects from which to build the\n    IntervalIndex.\nclosed : {'left', 'right', 'both', 'neither'}, default 'right'\n    Whether the intervals are closed on the left-side, right-side, both or\n    neither.\ndtype : dtype or None, default None\n    If None, dtype will be inferred.\n\n    .. versionadded:: 0.23.0\ncopy : bool, default False\n    Copy the input data.\nname : object, optional\n     Name to be stored in the index.\nverify_integrity : bool, default True\n    Verify that the IntervalIndex is valid.\n\nAttributes\n----------\nleft\nright\nclosed\nmid\nlength\nis_empty\nis_non_overlapping_monotonic\nis_overlapping\nvalues\n\nMethods\n-------\nfrom_arrays\nfrom_tuples\nfrom_breaks\ncontains\noverlaps\nset_closed\nto_tuples\n\nSee Also\n--------\nIndex : The base pandas Index type.\nInterval : A bounded slice-like interval; the elements of an IntervalIndex.\ninterval_range : Function to create a fixed frequency IntervalIndex.\ncut : Bin values into discrete Intervals.\nqcut : Bin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n-----\nSee the `user guide\n<http://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#intervalindex>`_\nfor more.\n\nExamples\n--------\nA new ``IntervalIndex`` is typically constructed using\n:func:`interval_range`:\n\n>>> pd.interval_range(start=0, end=5)\nIntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],\n              closed='right',\n              dtype='interval[int64]')\n\nIt may also be constructed using one of the constructor\nmethods: :meth:`IntervalIndex.from_arrays`,\n:meth:`IntervalIndex.from_breaks`, and :meth:`IntervalIndex.from_tuples`.\n\nSee further examples in the doc strings of ``interval_range`` and the\nmentioned constructor methods.\n",
        "klass": "pandas.IntervalIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.base.Index"
        ],
        "class_docstring": "\n    A multi-level, or hierarchical, index object for pandas objects.\n\n    Parameters\n    ----------\n    levels : sequence of arrays\n        The unique labels for each level.\n    codes : sequence of arrays\n        Integers for each level designating which label at each location.\n\n        .. versionadded:: 0.24.0\n    labels : sequence of arrays\n        Integers for each level designating which label at each location.\n\n        .. deprecated:: 0.24.0\n            Use ``codes`` instead\n    sortorder : optional int\n        Level of sortedness (must be lexicographically sorted by that\n        level).\n    names : optional sequence of objects\n        Names for each of the index levels. (name is accepted for compat).\n    copy : bool, default False\n        Copy the meta-data.\n    verify_integrity : bool, default True\n        Check that the levels/codes are consistent and valid.\n\n    Attributes\n    ----------\n    names\n    levels\n    codes\n    nlevels\n    levshape\n\n    Methods\n    -------\n    from_arrays\n    from_tuples\n    from_product\n    from_frame\n    set_levels\n    set_codes\n    to_frame\n    to_flat_index\n    is_lexsorted\n    sortlevel\n    droplevel\n    swaplevel\n    reorder_levels\n    remove_unused_levels\n\n    See Also\n    --------\n    MultiIndex.from_arrays  : Convert list of arrays to MultiIndex.\n    MultiIndex.from_product : Create a MultiIndex from the cartesian product\n                              of iterables.\n    MultiIndex.from_tuples  : Convert list of tuples to a MultiIndex.\n    MultiIndex.from_frame   : Make a MultiIndex from a DataFrame.\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    See the `user guide\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html>`_\n    for more.\n\n    Examples\n    --------\n    A new ``MultiIndex`` is typically constructed using one of the helper\n    methods :meth:`MultiIndex.from_arrays`, :meth:`MultiIndex.from_product`\n    and :meth:`MultiIndex.from_tuples`. For example (using ``.from_arrays``):\n\n    >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n    >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n    MultiIndex([(1,  'red'),\n                (1, 'blue'),\n                (2,  'red'),\n                (2, 'blue')],\n               names=['number', 'color'])\n\n    See further examples for how to construct a MultiIndex in the doc strings\n    of the mentioned helper methods.\n    ",
        "klass": "pandas.MultiIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.datetimelike.DatetimeIndexOpsMixin",
            "pandas.core.indexes.numeric.Int64Index",
            "pandas.core.indexes.period.PeriodDelegateMixin"
        ],
        "class_docstring": "\n    Immutable ndarray holding ordinal values indicating regular periods in\n    time such as particular years, quarters, months, etc.\n\n    Index keys are boxed to Period objects which carries the metadata (eg,\n    frequency information).\n\n    Parameters\n    ----------\n    data : array-like (1d integer np.ndarray or PeriodArray), optional\n        Optional period-like data to construct index with\n    copy : bool\n        Make a copy of input ndarray\n    freq : string or period object, optional\n        One of pandas period strings or corresponding objects\n    start : starting value, period-like, optional\n        If data is None, used as the start point in generating regular\n        period data.\n\n        .. deprecated:: 0.24.0\n\n    periods : int, optional, > 0\n        Number of periods to generate, if generating index. Takes precedence\n        over end argument\n\n        .. deprecated:: 0.24.0\n\n    end : end value, period-like, optional\n        If periods is none, generated index will extend to first conforming\n        period on or just past end argument\n\n        .. deprecated:: 0.24.0\n\n    year : int, array, or Series, default None\n    month : int, array, or Series, default None\n    quarter : int, array, or Series, default None\n    day : int, array, or Series, default None\n    hour : int, array, or Series, default None\n    minute : int, array, or Series, default None\n    second : int, array, or Series, default None\n    tz : object, default None\n        Timezone for converting datetime64 data to Periods\n    dtype : str or PeriodDtype, default None\n\n    Attributes\n    ----------\n    day\n    dayofweek\n    dayofyear\n    days_in_month\n    daysinmonth\n    end_time\n    freq\n    freqstr\n    hour\n    is_leap_year\n    minute\n    month\n    quarter\n    qyear\n    second\n    start_time\n    week\n    weekday\n    weekofyear\n    year\n\n    Methods\n    -------\n    asfreq\n    strftime\n    to_timestamp\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Period : Represents a period of time.\n    DatetimeIndex : Index with datetime64 data.\n    TimedeltaIndex : Index of timedelta64 data.\n    period_range : Create a fixed-frequency PeriodIndex.\n\n    Notes\n    -----\n    Creating a PeriodIndex based on `start`, `periods`, and `end` has\n    been deprecated in favor of :func:`period_range`.\n\n    Examples\n    --------\n    >>> idx = pd.PeriodIndex(year=year_arr, quarter=q_arr)\n    ",
        "klass": "pandas.PeriodIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.numeric.Int64Index"
        ],
        "class_docstring": "\n    Immutable Index implementing a monotonic integer range.\n\n    RangeIndex is a memory-saving special case of Int64Index limited to\n    representing monotonic ranges. Using RangeIndex may in some instances\n    improve computing speed.\n\n    This is the default index type used\n    by DataFrame and Series when no explicit index is provided by the user.\n\n    Parameters\n    ----------\n    start : int (default: 0), or other RangeIndex instance\n        If int and \"stop\" is not given, interpreted as \"stop\" instead.\n    stop : int (default: 0)\n    step : int (default: 1)\n    name : object, optional\n        Name to be stored in the index\n    copy : bool, default False\n        Unused, accepted for homogeneity with other index types.\n\n    Attributes\n    ----------\n    start\n    stop\n    step\n\n    Methods\n    -------\n    from_range\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Int64Index : Index of int64 data.\n    ",
        "klass": "pandas.RangeIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.base.IndexOpsMixin",
            "pandas.core.generic.NDFrame"
        ],
        "class_docstring": "\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, *, **) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series.\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    copy : bool, default False\n        Copy input data.\n    ",
        "klass": "pandas.Series",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.frame.DataFrame"
        ],
        "class_docstring": "\n    DataFrame containing sparse floating point data in the form of SparseSeries\n    objects\n\n    .. deprecated:: 0.25.0\n\n       Use a DataFrame with sparse values instead.\n\n    Parameters\n    ----------\n    data : same types as can be passed to DataFrame or scipy.sparse.spmatrix\n        .. versionchanged :: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    index : array-like, optional\n    column : array-like, optional\n    default_kind : {'block', 'integer'}, default 'block'\n        Default sparse kind for converting Series to SparseSeries. Will not\n        override SparseSeries passed into constructor\n    default_fill_value : float\n        Default fill_value for converting Series to SparseSeries\n        (default: nan). Will not override SparseSeries passed in.\n    ",
        "klass": "pandas.SparseDataFrame",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.series.Series"
        ],
        "class_docstring": "Data structure for labeled, sparse floating point data\n\n    .. deprecated:: 0.25.0\n\n       Use a Series with sparse values instead.\n\n    Parameters\n    ----------\n    data : {array-like, Series, SparseSeries, dict}\n        .. versionchanged :: 0.23.0\n           If data is a dict, argument order is maintained for Python 3.6\n           and later.\n\n    kind : {'block', 'integer'}\n    fill_value : float\n        Code for missing value. Defaults depends on dtype.\n        0 for int dtype, False for bool dtype, and NaN for other dtypes\n    sparse_index : {BlockIndex, IntIndex}, optional\n        Only if you have one. Mainly used internally\n\n    Notes\n    -----\n    SparseSeries objects are immutable via the typical Python means. If you\n    must change values, convert to dense, make your changes, then convert back\n    to sparse\n    ",
        "klass": "pandas.SparseSeries",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas._libs.tslibs.timedeltas._Timedelta"
        ],
        "class_docstring": "\n    Represents a duration, the difference between two dates or times.\n\n    Timedelta is the pandas equivalent of python's ``datetime.timedelta``\n    and is interchangeable with it in most cases.\n\n    Parameters\n    ----------\n    value : Timedelta, timedelta, np.timedelta64, string, or integer\n    unit : str, optional\n        Denote the unit of the input, if input is an integer. Default 'ns'.\n        Possible values:\n        {'Y', 'M', 'W', 'D', 'days', 'day', 'hours', hour', 'hr', 'h',\n        'm', 'minute', 'min', 'minutes', 'T', 'S', 'seconds', 'sec', 'second',\n        'ms', 'milliseconds', 'millisecond', 'milli', 'millis', 'L',\n        'us', 'microseconds', 'microsecond', 'micro', 'micros', 'U',\n        'ns', 'nanoseconds', 'nano', 'nanos', 'nanosecond', 'N'}\n    **kwargs\n        Available kwargs: {days, seconds, microseconds,\n        milliseconds, minutes, hours, weeks}.\n        Values for construction in compat with datetime.timedelta.\n        Numpy ints and floats will be coerced to python ints and floats.\n\n    Notes\n    -----\n    The ``.value`` attribute is always in ns.\n    ",
        "klass": "pandas.Timedelta",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.datetimelike.DatetimeIndexOpsMixin",
            "pandas.core.arrays.datetimelike.TimelikeOps",
            "pandas.core.indexes.numeric.Int64Index",
            "pandas.core.indexes.timedeltas.TimedeltaDelegateMixin"
        ],
        "class_docstring": "\n    Immutable ndarray of timedelta64 data, represented internally as int64, and\n    which can be boxed to timedelta objects\n\n    Parameters\n    ----------\n    data  : array-like (1-dimensional), optional\n        Optional timedelta-like data to construct index with\n    unit : unit of the arg (D,h,m,s,ms,us,ns) denote the unit, optional\n        which is an integer/float number\n    freq : string or pandas offset object, optional\n        One of pandas date offset strings or corresponding objects. The string\n        'infer' can be passed in order to set the frequency of the index as the\n        inferred frequency upon creation\n    copy  : bool\n        Make a copy of input ndarray\n    start : starting value, timedelta-like, optional\n        If data is None, start is used as the start point in generating regular\n        timedelta data.\n\n        .. deprecated:: 0.24.0\n\n    periods  : int, optional, > 0\n        Number of periods to generate, if generating index. Takes precedence\n        over end argument\n\n        .. deprecated:: 0.24.0\n\n    end : end time, timedelta-like, optional\n        If periods is none, generated index will extend to first conforming\n        time on or just past end argument\n\n        .. deprecated:: 0.24. 0\n\n    closed : string or None, default None\n        Make the interval closed with respect to the given frequency to\n        the 'left', 'right', or both sides (None)\n\n        .. deprecated:: 0.24. 0\n\n    name : object\n        Name to be stored in the index\n\n    Attributes\n    ----------\n    days\n    seconds\n    microseconds\n    nanoseconds\n    components\n    inferred_freq\n\n    Methods\n    -------\n    to_pytimedelta\n    to_series\n    round\n    floor\n    ceil\n    to_frame\n    mean\n\n    See Also\n    --------\n    Index : The base pandas Index type.\n    Timedelta : Represents a duration between two dates or times.\n    DatetimeIndex : Index of datetime64 data.\n    PeriodIndex : Index of Period data.\n    timedelta_range : Create a fixed-frequency TimedeltaIndex.\n\n    Notes\n    -----\n    To learn more about the frequency strings, please see `this link\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n    Creating a TimedeltaIndex based on `start`, `periods`, and `end` has\n    been deprecated in favor of :func:`timedelta_range`.\n    ",
        "klass": "pandas.TimedeltaIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas._libs.tslibs.c_timestamp._Timestamp"
        ],
        "class_docstring": "\n    Pandas replacement for python datetime.datetime object.\n\n    Timestamp is the pandas equivalent of python's Datetime\n    and is interchangeable with it in most cases. It's the type used\n    for the entries that make up a DatetimeIndex, and other timeseries\n    oriented data structures in pandas.\n\n    Parameters\n    ----------\n    ts_input : datetime-like, str, int, float\n        Value to be converted to Timestamp.\n    freq : str, DateOffset\n        Offset which Timestamp will have.\n    tz : str, pytz.timezone, dateutil.tz.tzfile or None\n        Time zone for time which Timestamp will have.\n    unit : str\n        Unit used for conversion if ts_input is of type int or float. The\n        valid values are 'D', 'h', 'm', 's', 'ms', 'us', and 'ns'. For\n        example, 's' means seconds and 'ms' means milliseconds.\n    year, month, day : int\n        .. versionadded:: 0.19.0\n    hour, minute, second, microsecond : int, optional, default 0\n        .. versionadded:: 0.19.0\n    nanosecond : int, optional, default 0\n        .. versionadded:: 0.23.0\n    tzinfo : datetime.tzinfo, optional, default None\n        .. versionadded:: 0.19.0\n\n    Notes\n    -----\n    There are essentially three calling conventions for the constructor. The\n    primary form accepts four parameters. They can be passed by position or\n    keyword.\n\n    The other two forms mimic the parameters from ``datetime.datetime``. They\n    can be passed by either position or keyword, but not both mixed together.\n\n    Examples\n    --------\n    Using the primary calling convention:\n\n    This converts a datetime-like string\n\n    >>> pd.Timestamp('2017-01-01T12')\n    Timestamp('2017-01-01 12:00:00')\n\n    This converts a float representing a Unix epoch in units of seconds\n\n    >>> pd.Timestamp(1513393355.5, unit='s')\n    Timestamp('2017-12-16 03:02:35.500000')\n\n    This converts an int representing a Unix-epoch in units of seconds\n    and for a particular timezone\n\n    >>> pd.Timestamp(1513393355, unit='s', tz='US/Pacific')\n    Timestamp('2017-12-15 19:02:35-0800', tz='US/Pacific')\n\n    Using the other two forms that mimic the API for ``datetime.datetime``:\n\n    >>> pd.Timestamp(2017, 1, 1, 12)\n    Timestamp('2017-01-01 12:00:00')\n\n    >>> pd.Timestamp(year=2017, month=1, day=1, hour=12)\n    Timestamp('2017-01-01 12:00:00')\n    ",
        "klass": "pandas.Timestamp",
        "module": "pandas"
    },
    {
        "base_classes": [
            "datetime.date"
        ],
        "class_docstring": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
        "klass": "pandas.datetime",
        "module": "datetime"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager to temporarily set options in the `with` statement context.\n\n    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.\n\n    Examples\n    --------\n\n    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n    ...     ...\n    ",
        "klass": "pandas.option_context",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas._libs.interval.IntervalMixin"
        ],
        "class_docstring": "A centered interval tree\n\n    Based off the algorithm described on Wikipedia:\n    http://en.wikipedia.org/wiki/Interval_tree\n\n    we are emulating the IndexEngine interface\n    ",
        "klass": "pandas._libs.interval.IntervalTree",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.arrays.base.ExtensionArray",
            "pandas.core.arrays.base.ExtensionOpsMixin",
            "numpy.lib.mixins.NDArrayOperatorsMixin"
        ],
        "class_docstring": "\n    A pandas ExtensionArray for NumPy data.\n\n    .. versionadded :: 0.24.0\n\n    This is mostly for internal compatibility, and is not especially\n    useful on its own.\n\n    Parameters\n    ----------\n    values : ndarray\n        The NumPy ndarray to wrap. Must be 1-dimensional.\n    copy : bool, default False\n        Whether to copy `values`.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n    ",
        "klass": "pandas.arrays.PandasArray",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.arrays.datetimelike.DatetimeLikeArrayMixin",
            "pandas.core.arrays.datetimelike.TimelikeOps",
            "pandas.core.arrays.datetimelike.DatelikeOps"
        ],
        "class_docstring": "\n    Pandas ExtensionArray for tz-naive or tz-aware datetime data.\n\n    .. versionadded:: 0.24.0\n\n    .. warning::\n\n       DatetimeArray is currently experimental, and its API may change\n       without warning. In particular, :attr:`DatetimeArray.dtype` is\n       expected to change to always be an instance of an ``ExtensionDtype``\n       subclass.\n\n    Parameters\n    ----------\n    values : Series, Index, DatetimeArray, ndarray\n        The datetime data.\n\n        For DatetimeArray `values` (or a Series or Index boxing one),\n        `dtype` and `freq` will be extracted from `values`, with\n        precedence given to\n\n    dtype : numpy.dtype or DatetimeTZDtype\n        Note that the only NumPy dtype allowed is 'datetime64[ns]'.\n    freq : str or Offset, optional\n    copy : bool, default False\n        Whether to copy the underlying array of values.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n    ",
        "klass": "pandas.core.arrays.DatetimeArray",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas._libs.interval.IntervalMixin",
            "pandas.core.arrays.base.ExtensionArray"
        ],
        "class_docstring": "\nPandas array for interval data that are closed on the same side.\n\n.. versionadded:: 0.24.0\n\nParameters\n----------\ndata : array-like (1-dimensional)\n    Array-like containing Interval objects from which to build the\n    IntervalArray.\nclosed : {'left', 'right', 'both', 'neither'}, default 'right'\n    Whether the intervals are closed on the left-side, right-side, both or\n    neither.\ndtype : dtype or None, default None\n    If None, dtype will be inferred.\n\n    .. versionadded:: 0.23.0\ncopy : bool, default False\n    Copy the input data.\nverify_integrity : bool, default True\n    Verify that the IntervalArray is valid.\n\nAttributes\n----------\nleft\nright\nclosed\nmid\nlength\nis_empty\nis_non_overlapping_monotonic\n\nMethods\n-------\nfrom_arrays\nfrom_tuples\nfrom_breaks\ncontains\noverlaps\nset_closed\nto_tuples\n\nSee Also\n--------\nIndex : The base pandas Index type.\nInterval : A bounded slice-like interval; the elements of an IntervalArray.\ninterval_range : Function to create a fixed frequency IntervalIndex.\ncut : Bin values into discrete Intervals.\nqcut : Bin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n-----\nSee the `user guide\n<http://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#intervalindex>`_\nfor more.\n\nExamples\n--------\nA new ``IntervalArray`` can be constructed directly from an array-like of\n``Interval`` objects:\n\n>>> pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\nIntervalArray([(0, 1], (1, 5]],\n              closed='right',\n              dtype='interval[int64]')\n\nIt may also be constructed using one of the constructor\nmethods: :meth:`IntervalArray.from_arrays`,\n:meth:`IntervalArray.from_breaks`, and :meth:`IntervalArray.from_tuples`.\n",
        "klass": "pandas.core.arrays.IntervalArray",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.arrays.datetimelike.DatetimeLikeArrayMixin",
            "pandas.core.arrays.datetimelike.TimelikeOps"
        ],
        "class_docstring": "\n    Pandas ExtensionArray for timedelta data.\n\n    .. versionadded:: 0.24.0\n\n    .. warning::\n\n       TimedeltaArray is currently experimental, and its API may change\n       without warning. In particular, :attr:`TimedeltaArray.dtype` is\n       expected to change to be an instance of an ``ExtensionDtype``\n       subclass.\n\n    Parameters\n    ----------\n    values : array-like\n        The timedelta data.\n\n    dtype : numpy.dtype\n        Currently, only ``numpy.dtype(\"timedelta64[ns]\")`` is accepted.\n    freq : Offset, optional\n    copy : bool, default False\n        Whether to copy the underlying array of data.\n\n    Attributes\n    ----------\n    None\n\n    Methods\n    -------\n    None\n    ",
        "klass": "pandas.core.arrays.TimedeltaArray",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.base.IndexOpsMixin",
            "pandas.core.base.PandasObject"
        ],
        "class_docstring": "\n    Immutable ndarray implementing an ordered, sliceable set. The basic object\n    storing axis labels for all pandas objects.\n\n    Parameters\n    ----------\n    data : array-like (1-dimensional)\n    dtype : NumPy dtype (default: object)\n        If dtype is None, we find the dtype that best fits the data.\n        If an actual dtype is provided, we coerce to that dtype if it's safe.\n        Otherwise, an error will be raised.\n    copy : bool\n        Make a copy of input ndarray\n    name : object\n        Name to be stored in the index\n    tupleize_cols : bool (default: True)\n        When True, attempt to create a MultiIndex if possible\n\n    See Also\n    --------\n    RangeIndex : Index implementing a monotonic integer range.\n    CategoricalIndex : Index of :class:`Categorical` s.\n    MultiIndex : A multi-level, or hierarchical, Index.\n    IntervalIndex : An Index of :class:`Interval` s.\n    DatetimeIndex, TimedeltaIndex, PeriodIndex\n    Int64Index, UInt64Index,  Float64Index\n\n    Notes\n    -----\n    An Index instance can **only** contain hashable objects\n\n    Examples\n    --------\n    >>> pd.Index([1, 2, 3])\n    Int64Index([1, 2, 3], dtype='int64')\n\n    >>> pd.Index(list('abc'))\n    Index(['a', 'b', 'c'], dtype='object')\n    ",
        "klass": "pandas.core.index.Index",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.indexes.base.Index"
        ],
        "class_docstring": "\n    A multi-level, or hierarchical, index object for pandas objects.\n\n    Parameters\n    ----------\n    levels : sequence of arrays\n        The unique labels for each level.\n    codes : sequence of arrays\n        Integers for each level designating which label at each location.\n\n        .. versionadded:: 0.24.0\n    labels : sequence of arrays\n        Integers for each level designating which label at each location.\n\n        .. deprecated:: 0.24.0\n            Use ``codes`` instead\n    sortorder : optional int\n        Level of sortedness (must be lexicographically sorted by that\n        level).\n    names : optional sequence of objects\n        Names for each of the index levels. (name is accepted for compat).\n    copy : bool, default False\n        Copy the meta-data.\n    verify_integrity : bool, default True\n        Check that the levels/codes are consistent and valid.\n\n    Attributes\n    ----------\n    names\n    levels\n    codes\n    nlevels\n    levshape\n\n    Methods\n    -------\n    from_arrays\n    from_tuples\n    from_product\n    from_frame\n    set_levels\n    set_codes\n    to_frame\n    to_flat_index\n    is_lexsorted\n    sortlevel\n    droplevel\n    swaplevel\n    reorder_levels\n    remove_unused_levels\n\n    See Also\n    --------\n    MultiIndex.from_arrays  : Convert list of arrays to MultiIndex.\n    MultiIndex.from_product : Create a MultiIndex from the cartesian product\n                              of iterables.\n    MultiIndex.from_tuples  : Convert list of tuples to a MultiIndex.\n    MultiIndex.from_frame   : Make a MultiIndex from a DataFrame.\n    Index : The base pandas Index type.\n\n    Notes\n    -----\n    See the `user guide\n    <http://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html>`_\n    for more.\n\n    Examples\n    --------\n    A new ``MultiIndex`` is typically constructed using one of the helper\n    methods :meth:`MultiIndex.from_arrays`, :meth:`MultiIndex.from_product`\n    and :meth:`MultiIndex.from_tuples`. For example (using ``.from_arrays``):\n\n    >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n    >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n    MultiIndex([(1,  'red'),\n                (1, 'blue'),\n                (2,  'red'),\n                (2, 'blue')],\n               names=['number', 'color'])\n\n    See further examples for how to construct a MultiIndex in the doc strings\n    of the mentioned helper methods.\n    ",
        "klass": "pandas.core.index.MultiIndex",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.io.common.BaseIterator"
        ],
        "class_docstring": "\n    Read SAS files in SAS7BDAT format.\n\n    Parameters\n    ----------\n    path_or_buf : path name or buffer\n        Name of SAS file or file-like object pointing to SAS file\n        contents.\n    index : column identifier, defaults to None\n        Column to use as index.\n    convert_dates : boolean, defaults to True\n        Attempt to convert dates to Pandas datetime values.  Note that\n        some rarely used SAS date formats may be unsupported.\n    blank_missing : boolean, defaults to True\n        Convert empty strings to missing values (SAS uses blanks to\n        indicate missing character variables).\n    chunksize : int, defaults to None\n        Return SAS7BDATReader object for iterations, returns chunks\n        with given number of lines.\n    encoding : string, defaults to None\n        String encoding.\n    convert_text : bool, defaults to True\n        If False, text variables are left as raw bytes.\n    convert_header_text : bool, defaults to True\n        If False, header text, including column names, are left as raw\n        bytes.\n    ",
        "klass": "pandas.io.sas.sas7bdat.SAS7BDATReader",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.io.common.BaseIterator"
        ],
        "class_docstring": "Class for reading SAS Xport files.\n\nParameters\n----------\nfilepath_or_buffer : string or file-like object\n    Path to SAS file or object implementing binary read method.\nindex : identifier of index column\n    Identifier of column that should be used as index of the DataFrame.\nencoding : string\n    Encoding for text data.\nchunksize : int\n    Read file `chunksize` lines at a time, returns iterator.\n\nAttributes\n----------\nmember_info : list\n    Contains information about the file\nfields : list\n    Contains information about the variables in the file\n",
        "klass": "pandas.io.sas.sas_xport.XportReader",
        "module": "pandas"
    },
    {
        "base_classes": [
            "pandas.core.generic.NDFrame"
        ],
        "class_docstring": "\n    Two-dimensional size-mutable, potentially heterogeneous tabular data\n    structure with labeled axes (rows and columns). Arithmetic operations\n    align on both row and column labels. Can be thought of as a dict-like\n    container for Series objects. The primary pandas data structure.\n\n    Parameters\n    ----------\n    data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n        Dict can contain Series, arrays, constants, or list-like objects\n\n        .. versionchanged :: 0.23.0\n           If data is a dict, column order follows insertion-order for\n           Python 3.6 and later.\n\n        .. versionchanged :: 0.25.0\n           If data is a list of dicts, column order follows insertion-order\n           for Python 3.6 and later.\n\n    index : Index or array-like\n        Index to use for resulting frame. Will default to RangeIndex if\n        no indexing information part of input data and no index provided\n    columns : Index or array-like\n        Column labels to use for resulting frame. Will default to\n        RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n    dtype : dtype, default None\n        Data type to force. Only a single dtype is allowed. If None, infer\n    copy : boolean, default False\n        Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n    See Also\n    --------\n    DataFrame.from_records : Constructor from tuples, also record arrays.\n    DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n    DataFrame.from_items : From sequence of (key, value) pairs\n        read_csv, pandas.read_table, pandas.read_clipboard.\n\n    Examples\n    --------\n    Constructing DataFrame from a dictionary.\n\n    >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n    >>> df = pd.DataFrame(data=d)\n    >>> df\n       col1  col2\n    0     1     3\n    1     2     4\n\n    Notice that the inferred dtype is int64.\n\n    >>> df.dtypes\n    col1    int64\n    col2    int64\n    dtype: object\n\n    To enforce a single dtype:\n\n    >>> df = pd.DataFrame(data=d, dtype=np.int8)\n    >>> df.dtypes\n    col1    int8\n    col2    int8\n    dtype: object\n\n    Constructing DataFrame from numpy ndarray:\n\n    >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n    ...                    columns=['a', 'b', 'c'])\n    >>> df2\n       a  b  c\n    0  1  2  3\n    1  4  5  6\n    2  7  8  9\n    ",
        "klass": "pandas.util.testing.DataFrame",
        "module": "pandas"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager to set the numpy random number generator speed. Returns\n    to the original value upon exiting the context manager.\n\n    Parameters\n    ----------\n    seed : int\n        Seed for numpy.random.seed\n\n    Examples\n    --------\n\n    with RNGContext(42):\n        np.random.randn()\n    ",
        "klass": "pandas.util.testing.RNGContext",
        "module": "pandas"
    },
    {
        "base_classes": [
            "greenlet.greenlet"
        ],
        "class_docstring": "Greenlet(run=None, *args, **kwargs)\n\n    A light-weight cooperatively-scheduled execution unit.\n    ",
        "klass": "gevent.Greenlet",
        "module": "gevent"
    },
    {
        "base_classes": [
            "BaseException"
        ],
        "class_docstring": "\n    Timeout(seconds=None, exception=None, ref=True, priority=-1)\n\n    Raise *exception* in the current greenlet after *seconds*\n    have elapsed::\n\n        timeout = Timeout(seconds, exception)\n        timeout.start()\n        try:\n            ...  # exception will be raised here, after *seconds* passed since start() call\n        finally:\n            timeout.close()\n\n    .. note::\n\n        If the code that the timeout was protecting finishes\n        executing before the timeout elapses, be sure to ``close`` the\n        timeout so it is not unexpectedly raised in the future. Even if it\n        is raised, it is a best practice to close it. This ``try/finally``\n        construct or a ``with`` statement is a recommended pattern. (If\n        the timeout object will be started again, use ``cancel`` instead\n        of ``close``; this is rare.)\n\n    When *exception* is omitted or ``None``, the ``Timeout`` instance\n    itself is raised::\n\n        >>> import gevent\n        >>> gevent.Timeout(0.1).start()\n        >>> gevent.sleep(0.2)  #doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n         ...\n        Timeout: 0.1 seconds\n\n    If the *seconds* argument is not given or is ``None`` (e.g.,\n    ``Timeout()``), then the timeout will never expire and never raise\n    *exception*. This is convenient for creating functions which take\n    an optional timeout parameter of their own. (Note that this is **not**\n    the same thing as a *seconds* value of ``0``.)\n\n    ::\n\n       def function(args, timeout=None):\n          \"A function with an optional timeout.\"\n          timer = Timeout(timeout)\n          with timer:\n             ...\n\n    .. caution::\n\n        A *seconds* value less than ``0.0`` (e.g., ``-1``) is poorly defined. In the future,\n        support for negative values is likely to do the same thing as a value\n        of ``None`` or ``0``\n\n    A *seconds* value of ``0`` requests that the event loop spin and poll for I/O;\n    it will immediately expire as soon as control returns to the event loop.\n\n    .. rubric:: Use As A Context Manager\n\n    To simplify starting and canceling timeouts, the ``with``\n    statement can be used::\n\n        with gevent.Timeout(seconds, exception) as timeout:\n            pass  # ... code block ...\n\n    This is equivalent to the try/finally block above with one\n    additional feature: if *exception* is the literal ``False``, the\n    timeout is still raised, but the context manager suppresses it, so\n    the code outside the with-block won't see it.\n\n    This is handy for adding a timeout to the functions that don't\n    support a *timeout* parameter themselves::\n\n        data = None\n        with gevent.Timeout(5, False):\n            data = mysock.makefile().readline()\n        if data is None:\n            ...  # 5 seconds passed without reading a line\n        else:\n            ...  # a line was read within 5 seconds\n\n    .. caution::\n\n        If ``readline()`` above catches and doesn't re-raise\n        :exc:`BaseException` (for example, with a bare ``except:``), then\n        your timeout will fail to function and control won't be returned\n        to you when you expect.\n\n    .. rubric:: Catching Timeouts\n\n    When catching timeouts, keep in mind that the one you catch may\n    not be the one you have set (a calling function may have set its\n    own timeout); if you going to silence a timeout, always check that\n    it's the instance you need::\n\n        timeout = Timeout(1)\n        timeout.start()\n        try:\n            ...\n        except Timeout as t:\n            if t is not timeout:\n                raise # not my timeout\n        finally:\n            timeout.close()\n\n\n    .. versionchanged:: 1.1b2\n\n        If *seconds* is not given or is ``None``, no longer allocate a\n        native timer object that will never be started.\n\n    .. versionchanged:: 1.1\n\n        Add warning about negative *seconds* values.\n\n    .. versionchanged:: 1.3a1\n\n        Timeout objects now have a :meth:`close`\n        method that must be called when the timeout will no longer be\n        used to properly clean up native resources.\n        The ``with`` statement does this automatically.\n\n    ",
        "klass": "gevent.Timeout",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.baseserver.BaseServer"
        ],
        "class_docstring": "\n    A generic TCP server.\n\n    Accepts connections on a listening socket and spawns user-provided\n    *handle* function for each connection with 2 arguments: the client\n    socket and the client address.\n\n    Note that although the errors in a successfully spawned handler\n    will not affect the server or other connections, the errors raised\n    by :func:`accept` and *spawn* cause the server to stop accepting\n    for a short amount of time. The exact period depends on the values\n    of :attr:`min_delay` and :attr:`max_delay` attributes.\n\n    The delay starts with :attr:`min_delay` and doubles with each\n    successive error until it reaches :attr:`max_delay`. A successful\n    :func:`accept` resets the delay to :attr:`min_delay` again.\n\n    See :class:`~gevent.baseserver.BaseServer` for information on defining the *handle*\n    function and important restrictions on it.\n\n    **SSL Support**\n\n    The server can optionally work in SSL mode when given the correct\n    keyword arguments. (That is, the presence of any keyword arguments\n    will trigger SSL mode.) On Python 2.7.9 and later (any Python\n    version that supports the :class:`ssl.SSLContext`), this can be\n    done with a configured ``SSLContext``. On any Python version, it\n    can be done by passing the appropriate arguments for\n    :func:`ssl.wrap_socket`.\n\n    The incoming socket will be wrapped into an SSL socket before\n    being passed to the *handle* function.\n\n    If the *ssl_context* keyword argument is present, it should\n    contain an :class:`ssl.SSLContext`. The remaining keyword\n    arguments are passed to the :meth:`ssl.SSLContext.wrap_socket`\n    method of that object. Depending on the Python version, supported arguments\n    may include:\n\n    - server_hostname\n    - suppress_ragged_eofs\n    - do_handshake_on_connect\n\n    .. caution:: When using an SSLContext, it should either be\n       imported from :mod:`gevent.ssl`, or the process needs to be monkey-patched.\n       If the process is not monkey-patched and you pass the standard library\n       SSLContext, the resulting client sockets will not cooperate with gevent.\n\n    Otherwise, keyword arguments are assumed to apply to :func:`ssl.wrap_socket`.\n    These keyword arguments may include:\n\n    - keyfile\n    - certfile\n    - cert_reqs\n    - ssl_version\n    - ca_certs\n    - suppress_ragged_eofs\n    - do_handshake_on_connect\n    - ciphers\n\n    .. versionchanged:: 1.2a2\n       Add support for the *ssl_context* keyword argument.\n\n    ",
        "klass": "gevent.server.StreamServer",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.__abstract_linkable.AbstractLinkable"
        ],
        "class_docstring": "AsyncResult()\nA one-time event that stores a value or an exception.\n\n    Like :class:`Event` it wakes up all the waiters when :meth:`set` or :meth:`set_exception`\n    is called. Waiters may receive the passed value or exception by calling :meth:`get`\n    instead of :meth:`wait`. An :class:`AsyncResult` instance cannot be reset.\n\n    To pass a value call :meth:`set`. Calls to :meth:`get` (those that are currently blocking as well as\n    those made in the future) will return the value:\n\n        >>> result = AsyncResult()\n        >>> result.set(100)\n        >>> result.get()\n        100\n\n    To pass an exception call :meth:`set_exception`. This will cause :meth:`get` to raise that exception:\n\n        >>> result = AsyncResult()\n        >>> result.set_exception(RuntimeError('failure'))\n        >>> result.get()\n        Traceback (most recent call last):\n         ...\n        RuntimeError: failure\n\n    :class:`AsyncResult` implements :meth:`__call__` and thus can be used as :meth:`link` target:\n\n        >>> import gevent\n        >>> result = AsyncResult()\n        >>> gevent.spawn(lambda : 1/0).link(result)\n        >>> try:\n        ...     result.get()\n        ... except ZeroDivisionError:\n        ...     print('ZeroDivisionError')\n        ZeroDivisionError\n\n    .. note::\n        The order and timing in which waiting greenlets are awakened is not determined.\n        As an implementation note, in gevent 1.1 and 1.0, waiting greenlets are awakened in a\n        undetermined order sometime *after* the current greenlet yields to the event loop. Other greenlets\n        (those not waiting to be awakened) may run between the current greenlet yielding and\n        the waiting greenlets being awakened. These details may change in the future.\n\n    .. versionchanged:: 1.1\n       The exact order in which waiting greenlets are awakened is not the same\n       as in 1.0.\n    .. versionchanged:: 1.1\n       Callbacks :meth:`linked <rawlink>` to this object are required to be hashable, and duplicates are\n       merged.\n    ",
        "klass": "gevent.event.AsyncResult",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.__abstract_linkable.AbstractLinkable"
        ],
        "class_docstring": "Event()\nA synchronization primitive that allows one greenlet to wake up one or more others.\n    It has the same interface as :class:`threading.Event` but works across greenlets.\n\n    An event object manages an internal flag that can be set to true with the\n    :meth:`set` method and reset to false with the :meth:`clear` method. The :meth:`wait` method\n    blocks until the flag is true.\n\n    .. note::\n        The order and timing in which waiting greenlets are awakened is not determined.\n        As an implementation note, in gevent 1.1 and 1.0, waiting greenlets are awakened in a\n        undetermined order sometime *after* the current greenlet yields to the event loop. Other greenlets\n        (those not waiting to be awakened) may run between the current greenlet yielding and\n        the waiting greenlets being awakened. These details may change in the future.\n    ",
        "klass": "gevent.event.Event",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.__abstract_linkable.AbstractLinkable"
        ],
        "class_docstring": "Semaphore(value=1)\n\n    Semaphore(value=1) -> Semaphore\n\n    A semaphore manages a counter representing the number of release()\n    calls minus the number of acquire() calls, plus an initial value.\n    The acquire() method blocks if necessary until it can return\n    without making the counter negative.\n\n    If not given, ``value`` defaults to 1.\n\n    The semaphore is a context manager and can be used in ``with`` statements.\n\n    This Semaphore's ``__exit__`` method does not call the trace function\n    on CPython, but does under PyPy.\n\n    .. seealso:: :class:`BoundedSemaphore` for a safer version that prevents\n       some classes of bugs.\n\n    .. versionchanged:: 1.4.0\n\n        The order in which waiters are awakened is not specified. It was not\n        specified previously, but usually went in FIFO order.\n    ",
        "klass": "gevent.lock.Semaphore",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.pool.GroupMappingMixin"
        ],
        "class_docstring": "\n    Maintain a group of greenlets that are still running, without\n    limiting their number.\n\n    Links to each item and removes it upon notification.\n\n    Groups can be iterated to discover what greenlets they are tracking,\n    they can be tested to see if they contain a greenlet, and they know the\n    number (len) of greenlets they are tracking. If they are not tracking any\n    greenlets, they are False in a boolean context.\n\n    .. attribute:: greenlet_class\n\n        Either :class:`gevent.Greenlet` (the default) or a subclass.\n        These are the type of\n        object we will :meth:`spawn`. This can be\n        changed on an instance or in a subclass.\n    ",
        "klass": "gevent.pool.Group",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent.server.StreamServer"
        ],
        "class_docstring": "\n    A WSGI server based on :class:`StreamServer` that supports HTTPS.\n\n\n    :keyword log: If given, an object with a ``write`` method to which\n        request (access) logs will be written. If not given, defaults\n        to :obj:`sys.stderr`. You may pass ``None`` to disable request\n        logging. You may use a wrapper, around e.g., :mod:`logging`,\n        to support objects that don't implement a ``write`` method.\n        (If you pass a :class:`~logging.Logger` instance, or in\n        general something that provides a ``log`` method but not a\n        ``write`` method, such a wrapper will automatically be created\n        and it will be logged to at the :data:`~logging.INFO` level.)\n\n    :keyword error_log: If given, a file-like object with ``write``,\n        ``writelines`` and ``flush`` methods to which error logs will\n        be written. If not given, defaults to :obj:`sys.stderr`. You\n        may pass ``None`` to disable error logging (not recommended).\n        You may use a wrapper, around e.g., :mod:`logging`, to support\n        objects that don't implement the proper methods. This\n        parameter will become the value for ``wsgi.errors`` in the\n        WSGI environment (if not already set). (As with *log*,\n        wrappers for :class:`~logging.Logger` instances and the like\n        will be created automatically and logged to at the :data:`~logging.ERROR`\n        level.)\n\n    .. seealso::\n\n        :class:`LoggingLogAdapter`\n            See important warnings before attempting to use :mod:`logging`.\n\n    .. versionchanged:: 1.1a3\n        Added the ``error_log`` parameter, and set ``wsgi.errors`` in the WSGI\n        environment to this value.\n    .. versionchanged:: 1.1a3\n        Add support for passing :class:`logging.Logger` objects to the ``log`` and\n        ``error_log`` arguments.\n    ",
        "klass": "gevent.pywsgi.WSGIServer",
        "module": "gevent"
    },
    {
        "base_classes": [
            "gevent._queue.Queue"
        ],
        "class_docstring": "JoinableQueue(maxsize=None, items=(), unfinished_tasks=None)\n\n    A subclass of :class:`Queue` that additionally has\n    :meth:`task_done` and :meth:`join` methods.\n    ",
        "klass": "gevent.queue.JoinableQueue",
        "module": "gevent"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Queue(maxsize=None, items=(), _warn_depth=2)\n\n    Create a queue object with a given maximum size.\n\n    If *maxsize* is less than or equal to zero or ``None``, the queue\n    size is infinite.\n\n    Queues have a ``len`` equal to the number of items in them (the :meth:`qsize`),\n    but in a boolean context they are always True.\n\n    .. versionchanged:: 1.1b3\n       Queues now support :func:`len`; it behaves the same as :meth:`qsize`.\n    .. versionchanged:: 1.1b3\n       Multiple greenlets that block on a call to :meth:`put` for a full queue\n       will now be awakened to put their items into the queue in the order in which\n       they arrived. Likewise, multiple greenlets that block on a call to :meth:`get` for\n       an empty queue will now receive items in the order in which they blocked. An\n       implementation quirk under CPython *usually* ensured this was roughly the case\n       previously anyway, but that wasn't the case for PyPy.\n    ",
        "klass": "gevent.queue.Queue",
        "module": "gevent"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "socket(family=AF_INET, type=SOCK_STREAM, proto=0) -> socket object\nsocket(family=-1, type=-1, proto=-1, fileno=None) -> socket object\n\nOpen a socket of the given type.  The family argument specifies the\naddress family; it defaults to AF_INET.  The type argument specifies\nwhether this is a stream (SOCK_STREAM, this is the default)\nor datagram (SOCK_DGRAM) socket.  The protocol argument defaults to 0,\nspecifying the default protocol.  Keyword arguments are accepted.\nThe socket is created as non-inheritable.\n\nWhen a fileno is passed in, family, type and proto are auto-detected,\nunless they are explicitly set.\n\nA socket object represents one endpoint of a network connection.\n\nMethods of socket objects (keyword arguments not allowed):\n\n_accept() -- accept connection, returning new socket fd and client address\nbind(addr) -- bind the socket to a local address\nclose() -- close the socket\nconnect(addr) -- connect the socket to a remote address\nconnect_ex(addr) -- connect, return an error code instead of an exception\ndup() -- return a new socket fd duplicated from fileno()\nfileno() -- return underlying file descriptor\ngetpeername() -- return remote address [*]\ngetsockname() -- return local address\ngetsockopt(level, optname[, buflen]) -- get socket options\ngettimeout() -- return timeout or None\nlisten([n]) -- start listening for incoming connections\nrecv(buflen[, flags]) -- receive data\nrecv_into(buffer[, nbytes[, flags]]) -- receive data (into a buffer)\nrecvfrom(buflen[, flags]) -- receive data and sender's address\nrecvfrom_into(buffer[, nbytes, [, flags])\n  -- receive data and sender's address (into a buffer)\nsendall(data[, flags]) -- send all data\nsend(data[, flags]) -- send data, may not send all of it\nsendto(data[, flags], addr) -- send data to a given address\nsetblocking(0 | 1) -- set or clear the blocking I/O flag\ngetblocking() -- return True if socket is blocking, False if non-blocking\nsetsockopt(level, optname, value[, optlen]) -- set socket options\nsettimeout(None | float) -- set or clear the timeout\nshutdown(how) -- shut down traffic in one or both directions\nif_nameindex() -- return all network interface indices and names\nif_nametoindex(name) -- return the corresponding interface index\nif_indextoname(index) -- return the corresponding interface name\n\n [*] not available on all platforms!",
        "klass": "gevent.socket.socket",
        "module": "_socket"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The underlying process creation and management in this module is\n    handled by the Popen class. It offers a lot of flexibility so that\n    developers are able to handle the less common cases not covered by\n    the convenience functions.\n\n    .. seealso:: :class:`subprocess.Popen`\n       This class should have the same interface as the standard library class.\n\n    .. versionchanged:: 1.2a1\n       Instances can now be used as context managers under Python 2.7. Previously\n       this was restricted to Python 3.\n\n    .. versionchanged:: 1.2a1\n       Instances now save the ``args`` attribute under Python 2.7. Previously this was\n       restricted to Python 3.\n\n    .. versionchanged:: 1.2b1\n        Add the ``encoding`` and ``errors`` parameters for Python 3.\n\n    .. versionchanged:: 1.3a1\n       Accept \"path-like\" objects for the *cwd* parameter on all platforms.\n       This was added to Python 3.6. Previously with gevent, it only worked\n       on POSIX platforms on 3.6.\n\n    .. versionchanged:: 1.3a1\n       Add the ``text`` argument as a synonym for ``universal_newlines``,\n       as added on Python 3.7.\n\n    .. versionchanged:: 1.3a2\n       Allow the same keyword arguments under Python 2 as Python 3:\n       ``pass_fds``, ``start_new_session``, ``restore_signals``, ``encoding``\n       and ``errors``. Under Python 2, ``encoding`` and ``errors`` are ignored\n       because native handling of universal newlines is used.\n\n    .. versionchanged:: 1.3a2\n       Under Python 2, ``restore_signals`` defaults to ``False``. Previously it\n       defaulted to ``True``, the same as it did in Python 3.\n    ",
        "klass": "gevent.subprocess.Popen",
        "module": "gevent"
    },
    {
        "base_classes": [
            "theano.gof.link.Linker"
        ],
        "class_docstring": "\n    Creates C code for an fgraph, compiles it and returns callables\n    through make_thunk and make_function that make use of the compiled\n    code.\n\n    no_recycling can contain a list of Variables that belong to the fgraph.\n    If a Variable is in no_recycling, CLinker will clear the output storage\n    associated to it during the computation (to avoid reusing it).\n\n    ",
        "klass": "theano.gof.cc.CLinker",
        "module": "theano"
    },
    {
        "base_classes": [
            "theano.gof.link.Linker"
        ],
        "class_docstring": "\n    Runs the fgraph in parallel using PerformLinker and CLinker.\n\n    The thunk/function produced by DualLinker uses PerformLinker as the\n    \"main\" implementation: the inputs and outputs are fed to/taken from\n    the Ops' perform. However, DualLinker also instantiates a copy of\n    the fgraph on which it runs OpWiseCLinker. At each step, the variables\n    of perform and of the C implementation are verified using a checker\n    function.\n\n    ",
        "klass": "theano.gof.cc.DualLinker",
        "module": "theano"
    },
    {
        "base_classes": [
            "theano.gof.utils.object2"
        ],
        "class_docstring": "\n    A FunctionGraph represents a subgraph bound by a set of input variables and\n    a set of output variables, ie a subgraph that specifies a theano function.\n    The inputs list should contain all the inputs on which the outputs depend.\n    Variables of type Constant are not counted as inputs.\n\n    The FunctionGraph supports the replace operation which allows to replace a\n    variable in the subgraph by another, e.g. replace (x + x).out by (2\n    * x).out. This is the basis for optimization in theano.\n\n    This class is also responsible for verifying that a graph is valid\n    (ie, all the dtypes and broadcast patterns are compatible with the\n    way the the Variables are used) and for annotating the Variables with\n    a .clients field that specifies which Apply nodes use the variable.\n    The .clients field combined with the .owner field and the Apply nodes'\n    .inputs field allows the graph to be traversed in both directions.\n\n    It can also be extended with new features using\n    FunctionGraph.attach_feature(<toolbox.Feature instance>).\n    See toolbox.Feature for event types and documentation.\n    Extra features allow the FunctionGraph to verify new properties of\n    a graph as it is optimized.\n    # TODO: are there other things features can do to the fgraph?\n\n    Historically, the FunctionGraph was called an Env. Keep this in mind\n    while reading out-of-date documentation, e-mail support threads, etc.\n\n    The constructor creates a FunctionGraph which operates on the subgraph\n    bound by the inputs and outputs sets.\n\n    This class keeps a pointer to the inputs and outputs, and also modifies\n    them.\n\n    #TODO: document what variables are[not] set in the FunctionGraph when a\n    feature is added via the constructor. How constructed is the\n    FunctionGraph?\n\n    Parameters\n    ----------\n    inputs\n        Inputs nodes of the graph, usually declared by the user.\n    outputs\n        Outputs nodes of the graph.\n    clone\n        If true, we will clone the graph. This is useful to remove the constant\n        cache problem.\n\n    Notes\n    -----\n    The intermediate nodes between 'inputs' and 'outputs' are not explicitely\n    passed.\n\n    ",
        "klass": "theano.gof.fg.FunctionGraph",
        "module": "theano"
    },
    {
        "base_classes": [
            "theano.gof.link.LocalLinker"
        ],
        "class_docstring": "\n    Uses CLinker on the individual Ops that comprise an fgraph and loops\n    over them in Python. The variable is slower than a compiled version of\n    the whole fgraph, but saves on compilation time because small changes\n    in the computation graph won't necessarily trigger any recompilation,\n    only local changes in the Variables or Ops that are used.\n\n    If fallback_on_perform is True, OpWiseCLinker will use an op's\n    perform method if no C version can be generated.\n\n    no_recycling can contain a list of Variables that belong to the fgraph.\n    If a Variable is in no_recycling, CLinker will clear the output storage\n    associated to it prior to computation (to avoid reusing it).\n\n    Notes\n    -----\n    This is in a sense the 'default' linker for Theano. The\n    overhead of using the OpWiseCLinker as compared with the CLinker\n    is only noticeable for graphs of very small tensors (such as 20\n    elements or less).\n\n    ",
        "klass": "theano.gof.cc.OpWiseCLinker",
        "module": "theano"
    },
    {
        "base_classes": [
            "theano.gof.link.Linker"
        ],
        "class_docstring": "\n    Creates C code for an fgraph, compiles it and returns callables\n    through make_thunk and make_function that make use of the compiled\n    code.\n\n    no_recycling can contain a list of Variables that belong to the fgraph.\n    If a Variable is in no_recycling, CLinker will clear the output storage\n    associated to it during the computation (to avoid reusing it).\n\n    ",
        "klass": "theano.gof.CLinker",
        "module": "theano"
    },
    {
        "base_classes": [
            "theano.tensor.raw_random.RandomStreamsBase"
        ],
        "class_docstring": "\n    Module component with similar interface to numpy.random\n    (numpy.random.RandomState)\n\n    Parameters\n    ----------\n    seed: None or int\n        A default seed to initialize the RandomState\n        instances after build.  See `RandomStreamsInstance.__init__`\n        for more details.\n\n    ",
        "klass": "theano.tensor.shared_randomstreams.RandomStreams",
        "module": "theano"
    },
    {
        "base_classes": [
            "requests.sessions.Session",
            "docker.api.build.BuildApiMixin",
            "docker.api.config.ConfigApiMixin",
            "docker.api.container.ContainerApiMixin",
            "docker.api.daemon.DaemonApiMixin",
            "docker.api.exec_api.ExecApiMixin",
            "docker.api.image.ImageApiMixin",
            "docker.api.network.NetworkApiMixin",
            "docker.api.plugin.PluginApiMixin",
            "docker.api.secret.SecretApiMixin",
            "docker.api.service.ServiceApiMixin",
            "docker.api.swarm.SwarmApiMixin",
            "docker.api.volume.VolumeApiMixin"
        ],
        "class_docstring": "\n    A low-level client for the Docker Engine API.\n\n    Example:\n\n        >>> import docker\n        >>> client = docker.APIClient(base_url='unix://var/run/docker.sock')\n        >>> client.version()\n        {u'ApiVersion': u'1.33',\n         u'Arch': u'amd64',\n         u'BuildTime': u'2017-11-19T18:46:37.000000000+00:00',\n         u'GitCommit': u'f4ffd2511c',\n         u'GoVersion': u'go1.9.2',\n         u'KernelVersion': u'4.14.3-1-ARCH',\n         u'MinAPIVersion': u'1.12',\n         u'Os': u'linux',\n         u'Version': u'17.10.0-ce'}\n\n    Args:\n        base_url (str): URL to the Docker server. For example,\n            ``unix:///var/run/docker.sock`` or ``tcp://127.0.0.1:1234``.\n        version (str): The version of the API to use. Set to ``auto`` to\n            automatically detect the server's version. Default: ``1.35``\n        timeout (int): Default timeout for API calls, in seconds.\n        tls (bool or :py:class:`~docker.tls.TLSConfig`): Enable TLS. Pass\n            ``True`` to enable it with default options, or pass a\n            :py:class:`~docker.tls.TLSConfig` object to use custom\n            configuration.\n        user_agent (str): Set a custom user agent for requests to the server.\n        credstore_env (dict): Override environment variables when calling the\n            credential store process.\n    ",
        "klass": "docker.APIClient",
        "module": "docker"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A client for communicating with a Docker server.\n\n    Example:\n\n        >>> import docker\n        >>> client = docker.DockerClient(base_url='unix://var/run/docker.sock')\n\n    Args:\n        base_url (str): URL to the Docker server. For example,\n            ``unix:///var/run/docker.sock`` or ``tcp://127.0.0.1:1234``.\n        version (str): The version of the API to use. Set to ``auto`` to\n            automatically detect the server's version. Default: ``1.35``\n        timeout (int): Default timeout for API calls, in seconds.\n        tls (bool or :py:class:`~docker.tls.TLSConfig`): Enable TLS. Pass\n            ``True`` to enable it with default options, or pass a\n            :py:class:`~docker.tls.TLSConfig` object to use custom\n            configuration.\n        user_agent (str): Set a custom user agent for requests to the server.\n        credstore_env (dict): Override environment variables when calling the\n            credential store process.\n    ",
        "klass": "docker.DockerClient",
        "module": "docker"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    Provides a thread-safe dict-like container which maintains up to\n    ``maxsize`` keys while throwing away the least-recently-used keys beyond\n    ``maxsize``.\n\n    :param maxsize:\n        Maximum number of recent elements to retain.\n\n    :param dispose_func:\n        Every time an item is evicted from the container,\n        ``dispose_func(value)`` is called.  Callback which will get called\n    ",
        "klass": "urllib3._collections.RecentlyUsedContainer",
        "module": "urllib3"
    },
    {
        "base_classes": [
            "contextlib.ContextDecorator"
        ],
        "class_docstring": "\n    errstate(**kwargs)\n\n    Context manager for floating-point error handling.\n\n    Using an instance of `errstate` as a context manager allows statements in\n    that context to execute with a known error handling behavior. Upon entering\n    the context the error handling is set with `seterr` and `seterrcall`, and\n    upon exiting it is reset to what it was before.\n\n    ..  versionchanged:: 1.17.0\n        `errstate` is also usable as a function decorator, saving\n        a level of indentation if an entire function is wrapped.\n        See :py:class:`contextlib.ContextDecorator` for more information.\n\n    Parameters\n    ----------\n    kwargs : {divide, over, under, invalid}\n        Keyword arguments. The valid keywords are the possible floating-point\n        exceptions. Each keyword should have a string value that defines the\n        treatment for the particular error. Possible values are\n        {'ignore', 'warn', 'raise', 'call', 'print', 'log'}.\n\n    See Also\n    --------\n    seterr, geterr, seterrcall, geterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> from collections import OrderedDict\n    >>> olderr = np.seterr(all='ignore')  # Set error handling to known state.\n\n    >>> np.arange(3) / 0.\n    array([nan, inf, inf])\n    >>> with np.errstate(divide='warn'):\n    ...     np.arange(3) / 0.\n    array([nan, inf, inf])\n\n    >>> np.sqrt(-1)\n    nan\n    >>> with np.errstate(invalid='raise'):\n    ...     np.sqrt(-1)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 2, in <module>\n    FloatingPointError: invalid value encountered in sqrt\n\n    Outside the context the error handling behavior has not changed:\n\n    >>> OrderedDict(sorted(np.geterr().items()))\n    OrderedDict([('divide', 'ignore'), ('invalid', 'ignore'), ('over', 'ignore'), ('under', 'ignore')])\n\n    ",
        "klass": "scipy.errstate",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager and decorator doing much the same as\n    ``warnings.catch_warnings``.\n\n    However, it also provides a filter mechanism to work around\n    https://bugs.python.org/issue4180.\n\n    This bug causes Python before 3.4 to not reliably show warnings again\n    after they have been ignored once (even within catch_warnings). It\n    means that no \"ignore\" filter can be used easily, since following\n    tests might need to see the warning. Additionally it allows easier\n    specificity for testing warnings and can be nested.\n\n    Parameters\n    ----------\n    forwarding_rule : str, optional\n        One of \"always\", \"once\", \"module\", or \"location\". Analogous to\n        the usual warnings module filter mode, it is useful to reduce\n        noise mostly on the outmost level. Unsuppressed and unrecorded\n        warnings will be forwarded based on this rule. Defaults to \"always\".\n        \"location\" is equivalent to the warnings \"default\", match by exact\n        location the warning warning originated from.\n\n    Notes\n    -----\n    Filters added inside the context manager will be discarded again\n    when leaving it. Upon entering all filters defined outside a\n    context will be applied automatically.\n\n    When a recording filter is added, matching warnings are stored in the\n    ``log`` attribute as well as in the list returned by ``record``.\n\n    If filters are added and the ``module`` keyword is given, the\n    warning registry of this module will additionally be cleared when\n    applying it, entering the context, or exiting it. This could cause\n    warnings to appear a second time after leaving the context if they\n    were configured to be printed once (default) and were already\n    printed before the context was entered.\n\n    Nesting this context manager will work as expected when the\n    forwarding rule is \"always\" (default). Unfiltered and unrecorded\n    warnings will be passed out and be matched by the outer level.\n    On the outmost level they will be printed (or caught by another\n    warnings context). The forwarding rule argument can modify this\n    behaviour.\n\n    Like ``catch_warnings`` this context manager is not threadsafe.\n\n    Examples\n    --------\n\n    With a context manager::\n\n        with np.testing.suppress_warnings() as sup:\n            sup.filter(DeprecationWarning, \"Some text\")\n            sup.filter(module=np.ma.core)\n            log = sup.record(FutureWarning, \"Does this occur?\")\n            command_giving_warnings()\n            # The FutureWarning was given once, the filtered warnings were\n            # ignored. All other warnings abide outside settings (may be\n            # printed/error)\n            assert_(len(log) == 1)\n            assert_(len(sup.log) == 1)  # also stored in log attribute\n\n    Or as a decorator::\n\n        sup = np.testing.suppress_warnings()\n        sup.filter(module=np.ma.core)  # module must match exactly\n        @sup\n        def some_function():\n            # do something which causes a warning in np.ma.core\n            pass\n    ",
        "klass": "scipy._lib._numpy_compat.suppress_warnings",
        "module": "numpy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Parallelisation wrapper for working with map-like callables, such as\n    `multiprocessing.Pool.map`.\n\n    Parameters\n    ----------\n    pool : int or map-like callable\n        If `pool` is an integer, then it specifies the number of threads to\n        use for parallelization. If ``int(pool) == 1``, then no parallel\n        processing is used and the map builtin is used.\n        If ``pool == -1``, then the pool will utilise all available CPUs.\n        If `pool` is a map-like callable that follows the same\n        calling sequence as the built-in map function, then this callable is\n        used for parallelisation.\n    ",
        "klass": "scipy._lib._util.MapWrapper",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.integrate._ode.ode"
        ],
        "class_docstring": "\n    A wrapper of ode for complex systems.\n\n    This functions similarly as `ode`, but re-maps a complex-valued\n    equation system to a real-valued one before using the integrators.\n\n    Parameters\n    ----------\n    f : callable ``f(t, y, *f_args)``\n        Rhs of the equation. t is a scalar, ``y.shape == (n,)``.\n        ``f_args`` is set by calling ``set_f_params(*args)``.\n    jac : callable ``jac(t, y, *jac_args)``\n        Jacobian of the rhs, ``jac[i,j] = d f[i] / d y[j]``.\n        ``jac_args`` is set by calling ``set_f_params(*args)``.\n\n    Attributes\n    ----------\n    t : float\n        Current time.\n    y : ndarray\n        Current variable values.\n\n    Examples\n    --------\n    For usage examples, see `ode`.\n\n    ",
        "klass": "scipy.integrate.complex_ode",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A generic interface class to numeric integrators.\n\n    Solve an equation system :math:`y'(t) = f(t,y)` with (optional) ``jac = df/dy``.\n\n    *Note*: The first two arguments of ``f(t, y, ...)`` are in the\n    opposite order of the arguments in the system definition function used\n    by `scipy.integrate.odeint`.\n\n    Parameters\n    ----------\n    f : callable ``f(t, y, *f_args)``\n        Right-hand side of the differential equation. t is a scalar,\n        ``y.shape == (n,)``.\n        ``f_args`` is set by calling ``set_f_params(*args)``.\n        `f` should return a scalar, array or list (not a tuple).\n    jac : callable ``jac(t, y, *jac_args)``, optional\n        Jacobian of the right-hand side, ``jac[i,j] = d f[i] / d y[j]``.\n        ``jac_args`` is set by calling ``set_jac_params(*args)``.\n\n    Attributes\n    ----------\n    t : float\n        Current time.\n    y : ndarray\n        Current variable values.\n\n    See also\n    --------\n    odeint : an integrator with a simpler interface based on lsoda from ODEPACK\n    quad : for finding the area under a curve\n\n    Notes\n    -----\n    Available integrators are listed below. They can be selected using\n    the `set_integrator` method.\n\n    \"vode\"\n\n        Real-valued Variable-coefficient Ordinary Differential Equation\n        solver, with fixed-leading-coefficient implementation. It provides\n        implicit Adams method (for non-stiff problems) and a method based on\n        backward differentiation formulas (BDF) (for stiff problems).\n\n        Source: http://www.netlib.org/ode/vode.f\n\n        .. warning::\n\n           This integrator is not re-entrant. You cannot have two `ode`\n           instances using the \"vode\" integrator at the same time.\n\n        This integrator accepts the following parameters in `set_integrator`\n        method of the `ode` class:\n\n        - atol : float or sequence\n          absolute tolerance for solution\n        - rtol : float or sequence\n          relative tolerance for solution\n        - lband : None or int\n        - uband : None or int\n          Jacobian band width, jac[i,j] != 0 for i-lband <= j <= i+uband.\n          Setting these requires your jac routine to return the jacobian\n          in packed format, jac_packed[i-j+uband, j] = jac[i,j]. The\n          dimension of the matrix must be (lband+uband+1, len(y)).\n        - method: 'adams' or 'bdf'\n          Which solver to use, Adams (non-stiff) or BDF (stiff)\n        - with_jacobian : bool\n          This option is only considered when the user has not supplied a\n          Jacobian function and has not indicated (by setting either band)\n          that the Jacobian is banded.  In this case, `with_jacobian` specifies\n          whether the iteration method of the ODE solver's correction step is\n          chord iteration with an internally generated full Jacobian or\n          functional iteration with no Jacobian.\n        - nsteps : int\n          Maximum number of (internally defined) steps allowed during one\n          call to the solver.\n        - first_step : float\n        - min_step : float\n        - max_step : float\n          Limits for the step sizes used by the integrator.\n        - order : int\n          Maximum order used by the integrator,\n          order <= 12 for Adams, <= 5 for BDF.\n\n    \"zvode\"\n\n        Complex-valued Variable-coefficient Ordinary Differential Equation\n        solver, with fixed-leading-coefficient implementation.  It provides\n        implicit Adams method (for non-stiff problems) and a method based on\n        backward differentiation formulas (BDF) (for stiff problems).\n\n        Source: http://www.netlib.org/ode/zvode.f\n\n        .. warning::\n\n           This integrator is not re-entrant. You cannot have two `ode`\n           instances using the \"zvode\" integrator at the same time.\n\n        This integrator accepts the same parameters in `set_integrator`\n        as the \"vode\" solver.\n\n        .. note::\n\n            When using ZVODE for a stiff system, it should only be used for\n            the case in which the function f is analytic, that is, when each f(i)\n            is an analytic function of each y(j).  Analyticity means that the\n            partial derivative df(i)/dy(j) is a unique complex number, and this\n            fact is critical in the way ZVODE solves the dense or banded linear\n            systems that arise in the stiff case.  For a complex stiff ODE system\n            in which f is not analytic, ZVODE is likely to have convergence\n            failures, and for this problem one should instead use DVODE on the\n            equivalent real system (in the real and imaginary parts of y).\n\n    \"lsoda\"\n\n        Real-valued Variable-coefficient Ordinary Differential Equation\n        solver, with fixed-leading-coefficient implementation. It provides\n        automatic method switching between implicit Adams method (for non-stiff\n        problems) and a method based on backward differentiation formulas (BDF)\n        (for stiff problems).\n\n        Source: http://www.netlib.org/odepack\n\n        .. warning::\n\n           This integrator is not re-entrant. You cannot have two `ode`\n           instances using the \"lsoda\" integrator at the same time.\n\n        This integrator accepts the following parameters in `set_integrator`\n        method of the `ode` class:\n\n        - atol : float or sequence\n          absolute tolerance for solution\n        - rtol : float or sequence\n          relative tolerance for solution\n        - lband : None or int\n        - uband : None or int\n          Jacobian band width, jac[i,j] != 0 for i-lband <= j <= i+uband.\n          Setting these requires your jac routine to return the jacobian\n          in packed format, jac_packed[i-j+uband, j] = jac[i,j].\n        - with_jacobian : bool\n          *Not used.*\n        - nsteps : int\n          Maximum number of (internally defined) steps allowed during one\n          call to the solver.\n        - first_step : float\n        - min_step : float\n        - max_step : float\n          Limits for the step sizes used by the integrator.\n        - max_order_ns : int\n          Maximum order used in the nonstiff case (default 12).\n        - max_order_s : int\n          Maximum order used in the stiff case (default 5).\n        - max_hnil : int\n          Maximum number of messages reporting too small step size (t + h = t)\n          (default 0)\n        - ixpr : int\n          Whether to generate extra printing at method switches (default False).\n\n    \"dopri5\"\n\n        This is an explicit runge-kutta method of order (4)5 due to Dormand &\n        Prince (with stepsize control and dense output).\n\n        Authors:\n\n            E. Hairer and G. Wanner\n            Universite de Geneve, Dept. de Mathematiques\n            CH-1211 Geneve 24, Switzerland\n            e-mail:  ernst.hairer@math.unige.ch, gerhard.wanner@math.unige.ch\n\n        This code is described in [HNW93]_.\n\n        This integrator accepts the following parameters in set_integrator()\n        method of the ode class:\n\n        - atol : float or sequence\n          absolute tolerance for solution\n        - rtol : float or sequence\n          relative tolerance for solution\n        - nsteps : int\n          Maximum number of (internally defined) steps allowed during one\n          call to the solver.\n        - first_step : float\n        - max_step : float\n        - safety : float\n          Safety factor on new step selection (default 0.9)\n        - ifactor : float\n        - dfactor : float\n          Maximum factor to increase/decrease step size by in one step\n        - beta : float\n          Beta parameter for stabilised step size control.\n        - verbosity : int\n          Switch for printing messages (< 0 for no messages).\n\n    \"dop853\"\n\n        This is an explicit runge-kutta method of order 8(5,3) due to Dormand\n        & Prince (with stepsize control and dense output).\n\n        Options and references the same as \"dopri5\".\n\n    Examples\n    --------\n\n    A problem to integrate and the corresponding jacobian:\n\n    >>> from scipy.integrate import ode\n    >>>\n    >>> y0, t0 = [1.0j, 2.0], 0\n    >>>\n    >>> def f(t, y, arg1):\n    ...     return [1j*arg1*y[0] + y[1], -arg1*y[1]**2]\n    >>> def jac(t, y, arg1):\n    ...     return [[1j*arg1, 1], [0, -arg1*2*y[1]]]\n\n    The integration:\n\n    >>> r = ode(f, jac).set_integrator('zvode', method='bdf')\n    >>> r.set_initial_value(y0, t0).set_f_params(2.0).set_jac_params(2.0)\n    >>> t1 = 10\n    >>> dt = 1\n    >>> while r.successful() and r.t < t1:\n    ...     print(r.t+dt, r.integrate(r.t+dt))\n    1 [-0.71038232+0.23749653j  0.40000271+0.j        ]\n    2.0 [0.19098503-0.52359246j 0.22222356+0.j        ]\n    3.0 [0.47153208+0.52701229j 0.15384681+0.j        ]\n    4.0 [-0.61905937+0.30726255j  0.11764744+0.j        ]\n    5.0 [0.02340997-0.61418799j 0.09523835+0.j        ]\n    6.0 [0.58643071+0.339819j 0.08000018+0.j      ]\n    7.0 [-0.52070105+0.44525141j  0.06896565+0.j        ]\n    8.0 [-0.15986733-0.61234476j  0.06060616+0.j        ]\n    9.0 [0.64850462+0.15048982j 0.05405414+0.j        ]\n    10.0 [-0.38404699+0.56382299j  0.04878055+0.j        ]\n\n    References\n    ----------\n    .. [HNW93] E. Hairer, S.P. Norsett and G. Wanner, Solving Ordinary\n        Differential Equations i. Nonstiff Problems. 2nd edition.\n        Springer Series in Computational Mathematics,\n        Springer-Verlag (1993)\n\n    ",
        "klass": "scipy.integrate.ode",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.fitpack2.UnivariateSpline"
        ],
        "class_docstring": "\n    One-dimensional interpolating spline for a given set of data points.\n\n    Fits a spline y = spl(x) of degree `k` to the provided `x`, `y` data.\n    Spline function passes through all provided points. Equivalent to\n    `UnivariateSpline` with  s=0.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        Input dimension of data points -- must be increasing\n    y : (N,) array_like\n        input dimension of data points\n    w : (N,) array_like, optional\n        Weights for spline fitting.  Must be positive.  If None (default),\n        weights are all equal.\n    bbox : (2,) array_like, optional\n        2-sequence specifying the boundary of the approximation interval. If\n        None (default), ``bbox=[x[0], x[-1]]``.\n    k : int, optional\n        Degree of the smoothing spline.  Must be 1 <= `k` <= 5.\n    ext : int or str, optional\n        Controls the extrapolation mode for elements\n        not in the interval defined by the knot sequence.\n\n        * if ext=0 or 'extrapolate', return the extrapolated value.\n        * if ext=1 or 'zeros', return 0\n        * if ext=2 or 'raise', raise a ValueError\n        * if ext=3 of 'const', return the boundary value.\n\n        The default value is 0.\n\n    check_finite : bool, optional\n        Whether to check that the input arrays contain only finite numbers.\n        Disabling may give a performance gain, but may result in problems\n        (crashes, non-termination or non-sensical results) if the inputs\n        do contain infinities or NaNs.\n        Default is False.\n\n    See Also\n    --------\n    UnivariateSpline : Superclass -- allows knots to be selected by a\n        smoothing condition\n    LSQUnivariateSpline : spline for which knots are user-selected\n    splrep : An older, non object-oriented wrapping of FITPACK\n    splev, sproot, splint, spalde\n    BivariateSpline : A similar class for two-dimensional spline interpolation\n\n    Notes\n    -----\n    The number of data points must be larger than the spline degree `k`.\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.interpolate import InterpolatedUnivariateSpline\n    >>> x = np.linspace(-3, 3, 50)\n    >>> y = np.exp(-x**2) + 0.1 * np.random.randn(50)\n    >>> spl = InterpolatedUnivariateSpline(x, y)\n    >>> plt.plot(x, y, 'ro', ms=5)\n    >>> xs = np.linspace(-3, 3, 1000)\n    >>> plt.plot(xs, spl(xs), 'g', lw=3, alpha=0.7)\n    >>> plt.show()\n\n    Notice that the ``spl(x)`` interpolates `y`:\n\n    >>> spl.get_residual()\n    0.0\n\n    ",
        "klass": "scipy.interpolate.InterpolatedUnivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.fitpack2.BivariateSpline"
        ],
        "class_docstring": "\n    Weighted least-squares bivariate spline approximation.\n\n    Parameters\n    ----------\n    x, y, z : array_like\n        1-D sequences of data points (order is not important).\n    tx, ty : array_like\n        Strictly ordered 1-D sequences of knots coordinates.\n    w : array_like, optional\n        Positive 1-D array of weights, of the same length as `x`, `y` and `z`.\n    bbox : (4,) array_like, optional\n        Sequence of length 4 specifying the boundary of the rectangular\n        approximation domain.  By default,\n        ``bbox=[min(x,tx),max(x,tx), min(y,ty),max(y,ty)]``.\n    kx, ky : ints, optional\n        Degrees of the bivariate spline. Default is 3.\n    eps : float, optional\n        A threshold for determining the effective rank of an over-determined\n        linear system of equations. `eps` should have a value between 0 and 1,\n        the default is 1e-16.\n\n    See Also\n    --------\n    bisplrep : an older wrapping of FITPACK\n    bisplev : an older wrapping of FITPACK\n    UnivariateSpline : a similar class for univariate spline interpolation\n    SmoothBivariateSpline : create a smoothing BivariateSpline\n\n    Notes\n    -----\n    The length of `x`, `y` and `z` should be at least ``(kx+1) * (ky+1)``.\n\n    ",
        "klass": "scipy.interpolate.fitpack2.LSQBivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.interpnd.NDInterpolatorBase"
        ],
        "class_docstring": "\n    LinearNDInterpolator(points, values, fill_value=np.nan, rescale=False)\n\n    Piecewise linear interpolant in N dimensions.\n\n    .. versionadded:: 0.9\n\n    Methods\n    -------\n    __call__\n\n    Parameters\n    ----------\n    points : ndarray of floats, shape (npoints, ndims); or Delaunay\n        Data point coordinates, or a precomputed Delaunay triangulation.\n    values : ndarray of float or complex, shape (npoints, ...)\n        Data values.\n    fill_value : float, optional\n        Value used to fill in for requested points outside of the\n        convex hull of the input points.  If not provided, then\n        the default is ``nan``.\n    rescale : bool, optional\n        Rescale points to unit cube before performing interpolation.\n        This is useful if some of the input dimensions have\n        incommensurable units and differ by many orders of magnitude.\n\n    Notes\n    -----\n    The interpolant is constructed by triangulating the input data\n    with Qhull [1]_, and on each triangle performing linear\n    barycentric interpolation.\n\n    References\n    ----------\n    .. [1] http://www.qhull.org/\n\n    ",
        "klass": "scipy.interpolate.LinearNDInterpolator",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.interpnd.NDInterpolatorBase"
        ],
        "class_docstring": "\n    NearestNDInterpolator(x, y)\n\n    Nearest-neighbour interpolation in N dimensions.\n\n    .. versionadded:: 0.9\n\n    Methods\n    -------\n    __call__\n\n    Parameters\n    ----------\n    x : (Npoints, Ndims) ndarray of floats\n        Data point coordinates.\n    y : (Npoints,) ndarray of float or complex\n        Data values.\n    rescale : boolean, optional\n        Rescale points to unit cube before performing interpolation.\n        This is useful if some of the input dimensions have\n        incommensurable units and differ by many orders of magnitude.\n\n        .. versionadded:: 0.14.0\n    tree_options : dict, optional\n        Options passed to the underlying ``cKDTree``.\n\n        .. versionadded:: 0.17.0\n\n\n    Notes\n    -----\n    Uses ``scipy.spatial.cKDTree``\n\n    ",
        "klass": "scipy.interpolate.NearestNDInterpolator",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate._cubic.CubicHermiteSpline"
        ],
        "class_docstring": "PCHIP 1-d monotonic cubic interpolation.\n\n    ``x`` and ``y`` are arrays of values used to approximate some function f,\n    with ``y = f(x)``. The interpolant uses monotonic cubic splines\n    to find the value of new points. (PCHIP stands for Piecewise Cubic\n    Hermite Interpolating Polynomial).\n\n    Parameters\n    ----------\n    x : ndarray\n        A 1-D array of monotonically increasing real values. ``x`` cannot\n        include duplicate values (otherwise f is overspecified)\n    y : ndarray\n        A 1-D array of real values. ``y``'s length along the interpolation\n        axis must be equal to the length of ``x``. If N-D array, use ``axis``\n        parameter to select correct axis.\n    axis : int, optional\n        Axis in the y array corresponding to the x-coordinate values.\n    extrapolate : bool, optional\n        Whether to extrapolate to out-of-bounds points based on first\n        and last intervals, or to return NaNs.\n\n    Methods\n    -------\n    __call__\n    derivative\n    antiderivative\n    roots\n\n    See Also\n    --------\n    CubicHermiteSpline\n    Akima1DInterpolator\n    CubicSpline\n    PPoly\n\n    Notes\n    -----\n    The interpolator preserves monotonicity in the interpolation data and does\n    not overshoot if the data is not smooth.\n\n    The first derivatives are guaranteed to be continuous, but the second\n    derivatives may jump at :math:`x_k`.\n\n    Determines the derivatives at the points :math:`x_k`, :math:`f'_k`,\n    by using PCHIP algorithm [1]_.\n\n    Let :math:`h_k = x_{k+1} - x_k`, and  :math:`d_k = (y_{k+1} - y_k) / h_k`\n    are the slopes at internal points :math:`x_k`.\n    If the signs of :math:`d_k` and :math:`d_{k-1}` are different or either of\n    them equals zero, then :math:`f'_k = 0`. Otherwise, it is given by the\n    weighted harmonic mean\n\n    .. math::\n\n        \\frac{w_1 + w_2}{f'_k} = \\frac{w_1}{d_{k-1}} + \\frac{w_2}{d_k}\n\n    where :math:`w_1 = 2 h_k + h_{k-1}` and :math:`w_2 = h_k + 2 h_{k-1}`.\n\n    The end slopes are set using a one-sided scheme [2]_.\n\n\n    References\n    ----------\n    .. [1] F. N. Fritsch and R. E. Carlson, Monotone Piecewise Cubic Interpolation,\n           SIAM J. Numer. Anal., 17(2), 238 (1980).\n           :doi:`10.1137/0717021`.\n    .. [2] see, e.g., C. Moler, Numerical Computing with Matlab, 2004.\n           :doi:`10.1137/1.9780898717952`\n\n\n    ",
        "klass": "scipy.interpolate.PchipInterpolator",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Rbf(*args)\n\n    A class for radial basis function approximation/interpolation of\n    n-dimensional scattered data.\n\n    Parameters\n    ----------\n    *args : arrays\n        x, y, z, ..., d, where x, y, z, ... are the coordinates of the nodes\n        and d is the array of values at the nodes\n    function : str or callable, optional\n        The radial basis function, based on the radius, r, given by the norm\n        (default is Euclidean distance); the default is 'multiquadric'::\n\n            'multiquadric': sqrt((r/self.epsilon)**2 + 1)\n            'inverse': 1.0/sqrt((r/self.epsilon)**2 + 1)\n            'gaussian': exp(-(r/self.epsilon)**2)\n            'linear': r\n            'cubic': r**3\n            'quintic': r**5\n            'thin_plate': r**2 * log(r)\n\n        If callable, then it must take 2 arguments (self, r).  The epsilon\n        parameter will be available as self.epsilon.  Other keyword\n        arguments passed in will be available as well.\n\n    epsilon : float, optional\n        Adjustable constant for gaussian or multiquadrics functions\n        - defaults to approximate average distance between nodes (which is\n        a good start).\n    smooth : float, optional\n        Values greater than zero increase the smoothness of the\n        approximation.  0 is for interpolation (default), the function will\n        always go through the nodal points in this case.\n    norm : str, callable, optional\n        A function that returns the 'distance' between two points, with\n        inputs as arrays of positions (x, y, z, ...), and an output as an\n        array of distance. E.g., the default: 'euclidean', such that the result\n        is a matrix of the distances from each point in ``x1`` to each point in\n        ``x2``. For more options, see documentation of\n        `scipy.spatial.distances.cdist`.\n\n    Attributes\n    ----------\n    N : int\n        The number of data points (as determined by the input arrays).\n    di : ndarray\n        The 1-D array of data values at each of the data coordinates `xi`.\n    xi : ndarray\n        The 2-D array of data coordinates.\n    function : str or callable\n        The radial basis function.  See description under Parameters.\n    epsilon : float\n        Parameter used by gaussian or multiquadrics functions.  See Parameters.\n    smooth : float\n        Smoothing parameter.  See description under Parameters.\n    norm : str or callable\n        The distance function.  See description under Parameters.\n    nodes : ndarray\n        A 1-D array of node values for the interpolation.\n    A : internal property, do not use\n\n    Examples\n    --------\n    >>> from scipy.interpolate import Rbf\n    >>> x, y, z, d = np.random.rand(4, 50)\n    >>> rbfi = Rbf(x, y, z, d)  # radial basis function interpolator instance\n    >>> xi = yi = zi = np.linspace(0, 1, 20)\n    >>> di = rbfi(xi, yi, zi)   # interpolated values\n    >>> di.shape\n    (20,)\n\n    ",
        "klass": "scipy.interpolate.Rbf",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.fitpack2.BivariateSpline"
        ],
        "class_docstring": "\n    Bivariate spline approximation over a rectangular mesh.\n\n    Can be used for both smoothing and interpolating data.\n\n    Parameters\n    ----------\n    x,y : array_like\n        1-D arrays of coordinates in strictly ascending order.\n    z : array_like\n        2-D array of data with shape (x.size,y.size).\n    bbox : array_like, optional\n        Sequence of length 4 specifying the boundary of the rectangular\n        approximation domain.  By default,\n        ``bbox=[min(x,tx),max(x,tx), min(y,ty),max(y,ty)]``.\n    kx, ky : ints, optional\n        Degrees of the bivariate spline. Default is 3.\n    s : float, optional\n        Positive smoothing factor defined for estimation condition:\n        ``sum((w[i]*(z[i]-s(x[i], y[i])))**2, axis=0) <= s``\n        Default is ``s=0``, which is for interpolation.\n\n    See Also\n    --------\n    SmoothBivariateSpline : a smoothing bivariate spline for scattered data\n    bisplrep : an older wrapping of FITPACK\n    bisplev : an older wrapping of FITPACK\n    UnivariateSpline : a similar class for univariate spline interpolation\n\n    ",
        "klass": "scipy.interpolate.RectBivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.fitpack2.SphereBivariateSpline"
        ],
        "class_docstring": "\n    Bivariate spline approximation over a rectangular mesh on a sphere.\n\n    Can be used for smoothing data.\n\n    .. versionadded:: 0.11.0\n\n    Parameters\n    ----------\n    u : array_like\n        1-D array of latitude coordinates in strictly ascending order.\n        Coordinates must be given in radians and lie within the interval\n        (0, pi).\n    v : array_like\n        1-D array of longitude coordinates in strictly ascending order.\n        Coordinates must be given in radians. First element (v[0]) must lie\n        within the interval [-pi, pi). Last element (v[-1]) must satisfy\n        v[-1] <= v[0] + 2*pi.\n    r : array_like\n        2-D array of data with shape ``(u.size, v.size)``.\n    s : float, optional\n        Positive smoothing factor defined for estimation condition\n        (``s=0`` is for interpolation).\n    pole_continuity : bool or (bool, bool), optional\n        Order of continuity at the poles ``u=0`` (``pole_continuity[0]``) and\n        ``u=pi`` (``pole_continuity[1]``).  The order of continuity at the pole\n        will be 1 or 0 when this is True or False, respectively.\n        Defaults to False.\n    pole_values : float or (float, float), optional\n        Data values at the poles ``u=0`` and ``u=pi``.  Either the whole\n        parameter or each individual element can be None.  Defaults to None.\n    pole_exact : bool or (bool, bool), optional\n        Data value exactness at the poles ``u=0`` and ``u=pi``.  If True, the\n        value is considered to be the right function value, and it will be\n        fitted exactly. If False, the value will be considered to be a data\n        value just like the other data values.  Defaults to False.\n    pole_flat : bool or (bool, bool), optional\n        For the poles at ``u=0`` and ``u=pi``, specify whether or not the\n        approximation has vanishing derivatives.  Defaults to False.\n\n    See Also\n    --------\n    RectBivariateSpline : bivariate spline approximation over a rectangular\n        mesh\n\n    Notes\n    -----\n    Currently, only the smoothing spline approximation (``iopt[0] = 0`` and\n    ``iopt[0] = 1`` in the FITPACK routine) is supported.  The exact\n    least-squares spline approximation is not implemented yet.\n\n    When actually performing the interpolation, the requested `v` values must\n    lie within the same length 2pi interval that the original `v` values were\n    chosen from.\n\n    For more information, see the FITPACK_ site about this function.\n\n    .. _FITPACK: http://www.netlib.org/dierckx/spgrid.f\n\n    Examples\n    --------\n    Suppose we have global data on a coarse grid\n\n    >>> lats = np.linspace(10, 170, 9) * np.pi / 180.\n    >>> lons = np.linspace(0, 350, 18) * np.pi / 180.\n    >>> data = np.dot(np.atleast_2d(90. - np.linspace(-80., 80., 18)).T,\n    ...               np.atleast_2d(180. - np.abs(np.linspace(0., 350., 9)))).T\n\n    We want to interpolate it to a global one-degree grid\n\n    >>> new_lats = np.linspace(1, 180, 180) * np.pi / 180\n    >>> new_lons = np.linspace(1, 360, 360) * np.pi / 180\n    >>> new_lats, new_lons = np.meshgrid(new_lats, new_lons)\n\n    We need to set up the interpolator object\n\n    >>> from scipy.interpolate import RectSphereBivariateSpline\n    >>> lut = RectSphereBivariateSpline(lats, lons, data)\n\n    Finally we interpolate the data.  The `RectSphereBivariateSpline` object\n    only takes 1-D arrays as input, therefore we need to do some reshaping.\n\n    >>> data_interp = lut.ev(new_lats.ravel(),\n    ...                      new_lons.ravel()).reshape((360, 180)).T\n\n    Looking at the original and the interpolated data, one can see that the\n    interpolant reproduces the original data very well:\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig = plt.figure()\n    >>> ax1 = fig.add_subplot(211)\n    >>> ax1.imshow(data, interpolation='nearest')\n    >>> ax2 = fig.add_subplot(212)\n    >>> ax2.imshow(data_interp, interpolation='nearest')\n    >>> plt.show()\n\n    Choosing the optimal value of ``s`` can be a delicate task. Recommended\n    values for ``s`` depend on the accuracy of the data values.  If the user\n    has an idea of the statistical errors on the data, she can also find a\n    proper estimate for ``s``. By assuming that, if she specifies the\n    right ``s``, the interpolator will use a spline ``f(u,v)`` which exactly\n    reproduces the function underlying the data, she can evaluate\n    ``sum((r(i,j)-s(u(i),v(j)))**2)`` to find a good estimate for this ``s``.\n    For example, if she knows that the statistical errors on her\n    ``r(i,j)``-values are not greater than 0.1, she may expect that a good\n    ``s`` should have a value not larger than ``u.size * v.size * (0.1)**2``.\n\n    If nothing is known about the statistical error in ``r(i,j)``, ``s`` must\n    be determined by trial and error.  The best is then to start with a very\n    large value of ``s`` (to determine the least-squares polynomial and the\n    corresponding upper bound ``fp0`` for ``s``) and then to progressively\n    decrease the value of ``s`` (say by a factor 10 in the beginning, i.e.\n    ``s = fp0 / 10, fp0 / 100, ...``  and more carefully as the approximation\n    shows more detail) to obtain closer fits.\n\n    The interpolation results for different values of ``s`` give some insight\n    into this process:\n\n    >>> fig2 = plt.figure()\n    >>> s = [3e9, 2e9, 1e9, 1e8]\n    >>> for ii in range(len(s)):\n    ...     lut = RectSphereBivariateSpline(lats, lons, data, s=s[ii])\n    ...     data_interp = lut.ev(new_lats.ravel(),\n    ...                          new_lons.ravel()).reshape((360, 180)).T\n    ...     ax = fig2.add_subplot(2, 2, ii+1)\n    ...     ax.imshow(data_interp, interpolation='nearest')\n    ...     ax.set_title(\"s = %g\" % s[ii])\n    >>> plt.show()\n\n    ",
        "klass": "scipy.interpolate.RectSphereBivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.fitpack2.BivariateSpline"
        ],
        "class_docstring": "\n    Smooth bivariate spline approximation.\n\n    Parameters\n    ----------\n    x, y, z : array_like\n        1-D sequences of data points (order is not important).\n    w : array_like, optional\n        Positive 1-D sequence of weights, of same length as `x`, `y` and `z`.\n    bbox : array_like, optional\n        Sequence of length 4 specifying the boundary of the rectangular\n        approximation domain.  By default,\n        ``bbox=[min(x,tx),max(x,tx), min(y,ty),max(y,ty)]``.\n    kx, ky : ints, optional\n        Degrees of the bivariate spline. Default is 3.\n    s : float, optional\n        Positive smoothing factor defined for estimation condition:\n        ``sum((w[i]*(z[i]-s(x[i], y[i])))**2, axis=0) <= s``\n        Default ``s=len(w)`` which should be a good value if ``1/w[i]`` is an\n        estimate of the standard deviation of ``z[i]``.\n    eps : float, optional\n        A threshold for determining the effective rank of an over-determined\n        linear system of equations. `eps` should have a value between 0 and 1,\n        the default is 1e-16.\n\n    See Also\n    --------\n    bisplrep : an older wrapping of FITPACK\n    bisplev : an older wrapping of FITPACK\n    UnivariateSpline : a similar class for univariate spline interpolation\n    LSQUnivariateSpline : to create a BivariateSpline using weighted\n\n    Notes\n    -----\n    The length of `x`, `y` and `z` should be at least ``(kx+1) * (ky+1)``.\n\n    ",
        "klass": "scipy.interpolate.fitpack2.SmoothBivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    One-dimensional smoothing spline fit to a given set of data points.\n\n    Fits a spline y = spl(x) of degree `k` to the provided `x`, `y` data.  `s`\n    specifies the number of knots by specifying a smoothing condition.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        1-D array of independent input data. Must be increasing.\n    y : (N,) array_like\n        1-D array of dependent input data, of the same length as `x`.\n    w : (N,) array_like, optional\n        Weights for spline fitting.  Must be positive.  If None (default),\n        weights are all equal.\n    bbox : (2,) array_like, optional\n        2-sequence specifying the boundary of the approximation interval. If\n        None (default), ``bbox=[x[0], x[-1]]``.\n    k : int, optional\n        Degree of the smoothing spline.  Must be <= 5.\n        Default is k=3, a cubic spline.\n    s : float or None, optional\n        Positive smoothing factor used to choose the number of knots.  Number\n        of knots will be increased until the smoothing condition is satisfied::\n\n            sum((w[i] * (y[i]-spl(x[i])))**2, axis=0) <= s\n\n        If None (default), ``s = len(w)`` which should be a good value if\n        ``1/w[i]`` is an estimate of the standard deviation of ``y[i]``.\n        If 0, spline will interpolate through all data points.\n    ext : int or str, optional\n        Controls the extrapolation mode for elements\n        not in the interval defined by the knot sequence.\n\n        * if ext=0 or 'extrapolate', return the extrapolated value.\n        * if ext=1 or 'zeros', return 0\n        * if ext=2 or 'raise', raise a ValueError\n        * if ext=3 of 'const', return the boundary value.\n\n        The default value is 0.\n\n    check_finite : bool, optional\n        Whether to check that the input arrays contain only finite numbers.\n        Disabling may give a performance gain, but may result in problems\n        (crashes, non-termination or non-sensical results) if the inputs\n        do contain infinities or NaNs.\n        Default is False.\n\n    See Also\n    --------\n    InterpolatedUnivariateSpline : Subclass with smoothing forced to 0\n    LSQUnivariateSpline : Subclass in which knots are user-selected instead of\n        being set by smoothing condition\n    splrep : An older, non object-oriented wrapping of FITPACK\n    splev, sproot, splint, spalde\n    BivariateSpline : A similar class for two-dimensional spline interpolation\n\n    Notes\n    -----\n    The number of data points must be larger than the spline degree `k`.\n\n    **NaN handling**: If the input arrays contain ``nan`` values, the result\n    is not useful, since the underlying spline fitting routines cannot deal\n    with ``nan`` . A workaround is to use zero weights for not-a-number\n    data points:\n\n    >>> from scipy.interpolate import UnivariateSpline\n    >>> x, y = np.array([1, 2, 3, 4]), np.array([1, np.nan, 3, 4])\n    >>> w = np.isnan(y)\n    >>> y[w] = 0.\n    >>> spl = UnivariateSpline(x, y, w=~w)\n\n    Notice the need to replace a ``nan`` by a numerical value (precise value\n    does not matter as long as the corresponding weight is zero.)\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.interpolate import UnivariateSpline\n    >>> x = np.linspace(-3, 3, 50)\n    >>> y = np.exp(-x**2) + 0.1 * np.random.randn(50)\n    >>> plt.plot(x, y, 'ro', ms=5)\n\n    Use the default value for the smoothing parameter:\n\n    >>> spl = UnivariateSpline(x, y)\n    >>> xs = np.linspace(-3, 3, 1000)\n    >>> plt.plot(xs, spl(xs), 'g', lw=3)\n\n    Manually change the amount of smoothing:\n\n    >>> spl.set_smoothing_factor(0.5)\n    >>> plt.plot(xs, spl(xs), 'b', lw=3)\n    >>> plt.show()\n\n    ",
        "klass": "scipy.interpolate.UnivariateSpline",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.interpolate.polyint._Interpolator1D"
        ],
        "class_docstring": "\n    Interpolate a 1-D function.\n\n    `x` and `y` are arrays of values used to approximate some function f:\n    ``y = f(x)``.  This class returns a function whose call method uses\n    interpolation to find the value of new points.\n\n    Note that calling `interp1d` with NaNs present in input values results in\n    undefined behaviour.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        A 1-D array of real values.\n    y : (...,N,...) array_like\n        A N-D array of real values. The length of `y` along the interpolation\n        axis must be equal to the length of `x`.\n    kind : str or int, optional\n        Specifies the kind of interpolation as a string\n        ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n        'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n        refer to a spline interpolation of zeroth, first, second or third\n        order; 'previous' and 'next' simply return the previous or next value\n        of the point) or as an integer specifying the order of the spline\n        interpolator to use.\n        Default is 'linear'.\n    axis : int, optional\n        Specifies the axis of `y` along which to interpolate.\n        Interpolation defaults to the last axis of `y`.\n    copy : bool, optional\n        If True, the class makes internal copies of x and y.\n        If False, references to `x` and `y` are used. The default is to copy.\n    bounds_error : bool, optional\n        If True, a ValueError is raised any time interpolation is attempted on\n        a value outside of the range of x (where extrapolation is\n        necessary). If False, out of bounds values are assigned `fill_value`.\n        By default, an error is raised unless ``fill_value=\"extrapolate\"``.\n    fill_value : array-like or (array-like, array_like) or \"extrapolate\", optional\n        - if a ndarray (or float), this value will be used to fill in for\n          requested points outside of the data range. If not provided, then\n          the default is NaN. The array-like must broadcast properly to the\n          dimensions of the non-interpolation axes.\n        - If a two-element tuple, then the first element is used as a\n          fill value for ``x_new < x[0]`` and the second element is used for\n          ``x_new > x[-1]``. Anything that is not a 2-element tuple (e.g.,\n          list or ndarray, regardless of shape) is taken to be a single\n          array-like argument meant to be used for both bounds as\n          ``below, above = fill_value, fill_value``.\n\n          .. versionadded:: 0.17.0\n        - If \"extrapolate\", then points outside the data range will be\n          extrapolated.\n\n          .. versionadded:: 0.17.0\n    assume_sorted : bool, optional\n        If False, values of `x` can be in any order and they are sorted first.\n        If True, `x` has to be an array of monotonically increasing values.\n\n    Attributes\n    ----------\n    fill_value\n\n    Methods\n    -------\n    __call__\n\n    See Also\n    --------\n    splrep, splev\n        Spline interpolation/smoothing based on FITPACK.\n    UnivariateSpline : An object-oriented wrapper of the FITPACK routines.\n    interp2d : 2-D interpolation\n\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import interpolate\n    >>> x = np.arange(0, 10)\n    >>> y = np.exp(-x/3.0)\n    >>> f = interpolate.interp1d(x, y)\n\n    >>> xnew = np.arange(0, 9, 0.1)\n    >>> ynew = f(xnew)   # use interpolation function returned by `interp1d`\n    >>> plt.plot(x, y, 'o', xnew, ynew, '-')\n    >>> plt.show()\n    ",
        "klass": "scipy.interpolate.interp1d",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    interp2d(x, y, z, kind='linear', copy=True, bounds_error=False,\n             fill_value=None)\n\n    Interpolate over a 2-D grid.\n\n    `x`, `y` and `z` are arrays of values used to approximate some function\n    f: ``z = f(x, y)``. This class returns a function whose call method uses\n    spline interpolation to find the value of new points.\n\n    If `x` and `y` represent a regular grid, consider using\n    RectBivariateSpline.\n\n    Note that calling `interp2d` with NaNs present in input values results in\n    undefined behaviour.\n\n    Methods\n    -------\n    __call__\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays defining the data point coordinates.\n\n        If the points lie on a regular grid, `x` can specify the column\n        coordinates and `y` the row coordinates, for example::\n\n          >>> x = [0,1,2];  y = [0,3]; z = [[1,2,3], [4,5,6]]\n\n        Otherwise, `x` and `y` must specify the full coordinates for each\n        point, for example::\n\n          >>> x = [0,1,2,0,1,2];  y = [0,0,0,3,3,3]; z = [1,2,3,4,5,6]\n\n        If `x` and `y` are multi-dimensional, they are flattened before use.\n    z : array_like\n        The values of the function to interpolate at the data points. If\n        `z` is a multi-dimensional array, it is flattened before use.  The\n        length of a flattened `z` array is either\n        len(`x`)*len(`y`) if `x` and `y` specify the column and row coordinates\n        or ``len(z) == len(x) == len(y)`` if `x` and `y` specify coordinates\n        for each point.\n    kind : {'linear', 'cubic', 'quintic'}, optional\n        The kind of spline interpolation to use. Default is 'linear'.\n    copy : bool, optional\n        If True, the class makes internal copies of x, y and z.\n        If False, references may be used. The default is to copy.\n    bounds_error : bool, optional\n        If True, when interpolated values are requested outside of the\n        domain of the input data (x,y), a ValueError is raised.\n        If False, then `fill_value` is used.\n    fill_value : number, optional\n        If provided, the value to use for points outside of the\n        interpolation domain. If omitted (None), values outside\n        the domain are extrapolated.\n\n    See Also\n    --------\n    RectBivariateSpline :\n        Much faster 2D interpolation if your input data is on a grid\n    bisplrep, bisplev :\n        Spline interpolation based on FITPACK\n    BivariateSpline : a more recent wrapper of the FITPACK routines\n    interp1d : one dimension version of this function\n\n    Notes\n    -----\n    The minimum number of data points required along the interpolation\n    axis is ``(k+1)**2``, with k=1 for linear, k=3 for cubic and k=5 for\n    quintic interpolation.\n\n    The interpolator is constructed by `bisplrep`, with a smoothing factor\n    of 0. If more control over smoothing is needed, `bisplrep` should be\n    used directly.\n\n    Examples\n    --------\n    Construct a 2-D grid and interpolate on it:\n\n    >>> from scipy import interpolate\n    >>> x = np.arange(-5.01, 5.01, 0.25)\n    >>> y = np.arange(-5.01, 5.01, 0.25)\n    >>> xx, yy = np.meshgrid(x, y)\n    >>> z = np.sin(xx**2+yy**2)\n    >>> f = interpolate.interp2d(x, y, z, kind='cubic')\n\n    Now use the obtained interpolation function and plot the result:\n\n    >>> import matplotlib.pyplot as plt\n    >>> xnew = np.arange(-5.01, 5.01, 1e-2)\n    >>> ynew = np.arange(-5.01, 5.01, 1e-2)\n    >>> znew = f(xnew, ynew)\n    >>> plt.plot(x, z[0, :], 'ro-', xnew, znew[0, :], 'b-')\n    >>> plt.show()\n    ",
        "klass": "scipy.interpolate.interp2d",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A file object for unformatted sequential files from Fortran code.\n\n    Parameters\n    ----------\n    filename : file or str\n        Open file object or filename.\n    mode : {'r', 'w'}, optional\n        Read-write mode, default is 'r'.\n    header_dtype : dtype, optional\n        Data type of the header. Size and endiness must match the input/output file.\n\n    Notes\n    -----\n    These files are broken up into records of unspecified types. The size of\n    each record is given at the start (although the size of this header is not\n    standard) and the data is written onto disk without any formatting. Fortran\n    compilers supporting the BACKSPACE statement will write a second copy of\n    the size to facilitate backwards seeking.\n\n    This class only supports files written with both sizes for the record.\n    It also does not support the subrecords used in Intel and gfortran compilers\n    for records which are greater than 2GB with a 4-byte header.\n\n    An example of an unformatted sequential file in Fortran would be written as::\n\n        OPEN(1, FILE=myfilename, FORM='unformatted')\n\n        WRITE(1) myvariable\n\n    Since this is a non-standard file format, whose contents depend on the\n    compiler and the endianness of the machine, caution is advised. Files from\n    gfortran 4.8.0 and gfortran 4.1.2 on x86_64 are known to work.\n\n    Consider using Fortran direct-access files or files from the newer Stream\n    I/O, which can be easily read by `numpy.fromfile`.\n\n    Examples\n    --------\n    To create an unformatted sequential Fortran file:\n\n    >>> from scipy.io import FortranFile\n    >>> f = FortranFile('test.unf', 'w')\n    >>> f.write_record(np.array([1,2,3,4,5], dtype=np.int32))\n    >>> f.write_record(np.linspace(0,1,20).reshape((5,4)).T)\n    >>> f.close()\n\n    To read this file:\n\n    >>> f = FortranFile('test.unf', 'r')\n    >>> print(f.read_ints(np.int32))\n    [1 2 3 4 5]\n    >>> print(f.read_reals(float).reshape((5,4), order=\"F\"))\n    [[0.         0.05263158 0.10526316 0.15789474]\n     [0.21052632 0.26315789 0.31578947 0.36842105]\n     [0.42105263 0.47368421 0.52631579 0.57894737]\n     [0.63157895 0.68421053 0.73684211 0.78947368]\n     [0.84210526 0.89473684 0.94736842 1.        ]]\n    >>> f.close()\n\n    Or, in Fortran::\n\n        integer :: a(5), i\n        double precision :: b(5,4)\n        open(1, file='test.unf', form='unformatted')\n        read(1) a\n        read(1) b\n        close(1)\n        write(*,*) a\n        do i = 1, 5\n            write(*,*) b(i,:)\n        end do\n\n    ",
        "klass": "scipy.io.FortranFile",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A file object for NetCDF data.\n\n    A `netcdf_file` object has two standard attributes: `dimensions` and\n    `variables`. The values of both are dictionaries, mapping dimension\n    names to their associated lengths and variable names to variables,\n    respectively. Application programs should never modify these\n    dictionaries.\n\n    All other attributes correspond to global attributes defined in the\n    NetCDF file. Global file attributes are created by assigning to an\n    attribute of the `netcdf_file` object.\n\n    Parameters\n    ----------\n    filename : string or file-like\n        string -> filename\n    mode : {'r', 'w', 'a'}, optional\n        read-write-append mode, default is 'r'\n    mmap : None or bool, optional\n        Whether to mmap `filename` when reading.  Default is True\n        when `filename` is a file name, False when `filename` is a\n        file-like object. Note that when mmap is in use, data arrays\n        returned refer directly to the mmapped data on disk, and the\n        file cannot be closed as long as references to it exist.\n    version : {1, 2}, optional\n        version of netcdf to read / write, where 1 means *Classic\n        format* and 2 means *64-bit offset format*.  Default is 1.  See\n        `here <https://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html#select_format>`__\n        for more info.\n    maskandscale : bool, optional\n        Whether to automatically scale and/or mask data based on attributes.\n        Default is False.\n\n    Notes\n    -----\n    The major advantage of this module over other modules is that it doesn't\n    require the code to be linked to the NetCDF libraries. This module is\n    derived from `pupynere <https://bitbucket.org/robertodealmeida/pupynere/>`_.\n\n    NetCDF files are a self-describing binary data format. The file contains\n    metadata that describes the dimensions and variables in the file. More\n    details about NetCDF files can be found `here\n    <https://www.unidata.ucar.edu/software/netcdf/docs/user_guide.html>`__. There\n    are three main sections to a NetCDF data structure:\n\n    1. Dimensions\n    2. Variables\n    3. Attributes\n\n    The dimensions section records the name and length of each dimension used\n    by the variables. The variables would then indicate which dimensions it\n    uses and any attributes such as data units, along with containing the data\n    values for the variable. It is good practice to include a\n    variable that is the same name as a dimension to provide the values for\n    that axes. Lastly, the attributes section would contain additional\n    information such as the name of the file creator or the instrument used to\n    collect the data.\n\n    When writing data to a NetCDF file, there is often the need to indicate the\n    'record dimension'. A record dimension is the unbounded dimension for a\n    variable. For example, a temperature variable may have dimensions of\n    latitude, longitude and time. If one wants to add more temperature data to\n    the NetCDF file as time progresses, then the temperature variable should\n    have the time dimension flagged as the record dimension.\n\n    In addition, the NetCDF file header contains the position of the data in\n    the file, so access can be done in an efficient manner without loading\n    unnecessary data into memory. It uses the ``mmap`` module to create\n    Numpy arrays mapped to the data on disk, for the same purpose.\n\n    Note that when `netcdf_file` is used to open a file with mmap=True\n    (default for read-only), arrays returned by it refer to data\n    directly on the disk. The file should not be closed, and cannot be cleanly\n    closed when asked, if such arrays are alive. You may want to copy data arrays\n    obtained from mmapped Netcdf file if they are to be processed after the file\n    is closed, see the example below.\n\n    Examples\n    --------\n    To create a NetCDF file:\n\n    >>> from scipy.io import netcdf\n    >>> f = netcdf.netcdf_file('simple.nc', 'w')\n    >>> f.history = 'Created for a test'\n    >>> f.createDimension('time', 10)\n    >>> time = f.createVariable('time', 'i', ('time',))\n    >>> time[:] = np.arange(10)\n    >>> time.units = 'days since 2008-01-01'\n    >>> f.close()\n\n    Note the assignment of ``arange(10)`` to ``time[:]``.  Exposing the slice\n    of the time variable allows for the data to be set in the object, rather\n    than letting ``arange(10)`` overwrite the ``time`` variable.\n\n    To read the NetCDF file we just created:\n\n    >>> from scipy.io import netcdf\n    >>> f = netcdf.netcdf_file('simple.nc', 'r')\n    >>> print(f.history)\n    b'Created for a test'\n    >>> time = f.variables['time']\n    >>> print(time.units)\n    b'days since 2008-01-01'\n    >>> print(time.shape)\n    (10,)\n    >>> print(time[-1])\n    9\n\n    NetCDF files, when opened read-only, return arrays that refer\n    directly to memory-mapped data on disk:\n\n    >>> data = time[:]\n    >>> data.base.base\n    <mmap.mmap object at 0x7fe753763180>\n\n    If the data is to be processed after the file is closed, it needs\n    to be copied to main memory:\n\n    >>> data = time[:].copy()\n    >>> f.close()\n    >>> data.mean()\n    4.5\n\n    A NetCDF file can also be used as context manager:\n\n    >>> from scipy.io import netcdf\n    >>> with netcdf.netcdf_file('simple.nc', 'r') as f:\n    ...     print(f.history)\n    b'Created for a test'\n\n    ",
        "klass": "scipy.io.netcdf.netcdf_file",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A file object for NetCDF data.\n\n    A `netcdf_file` object has two standard attributes: `dimensions` and\n    `variables`. The values of both are dictionaries, mapping dimension\n    names to their associated lengths and variable names to variables,\n    respectively. Application programs should never modify these\n    dictionaries.\n\n    All other attributes correspond to global attributes defined in the\n    NetCDF file. Global file attributes are created by assigning to an\n    attribute of the `netcdf_file` object.\n\n    Parameters\n    ----------\n    filename : string or file-like\n        string -> filename\n    mode : {'r', 'w', 'a'}, optional\n        read-write-append mode, default is 'r'\n    mmap : None or bool, optional\n        Whether to mmap `filename` when reading.  Default is True\n        when `filename` is a file name, False when `filename` is a\n        file-like object. Note that when mmap is in use, data arrays\n        returned refer directly to the mmapped data on disk, and the\n        file cannot be closed as long as references to it exist.\n    version : {1, 2}, optional\n        version of netcdf to read / write, where 1 means *Classic\n        format* and 2 means *64-bit offset format*.  Default is 1.  See\n        `here <https://www.unidata.ucar.edu/software/netcdf/docs/netcdf_introduction.html#select_format>`__\n        for more info.\n    maskandscale : bool, optional\n        Whether to automatically scale and/or mask data based on attributes.\n        Default is False.\n\n    Notes\n    -----\n    The major advantage of this module over other modules is that it doesn't\n    require the code to be linked to the NetCDF libraries. This module is\n    derived from `pupynere <https://bitbucket.org/robertodealmeida/pupynere/>`_.\n\n    NetCDF files are a self-describing binary data format. The file contains\n    metadata that describes the dimensions and variables in the file. More\n    details about NetCDF files can be found `here\n    <https://www.unidata.ucar.edu/software/netcdf/docs/user_guide.html>`__. There\n    are three main sections to a NetCDF data structure:\n\n    1. Dimensions\n    2. Variables\n    3. Attributes\n\n    The dimensions section records the name and length of each dimension used\n    by the variables. The variables would then indicate which dimensions it\n    uses and any attributes such as data units, along with containing the data\n    values for the variable. It is good practice to include a\n    variable that is the same name as a dimension to provide the values for\n    that axes. Lastly, the attributes section would contain additional\n    information such as the name of the file creator or the instrument used to\n    collect the data.\n\n    When writing data to a NetCDF file, there is often the need to indicate the\n    'record dimension'. A record dimension is the unbounded dimension for a\n    variable. For example, a temperature variable may have dimensions of\n    latitude, longitude and time. If one wants to add more temperature data to\n    the NetCDF file as time progresses, then the temperature variable should\n    have the time dimension flagged as the record dimension.\n\n    In addition, the NetCDF file header contains the position of the data in\n    the file, so access can be done in an efficient manner without loading\n    unnecessary data into memory. It uses the ``mmap`` module to create\n    Numpy arrays mapped to the data on disk, for the same purpose.\n\n    Note that when `netcdf_file` is used to open a file with mmap=True\n    (default for read-only), arrays returned by it refer to data\n    directly on the disk. The file should not be closed, and cannot be cleanly\n    closed when asked, if such arrays are alive. You may want to copy data arrays\n    obtained from mmapped Netcdf file if they are to be processed after the file\n    is closed, see the example below.\n\n    Examples\n    --------\n    To create a NetCDF file:\n\n    >>> from scipy.io import netcdf\n    >>> f = netcdf.netcdf_file('simple.nc', 'w')\n    >>> f.history = 'Created for a test'\n    >>> f.createDimension('time', 10)\n    >>> time = f.createVariable('time', 'i', ('time',))\n    >>> time[:] = np.arange(10)\n    >>> time.units = 'days since 2008-01-01'\n    >>> f.close()\n\n    Note the assignment of ``arange(10)`` to ``time[:]``.  Exposing the slice\n    of the time variable allows for the data to be set in the object, rather\n    than letting ``arange(10)`` overwrite the ``time`` variable.\n\n    To read the NetCDF file we just created:\n\n    >>> from scipy.io import netcdf\n    >>> f = netcdf.netcdf_file('simple.nc', 'r')\n    >>> print(f.history)\n    b'Created for a test'\n    >>> time = f.variables['time']\n    >>> print(time.units)\n    b'days since 2008-01-01'\n    >>> print(time.shape)\n    (10,)\n    >>> print(time[-1])\n    9\n\n    NetCDF files, when opened read-only, return arrays that refer\n    directly to memory-mapped data on disk:\n\n    >>> data = time[:]\n    >>> data.base.base\n    <mmap.mmap object at 0x7fe753763180>\n\n    If the data is to be processed after the file is closed, it needs\n    to be copied to main memory:\n\n    >>> data = time[:].copy()\n    >>> f.close()\n    >>> data.mean()\n    4.5\n\n    A NetCDF file can also be used as context manager:\n\n    >>> from scipy.io import netcdf\n    >>> with netcdf.netcdf_file('simple.nc', 'r') as f:\n    ...     print(f.history)\n    b'Created for a test'\n\n    ",
        "klass": "scipy.io.netcdf_file",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.io.matlab.streams.GenericStream"
        ],
        "class_docstring": "\n    File-like object uncompressing bytes from a zlib compressed stream.\n\n    Parameters\n    ----------\n    stream : file-like\n        Stream to read compressed data from.\n    max_length : int\n        Maximum number of bytes to read from the stream.\n\n    Notes\n    -----\n    Some matlab files contain zlib streams without valid Z_STREAM_END\n    termination.  To get round this, we use the decompressobj object, that\n    allows you to decode an incomplete stream.  See discussion at\n    https://bugs.python.org/issue8672\n\n    ",
        "klass": "scipy.io.matlab.streams.ZlibInputStream",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy"
        ],
        "class_docstring": "Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n\n    Parameters\n    ----------\n    exception_strategy : {'skip_update', 'damp_update'}, optional\n        Define how to proceed when the curvature condition is violated.\n        Set it to 'skip_update' to just skip the update. Or, alternatively,\n        set it to 'damp_update' to interpolate between the actual BFGS\n        result and the unmodified matrix. Both exceptions strategies\n        are explained  in [1]_, p.536-537.\n    min_curvature : float\n        This number, scaled by a normalization factor, defines the\n        minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n        unaffected by the exception strategy. By default is equal to\n        1e-8 when ``exception_strategy = 'skip_update'`` and equal\n        to 0.2 when ``exception_strategy = 'damp_update'``.\n    init_scale : {float, 'auto'}\n        Matrix scale at first iteration. At the first\n        iteration the Hessian matrix or its inverse will be initialized\n        with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n        Set it to 'auto' in order to use an automatic heuristic for choosing\n        the initial scale. The heuristic is described in [1]_, p.143.\n        By default uses 'auto'.\n\n    Notes\n    -----\n    The update is based on the description in [1]_, p.140.\n\n    References\n    ----------\n    .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n           Second Edition (2006).\n    ",
        "klass": "scipy.optimize.BFGS",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy"
        ],
        "class_docstring": "Symmetric-rank-1 Hessian update strategy.\n\n    Parameters\n    ----------\n    min_denominator : float\n        This number, scaled by a normalization factor,\n        defines the minimum denominator magnitude allowed\n        in the update. When the condition is violated we skip\n        the update. By default uses ``1e-8``.\n    init_scale : {float, 'auto'}, optional\n        Matrix scale at first iteration. At the first\n        iteration the Hessian matrix or its inverse will be initialized\n        with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n        Set it to 'auto' in order to use an automatic heuristic for choosing\n        the initial scale. The heuristic is described in [1]_, p.143.\n        By default uses 'auto'.\n\n    Notes\n    -----\n    The update is based on the description in [1]_, p.144-146.\n\n    References\n    ----------\n    .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n           Second Edition (2006).\n    ",
        "klass": "scipy.optimize.SR1",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.compressed._cs_matrix",
            "scipy.sparse.data._minmax_mixin"
        ],
        "class_docstring": "Block Sparse Row matrix\n\n    This can be instantiated in several ways:\n        bsr_matrix(D, [blocksize=(R,C)])\n            where D is a dense matrix or 2-D ndarray.\n\n        bsr_matrix(S, [blocksize=(R,C)])\n            with another sparse matrix S (equivalent to S.tobsr())\n\n        bsr_matrix((M, N), [blocksize=(R,C), dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n        bsr_matrix((data, ij), [blocksize=(R,C), shape=(M, N)])\n            where ``data`` and ``ij`` satisfy ``a[ij[0, k], ij[1, k]] = data[k]``\n\n        bsr_matrix((data, indices, indptr), [shape=(M, N)])\n            is the standard BSR representation where the block column\n            indices for row i are stored in ``indices[indptr[i]:indptr[i+1]]``\n            and their corresponding block values are stored in\n            ``data[ indptr[i]: indptr[i+1] ]``.  If the shape parameter is not\n            supplied, the matrix dimensions are inferred from the index arrays.\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        Data array of the matrix\n    indices\n        BSR format index array\n    indptr\n        BSR format index pointer array\n    blocksize\n        Block size of the matrix\n    has_sorted_indices\n        Whether indices are sorted\n\n    Notes\n    -----\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    **Summary of BSR format**\n\n    The Block Compressed Row (BSR) format is very similar to the Compressed\n    Sparse Row (CSR) format.  BSR is appropriate for sparse matrices with dense\n    sub matrices like the last example below.  Block matrices often arise in\n    vector-valued finite element discretizations.  In such cases, BSR is\n    considerably more efficient than CSR and CSC for many sparse arithmetic\n    operations.\n\n    **Blocksize**\n\n    The blocksize (R,C) must evenly divide the shape of the matrix (M,N).\n    That is, R and C must satisfy the relationship ``M % R = 0`` and\n    ``N % C = 0``.\n\n    If no blocksize is specified, a simple heuristic is applied to determine\n    an appropriate blocksize.\n\n    Examples\n    --------\n    >>> from scipy.sparse import bsr_matrix\n    >>> bsr_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> row = np.array([0, 0, 1, 2, 2, 2])\n    >>> col = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3 ,4, 5, 6])\n    >>> bsr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n    array([[1, 0, 2],\n           [0, 0, 3],\n           [4, 5, 6]])\n\n    >>> indptr = np.array([0, 2, 3, 6])\n    >>> indices = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6]).repeat(4).reshape(6, 2, 2)\n    >>> bsr_matrix((data,indices,indptr), shape=(6, 6)).toarray()\n    array([[1, 1, 0, 0, 2, 2],\n           [1, 1, 0, 0, 2, 2],\n           [0, 0, 0, 0, 3, 3],\n           [0, 0, 0, 0, 3, 3],\n           [4, 4, 5, 5, 6, 6],\n           [4, 4, 5, 5, 6, 6]])\n\n    ",
        "klass": "scipy.sparse.bsr_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.data._data_matrix",
            "scipy.sparse.data._minmax_mixin"
        ],
        "class_docstring": "\n    A sparse matrix in COOrdinate format.\n\n    Also known as the 'ijv' or 'triplet' format.\n\n    This can be instantiated in several ways:\n        coo_matrix(D)\n            with a dense matrix D\n\n        coo_matrix(S)\n            with another sparse matrix S (equivalent to S.tocoo())\n\n        coo_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n        coo_matrix((data, (i, j)), [shape=(M, N)])\n            to construct from three arrays:\n                1. data[:]   the entries of the matrix, in any order\n                2. i[:]      the row indices of the matrix entries\n                3. j[:]      the column indices of the matrix entries\n\n            Where ``A[i[k], j[k]] = data[k]``.  When shape is not\n            specified, it is inferred from the index arrays\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        COO format data array of the matrix\n    row\n        COO format row index array of the matrix\n    col\n        COO format column index array of the matrix\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Advantages of the COO format\n        - facilitates fast conversion among sparse formats\n        - permits duplicate entries (see example)\n        - very fast conversion to and from CSR/CSC formats\n\n    Disadvantages of the COO format\n        - does not directly support:\n            + arithmetic operations\n            + slicing\n\n    Intended Usage\n        - COO is a fast format for constructing sparse matrices\n        - Once a matrix has been constructed, convert to CSR or\n          CSC format for fast arithmetic and matrix vector operations\n        - By default when converting to CSR or CSC format, duplicate (i,j)\n          entries will be summed together.  This facilitates efficient\n          construction of finite element matrices and the like. (see example)\n\n    Examples\n    --------\n\n    >>> # Constructing an empty matrix\n    >>> from scipy.sparse import coo_matrix\n    >>> coo_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> # Constructing a matrix using ijv format\n    >>> row  = np.array([0, 3, 1, 0])\n    >>> col  = np.array([0, 3, 1, 2])\n    >>> data = np.array([4, 5, 7, 9])\n    >>> coo_matrix((data, (row, col)), shape=(4, 4)).toarray()\n    array([[4, 0, 9, 0],\n           [0, 7, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 5]])\n\n    >>> # Constructing a matrix with duplicate indices\n    >>> row  = np.array([0, 0, 1, 3, 1, 0, 0])\n    >>> col  = np.array([0, 2, 1, 3, 1, 0, 0])\n    >>> data = np.array([1, 1, 1, 1, 1, 1, 1])\n    >>> coo = coo_matrix((data, (row, col)), shape=(4, 4))\n    >>> # Duplicate indices are maintained until implicitly or explicitly summed\n    >>> np.max(coo.data)\n    1\n    >>> coo.toarray()\n    array([[3, 0, 1, 0],\n           [0, 2, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    ",
        "klass": "scipy.sparse.coo_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.compressed._cs_matrix"
        ],
        "class_docstring": "\n    Compressed Sparse Column matrix\n\n    This can be instantiated in several ways:\n\n        csc_matrix(D)\n            with a dense matrix or rank-2 ndarray D\n\n        csc_matrix(S)\n            with another sparse matrix S (equivalent to S.tocsc())\n\n        csc_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n        csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n            where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n            relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n\n        csc_matrix((data, indices, indptr), [shape=(M, N)])\n            is the standard CSC representation where the row indices for\n            column i are stored in ``indices[indptr[i]:indptr[i+1]]``\n            and their corresponding values are stored in\n            ``data[indptr[i]:indptr[i+1]]``.  If the shape parameter is\n            not supplied, the matrix dimensions are inferred from\n            the index arrays.\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        Data array of the matrix\n    indices\n        CSC format index array\n    indptr\n        CSC format index pointer array\n    has_sorted_indices\n        Whether indices are sorted\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Advantages of the CSC format\n        - efficient arithmetic operations CSC + CSC, CSC * CSC, etc.\n        - efficient column slicing\n        - fast matrix vector products (CSR, BSR may be faster)\n\n    Disadvantages of the CSC format\n      - slow row slicing operations (consider CSR)\n      - changes to the sparsity structure are expensive (consider LIL or DOK)\n\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy.sparse import csc_matrix\n    >>> csc_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> row = np.array([0, 2, 2, 0, 1, 2])\n    >>> col = np.array([0, 0, 1, 2, 2, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csc_matrix((data, (row, col)), shape=(3, 3)).toarray()\n    array([[1, 0, 4],\n           [0, 0, 5],\n           [2, 3, 6]])\n\n    >>> indptr = np.array([0, 2, 3, 6])\n    >>> indices = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csc_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n    array([[1, 0, 4],\n           [0, 0, 5],\n           [2, 3, 6]])\n\n    ",
        "klass": "scipy.sparse.csc_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.compressed._cs_matrix"
        ],
        "class_docstring": "\n    Compressed Sparse Row matrix\n\n    This can be instantiated in several ways:\n        csr_matrix(D)\n            with a dense matrix or rank-2 ndarray D\n\n        csr_matrix(S)\n            with another sparse matrix S (equivalent to S.tocsr())\n\n        csr_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n        csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n            where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n            relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n\n        csr_matrix((data, indices, indptr), [shape=(M, N)])\n            is the standard CSR representation where the column indices for\n            row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their\n            corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``.\n            If the shape parameter is not supplied, the matrix dimensions\n            are inferred from the index arrays.\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        CSR format data array of the matrix\n    indices\n        CSR format index array of the matrix\n    indptr\n        CSR format index pointer array of the matrix\n    has_sorted_indices\n        Whether indices are sorted\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Advantages of the CSR format\n      - efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n      - efficient row slicing\n      - fast matrix vector products\n\n    Disadvantages of the CSR format\n      - slow column slicing operations (consider CSC)\n      - changes to the sparsity structure are expensive (consider LIL or DOK)\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy.sparse import csr_matrix\n    >>> csr_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> row = np.array([0, 0, 1, 2, 2, 2])\n    >>> col = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n    array([[1, 0, 2],\n           [0, 0, 3],\n           [4, 5, 6]])\n\n    >>> indptr = np.array([0, 2, 3, 6])\n    >>> indices = np.array([0, 2, 2, 0, 1, 2])\n    >>> data = np.array([1, 2, 3, 4, 5, 6])\n    >>> csr_matrix((data, indices, indptr), shape=(3, 3)).toarray()\n    array([[1, 0, 2],\n           [0, 0, 3],\n           [4, 5, 6]])\n\n    As an example of how to construct a CSR matrix incrementally,\n    the following snippet builds a term-document matrix from texts:\n\n    >>> docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n    >>> indptr = [0]\n    >>> indices = []\n    >>> data = []\n    >>> vocabulary = {}\n    >>> for d in docs:\n    ...     for term in d:\n    ...         index = vocabulary.setdefault(term, len(vocabulary))\n    ...         indices.append(index)\n    ...         data.append(1)\n    ...     indptr.append(len(indices))\n    ...\n    >>> csr_matrix((data, indices, indptr), dtype=int).toarray()\n    array([[2, 1, 0, 0],\n           [0, 1, 1, 1]])\n\n    ",
        "klass": "scipy.sparse.csr_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.data._data_matrix"
        ],
        "class_docstring": "Sparse matrix with DIAgonal storage\n\n    This can be instantiated in several ways:\n        dia_matrix(D)\n            with a dense matrix\n\n        dia_matrix(S)\n            with another sparse matrix S (equivalent to S.todia())\n\n        dia_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N),\n            dtype is optional, defaulting to dtype='d'.\n\n        dia_matrix((data, offsets), shape=(M, N))\n            where the ``data[k,:]`` stores the diagonal entries for\n            diagonal ``offsets[k]`` (See example below)\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        DIA format data array of the matrix\n    offsets\n        DIA format offset array of the matrix\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy.sparse import dia_matrix\n    >>> dia_matrix((3, 4), dtype=np.int8).toarray()\n    array([[0, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 0]], dtype=int8)\n\n    >>> data = np.array([[1, 2, 3, 4]]).repeat(3, axis=0)\n    >>> offsets = np.array([0, -1, 2])\n    >>> dia_matrix((data, offsets), shape=(4, 4)).toarray()\n    array([[1, 0, 3, 0],\n           [1, 2, 0, 4],\n           [0, 2, 3, 0],\n           [0, 0, 3, 4]])\n\n    ",
        "klass": "scipy.sparse.dia_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.base.spmatrix",
            "scipy.sparse._index.IndexMixin",
            "dict"
        ],
        "class_docstring": "\n    Dictionary Of Keys based sparse matrix.\n\n    This is an efficient structure for constructing sparse\n    matrices incrementally.\n\n    This can be instantiated in several ways:\n        dok_matrix(D)\n            with a dense matrix, D\n\n        dok_matrix(S)\n            with a sparse matrix, S\n\n        dok_matrix((M,N), [dtype])\n            create the matrix with initial shape (M,N)\n            dtype is optional, defaulting to dtype='d'\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Allows for efficient O(1) access of individual elements.\n    Duplicates are not allowed.\n    Can be efficiently converted to a coo_matrix once constructed.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.sparse import dok_matrix\n    >>> S = dok_matrix((5, 5), dtype=np.float32)\n    >>> for i in range(5):\n    ...     for j in range(5):\n    ...         S[i, j] = i + j    # Update element\n\n    ",
        "klass": "scipy.sparse.dok_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.base.spmatrix",
            "scipy.sparse._index.IndexMixin"
        ],
        "class_docstring": "Row-based linked list sparse matrix\n\n    This is a structure for constructing sparse matrices incrementally.\n    Note that inserting a single item can take linear time in the worst case;\n    to construct a matrix efficiently, make sure the items are pre-sorted by\n    index, per row.\n\n    This can be instantiated in several ways:\n        lil_matrix(D)\n            with a dense matrix or rank-2 ndarray D\n\n        lil_matrix(S)\n            with another sparse matrix S (equivalent to S.tolil())\n\n        lil_matrix((M, N), [dtype])\n            to construct an empty matrix with shape (M, N)\n            dtype is optional, defaulting to dtype='d'.\n\n    Attributes\n    ----------\n    dtype : dtype\n        Data type of the matrix\n    shape : 2-tuple\n        Shape of the matrix\n    ndim : int\n        Number of dimensions (this is always 2)\n    nnz\n        Number of nonzero elements\n    data\n        LIL format data array of the matrix\n    rows\n        LIL format row index array of the matrix\n\n    Notes\n    -----\n\n    Sparse matrices can be used in arithmetic operations: they support\n    addition, subtraction, multiplication, division, and matrix power.\n\n    Advantages of the LIL format\n        - supports flexible slicing\n        - changes to the matrix sparsity structure are efficient\n\n    Disadvantages of the LIL format\n        - arithmetic operations LIL + LIL are slow (consider CSR or CSC)\n        - slow column slicing (consider CSC)\n        - slow matrix vector products (consider CSR or CSC)\n\n    Intended Usage\n        - LIL is a convenient format for constructing sparse matrices\n        - once a matrix has been constructed, convert to CSR or\n          CSC format for fast arithmetic and matrix vector operations\n        - consider using the COO format when constructing large matrices\n\n    Data Structure\n        - An array (``self.rows``) of rows, each of which is a sorted\n          list of column indices of non-zero elements.\n        - The corresponding nonzero values are stored in similar\n          fashion in ``self.data``.\n\n\n    ",
        "klass": "scipy.sparse.lil_matrix",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.sparse.linalg.interface.LinearOperator"
        ],
        "class_docstring": "\n    For now, this is limited to products of multiple square matrices.\n    ",
        "klass": "scipy.sparse.linalg.matfuncs.ProductOperator",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    kd-tree for quick nearest-neighbor lookup\n\n    This class provides an index into a set of k-dimensional points which\n    can be used to rapidly look up the nearest neighbors of any point.\n\n    Parameters\n    ----------\n    data : (N,K) array_like\n        The data points to be indexed. This array is not copied, and\n        so modifying this data will result in bogus results.\n    leafsize : int, optional\n        The number of points at which the algorithm switches over to\n        brute-force.  Has to be positive.\n\n    Raises\n    ------\n    RuntimeError\n        The maximum recursion limit can be exceeded for large data\n        sets.  If this happens, either increase the value for the `leafsize`\n        parameter or increase the recursion limit by::\n\n            >>> import sys\n            >>> sys.setrecursionlimit(10000)\n\n    See Also\n    --------\n    cKDTree : Implementation of `KDTree` in Cython\n\n    Notes\n    -----\n    The algorithm used is described in Maneewongvatana and Mount 1999.\n    The general idea is that the kd-tree is a binary tree, each of whose\n    nodes represents an axis-aligned hyperrectangle. Each node specifies\n    an axis and splits the set of points based on whether their coordinate\n    along that axis is greater than or less than a particular value.\n\n    During construction, the axis and splitting point are chosen by the\n    \"sliding midpoint\" rule, which ensures that the cells do not all\n    become long and thin.\n\n    The tree can be queried for the r closest neighbors of any given point\n    (optionally returning only those within some maximum distance of the\n    point). It can also be queried, with a substantial gain in efficiency,\n    for the r approximate closest neighbors.\n\n    For large dimensions (20 is already large) do not expect this to run\n    significantly faster than brute force. High-dimensional nearest-neighbor\n    queries are a substantial open problem in computer science.\n\n    The tree also supports all-neighbors queries, both with arrays of points\n    and with other kd-trees. These do use a reasonably efficient algorithm,\n    but the kd-tree is not necessarily the best data structure for this\n    sort of calculation.\n\n    ",
        "klass": "scipy.spatial.KDTree",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    cKDTree(data, leafsize=16, compact_nodes=True, copy_data=False,\n            balanced_tree=True, boxsize=None)\n\n    kd-tree for quick nearest-neighbor lookup\n\n    This class provides an index into a set of k-dimensional points\n    which can be used to rapidly look up the nearest neighbors of any\n    point.\n\n    The algorithm used is described in Maneewongvatana and Mount 1999.\n    The general idea is that the kd-tree is a binary trie, each of whose\n    nodes represents an axis-aligned hyperrectangle. Each node specifies\n    an axis and splits the set of points based on whether their coordinate\n    along that axis is greater than or less than a particular value.\n\n    During construction, the axis and splitting point are chosen by the\n    \"sliding midpoint\" rule, which ensures that the cells do not all\n    become long and thin.\n\n    The tree can be queried for the r closest neighbors of any given point\n    (optionally returning only those within some maximum distance of the\n    point). It can also be queried, with a substantial gain in efficiency,\n    for the r approximate closest neighbors.\n\n    For large dimensions (20 is already large) do not expect this to run\n    significantly faster than brute force. High-dimensional nearest-neighbor\n    queries are a substantial open problem in computer science.\n\n    Parameters\n    ----------\n    data : array_like, shape (n,m)\n        The n data points of dimension m to be indexed. This array is\n        not copied unless this is necessary to produce a contiguous\n        array of doubles, and so modifying this data will result in\n        bogus results. The data are also copied if the kd-tree is built\n        with copy_data=True.\n    leafsize : positive int, optional\n        The number of points at which the algorithm switches over to\n        brute-force. Default: 16.\n    compact_nodes : bool, optional\n        If True, the kd-tree is built to shrink the hyperrectangles to\n        the actual data range. This usually gives a more compact tree that\n        is robust against degenerated input data and gives faster queries\n        at the expense of longer build time. Default: True.\n    copy_data : bool, optional\n        If True the data is always copied to protect the kd-tree against\n        data corruption. Default: False.\n    balanced_tree : bool, optional\n        If True, the median is used to split the hyperrectangles instead of\n        the midpoint. This usually gives a more compact tree and\n        faster queries at the expense of longer build time. Default: True.\n    boxsize : array_like or scalar, optional\n        Apply a m-d toroidal topology to the KDTree.. The topology is generated\n        by :math:`x_i + n_i L_i` where :math:`n_i` are integers and :math:`L_i`\n        is the boxsize along i-th dimension. The input data shall be wrapped\n        into :math:`[0, L_i)`. A ValueError is raised if any of the data is\n        outside of this bound.\n\n    Attributes\n    ----------\n    data : ndarray, shape (n,m)\n        The n data points of dimension m to be indexed. This array is\n        not copied unless this is necessary to produce a contiguous\n        array of doubles. The data are also copied if the kd-tree is built\n        with `copy_data=True`.\n    leafsize : positive int\n        The number of points at which the algorithm switches over to\n        brute-force.\n    m : int\n        The dimension of a single data-point.\n    n : int\n        The number of data points.\n    maxes : ndarray, shape (m,)\n        The maximum value in each dimension of the n data points.\n    mins : ndarray, shape (m,)\n        The minimum value in each dimension of the n data points.\n    tree : object, class cKDTreeNode\n        This class exposes a Python view of the root node in the cKDTree object.\n    size : int\n        The number of nodes in the tree.\n\n    See Also\n    --------\n    KDTree : Implementation of `cKDTree` in pure Python\n\n    ",
        "klass": "scipy.spatial.cKDTree",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager for special-function error handling.\n\n    Using an instance of `errstate` as a context manager allows\n    statements in that context to execute with a known error handling\n    behavior. Upon entering the context the error handling is set with\n    `seterr`, and upon exiting it is restored to what it was before.\n\n    Parameters\n    ----------\n    kwargs : {all, singular, underflow, overflow, slow, loss, no_result, domain, arg, other}\n        Keyword arguments. The valid keywords are possible\n        special-function errors. Each keyword should have a string\n        value that defines the treatement for the particular type of\n        error. Values must be 'ignore', 'warn', or 'other'. See\n        `seterr` for details.\n\n    See Also\n    --------\n    geterr : get the current way of handling special-function errors\n    seterr : set how special-function errors are handled\n    numpy.errstate : similar numpy function for floating-point errors\n\n    Examples\n    --------\n    >>> import scipy.special as sc\n    >>> from pytest import raises\n    >>> sc.gammaln(0)\n    inf\n    >>> with sc.errstate(singular='raise'):\n    ...     with raises(sc.SpecialFunctionError):\n    ...         sc.gammaln(0)\n    ...\n    >>> sc.gammaln(0)\n    inf\n\n    We can also raise on every category except one.\n\n    >>> with sc.errstate(all='raise', singular='ignore'):\n    ...     sc.gammaln(0)\n    ...     with raises(sc.SpecialFunctionError):\n    ...         sc.spence(-1)\n    ...\n    inf\n\n    ",
        "klass": "scipy.special.errstate",
        "module": "scipy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Representation of a kernel-density estimate using Gaussian kernels.\n\n    Kernel density estimation is a way to estimate the probability density\n    function (PDF) of a random variable in a non-parametric way.\n    `gaussian_kde` works for both uni-variate and multi-variate data.   It\n    includes automatic bandwidth determination.  The estimation works best for\n    a unimodal distribution; bimodal or multi-modal distributions tend to be\n    oversmoothed.\n\n    Parameters\n    ----------\n    dataset : array_like\n        Datapoints to estimate from. In case of univariate data this is a 1-D\n        array, otherwise a 2-D array with shape (# of dims, # of data).\n    bw_method : str, scalar or callable, optional\n        The method used to calculate the estimator bandwidth.  This can be\n        'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n        this will be used directly as `kde.factor`.  If a callable, it should\n        take a `gaussian_kde` instance as only parameter and return a scalar.\n        If None (default), 'scott' is used.  See Notes for more details.\n    weights : array_like, optional\n        weights of datapoints. This must be the same shape as dataset.\n        If None (default), the samples are assumed to be equally weighted\n\n    Attributes\n    ----------\n    dataset : ndarray\n        The dataset with which `gaussian_kde` was initialized.\n    d : int\n        Number of dimensions.\n    n : int\n        Number of datapoints.\n    neff : int\n        Effective number of datapoints.\n\n        .. versionadded:: 1.2.0\n    factor : float\n        The bandwidth factor, obtained from `kde.covariance_factor`, with which\n        the covariance matrix is multiplied.\n    covariance : ndarray\n        The covariance matrix of `dataset`, scaled by the calculated bandwidth\n        (`kde.factor`).\n    inv_cov : ndarray\n        The inverse of `covariance`.\n\n    Methods\n    -------\n    evaluate\n    __call__\n    integrate_gaussian\n    integrate_box_1d\n    integrate_box\n    integrate_kde\n    pdf\n    logpdf\n    resample\n    set_bandwidth\n    covariance_factor\n\n    Notes\n    -----\n    Bandwidth selection strongly influences the estimate obtained from the KDE\n    (much more so than the actual shape of the kernel).  Bandwidth selection\n    can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n    methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n    uses a rule of thumb, the default is Scott's Rule.\n\n    Scott's Rule [1]_, implemented as `scotts_factor`, is::\n\n        n**(-1./(d+4)),\n\n    with ``n`` the number of data points and ``d`` the number of dimensions.\n    In the case of unequally weighted points, `scotts_factor` becomes::\n\n        neff**(-1./(d+4)),\n\n    with ``neff`` the effective number of datapoints.\n    Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n\n        (n * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    or in the case of unequally weighted points::\n\n        (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    Good general descriptions of kernel density estimation can be found in [1]_\n    and [2]_, the mathematics for this multi-dimensional implementation can be\n    found in [1]_.\n\n    With a set of weighted samples, the effective number of datapoints ``neff``\n    is defined by::\n\n        neff = sum(weights)^2 / sum(weights^2)\n\n    as detailed in [5]_.\n\n    References\n    ----------\n    .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n           Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n    .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n           Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n           Chapman and Hall, London, 1986.\n    .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n           Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n    .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n           conditional density estimation\", Computational Statistics & Data\n           Analysis, Vol. 36, pp. 279-298, 2001.\n    .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n           Series A (General), 132, 272\n\n    Examples\n    --------\n    Generate some random two-dimensional data:\n\n    >>> from scipy import stats\n    >>> def measure(n):\n    ...     \"Measurement model, return two coupled measurements.\"\n    ...     m1 = np.random.normal(size=n)\n    ...     m2 = np.random.normal(scale=0.5, size=n)\n    ...     return m1+m2, m1-m2\n\n    >>> m1, m2 = measure(2000)\n    >>> xmin = m1.min()\n    >>> xmax = m1.max()\n    >>> ymin = m2.min()\n    >>> ymax = m2.max()\n\n    Perform a kernel density estimate on the data:\n\n    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n    >>> values = np.vstack([m1, m2])\n    >>> kernel = stats.gaussian_kde(values)\n    >>> Z = np.reshape(kernel(positions).T, X.shape)\n\n    Plot the results:\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n    ...           extent=[xmin, xmax, ymin, ymax])\n    >>> ax.plot(m1, m2, 'k.', markersize=2)\n    >>> ax.set_xlim([xmin, xmax])\n    >>> ax.set_ylim([ymin, ymax])\n    >>> plt.show()\n\n    ",
        "klass": "scipy.stats.gaussian_kde",
        "module": "scipy"
    },
    {
        "base_classes": [
            "scipy.stats._distn_infrastructure.rv_generic"
        ],
        "class_docstring": "\n    A generic discrete random variable class meant for subclassing.\n\n    `rv_discrete` is a base class to construct specific distribution classes\n    and instances for discrete random variables. It can also be used\n    to construct an arbitrary distribution defined by a list of support\n    points and corresponding probabilities.\n\n    Parameters\n    ----------\n    a : float, optional\n        Lower bound of the support of the distribution, default: 0\n    b : float, optional\n        Upper bound of the support of the distribution, default: plus infinity\n    moment_tol : float, optional\n        The tolerance for the generic calculation of moments.\n    values : tuple of two array_like, optional\n        ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero\n        probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``\n        and ``pk`` must have the same shape.\n    inc : integer, optional\n        Increment for the support of the distribution.\n        Default is 1. (other values have not been tested)\n    badvalue : float, optional\n        The value in a result arrays that indicates a value that for which\n        some argument restriction is violated, default is np.nan.\n    name : str, optional\n        The name of the instance. This string is used to construct the default\n        example for distributions.\n    longname : str, optional\n        This string is used as part of the first line of the docstring returned\n        when a subclass has no docstring of its own. Note: `longname` exists\n        for backwards compatibility, do not use for new subclasses.\n    shapes : str, optional\n        The shape of the distribution. For example \"m, n\" for a distribution\n        that takes two integers as the two shape arguments for all its methods\n        If not provided, shape parameters will be inferred from\n        the signatures of the private methods, ``_pmf`` and ``_cdf`` of\n        the instance.\n    extradoc :  str, optional\n        This string is used as the last part of the docstring returned when a\n        subclass has no docstring of its own. Note: `extradoc` exists for\n        backwards compatibility, do not use for new subclasses.\n    seed : None or int or ``numpy.random.RandomState`` instance, optional\n        This parameter defines the RandomState object to use for drawing\n        random variates.\n        If None, the global np.random state is used.\n        If integer, it is used to seed the local RandomState instance.\n        Default is None.\n\n    Methods\n    -------\n    rvs\n    pmf\n    logpmf\n    cdf\n    logcdf\n    sf\n    logsf\n    ppf\n    isf\n    moment\n    stats\n    entropy\n    expect\n    median\n    mean\n    std\n    var\n    interval\n    __call__\n    support\n\n\n    Notes\n    -----\n\n    This class is similar to `rv_continuous`. Whether a shape parameter is\n    valid is decided by an ``_argcheck`` method (which defaults to checking\n    that its arguments are strictly positive.)\n    The main differences are:\n\n    - the support of the distribution is a set of integers\n    - instead of the probability density function, ``pdf`` (and the\n      corresponding private ``_pdf``), this class defines the\n      *probability mass function*, `pmf` (and the corresponding\n      private ``_pmf``.)\n    - scale parameter is not defined.\n\n    To create a new discrete distribution, we would do the following:\n\n    >>> from scipy.stats import rv_discrete\n    >>> class poisson_gen(rv_discrete):\n    ...     \"Poisson distribution\"\n    ...     def _pmf(self, k, mu):\n    ...         return exp(-mu) * mu**k / factorial(k)\n\n    and create an instance::\n\n    >>> poisson = poisson_gen(name=\"poisson\")\n\n    Note that above we defined the Poisson distribution in the standard form.\n    Shifting the distribution can be done by providing the ``loc`` parameter\n    to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``\n    delegates the work to ``poisson._pmf(x-loc, mu)``.\n\n    **Discrete distributions from a list of probabilities**\n\n    Alternatively, you can construct an arbitrary discrete rv defined\n    on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the\n    ``values`` keyword argument to the `rv_discrete` constructor.\n\n    Examples\n    --------\n\n    Custom made discrete distribution:\n\n    >>> from scipy import stats\n    >>> xk = np.arange(7)\n    >>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\n    >>> custm = stats.rv_discrete(name='custm', values=(xk, pk))\n    >>>\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')\n    >>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)\n    >>> plt.show()\n\n    Random number generation:\n\n    >>> R = custm.rvs(size=100)\n\n    ",
        "klass": "scipy.stats.rv_discrete",
        "module": "scipy"
    },
    {
        "base_classes": [
            "socks._BaseSocket"
        ],
        "class_docstring": "socksocket([family[, type[, proto]]]) -> socket object\n\n    Open a SOCKS enabled socket. The parameters are the same as\n    those of the standard socket init. In order for SOCKS to work,\n    you must specify family=AF_INET and proto=0.\n    The \"type\" argument must be either SOCK_STREAM or SOCK_DGRAM.\n    ",
        "klass": "socks.socksocket",
        "module": "socks"
    },
    {
        "base_classes": [
            "str"
        ],
        "class_docstring": " Represents a filesystem path.\n\n    For documentation on individual methods, consult their\n    counterparts in os.path.\n    ",
        "klass": "path.path",
        "module": "path"
    },
    {
        "base_classes": [
            "path.path"
        ],
        "class_docstring": "\n    A temporary directory via tempfile.mkdtemp, and constructed with the\n    same parameters that you can use as a context manager.\n\n    Example:\n\n        with tempdir() as d:\n            # do stuff with the path object \"d\"\n\n        # here the directory is deleted automatically\n\n    .. seealso:: :func:`tempfile.mkdtemp`\n    ",
        "klass": "path.tempdir",
        "module": "path"
    },
    {
        "base_classes": [
            "pyparsing.Literal"
        ],
        "class_docstring": "\n    Token to match a specified string, ignoring case of letters.\n    Note: the matched results will always be in the case of the given\n    match string, NOT the case of the input text.\n\n    Example::\n        OneOrMore(CaselessLiteral(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD', 'CMD']\n        \n    (Contrast with example for L{CaselessKeyword}.)\n    ",
        "klass": "pyparsing.CaselessLiteral",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A low-level WebSocket connection object.\n\n    This wraps two other protocol objects, an HTTP/1.1 protocol object used\n    to do the initial HTTP upgrade handshake and a WebSocket frame protocol\n    object used to exchange messages and other control frames.\n\n    :param conn_type: Whether this object is on the client- or server-side of\n        a connection. To initialise as a client pass ``CLIENT`` otherwise\n        pass ``SERVER``.\n    :type conn_type: ``ConnectionType``\n\n    :param host: The hostname to pass to the server when acting as a client.\n    :type host: ``str``\n\n    :param resource: The resource (aka path) to pass to the server when acting\n        as a client.\n    :type resource: ``str``\n\n    :param extensions: A list of  extensions to use on this connection.\n        Extensions should be instances of a subclass of\n        :class:`Extension <wsproto.extensions.Extension>`.\n\n    :param subprotocols: A list of subprotocols to request when acting as a\n        client, ordered by preference. This has no impact on the connection\n        itself.\n    :type subprotocol: ``list`` of ``str``\n    ",
        "klass": "wsproto.connection.WSConnection",
        "module": "wsproto"
    },
    {
        "base_classes": [
            "sklearn.decomposition.base._BasePCA"
        ],
        "class_docstring": "Principal component analysis (PCA)\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float, None or string\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool (default True)\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, optional (default False)\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float >= 0, optional (default .0)\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int >= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    random_state : int, RandomState instance or None, optional (default None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. The components are sorted by\n        ``explained_variance_``.\n\n    explained_variance_ : array, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method of *Minka, T. P.\n    \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n\n    Implements the probabilistic PCA model from:\n    Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    via the score and score_samples methods.\n    See http://www.miketipping.com/papers/met-mppca.pdf\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    *Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.* and also\n    *Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='auto', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='full', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n    PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n      svd_solver='arpack', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.99244...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061...]\n\n    See also\n    --------\n    KernelPCA\n    SparsePCA\n    TruncatedSVD\n    IncrementalPCA\n    ",
        "klass": "sklearn.decomposition.pca.PCA",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " This is the main class to embed modeling objects.\n\n    The :class:`Model` class acts as a factory to create optimization objects,\n    decision variables, and constraints.\n    It provides various accessors and iterators to the modeling objects.\n    It also manages solving operations and solution management.\n\n    The Model class is the context manager and can be used with the Python `with` statement:\n\n    .. code-block:: python\n\n       with Model() as mdl:\n         # start modeling...\n\n    When the `with` block is finished, the :func:`end` method is called automatically, and all resources\n    allocated by the model are destroyed.\n\n    When a model is created without a specified ``context``, a default\n    ``Context`` is created and initialized as described in :func:`docplex.mp.context.Context.read_settings`.\n\n    Example::\n\n        # Creates a model named 'my model' with default context\n        model = Model('my model')\n\n    In this example, we create a model to solve with just 2 threads::\n\n        context = Context.make_default_context()\n        context.cplex_parameters.threads = 2\n        model = Model(context=context)\n\n    Alternatively, this can be coded as::\n\n        model = Model()\n        model.context.cplex_parameters.threads = 2\n\n    Args:\n        name (optional): The name of the model.\n        context (optional): The solve context to be used. If no ``context`` is\n            passed, a default context is created.\n        log_output (optional): If ``True``, solver logs are output to\n            stdout. If this is a stream, solver logs are output to that\n            stream object.\n        checker (optional): If ``off``, then checking is disabled everywhere. Turning off checking\n            may improve performance but should be done only with extreme caution.\n            Possible values for the `checker` keyword argument are:\n\n                - `default` (or `std`, or `on`): detects modeling errors, but doe snot check\n                  numerical values for infinities or NaNs. This is the default value\n                - `numerical`: checks modeling errors and also checks all number values for  infinities\n                  or NaNs. This option should be used when data are not trusted.\n                - `off`: no typechecking is performed. This options must be used when the model has been\n                  thorougly tested and numerical data are trusted.\n        cts_by_name (optional): a flag which control whether the constraint name dictionary is enabled.\n            Default is False.\n    ",
        "klass": "docplex.mp.model.Model",
        "module": "docplex"
    },
    {
        "base_classes": [
            "matplotlib.collections.PolyCollection"
        ],
        "class_docstring": "\n    A collection of 3D polygons.\n    ",
        "klass": "mpl_toolkits.mplot3d.art3d.Poly3DCollection",
        "module": "mpl_toolkits"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.TransformerMixin"
        ],
        "class_docstring": "Binarize labels in a one-vs-all fashion\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    At learning time, this simply consists in learning one regressor\n    or binary classifier per class. In doing so, one needs to convert\n    multi-class labels to binary labels (belong or does not belong\n    to the class). LabelBinarizer makes this process easy with the\n    transform method.\n\n    At prediction time, one assigns the class for which the corresponding\n    model gave the greatest confidence. LabelBinarizer makes this easy\n    with the inverse_transform method.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    Parameters\n    ----------\n\n    neg_label : int (default: 0)\n        Value with which negative labels must be encoded.\n\n    pos_label : int (default: 1)\n        Value with which positive labels must be encoded.\n\n    sparse_output : boolean (default: False)\n        True if the returned array from transform is desired to be in sparse\n        CSR format.\n\n    Attributes\n    ----------\n\n    classes_ : array of shape [n_class]\n        Holds the label for each class.\n\n    y_type_ : str,\n        Represents the type of the target data as evaluated by\n        utils.multiclass.type_of_target. Possible type are 'continuous',\n        'continuous-multioutput', 'binary', 'multiclass',\n        'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n\n    sparse_input_ : boolean,\n        True if the input data to transform is given as a sparse matrix, False\n        otherwise.\n\n    Examples\n    --------\n    >>> from sklearn import preprocessing\n    >>> lb = preprocessing.LabelBinarizer()\n    >>> lb.fit([1, 2, 6, 4, 2])\n    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n    >>> lb.classes_\n    array([1, 2, 4, 6])\n    >>> lb.transform([1, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    Binary targets transform to a column vector\n\n    >>> lb = preprocessing.LabelBinarizer()\n    >>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])\n\n    Passing a 2D matrix for multilabel classification\n\n    >>> import numpy as np\n    >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\n    LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n    >>> lb.classes_\n    array([0, 1, 2])\n    >>> lb.transform([0, 1, 2, 1])\n    array([[1, 0, 0],\n           [0, 1, 0],\n           [0, 0, 1],\n           [0, 1, 0]])\n\n    See also\n    --------\n    label_binarize : function to perform the transform operation of\n        LabelBinarizer with fixed classes.\n    sklearn.preprocessing.OneHotEncoder : encode categorical features\n        using a one-hot aka one-of-K scheme.\n    ",
        "klass": "sklearn.preprocessing.label.LabelBinarizer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "PIL.ImageEnhance._Enhance"
        ],
        "class_docstring": "Adjust image brightness.\n\n    This class can be used to control the brightness of an image.  An\n    enhancement factor of 0.0 gives a black image. A factor of 1.0 gives the\n    original image.\n    ",
        "klass": "PIL.ImageEnhance.Brightness",
        "module": "PIL"
    },
    {
        "base_classes": [
            "PIL.ImageEnhance._Enhance"
        ],
        "class_docstring": "Adjust image color balance.\n\n    This class can be used to adjust the colour balance of an image, in\n    a manner similar to the controls on a colour TV set. An enhancement\n    factor of 0.0 gives a black and white image. A factor of 1.0 gives\n    the original image.\n    ",
        "klass": "PIL.ImageEnhance.Color",
        "module": "PIL"
    },
    {
        "base_classes": [
            "PIL.ImageEnhance._Enhance"
        ],
        "class_docstring": "Adjust image contrast.\n\n    This class can be used to control the contrast of an image, similar\n    to the contrast control on a TV set. An enhancement factor of 0.0\n    gives a solid grey image. A factor of 1.0 gives the original image.\n    ",
        "klass": "PIL.ImageEnhance.Contrast",
        "module": "PIL"
    },
    {
        "base_classes": [
            "PIL.ImageEnhance._Enhance"
        ],
        "class_docstring": "Adjust image sharpness.\n\n    This class can be used to adjust the sharpness of an image. An\n    enhancement factor of 0.0 gives a blurred image, a factor of 1.0 gives the\n    original image, and a factor of 2.0 gives a sharpened image.\n    ",
        "klass": "PIL.ImageEnhance.Sharpness",
        "module": "PIL"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A Windows bitmap with the given mode and size.  The mode can be one of \"1\",\n    \"L\", \"P\", or \"RGB\".\n\n    If the display requires a palette, this constructor creates a suitable\n    palette and associates it with the image. For an \"L\" image, 128 greylevels\n    are allocated. For an \"RGB\" image, a 6x6x6 colour cube is used, together\n    with 20 greylevels.\n\n    To make sure that palettes work properly under Windows, you must call the\n    **palette** method upon certain events from Windows.\n\n    :param image: Either a PIL image, or a mode string. If a mode string is\n                  used, a size must also be given.  The mode can be one of \"1\",\n                  \"L\", \"P\", or \"RGB\".\n    :param size: If the first argument is a mode string, this\n                 defines the size of the image.\n    ",
        "klass": "PIL.ImageWin.Dib",
        "module": "PIL"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Incremental image parser.  This class implements the standard\n    feed/close consumer interface.\n    ",
        "klass": "PIL.ImageFile.Parser",
        "module": "PIL"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": ".. class:: Color(rgb)\n\n   This object gives access to Colors in Blender.\n\n   :param rgb: (r, g, b) color values\n   :type rgb: 3d vector\n",
        "klass": "Color",
        "module": "Color"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": ".. class:: Matrix([rows])\n\n   This object gives access to Matrices in Blender, supporting square and rectangular\n   matrices from 2x2 up to 4x4.\n\n   :param rows: Sequence of rows.\n      When omitted, a 4x4 identity matrix is constructed.\n   :type rows: 2d number sequence\n",
        "klass": "Matrix",
        "module": "Matrix"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": ".. class:: Vector(seq)\n\n   This object gives access to Vectors in Blender.\n\n   :param seq: Components of the vector, must be a sequence of at least two\n   :type seq: sequence of numbers\n",
        "klass": "Vector",
        "module": "Vector"
    },
    {
        "base_classes": [
            "dfvfs.file_io.file_io.FileIO"
        ],
        "class_docstring": "File-like object of a encrypted stream.",
        "klass": "dfvfs.file_io.encrypted_stream_io.EncryptedStream",
        "module": "dfvfs"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A RingBuffer containing complex values.\n    ",
        "klass": "urh.util.RingBuffer.RingBuffer",
        "module": "urh"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Kernel Density Estimation\n\n    Read more in the :ref:`User Guide <kernel_density>`.\n\n    Parameters\n    ----------\n    bandwidth : float\n        The bandwidth of the kernel.\n\n    algorithm : string\n        The tree algorithm to use.  Valid options are\n        ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.\n\n    kernel : string\n        The kernel to use.  Valid kernels are\n        ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']\n        Default is 'gaussian'.\n\n    metric : string\n        The distance metric to use.  Note that not all metrics are\n        valid with all algorithms.  Refer to the documentation of\n        :class:`BallTree` and :class:`KDTree` for a description of\n        available algorithms.  Note that the normalization of the density\n        output is correct only for the Euclidean distance metric. Default\n        is 'euclidean'.\n\n    atol : float\n        The desired absolute tolerance of the result.  A larger tolerance will\n        generally lead to faster execution. Default is 0.\n\n    rtol : float\n        The desired relative tolerance of the result.  A larger tolerance will\n        generally lead to faster execution.  Default is 1E-8.\n\n    breadth_first : boolean\n        If true (default), use a breadth-first approach to the problem.\n        Otherwise use a depth-first approach.\n\n    leaf_size : int\n        Specify the leaf size of the underlying tree.  See :class:`BallTree`\n        or :class:`KDTree` for details.  Default is 40.\n\n    metric_params : dict\n        Additional parameters to be passed to the tree for use with the\n        metric.  For more information, see the documentation of\n        :class:`BallTree` or :class:`KDTree`.\n    ",
        "klass": "sklearn.neighbors.kde.KernelDensity",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "pySDC.implementations.controller_es.controller_nonMPI.controller_nonMPI"
        ],
        "class_docstring": "\n\n    PFASST controller, running serialized version of PFASST in classical style, allows node failure before fine sweep\n\n    ",
        "klass": "pySDC.projects.node_failure.controller_nonMPI_hard_faults.controller_nonMPI_hard_faults",
        "module": "pySDC"
    },
    {
        "base_classes": [
            "pySDC.implementations.controller_es.controller_nonMPI.controller_nonMPI"
        ],
        "class_docstring": "\n\n    PFASST controller, running serial matrix-based versions\n\n    ",
        "klass": "pySDC.projects.matrixPFASST.controller_matrix_nonMPI.controller_matrix_nonMPI",
        "module": "pySDC"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A wrapper class for :class:`Timer` that enables its use as a\n    context manager.\n\n    For example, instead of\n\n    >>> t = Timer()\n    >>> t.start()\n    >>> do_something()\n    >>> t.stop()\n    >>> elapsed = t.elapsed()\n\n    one can use\n\n    >>> t = Timer()\n    >>> with ContextTimer(t):\n    ...   do_something()\n    >>> elapsed = t.elapsed()\n    ",
        "klass": "sporco.util.ContextTimer",
        "module": "sporco"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Timer class supporting multiple independent labelled timers.\n\n    The timer is based on the relative time returned by\n    :func:`timeit.default_timer`.\n    ",
        "klass": "sporco.util.Timer",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.cbpdntv.ConvBPDNScalarTV"
        ],
        "class_docstring": "\n    ADMM algorithm for an extension of Convolutional BPDN including\n    a term penalising the vector total variation of the coefficient maps\n    :cite:`wohlberg-2017-convolutional`.\n\n    |\n\n    .. inheritance-diagram:: ConvBPDNVectorTV\n       :parts: 2\n\n    |\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{x} \\; \\frac{1}{2}\n       \\left\\| \\sum_m \\mathbf{d}_m * \\mathbf{x}_m - \\mathbf{s}\n       \\right\\|_2^2 + \\lambda \\sum_m \\| \\mathbf{x}_m \\|_1 +\n       \\mu \\left\\| \\sqrt{\\sum_m \\sum_i (G_i \\mathbf{x}_m)^2} \\right\\|_1\n       \\;\\;,\n\n    where :math:`G_i` is an operator computing the derivative along index\n    :math:`i`, via the ADMM problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{x} \\; (1/2) \\left\\| D \\mathbf{x} -\n       \\mathbf{s} \\right\\|_2^2 + \\lambda\n       \\| \\mathbf{y}_L \\|_1 + \\mu \\left\\| \\sqrt{\\sum_{i=0}^{L-1}\n       I_B \\mathbf{y}_i^2} \\right\\|_1 \\quad \\text{ such that } \\quad\n       \\left( \\begin{array}{c} \\Gamma_0 \\\\ \\Gamma_1 \\\\ \\vdots \\\\ I\n       \\end{array} \\right) \\mathbf{x} =\n       \\left( \\begin{array}{c} \\mathbf{y}_0 \\\\\n       \\mathbf{y}_1 \\\\ \\vdots \\\\ \\mathbf{y}_L \\end{array}\n       \\right)  \\;\\;,\n\n    where\n\n    .. math::\n       D = \\left( \\begin{array}{ccc} D_0 & D_1 & \\ldots \\end{array} \\right)\n       \\qquad\n       \\mathbf{x} = \\left( \\begin{array}{c} \\mathbf{x}_0 \\\\ \\mathbf{x}_1 \\\\\n       \\vdots \\end{array} \\right) \\qquad\n       \\Gamma_i = \\left( \\begin{array}{ccc}\n          G_i & 0 & \\ldots \\\\  0 & G_i & \\ldots \\\\ \\vdots & \\vdots & \\ddots\n       \\end{array} \\right) \\qquad\n       I_B = \\left( \\begin{array}{ccc} I & I & \\ldots \\end{array} \\right)\n       \\;\\;.\n\n\n    For multi-channel signals with a single-channel dictionary, vector TV is\n    applied jointly over the coefficient maps for channel :math:`c` and\n    filter :math:`m`. Since multi-channel signals with a multi-channel\n    dictionary also have one coefficient map per filter, the behaviour is\n    the same as for single-channel signals.\n\n\n    After termination of the :meth:`solve` method, attribute :attr:`itstat`\n    is a list of tuples representing statistics of each iteration. The\n    fields of the named tuple ``IterationStats`` are:\n\n       ``Iter`` : Iteration number\n\n       ``ObjFun`` : Objective function value\n\n       ``DFid`` :  Value of data fidelity term :math:`(1/2) \\| \\sum_m\n       \\mathbf{d}_m * \\mathbf{x}_m - \\mathbf{s} \\|_2^2`\n\n       ``RegL1`` : Value of regularisation term :math:`\\sum_m \\|\n       \\mathbf{x}_m \\|_1`\n\n       ``RegTV`` : Value of regularisation term :math:`\\left\\|\n       \\sqrt{\\sum_m \\sum_i (G_i \\mathbf{x}_m)^2} \\right\\|_1`\n\n       ``PrimalRsdl`` : Norm of primal residual\n\n       ``DualRsdl`` : Norm of dual residual\n\n       ``EpsPrimal`` : Primal residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{pri}}`\n\n       ``EpsDual`` : Dual residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{dua}}`\n\n       ``Rho`` : Penalty parameter\n\n       ``XSlvRelRes`` : Relative residual of X step solver\n\n       ``Time`` : Cumulative run time\n    ",
        "klass": "sporco.admm.cbpdntv.ConvBPDNVectorTV",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.fista.cbpdn.ConvBPDN"
        ],
        "class_docstring": "\n    FISTA algorithm for Convolutional BPDN with a spatial mask.\n\n    |\n\n    .. inheritance-diagram:: ConvBPDNMask\n       :parts: 2\n\n    |\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{x} \\;\n       (1/2) \\left\\|  W \\left(\\sum_m \\mathbf{d}_m * \\mathbf{x}_m -\n       \\mathbf{s}\\right) \\right\\|_2^2 + \\lambda \\sum_m\n       \\| \\mathbf{x}_m \\|_1 \\;\\;,\n\n    where :math:`W` is a mask array.\n\n    See :class:`ConvBPDN` for interface details.\n    ",
        "klass": "sporco.fista.cbpdn.ConvBPDNMask",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.ccmod.ConvCnstrMOD_Consensus"
        ],
        "class_docstring": "\n    Hybrid ADMM Consensus algorithm for Convolutional Constrained MOD with\n    Mask Decoupling :cite:`garcia-2018-convolutional1`.\n\n    |\n\n    .. inheritance-diagram:: ConvCnstrMODMaskDcpl_Consensus\n       :parts: 2\n\n    |\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{d} \\;\n       (1/2) \\left\\|  W \\left(\\sum_m \\mathbf{d}_m * \\mathbf{x}_m -\n       \\mathbf{s} \\right) \\right\\|_2^2 \\quad \\text{such that} \\quad\n       \\mathbf{d}_m \\in C \\;\\; \\forall m\n\n    where :math:`C` is the feasible set consisting of filters with unit\n    norm and constrained support, and :math:`W` is a mask array, via a\n    hybrid ADMM Consensus problem.\n\n    See the documentation of :class:`.ConvCnstrMODMaskDcplBase` for a\n    detailed discussion of the implementational complications resulting\n    from the support of multi-channel signals.\n    ",
        "klass": "sporco.admm.ccmodmd.ConvCnstrMODMaskDcpl_Consensus",
        "module": "sporco"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Boundary masking for convolutional representations using the\n    Additive Mask Simulation (AMS) technique described in\n    :cite:`wohlberg-2016-boundary`. Implemented as a wrapper about a\n    cbpdn.ConvBPDN or derived object (or any other object with\n    sufficiently similar interface and internals). The wrapper is largely\n    transparent, but must be taken into account when setting some of the\n    options for the inner object, e.g. the shape of the ``L1Weight``\n    option array must take into account the extra dictionary atom appended\n    by the wrapper.\n    ",
        "klass": "sporco.admm.cbpdn.AddMaskSim",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.cbpdn.ConvBPDN"
        ],
        "class_docstring": "\n    ADMM algorithm for a convolutional form of the elastic net problem\n    :cite:`zou-2005-regularization`.\n\n    |\n\n    .. inheritance-diagram:: ConvElasticNet\n       :parts: 2\n\n    |\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{x} \\;\n       (1/2) \\left\\| \\sum_m \\mathbf{d}_m * \\mathbf{x}_m - \\mathbf{s}\n       \\right\\|_2^2 + \\lambda \\sum_m \\| \\mathbf{x}_m \\|_1 +\n       (\\mu/2) \\sum_m \\| \\mathbf{x}_m \\|_2^2\n\n    via the ADMM problem\n\n    .. math::\n       \\mathrm{argmin}_{\\mathbf{x}, \\mathbf{y}} \\;\n       (1/2) \\left\\| \\sum_m \\mathbf{d}_m * \\mathbf{x}_m -\n       \\mathbf{s} \\right\\|_2^2 + \\lambda \\sum_m \\| \\mathbf{y}_m \\|_1\n       + (\\mu/2) \\sum_m \\| \\mathbf{x}_m \\|_2^2\n       \\quad \\text{such that} \\quad \\mathbf{x}_m = \\mathbf{y}_m \\;\\;.\n\n    After termination of the :meth:`solve` method, attribute :attr:`itstat`\n    is a list of tuples representing statistics of each iteration. The\n    fields of the named tuple ``IterationStats`` are:\n\n       ``Iter`` : Iteration number\n\n       ``ObjFun`` : Objective function value\n\n       ``DFid`` : Value of data fidelity term :math:`(1/2) \\| \\sum_m\n       \\mathbf{d}_m * \\mathbf{x}_m - \\mathbf{s} \\|_2^2`\n\n       ``RegL1`` : Value of regularisation term :math:`\\sum_m \\|\n       \\mathbf{x}_m \\|_1`\n\n       ``RegL2`` : Value of regularisation term :math:`(1/2) \\sum_m \\|\n       \\mathbf{x}_m \\|_2^2`\n\n       ``PrimalRsdl`` : Norm of primal residual\n\n       ``DualRsdl`` : Norm of dual residual\n\n       ``EpsPrimal`` : Primal residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{pri}}`\n\n       ``EpsDual`` : Dual residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{dua}}`\n\n       ``Rho`` : Penalty parameter\n\n       ``XSlvRelRes`` : Relative residual of X step solver\n\n       ``Time`` : Cumulative run time\n    ",
        "klass": "sporco.admm.cbpdn.ConvElasticNet",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.cbpdn.ConvBPDN"
        ],
        "class_docstring": "\n    ADMM algorithm for the Convolutional BPDN (CBPDN) for multi-channel\n    signals with a dictionary consisting of a product of convolutional\n    and standard dictionaries :cite:`garcia-2018-convolutional2`.\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_X \\; (1/2) \\left\\| D X B^T - S \\right\\|_2^2 +\n       \\lambda \\| X \\|_1\n\n    where :math:`D` is a convolutional dictionary, :math:`B` is a\n    standard dictionary, and :math:`S` is a multi-channel input image\n    with\n\n    .. math::\n       S = \\left( \\begin{array}{ccc} \\mathbf{s}_0 & \\mathbf{s}_1 & \\ldots\n       \\end{array} \\right) \\;.\n\n    where the signal channels form the columns, :math:`\\mathbf{s}_c`, of\n    :math:`S`. This problem is solved via the ADMM problem\n    :cite:`garcia-2018-convolutional2`\n\n    .. math::\n       \\mathrm{argmin}_{X,Y} \\;\n       (1/2) \\left\\| D X B^T - S \\right\\|_2^2 + \\lambda \\| Y \\|_1\n       \\quad \\text{such that} \\quad X = Y \\;\\;.\n    ",
        "klass": "sporco.admm.pdcsc.ConvProdDictBPDN",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.pdcsc.ConvProdDictBPDN"
        ],
        "class_docstring": "\n    ADMM algorithm for the Convolutional BPDN (CBPDN) for multi-channel\n    signals with a dictionary consisting of a product of convolutional\n    and standard dictionaries, and with joint sparsity via an\n    :math:`\\ell_{2,1}` norm term :cite:`garcia-2018-convolutional2`.\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_X \\; (1/2) \\left\\| D X B^T - S \\right\\|_2^2 +\n       \\lambda \\| X \\|_1 + \\mu \\| X \\|_{2,1}\n\n    where :math:`D` is a convolutional dictionary, :math:`B` is a\n    standard dictionary, and :math:`S` is a multi-channel input image\n    with\n\n    .. math::\n       S = \\left( \\begin{array}{ccc} \\mathbf{s}_0 & \\mathbf{s}_1 & \\ldots\n       \\end{array} \\right) \\;.\n\n    where the signal channels form the columns, :math:`\\mathbf{s}_c`, of\n    :math:`S`. This problem is solved via the ADMM problem\n    :cite:`garcia-2018-convolutional2`\n\n    .. math::\n       \\mathrm{argmin}_{X,Y} \\;\n       (1/2) \\left\\| D X B^T - S \\right\\|_2^2 + \\lambda \\| Y \\|_1\n       + \\mu \\| Y \\|_{2,1} \\quad \\text{such that} \\quad X = Y \\;\\;.\n    ",
        "klass": "sporco.admm.pdcsc.ConvProdDictBPDNJoint",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.cbpdn.ConvL1L1Grd"
        ],
        "class_docstring": "\n    ADMM algorithm for a Convolutional Sparse Coding problem for\n    multi-channel signals with a dictionary consisting of a product\n    of convolutional and standard dictionaries and with an :math:`\\ell_1`\n    data fidelity term and both :math:`\\ell_1` and :math:`\\ell_2` of\n    gradient regularisation terms :cite:`garcia-2018-convolutional2`.\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_X \\; \\left\\| D X B^T - S \\right\\|_1 +\n       \\lambda \\| X \\|_1 + (\\mu / 2) \\sum_i \\| G_i X \\|_2^2\n\n    where :math:`D` is a convolutional dictionary, :math:`B` is a\n    standard dictionary, :math:`G_i` is an operator that computes the\n    gradient along array axis :math:`i`, and :math:`S` is a multi-channel\n    input image with\n\n    .. math::\n       S = \\left( \\begin{array}{ccc} \\mathbf{s}_0 & \\mathbf{s}_1 & \\ldots\n       \\end{array} \\right) \\;.\n\n    where the signal channels form the columns, :math:`\\mathbf{s}_c`, of\n    :math:`S`. This problem is solved via the ADMM problem\n    :cite:`garcia-2018-convolutional2`\n\n    .. math::\n       \\mathrm{argmin}_{X,Y} \\;\n       \\left\\| Y_0 \\right\\|_1 + \\lambda \\| Y_1 \\|_1 + (\\mu / 2)\n       \\sum_i \\| G_i X \\|_2^2 \\quad \\text{such that} \\quad\n       Y_0 = D X B^T - S \\;\\;\\; Y_1 = X \\;\\;.\n    ",
        "klass": "sporco.admm.pdcsc.ConvProdDictL1L1Grd",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.bpdn.BPDN"
        ],
        "class_docstring": "\n    ADMM algorithm for BPDN with joint sparsity via an :math:`\\ell_{2,1}`\n    norm term.\n\n    |\n\n    .. inheritance-diagram:: BPDNJoint\n       :parts: 2\n\n    |\n\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_X \\; (1/2) \\| D X - S \\|_2^2 + \\lambda \\| X \\|_1\n       + \\mu \\| X \\|_{2,1}\n\n    via the ADMM problem\n\n    .. math::\n       \\mathrm{argmin}_{X, Y} \\; (1/2) \\| D X - S \\|_2^2 +\n       \\lambda \\| Y \\|_1 + \\mu \\| Y \\|_{2,1} \\quad \\text{such that} \\quad\n       X = Y \\;\\;.\n\n    After termination of the :meth:`solve` method, attribute\n    :attr:`itstat` is a list of tuples representing statistics of each\n    iteration. The fields of the named tuple ``IterationStats`` are:\n\n       ``Iter`` : Iteration number\n\n       ``ObjFun`` : Objective function value\n\n       ``DFid`` :  Value of data fidelity term :math:`(1/2) \\| D X - S\n       \\|_2^2`\n\n       ``RegL1`` : Value of regularisation term :math:`\\| X \\|_1`\n\n       ``RegL21`` : Value of regularisation term :math:`\\| X \\|_{2,1}`\n\n       ``PrimalRsdl`` : Norm of primal residual\n\n       ``DualRsdl`` : Norm of dual Residual\n\n       ``EpsPrimal`` : Primal residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{pri}}`\n\n       ``EpsDual`` : Dual residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{dua}}`\n\n       ``Rho`` : Penalty parameter\n\n       ``Time`` : Cumulative run time\n    ",
        "klass": "sporco.admm.bpdn.BPDNJoint",
        "module": "sporco"
    },
    {
        "base_classes": [
            "sporco.admm.bpdn.BPDN"
        ],
        "class_docstring": "\n    ADMM algorithm for the elastic net :cite:`zou-2005-regularization`\n    problem.\n\n    |\n\n    .. inheritance-diagram:: ElasticNet\n       :parts: 2\n\n    |\n\n    Solve the optimisation problem\n\n    .. math::\n       \\mathrm{argmin}_\\mathbf{x} \\;\n       (1/2) \\| D \\mathbf{x} - \\mathbf{s} \\|_2^2 + \\lambda \\| \\mathbf{x}\n       \\|_1 + (\\mu/2) \\| \\mathbf{x} \\|_2^2\n\n    via the ADMM problem\n\n    .. math::\n       \\mathrm{argmin}_{\\mathbf{x}, \\mathbf{y}} \\;\n       (1/2) \\| D \\mathbf{x} - \\mathbf{s} \\|_2^2 + \\lambda \\| \\mathbf{y}\n       \\|_1 + (\\mu/2) \\| \\mathbf{x} \\|_2^2 \\quad \\text{such that} \\quad\n       \\mathbf{x} = \\mathbf{y} \\;\\;.\n\n    After termination of the :meth:`solve` method, attribute\n    :attr:`itstat` is a list of tuples representing statistics of each\n    iteration. The fields of the named tuple ``IterationStats`` are:\n\n       ``Iter`` : Iteration number\n\n       ``ObjFun`` : Objective function value\n\n       ``DFid`` : Value of data fidelity term :math:`(1/2) \\| D\n       \\mathbf{x} - \\mathbf{s} \\|_2^2`\n\n       ``RegL1`` : Value of regularisation term :math:`\\| \\mathbf{x}\n       \\|_1`\n\n       ``RegL2`` : Value of regularisation term :math:`(1/2) \\|\n       \\mathbf{x} \\|_2^2`\n\n       ``PrimalRsdl`` : Norm of primal residual\n\n       ``DualRsdl`` : Norm of dual Residual\n\n       ``EpsPrimal`` : Primal residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{pri}}`\n\n       ``EpsDual`` : Dual residual stopping tolerance\n       :math:`\\epsilon_{\\mathrm{dua}}`\n\n       ``Rho`` : Penalty parameter\n\n       ``Time`` : Cumulative run time\n    ",
        "klass": "sporco.admm.bpdn.ElasticNet",
        "module": "sporco"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "\n    A Tree represents a hierarchical grouping of leaves and subtrees.\n    For example, each constituent in a syntax tree is represented by a single Tree.\n\n    A tree's children are encoded as a list of leaves and subtrees,\n    where a leaf is a basic (non-tree) value; and a subtree is a\n    nested Tree.\n\n        >>> from nltk.tree import Tree\n        >>> print(Tree(1, [2, Tree(3, [4]), 5]))\n        (1 2 (3 4) 5)\n        >>> vp = Tree('VP', [Tree('V', ['saw']),\n        ...                  Tree('NP', ['him'])])\n        >>> s = Tree('S', [Tree('NP', ['I']), vp])\n        >>> print(s)\n        (S (NP I) (VP (V saw) (NP him)))\n        >>> print(s[1])\n        (VP (V saw) (NP him))\n        >>> print(s[1,1])\n        (NP him)\n        >>> t = Tree.fromstring(\"(S (NP I) (VP (V saw) (NP him)))\")\n        >>> s == t\n        True\n        >>> t[1][1].set_label('X')\n        >>> t[1][1].label()\n        'X'\n        >>> print(t)\n        (S (NP I) (VP (V saw) (X him)))\n        >>> t[0], t[1,1] = t[1,1], t[0]\n        >>> print(t)\n        (S (X him) (VP (V saw) (NP I)))\n\n    The length of a tree is the number of children it has.\n\n        >>> len(t)\n        2\n\n    The set_label() and label() methods allow individual constituents\n    to be labeled.  For example, syntax trees use this label to specify\n    phrase tags, such as \"NP\" and \"VP\".\n\n    Several Tree methods use \"tree positions\" to specify\n    children or descendants of a tree.  Tree positions are defined as\n    follows:\n\n      - The tree position *i* specifies a Tree's *i*\\ th child.\n      - The tree position ``()`` specifies the Tree itself.\n      - If *p* is the tree position of descendant *d*, then\n        *p+i* specifies the *i*\\ th child of *d*.\n\n    I.e., every tree position is either a single index *i*,\n    specifying ``tree[i]``; or a sequence *i1, i2, ..., iN*,\n    specifying ``tree[i1][i2]...[iN]``.\n\n    Construct a new tree.  This constructor can be called in one\n    of two ways:\n\n    - ``Tree(label, children)`` constructs a new tree with the\n        specified label and list of children.\n\n    - ``Tree.fromstring(s)`` constructs a new tree by parsing the string ``s``.\n    ",
        "klass": "nltk.tree.Tree",
        "module": "nltk"
    },
    {
        "base_classes": [
            "collections.defaultdict"
        ],
        "class_docstring": "\n    A collection of frequency distributions for a single experiment\n    run under different conditions.  Conditional frequency\n    distributions are used to record the number of times each sample\n    occurred, given the condition under which the experiment was run.\n    For example, a conditional frequency distribution could be used to\n    record the frequency of each word (type) in a document, given its\n    length.  Formally, a conditional frequency distribution can be\n    defined as a function that maps from each condition to the\n    FreqDist for the experiment under that condition.\n\n    Conditional frequency distributions are typically constructed by\n    repeatedly running an experiment under a variety of conditions,\n    and incrementing the sample outcome counts for the appropriate\n    conditions.  For example, the following code will produce a\n    conditional frequency distribution that encodes how often each\n    word type occurs, given the length of that word type:\n\n        >>> from nltk.probability import ConditionalFreqDist\n        >>> from nltk.tokenize import word_tokenize\n        >>> sent = \"the the the dog dog some other words that we do not care about\"\n        >>> cfdist = ConditionalFreqDist()\n        >>> for word in word_tokenize(sent):\n        ...     condition = len(word)\n        ...     cfdist[condition][word] += 1\n\n    An equivalent way to do this is with the initializer:\n\n        >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n\n    The frequency distribution for each condition is accessed using\n    the indexing operator:\n\n        >>> cfdist[3]\n        FreqDist({'the': 3, 'dog': 2, 'not': 1})\n        >>> cfdist[3].freq('the')\n        0.5\n        >>> cfdist[3]['dog']\n        2\n\n    When the indexing operator is used to access the frequency\n    distribution for a condition that has not been accessed before,\n    ``ConditionalFreqDist`` creates a new empty FreqDist for that\n    condition.\n\n    ",
        "klass": "nltk.probability.ConditionalFreqDist",
        "module": "nltk"
    },
    {
        "base_classes": [
            "collections.Counter"
        ],
        "class_docstring": "\n    A frequency distribution for the outcomes of an experiment.  A\n    frequency distribution records the number of times each outcome of\n    an experiment has occurred.  For example, a frequency distribution\n    could be used to record the frequency of each word type in a\n    document.  Formally, a frequency distribution can be defined as a\n    function mapping from each sample to the number of times that\n    sample occurred as an outcome.\n\n    Frequency distributions are generally constructed by running a\n    number of experiments, and incrementing the count for a sample\n    every time it is an outcome of an experiment.  For example, the\n    following code will produce a frequency distribution that encodes\n    how often each word occurs in a text:\n\n        >>> from nltk.tokenize import word_tokenize\n        >>> from nltk.probability import FreqDist\n        >>> sent = 'This is an example sentence'\n        >>> fdist = FreqDist()\n        >>> for word in word_tokenize(sent):\n        ...    fdist[word.lower()] += 1\n\n    An equivalent way to do this is with the initializer:\n\n        >>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n\n    ",
        "klass": "nltk.probability.FreqDist",
        "module": "nltk"
    },
    {
        "base_classes": [
            "nltk.tag.api.TaggerI"
        ],
        "class_docstring": "\n    TnT - Statistical POS tagger\n\n    IMPORTANT NOTES:\n\n    * DOES NOT AUTOMATICALLY DEAL WITH UNSEEN WORDS\n\n      - It is possible to provide an untrained POS tagger to\n        create tags for unknown words, see __init__ function\n\n    * SHOULD BE USED WITH SENTENCE-DELIMITED INPUT\n\n      - Due to the nature of this tagger, it works best when\n        trained over sentence delimited input.\n      - However it still produces good results if the training\n        data and testing data are separated on all punctuation eg: [,.?!]\n      - Input for training is expected to be a list of sentences\n        where each sentence is a list of (word, tag) tuples\n      - Input for tag function is a single sentence\n        Input for tagdata function is a list of sentences\n        Output is of a similar form\n\n    * Function provided to process text that is unsegmented\n\n      - Please see basic_sent_chop()\n\n\n    TnT uses a second order Markov model to produce tags for\n    a sequence of input, specifically:\n\n      argmax [Proj(P(t_i|t_i-1,t_i-2)P(w_i|t_i))] P(t_T+1 | t_T)\n\n    IE: the maximum projection of a set of probabilities\n\n    The set of possible tags for a given word is derived\n    from the training data. It is the set of all tags\n    that exact word has been assigned.\n\n    To speed up and get more precision, we can use log addition\n    to instead multiplication, specifically:\n\n      argmax [Sigma(log(P(t_i|t_i-1,t_i-2))+log(P(w_i|t_i)))] +\n             log(P(t_T+1|t_T))\n\n    The probability of a tag for a given word is the linear\n    interpolation of 3 markov models; a zero-order, first-order,\n    and a second order model.\n\n      P(t_i| t_i-1, t_i-2) = l1*P(t_i) + l2*P(t_i| t_i-1) +\n                             l3*P(t_i| t_i-1, t_i-2)\n\n    A beam search is used to limit the memory usage of the algorithm.\n    The degree of the beam can be changed using N in the initialization.\n    N represents the maximum number of possible solutions to maintain\n    while tagging.\n\n    It is possible to differentiate the tags which are assigned to\n    capitalized words. However this does not result in a significant\n    gain in the accuracy of the results.\n    ",
        "klass": "nltk.tag.tnt.TnT",
        "module": "nltk"
    },
    {
        "base_classes": [
            "nltk.tokenize.punkt.PunktBaseClass",
            "nltk.tokenize.api.TokenizerI"
        ],
        "class_docstring": "\n    A sentence tokenizer which uses an unsupervised algorithm to build\n    a model for abbreviation words, collocations, and words that start\n    sentences; and then uses that model to find sentence boundaries.\n    This approach has been shown to work well for many European\n    languages.\n    ",
        "klass": "nltk.tokenize.punkt.PunktSentenceTokenizer",
        "module": "nltk"
    },
    {
        "base_classes": [
            "nltk.tokenize.api.TokenizerI"
        ],
        "class_docstring": "\n    A tokenizer that splits a string using a regular expression, which\n    matches either the tokens or the separators between tokens.\n\n        >>> tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n\n    :type pattern: str\n    :param pattern: The pattern used to build this tokenizer.\n        (This pattern must not contain capturing parentheses;\n        Use non-capturing parentheses, e.g. (?:...), instead)\n    :type gaps: bool\n    :param gaps: True if this tokenizer's pattern should be used\n        to find separators between tokens; False if this\n        tokenizer's pattern should be used to find the tokens\n        themselves.\n    :type discard_empty: bool\n    :param discard_empty: True if any empty tokens `''`\n        generated by the tokenizer should be discarded.  Empty\n        tokens can only be generated if `_gaps == True`.\n    :type flags: int\n    :param flags: The regexp flags used to compile this\n        tokenizer's pattern.  By default, the following flags are\n        used: `re.UNICODE | re.MULTILINE | re.DOTALL`.\n\n    ",
        "klass": "nltk.tokenize.regexp.RegexpTokenizer",
        "module": "nltk"
    },
    {
        "base_classes": [
            "nltk.tree.AbstractParentedTree"
        ],
        "class_docstring": "\n    A ``Tree`` that automatically maintains parent pointers for\n    single-parented trees.  The following are methods for querying\n    the structure of a parented tree: ``parent``, ``parent_index``,\n    ``left_sibling``, ``right_sibling``, ``root``, ``treeposition``.\n\n    Each ``ParentedTree`` may have at most one parent.  In\n    particular, subtrees may not be shared.  Any attempt to reuse a\n    single ``ParentedTree`` as a child of more than one parent (or\n    as multiple children of the same parent) will cause a\n    ``ValueError`` exception to be raised.\n\n    ``ParentedTrees`` should never be used in the same tree as ``Trees``\n    or ``MultiParentedTrees``.  Mixing tree implementations may result\n    in incorrect parent pointers and in ``TypeError`` exceptions.\n    ",
        "klass": "nltk.tree.ParentedTree",
        "module": "nltk"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.TransformerMixin"
        ],
        "class_docstring": "Encode labels with value between 0 and n_classes-1.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_class,)\n        Holds the label for each class.\n\n    Examples\n    --------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn import preprocessing\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    ['amsterdam', 'paris', 'tokyo']\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) #doctest: +ELLIPSIS\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    ['tokyo', 'tokyo', 'paris']\n\n    See also\n    --------\n    sklearn.preprocessing.OrdinalEncoder : encode categorical features\n        using a one-hot or ordinal encoding scheme.\n    ",
        "klass": "sklearn.preprocessing.label.LabelEncoder",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "plistlib._InternalDict"
        ],
        "class_docstring": "This class has been deprecated. Use dump() and load()\n    functions instead, together with regular dict objects.\n    ",
        "klass": "plistlib.Plist",
        "module": "plistlib"
    },
    {
        "base_classes": [
            "sleekxmpp.xmlstream.stanzabase.ElementBase"
        ],
        "class_docstring": "\n    StanzaBase provides the foundation for all other stanza objects used\n    by SleekXMPP, and defines a basic set of interfaces common to nearly\n    all stanzas. These interfaces are the ``'id'``, ``'type'``, ``'to'``,\n    and ``'from'`` attributes. An additional interface, ``'payload'``, is\n    available to access the XML contents of the stanza. Most stanza objects\n    will provided more specific interfaces, however.\n\n    **Stanza Interfaces:**\n\n        :id: An optional id value that can be used to associate stanzas\n        :to: A JID object representing the recipient's JID.\n        :from: A JID object representing the sender's JID.\n               with their replies.\n        :type: The type of stanza, typically will be ``'normal'``,\n               ``'error'``, ``'get'``, or ``'set'``, etc.\n        :payload: The XML contents of the stanza.\n\n    :param XMLStream stream: Optional :class:`sleekxmpp.xmlstream.XMLStream`\n                             object responsible for sending this stanza.\n    :param XML xml: Optional XML contents to initialize stanza values.\n    :param string stype: Optional stanza type value.\n    :param sto: Optional string or :class:`sleekxmpp.xmlstream.JID`\n                object of the recipient's JID.\n    :param sfrom: Optional string or :class:`sleekxmpp.xmlstream.JID`\n                  object of the sender's JID.\n    :param string sid: Optional ID value for the stanza.\n    :param parent: Optionally specify a parent stanza object will\n                   contain this substanza.\n    ",
        "klass": "sleekxmpp.xmlstream.stanzabase.StanzaBase",
        "module": "sleekxmpp"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Mersenne Twister pseudo-random number generator\n\n    This object contains state to deterministically generate pseudo-random\n    numbers from a variety of probability distributions.  It is identical to\n    ``np.random.RandomState`` except that all functions also take a ``chunks=``\n    keyword argument.\n\n    Parameters\n    ----------\n    seed: Number\n        Object to pass to RandomState to serve as deterministic seed\n    RandomState: Callable[seed] -> RandomState\n        A callable that, when provided with a ``seed`` keyword provides an\n        object that operates identically to ``np.random.RandomState`` (the\n        default).  This might also be a function that returns a\n        ``randomgen.RandomState``, ``mkl_random``, or\n        ``cupy.random.RandomState`` object.\n\n    Examples\n    --------\n    >>> import dask.array as da\n    >>> state = da.random.RandomState(1234)  # a seed\n    >>> x = state.normal(10, 0.1, size=3, chunks=(2,))\n    >>> x.compute()\n    array([10.01867852, 10.04812289,  9.89649746])\n\n    See Also\n    --------\n    np.random.RandomState\n    ",
        "klass": "dask.array.random.RandomState",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "API spec for the methods a filesystem\n\n    A filesystem must provide these methods, if it is to be registered as\n    a backend for dask.\n\n    Implementation for local disc",
        "klass": "dask.bytes.local.LocalFileSystem",
        "module": "dask"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Has methods to ``enumerate`` and ``count`` the partitions of a multiset.\n\n    This implements a refactored and extended version of Knuth's algorithm\n    7.1.2.5M.\n\n    The enumeration methods of this class are generators and return\n    data structures which can be interpreted by the same visitor\n    functions used for the output of ``multiset_partitions_taocp``.\n\n    See Also\n    ========\n\n    multiset_partitions_taocp\n\n    Examples\n    ========\n\n    >>> m = MultisetPartitionTraverser()\n    >>> m.count_partitions([4, 4, 4, 2])\n    127750\n    >>> m.count_partitions([3, 3, 3])\n    686\n\n    References\n    ==========\n\n    * Algorithm 7.1.2.5M in Volume 4A, Combinatoral Algorithms,\n      Part 1, of The Art of Computer Programming, by Donald Knuth.\n\n    * On a Problem of Oppenheim concerning\n      \"Factorisatio Numerorum\" E. R. Canfield, Paul Erd\u0151s, Carl\n      Pomerance, JOURNAL OF NUMBER THEORY, Vol. 17, No. 1. August\n      1983.  See section 7 for a description of an algorithm\n      similar to Knuth's.\n\n    * Generating Multiset Partitions, Brent Yorgey, The\n      Monad.Reader, Issue 8, September 2007.\n\n    ",
        "klass": "diofant.utilities.enumerative.MultisetPartitionTraverser",
        "module": "diofant"
    },
    {
        "base_classes": [
            "pyparsing.TokenConverter"
        ],
        "class_docstring": "\n    Converter to return a repetitive expression as a list, but also as a dictionary.\n    Each element can also be referenced using the first token in the expression as its key.\n    Useful for tabular report scraping when the first column can be used as a item key.\n\n    Example::\n        data_word = Word(alphas)\n        label = data_word + FollowedBy(':')\n        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))\n\n        text = \"shape: SQUARE posn: upper left color: light blue texture: burlap\"\n        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n        \n        # print attributes as plain groups\n        print(OneOrMore(attr_expr).parseString(text).dump())\n        \n        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names\n        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)\n        print(result.dump())\n        \n        # access named fields as dict entries, or output as dict\n        print(result['shape'])        \n        print(result.asDict())\n    prints::\n        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']\n\n        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]\n        - color: light blue\n        - posn: upper left\n        - shape: SQUARE\n        - texture: burlap\n        SQUARE\n        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}\n    See more examples at L{ParseResults} of accessing fields by results name.\n    ",
        "klass": "pyparsing.Dict",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "pyparsing.ParseExpression"
        ],
        "class_docstring": "\n    Requires that at least one C{ParseExpression} is found.\n    If two expressions match, the expression that matches the longest string will be used.\n    May be constructed using the C{'^'} operator.\n\n    Example::\n        # construct Or using '^' operator\n        \n        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))\n        print(number.searchString(\"123 3.1416 789\"))\n    prints::\n        [['123'], ['3.1416'], ['789']]\n    ",
        "klass": "pyparsing.Or",
        "module": "pyparsing"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A credentials object for the default authentication methodology with\n    RabbitMQ.\n\n    If you do not pass in credentials to the ConnectionParameters object, it\n    will create credentials for 'guest' with the password of 'guest'.\n\n    If you pass True to erase_on_connect the credentials will not be stored\n    in memory after the Connection attempt has been made.\n\n    :param str username: The username to authenticate with\n    :param str password: The password to authenticate with\n    :param bool erase_on_connect: erase credentials on connect.\n\n    ",
        "klass": "pika.credentials.PlainCredentials",
        "module": "pika"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The ExternalCredentials class allows the connection to use EXTERNAL\n    authentication, generally with a client SSL certificate.\n\n    ",
        "klass": "pika.credentials.ExternalCredentials",
        "module": "pika"
    },
    {
        "base_classes": [
            "pygeodesy.sphericalBase.LatLonSphericalBase"
        ],
        "class_docstring": "New point on spherical model earth model.\n\n       @example:\n\n       >>> p = LatLon(52.205, 0.119)  # height=0\n    ",
        "klass": "pygeodesy.sphericalTrigonometry.LatLon",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.nvectorBase.LatLonNvectorBase",
            "pygeodesy.sphericalBase.LatLonSphericalBase"
        ],
        "class_docstring": "New n-vector based point on a spherical earth model.\n\n       Tools for working with points and paths on (a spherical\n       model of) the earth's surface using vector-based methods.\n\n       @example:\n\n       >>> from sphericalNvector import LatLon\n       >>> p = LatLon(52.205, 0.119)\n    ",
        "klass": "pygeodesy.sphericalNvector.LatLon",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.ellipsoidalBase.CartesianEllipsoidalBase"
        ],
        "class_docstring": "Extended to convert C{Karney}-based L{Cartesian} to\n       C{Karney}-based L{LatLon} points.\n    ",
        "klass": "pygeodesy.ellipsoidalKarney.Cartesian",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.ellipsoidalBase.LatLonEllipsoidalBase"
        ],
        "class_docstring": "An ellipsoidal L{LatLon} similar to L{ellipsoidalVincenty.LatLon}\n       but using I{Charles F. F. Karney's} Python U{GeographicLib\n       <https://PyPI.org/project/geographiclib>} to compute the geodesic\n       distance, initial and final bearing (azimuths) between two given\n       points or the destination point given a start point and an initial\n       bearing.\n\n       @note: This L{LatLon}'s methods require the U{GeographicLib\n              <https://PyPI.org/project/geographiclib>} package.\n    ",
        "klass": "pygeodesy.ellipsoidalKarney.LatLon",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.named._NamedBase"
        ],
        "class_docstring": "Ordinance Survey Grid References (OSGR) coordinate.\n    ",
        "klass": "pygeodesy.osgr.Osgr",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.ellipsoidalBase.CartesianEllipsoidalBase"
        ],
        "class_docstring": "Extended to convert geocentric, L{Cartesian} points to\n       Vincenty-based, ellipsoidal, geodetic L{LatLon}.\n    ",
        "klass": "pygeodesy.ellipsoidalVincenty.Cartesian",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.ellipsoidalBase.LatLonEllipsoidalBase"
        ],
        "class_docstring": "Using the formulae devised by Thaddeus Vincenty (1975) with an\n       ellipsoidal model of the earth to compute the geodesic distance\n       and bearings between two given points or the destination point\n       given an start point and initial bearing.\n\n       Set the earth model to be used with the keyword argument\n       datum.  The default is Datums.WGS84, which is the most globally\n       accurate.  For other models, see the Datums in module datum.\n\n       Note: This implementation of the Vincenty methods may not\n       converge for some valid points, raising a VincentyError.  In\n       that case, a result may be obtained by increasing the epsilon\n       and/or the iteration limit, see properties L{LatLon.epsilon}\n       and L{LatLon.iterations}.\n    ",
        "klass": "pygeodesy.ellipsoidalVincenty.LatLon",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.ellipsoidalBase.CartesianEllipsoidalBase"
        ],
        "class_docstring": "Extended to convert geocentric, L{Cartesian} points to\n       L{Nvector} and n-vector-based, geodetic L{LatLon}.\n    ",
        "klass": "pygeodesy.ellipsoidalNvector.Cartesian",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.nvectorBase.LatLonNvectorBase",
            "pygeodesy.ellipsoidalBase.LatLonEllipsoidalBase"
        ],
        "class_docstring": "An n-vector-based, ellipsoidal L{LatLon} point.\n\n       @example:\n\n       >>> from ellipsoidalNvector import LatLon\n       >>> p = LatLon(52.205, 0.119)  # height=0, datum=Datums.WGS84\n    ",
        "klass": "pygeodesy.ellipsoidalNvector.LatLon",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "pygeodesy.nvectorBase.NvectorBase"
        ],
        "class_docstring": "An n-vector is a position representation using a (unit) vector\n       normal to the earth ellipsoid.  Unlike lat-/longitude points,\n       n-vectors have no singularities or discontinuities.\n\n       For many applications, n-vectors are more convenient to work\n       with than other position representations like lat-/longitude,\n       earth-centred earth-fixed (ECEF) vectors, UTM coordinates, etc.\n\n       Note commonality with L{sphericalNvector.Nvector}.\n    ",
        "klass": "pygeodesy.ellipsoidalNvector.Nvector",
        "module": "pygeodesy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Undirected graph structure that stores neural gas network's\n    neurons and connections between them.\n\n    Attributes\n    ----------\n    edges_per_node : dict\n        Dictionary that where key is a unique node and value is a list\n        of nodes that connection to this edge.\n\n    edges : dict\n        Dictonary that stores age per each connection. Ech key will have\n        the following format: ``(node_1, node_2)``.\n\n    nodes : list\n        List of all nodes in the graph (read-only attribute).\n\n    n_nodes : int\n        Number of nodes in the network (read-only attribute).\n\n    n_edges : int\n        Number of edges in the network (read-only attribute).\n    ",
        "klass": "neupy.algorithms.competitive.growing_neural_gas.NeuralGasGraph",
        "module": "neupy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Curl() -> New Curl object\n\nCreates a new :ref:`curlobject` which corresponds to a\n``CURL`` handle in libcurl. Curl objects automatically set\nCURLOPT_VERBOSE to 0, CURLOPT_NOPROGRESS to 1, provide a default\nCURLOPT_USERAGENT and setup CURLOPT_ERRORBUFFER to point to a\nprivate error buffer.\n\nImplicitly calls :py:func:`pycurl.global_init` if the latter has not yet been called.",
        "klass": "pycurl.Curl",
        "module": "pycurl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "CurlMulti() -> New CurlMulti object\n\nCreates a new :ref:`curlmultiobject` which corresponds to\na ``CURLM`` handle in libcurl.",
        "klass": "pycurl.CurlMulti",
        "module": "pycurl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Methods for building a CP model.\n\n  Methods beginning with:\n\n  * ```New``` create integer, boolean, or interval variables.\n  * ```Add``` create new constraints and add them to the model.\n  ",
        "klass": "ortools.sat.python.cp_model.CpModel",
        "module": "ortools"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Main solver class.\n\n  The purpose of this class is to search for a solution to the model provided\n  to the Solve() method.\n\n  Once Solve() is called, this class allows inspecting the solution found\n  with the Value() and BooleanValue() methods, as well as general statistics\n  about the solve procedure.\n  ",
        "klass": "ortools.sat.python.cp_model.CpSolver",
        "module": "ortools"
    },
    {
        "base_classes": [
            "io.BufferedIOBase"
        ],
        "class_docstring": "\n    Weak replacement for the Python's tempfile.TemporaryFile.\n\n    This class is a replacement for :func:`tempfile.NamedTemporaryFile` but\n    will work also with Windows 7/Vista's UAC.\n\n    :type dir: str\n    :param dir: If specified, the file will be created in that directory,\n        otherwise the default directory for temporary files is used.\n    :type suffix: str\n    :param suffix: The temporary file name will end with that suffix. Defaults\n        to ``'.tmp'``.\n\n    .. rubric:: Example\n\n    >>> with NamedTemporaryFile() as tf:\n    ...     _ = tf.write(b\"test\")\n    ...     os.path.exists(tf.name)\n    True\n    >>> # when using the with statement, the file is deleted at the end:\n    >>> os.path.exists(tf.name)\n    False\n\n    >>> with NamedTemporaryFile() as tf:\n    ...     filename = tf.name\n    ...     with open(filename, 'wb') as fh:\n    ...         _ = fh.write(b\"just a test\")\n    ...     with open(filename, 'r') as fh:\n    ...         print(fh.read())\n    just a test\n    >>> # when using the with statement, the file is deleted at the end:\n    >>> os.path.exists(tf.name)\n    False\n    ",
        "klass": "obspy.core.util.base.NamedTemporaryFile",
        "module": "obspy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class for collecting and sending usage analytics.\n\n    Args:\n        info (dict): optional existing analytics report.\n    ",
        "klass": "dvc.analytics.Analytics",
        "module": "dvc"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class to display a progress bar in the terminal.\n\n    It is designed to be used either with the ``with`` statement::\n\n        with ProgressBar(len(items)) as bar:\n            for item in enumerate(items):\n                bar.update()\n\n    or as a generator::\n\n        for item in ProgressBar(items):\n            item.process()\n    ",
        "klass": "astropy.utils.console.ProgressBar",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A class to display a spinner in the terminal.\n\n    It is designed to be used with the ``with`` statement::\n\n        with Spinner(\"Reticulating splines\", \"green\") as s:\n            for item in enumerate(items):\n                s.next()\n    ",
        "klass": "astropy.utils.console.Spinner",
        "module": "astropy"
    },
    {
        "base_classes": [
            "warnings.catch_warnings"
        ],
        "class_docstring": "\n    A high-powered version of warnings.catch_warnings to use for testing\n    and to make sure that there is no dependence on the order in which\n    the tests are run.\n\n    This completely blitzes any memory of any warnings that have\n    appeared before so that all warnings will be caught and displayed.\n\n    ``*args`` is a set of warning classes to collect.  If no arguments are\n    provided, all warnings are collected.\n\n    Use as follows::\n\n        with catch_warnings(MyCustomWarning) as w:\n            do.something.bad()\n        assert len(w) > 0\n    ",
        "klass": "astropy.tests.helper.catch_warnings",
        "module": "astropy"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "switch_db alias context manager.\n\n    Example ::\n\n        # Register connections\n        register_connection('default', 'mongoenginetest')\n        register_connection('testdb-1', 'mongoenginetest2')\n\n        class Group(Document):\n            name = StringField()\n\n        Group(name='test').save()  # Saves in the default db\n\n        with switch_db(Group, 'testdb-1') as Group:\n            Group(name='hello testdb!').save()  # Saves in testdb-1\n    ",
        "klass": "mongoengine.context_managers.switch_db",
        "module": "mongoengine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "no_dereference context manager.\n\n    Turns off all dereferencing in Documents for the duration of the context\n    manager::\n\n        with no_dereference(Group) as Group:\n            Group.objects.find()\n    ",
        "klass": "mongoengine.context_managers.no_dereference",
        "module": "mongoengine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Query_counter context manager to get the number of queries.\n    This works by updating the `profiling_level` of the database so that all queries get logged,\n    resetting the db.system.profile collection at the beginnig of the context and counting the new entries.\n\n    This was designed for debugging purpose. In fact it is a global counter so queries issued by other threads/processes\n    can interfere with it\n\n    Be aware that:\n    - Iterating over large amount of documents (>101) makes pymongo issue `getmore` queries to fetch the next batch of\n        documents (https://docs.mongodb.com/manual/tutorial/iterate-a-cursor/#cursor-batches)\n    - Some queries are ignored by default by the counter (killcursors, db.system.indexes)\n    ",
        "klass": "mongoengine.context_managers.query_counter",
        "module": "mongoengine"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "switch_collection alias context manager.\n\n    Example ::\n\n        class Group(Document):\n            name = StringField()\n\n        Group(name='test').save()  # Saves in the default db\n\n        with switch_collection(Group, 'group1') as Group:\n            Group(name='hello testdb!').save()  # Saves in group1 collection\n    ",
        "klass": "mongoengine.context_managers.switch_collection",
        "module": "mongoengine"
    },
    {
        "base_classes": [
            "configparser.RawConfigParser",
            "object"
        ],
        "class_docstring": "Enhanced ConfigParser class that supports the addition of default\n    sections and default values.\n\n    By default, the kivy ConfigParser instance, :attr:`~kivy.config.Config`,\n    is named `'kivy'` and the ConfigParser instance used by the\n    :meth:`App.build_settings <~kivy.app.App.build_settings>` method is named\n    `'app'`.\n\n    :Parameters:\n        `name`: string\n            The name of the instance. See :attr:`name`. Defaults to `''`.\n\n    .. versionchanged:: 1.9.0\n        Each ConfigParser can now be :attr:`named <name>`. You can get the\n        ConfigParser associated with a name using :meth:`get_configparser`.\n        In addition, you can now control the config values with\n        :class:`~kivy.properties.ConfigParserProperty`.\n\n    .. versionadded:: 1.0.7\n    ",
        "klass": "kivy.config.ConfigParser",
        "module": "kivy"
    },
    {
        "base_classes": [
            "list"
        ],
        "class_docstring": "Vector class. See module documentation for more information.\n    ",
        "klass": "kivy.vector.Vector",
        "module": "kivy"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    :param headers:\n        An iterable of field-value pairs. Must not contain multiple field names\n        when compared case-insensitively.\n\n    :param kwargs:\n        Additional field-value pairs to pass in to ``dict.update``.\n\n    A ``dict`` like container for storing HTTP Headers.\n\n    Field names are stored and compared case-insensitively in compliance with\n    RFC 7230. Iteration provides the first case-sensitive key seen for each\n    case-insensitive pair.\n\n    Using ``__setitem__`` syntax overwrites fields that compare equal\n    case-insensitively in order to maintain ``dict``'s api. For fields that\n    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``\n    in a loop.\n\n    If multiple fields that are equal case-insensitively are passed to the\n    constructor or ``.update``, the behavior is undefined and some will be\n    lost.\n\n    >>> headers = HTTPHeaderDict()\n    >>> headers.add('Set-Cookie', 'foo=bar')\n    >>> headers.add('set-cookie', 'baz=quxx')\n    >>> headers['content-length'] = '7'\n    >>> headers['SET-cookie']\n    'foo=bar, baz=quxx'\n    >>> headers['Content-Length']\n    '7'\n    ",
        "klass": "urllib3._collections.HTTPHeaderDict",
        "module": "urllib3"
    },
    {
        "base_classes": [
            "collections.abc.MutableMapping"
        ],
        "class_docstring": "\n    Representation of an OpenSSH-style \"known hosts\" file.  Host keys can be\n    read from one or more files, and then individual hosts can be looked up to\n    verify server keys during SSH negotiation.\n\n    A `.HostKeys` object can be treated like a dict; any dict lookup is\n    equivalent to calling `lookup`.\n\n    .. versionadded:: 1.5.3\n    ",
        "klass": "paramiko.hostkeys.HostKeys",
        "module": "paramiko"
    },
    {
        "base_classes": [
            "threading.Thread",
            "paramiko.util.ClosingContextManager"
        ],
        "class_docstring": "\n    An SSH Transport attaches to a stream (usually a socket), negotiates an\n    encrypted session, authenticates, and then creates stream tunnels, called\n    `channels <.Channel>`, across the session.  Multiple channels can be\n    multiplexed across a single session (and often are, in the case of port\n    forwardings).\n\n    Instances of this class may be used as context managers.\n    ",
        "klass": "paramiko.transport.Transport",
        "module": "paramiko"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A buffer that obeys normal read (with timeout) & close semantics for a\n    file or socket, but is fed data from another thread.  This is used by\n    `.Channel`.\n    ",
        "klass": "paramiko.buffered_pipe.BufferedPipe",
        "module": "paramiko"
    },
    {
        "base_classes": [
            "config.Mapping"
        ],
        "class_docstring": "\n    This class represents a configuration, and is the only one which clients\n    need to interface to, under normal circumstances.\n    ",
        "klass": "config.Config",
        "module": "config"
    },
    {
        "base_classes": [
            "girder.models.model_base.AccessControlledModel"
        ],
        "class_docstring": "\n    Groups are simply groups of users. The primary use of grouping users is\n    to simplify access control for resources in the system, but they can\n    be used for other purposes that require groupings of users as well.\n\n    Group membership is stored in the database on the user document only;\n    there is no \"users\" field in this model. This is to optimize for the most\n    common use case for querying membership, which involves checking access\n    control policies, which is always done relative to a specific user. The\n    task of querying all members within a group is much less common and\n    typically only performed on a single group at a time, so doing a find on the\n    indexed group list in the user collection is sufficiently fast.\n\n    Users with READ access on the group can see the group and its members.\n    Users with WRITE access on the group can add and remove members and\n    change the name or description.\n    Users with ADMIN access can promote group members to grant them WRITE or\n    ADMIN access, and can also delete the entire group.\n\n    This model uses a custom implementation of the access control methods,\n    because it uses only a subset of its capabilities and provides a more\n    optimized implementation for that subset. Specifically: read access is\n    implied by membership in the group or having an invitation to join the\n    group, so we don't store read access in the access document as normal.\n    Another constraint is that write and admin access on the group can only be\n    granted to members of the group. Also, group permissions are not allowed\n    on groups for the sake of simplicity.\n    ",
        "klass": "girder.models.group.Group",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.models.model_base.Model"
        ],
        "class_docstring": "\n    This model represents server-wide configuration settings as key/value pairs.\n    ",
        "klass": "girder.models.setting.Setting",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.models.model_base.AccessControlledModel"
        ],
        "class_docstring": "\n    This model represents the users of the system.\n    ",
        "klass": "girder.models.user.User",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.utility.acl_mixin.AccessControlMixin",
            "girder.models.model_base.Model"
        ],
        "class_docstring": "\n    This model represents a File, which is stored in an assetstore.\n    ",
        "klass": "girder.models.file.File",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.models.model_base.AccessControlledModel"
        ],
        "class_docstring": "\n    Collections are the top level roots of the data hierarchy. They are used\n    to group and organize data that is meant to be shared amongst users.\n    ",
        "klass": "girder.models.collection.Collection",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.models.model_base.AccessControlledModel"
        ],
        "class_docstring": "\n    Folders are used to store items and can also store other folders in\n    a hierarchical way, like a directory on a filesystem. Every folder has\n    its own set of access control policies, but by default the access\n    control list is inherited from the folder's parent folder, if it has one.\n    Top-level folders are ones whose parent is a user or a collection.\n    ",
        "klass": "girder.models.folder.Folder",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.utility.acl_mixin.AccessControlMixin",
            "girder.models.model_base.Model"
        ],
        "class_docstring": "\n    Items are leaves in the data hierarchy. They can contain 0 or more\n    files within them, and can also contain arbitrary metadata.\n    ",
        "klass": "girder.models.item.Item",
        "module": "girder"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This class is a context manager that can be used to update progress in a way\n    that rate-limits writes to the database and guarantees a flush when the\n    context is exited. This is a no-op if \"on\" is set to False, which is\n    meant as a convenience for callers. Any additional kwargs passed to this\n    constructor are passed through to the ``initProgress`` method of the\n    notification model.\n\n    :param on: Whether to record progress.\n    :type on: bool\n    :param interval: Minimum time interval at which to write updates to the\n        database, in seconds.\n    :type interval: int or float\n    :param user: The user creating this progress.\n    :type user: dict\n    :param title: The title for the task being tracked.\n    :type title: str\n    ",
        "klass": "girder.utility.progress.ProgressContext",
        "module": "girder"
    },
    {
        "base_classes": [
            "girder.models.model_base.AccessControlledModel"
        ],
        "class_docstring": "\n    This model stores session tokens for user authentication.\n    ",
        "klass": "girder.models.token.Token",
        "module": "girder"
    },
    {
        "base_classes": [
            "bitstring.Bits"
        ],
        "class_docstring": "A container holding a mutable sequence of bits.\n\n    Subclass of the immutable Bits class. Inherits all of its\n    methods (except __hash__) and adds mutating methods.\n\n    Mutating methods:\n\n    append() -- Append a bitstring.\n    byteswap() -- Change byte endianness in-place.\n    insert() -- Insert a bitstring.\n    invert() -- Flip bit(s) between one and zero.\n    overwrite() -- Overwrite a section with a new bitstring.\n    prepend() -- Prepend a bitstring.\n    replace() -- Replace occurrences of one bitstring with another.\n    reverse() -- Reverse bits in-place.\n    rol() -- Rotate bits to the left.\n    ror() -- Rotate bits to the right.\n    set() -- Set bit(s) to 1 or 0.\n\n    Methods inherited from Bits:\n\n    all() -- Check if all specified bits are set to 1 or 0.\n    any() -- Check if any of specified bits are set to 1 or 0.\n    count() -- Count the number of bits set to 1 or 0.\n    cut() -- Create generator of constant sized chunks.\n    endswith() -- Return whether the bitstring ends with a sub-string.\n    find() -- Find a sub-bitstring in the current bitstring.\n    findall() -- Find all occurrences of a sub-bitstring in the current bitstring.\n    join() -- Join bitstrings together using current bitstring.\n    rfind() -- Seek backwards to find a sub-bitstring.\n    split() -- Create generator of chunks split by a delimiter.\n    startswith() -- Return whether the bitstring starts with a sub-bitstring.\n    tobytes() -- Return bitstring as bytes, padding if needed.\n    tofile() -- Write bitstring to file, padding if needed.\n    unpack() -- Interpret bits using format string.\n\n    Special methods:\n\n    Mutating operators are available: [], <<=, >>=, +=, *=, &=, |= and ^=\n    in addition to the inherited [], ==, !=, +, *, ~, <<, >>, &, | and ^.\n\n    Properties:\n\n    bin -- The bitstring as a binary string.\n    bool -- For single bit bitstrings, interpret as True or False.\n    bytepos -- The current byte position in the bitstring.\n    bytes -- The bitstring as a bytes object.\n    float -- Interpret as a floating point number.\n    floatbe -- Interpret as a big-endian floating point number.\n    floatle -- Interpret as a little-endian floating point number.\n    floatne -- Interpret as a native-endian floating point number.\n    hex -- The bitstring as a hexadecimal string.\n    int -- Interpret as a two's complement signed integer.\n    intbe -- Interpret as a big-endian signed integer.\n    intle -- Interpret as a little-endian signed integer.\n    intne -- Interpret as a native-endian signed integer.\n    len -- Length of the bitstring in bits.\n    oct -- The bitstring as an octal string.\n    pos -- The current bit position in the bitstring.\n    se -- Interpret as a signed exponential-Golomb code.\n    ue -- Interpret as an unsigned exponential-Golomb code.\n    sie -- Interpret as a signed interleaved exponential-Golomb code.\n    uie -- Interpret as an unsigned interleaved exponential-Golomb code.\n    uint -- Interpret as a two's complement unsigned integer.\n    uintbe -- Interpret as a big-endian unsigned integer.\n    uintle -- Interpret as a little-endian unsigned integer.\n    uintne -- Interpret as a native-endian unsigned integer.\n\n    ",
        "klass": "bitstring.BitArray",
        "module": "bitstring"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A container holding an immutable sequence of bits.\n\n    For a mutable container use the BitArray class instead.\n\n    Methods:\n\n    all() -- Check if all specified bits are set to 1 or 0.\n    any() -- Check if any of specified bits are set to 1 or 0.\n    count() -- Count the number of bits set to 1 or 0.\n    cut() -- Create generator of constant sized chunks.\n    endswith() -- Return whether the bitstring ends with a sub-string.\n    find() -- Find a sub-bitstring in the current bitstring.\n    findall() -- Find all occurrences of a sub-bitstring in the current bitstring.\n    join() -- Join bitstrings together using current bitstring.\n    rfind() -- Seek backwards to find a sub-bitstring.\n    split() -- Create generator of chunks split by a delimiter.\n    startswith() -- Return whether the bitstring starts with a sub-bitstring.\n    tobytes() -- Return bitstring as bytes, padding if needed.\n    tofile() -- Write bitstring to file, padding if needed.\n    unpack() -- Interpret bits using format string.\n\n    Special methods:\n\n    Also available are the operators [], ==, !=, +, *, ~, <<, >>, &, |, ^.\n\n    Properties:\n\n    bin -- The bitstring as a binary string.\n    bool -- For single bit bitstrings, interpret as True or False.\n    bytes -- The bitstring as a bytes object.\n    float -- Interpret as a floating point number.\n    floatbe -- Interpret as a big-endian floating point number.\n    floatle -- Interpret as a little-endian floating point number.\n    floatne -- Interpret as a native-endian floating point number.\n    hex -- The bitstring as a hexadecimal string.\n    int -- Interpret as a two's complement signed integer.\n    intbe -- Interpret as a big-endian signed integer.\n    intle -- Interpret as a little-endian signed integer.\n    intne -- Interpret as a native-endian signed integer.\n    len -- Length of the bitstring in bits.\n    oct -- The bitstring as an octal string.\n    se -- Interpret as a signed exponential-Golomb code.\n    ue -- Interpret as an unsigned exponential-Golomb code.\n    sie -- Interpret as a signed interleaved exponential-Golomb code.\n    uie -- Interpret as an unsigned interleaved exponential-Golomb code.\n    uint -- Interpret as a two's complement unsigned integer.\n    uintbe -- Interpret as a big-endian unsigned integer.\n    uintle -- Interpret as a little-endian unsigned integer.\n    uintne -- Interpret as a native-endian unsigned integer.\n\n    ",
        "klass": "bitstring.Bits",
        "module": "bitstring"
    },
    {
        "base_classes": [
            "bitstring.Bits"
        ],
        "class_docstring": "A container or stream holding an immutable sequence of bits.\n\n    For a mutable container use the BitStream class instead.\n\n    Methods inherited from Bits:\n\n    all() -- Check if all specified bits are set to 1 or 0.\n    any() -- Check if any of specified bits are set to 1 or 0.\n    count() -- Count the number of bits set to 1 or 0.\n    cut() -- Create generator of constant sized chunks.\n    endswith() -- Return whether the bitstring ends with a sub-string.\n    find() -- Find a sub-bitstring in the current bitstring.\n    findall() -- Find all occurrences of a sub-bitstring in the current bitstring.\n    join() -- Join bitstrings together using current bitstring.\n    rfind() -- Seek backwards to find a sub-bitstring.\n    split() -- Create generator of chunks split by a delimiter.\n    startswith() -- Return whether the bitstring starts with a sub-bitstring.\n    tobytes() -- Return bitstring as bytes, padding if needed.\n    tofile() -- Write bitstring to file, padding if needed.\n    unpack() -- Interpret bits using format string.\n\n    Other methods:\n\n    bytealign() -- Align to next byte boundary.\n    peek() -- Peek at and interpret next bits as a single item.\n    peeklist() -- Peek at and interpret next bits as a list of items.\n    read() -- Read and interpret next bits as a single item.\n    readlist() -- Read and interpret next bits as a list of items.\n\n    Special methods:\n\n    Also available are the operators [], ==, !=, +, *, ~, <<, >>, &, |, ^.\n\n    Properties:\n\n    bin -- The bitstring as a binary string.\n    bool -- For single bit bitstrings, interpret as True or False.\n    bytepos -- The current byte position in the bitstring.\n    bytes -- The bitstring as a bytes object.\n    float -- Interpret as a floating point number.\n    floatbe -- Interpret as a big-endian floating point number.\n    floatle -- Interpret as a little-endian floating point number.\n    floatne -- Interpret as a native-endian floating point number.\n    hex -- The bitstring as a hexadecimal string.\n    int -- Interpret as a two's complement signed integer.\n    intbe -- Interpret as a big-endian signed integer.\n    intle -- Interpret as a little-endian signed integer.\n    intne -- Interpret as a native-endian signed integer.\n    len -- Length of the bitstring in bits.\n    oct -- The bitstring as an octal string.\n    pos -- The current bit position in the bitstring.\n    se -- Interpret as a signed exponential-Golomb code.\n    ue -- Interpret as an unsigned exponential-Golomb code.\n    sie -- Interpret as a signed interleaved exponential-Golomb code.\n    uie -- Interpret as an unsigned interleaved exponential-Golomb code.\n    uint -- Interpret as a two's complement unsigned integer.\n    uintbe -- Interpret as a big-endian unsigned integer.\n    uintle -- Interpret as a little-endian unsigned integer.\n    uintne -- Interpret as a native-endian unsigned integer.\n\n    ",
        "klass": "bitstring.ConstBitStream",
        "module": "bitstring"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "    Terminal application. Copy data from serial port to console and vice versa.\n    Handle special keys from the console to show menu etc.\n    ",
        "klass": "serial.tools.miniterm.Miniterm",
        "module": "serial"
    },
    {
        "base_classes": [
            "threading.Thread"
        ],
        "class_docstring": "    Implement a serial port read loop and dispatch to a Protocol instance (like\n    the asyncio.Protocol) but do it with threads.\n\n    Calls to close() will close the serial port but it is also possible to just\n    stop() this thread and continue the serial port instance otherwise.\n    ",
        "klass": "serial.threaded.ReaderThread",
        "module": "serial"
    },
    {
        "base_classes": [
            "numba._DUFunc"
        ],
        "class_docstring": "\n    Dynamic universal function (DUFunc) intended to act like a normal\n    Numpy ufunc, but capable of call-time (just-in-time) compilation\n    of fast loops specialized to inputs.\n    ",
        "klass": "numba.npyufunc.dufunc.DUFunc",
        "module": "numba"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An object matching an actual signature against a registry of formal\n    signatures and choosing the best candidate, if any.\n\n    In the current implementation:\n    - a \"signature\" is a tuple of type classes or type instances\n    - the \"best candidate\" is the most specific match\n    ",
        "klass": "numba.targets.base.OverloadSelector",
        "module": "numba"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An X.509 certificate.\n    ",
        "klass": "OpenSSL.crypto.X509",
        "module": "OpenSSL"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An X.509 certificate signing requests.\n    ",
        "klass": "OpenSSL.crypto.X509Req",
        "module": "OpenSSL"
    },
    {
        "base_classes": [
            "zmq.sugar.context.Context"
        ],
        "class_docstring": "Subclass of :class:`zmq.Context`\n    ",
        "klass": "eventlet.green.zmq.Context",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "zmq.sugar.socket.Socket"
        ],
        "class_docstring": "Green version of :class:`zmq.core.socket.Socket\n\n    The following three methods are always overridden:\n        * send\n        * recv\n        * getsockopt\n    To ensure that the ``zmq.NOBLOCK`` flag is set and that sending or receiving\n    is deferred to the hub (using :func:`eventlet.hubs.trampoline`) if a\n    ``zmq.EAGAIN`` (retry) error is raised\n\n    For some socket types, the following methods are also overridden:\n        * send_multipart\n        * recv_multipart\n    ",
        "klass": "eventlet.green.zmq.Socket",
        "module": "eventlet"
    },
    {
        "base_classes": [
            "neutronclient.neutron.v2_0.ShowCommand"
        ],
        "class_docstring": "Retrieve stats for a given loadbalancer.",
        "klass": "neutronclient.neutron.v2_0.lb.v2.loadbalancer.RetrieveLoadBalancerStats",
        "module": "neutronclient"
    },
    {
        "base_classes": [
            "neutronclient.neutron.v2_0.NeutronCommand"
        ],
        "class_docstring": "Retrieve status for a given loadbalancer.\n\n    The only output is a formatted JSON tree, and the table format\n    does not support this type of data.\n    ",
        "klass": "neutronclient.neutron.v2_0.lb.v2.loadbalancer.RetrieveLoadBalancerStatus",
        "module": "neutronclient"
    },
    {
        "base_classes": [
            "neutronclient.neutron.v2_0.ShowCommand"
        ],
        "class_docstring": "Retrieve stats for a given pool.",
        "klass": "neutronclient.neutron.v2_0.lb.pool.RetrievePoolStats",
        "module": "neutronclient"
    },
    {
        "base_classes": [
            "neutronclient.neutron.v2_0.NeutronCommand"
        ],
        "class_docstring": "Create a mapping between a health monitor and a pool.",
        "klass": "neutronclient.neutron.v2_0.lb.healthmonitor.AssociateHealthMonitor",
        "module": "neutronclient"
    },
    {
        "base_classes": [
            "neutronclient.neutron.v2_0.NeutronCommand"
        ],
        "class_docstring": "Remove a mapping from a health monitor to a pool.",
        "klass": "neutronclient.neutron.v2_0.lb.healthmonitor.DisassociateHealthMonitor",
        "module": "neutronclient"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Server class to manage multiple WSGI sockets and applications.",
        "klass": "senlin.api.common.wsgi.Server",
        "module": "senlin"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    The main class to use when interacting with a Cassandra cluster.\n    Typically, one instance of this class will be created for each\n    separate Cassandra cluster that your application interacts with.\n\n    Example usage::\n\n        >>> from cassandra.cluster import Cluster\n        >>> cluster = Cluster(['192.168.1.1', '192.168.1.2'])\n        >>> session = cluster.connect()\n        >>> session.execute(\"CREATE KEYSPACE ...\")\n        >>> ...\n        >>> cluster.shutdown()\n\n    ``Cluster`` and ``Session`` also provide context management functions\n    which implicitly handle shutdown when leaving scope.\n    ",
        "klass": "cassandra.cluster.Cluster",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.BaseCQLStatement"
        ],
        "class_docstring": " a cql delete statement ",
        "klass": "cassandra.cqlengine.statements.DeleteStatement",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.AssignmentStatement"
        ],
        "class_docstring": " an cql insert statement ",
        "klass": "cassandra.cqlengine.statements.InsertStatement",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.ContainerUpdateClause"
        ],
        "class_docstring": " updates a list collection ",
        "klass": "cassandra.cqlengine.statements.ListUpdateClause",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.BaseDeleteClause"
        ],
        "class_docstring": " removes keys from a map ",
        "klass": "cassandra.cqlengine.statements.MapDeleteClause",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.ContainerUpdateClause"
        ],
        "class_docstring": " updates a map collection ",
        "klass": "cassandra.cqlengine.statements.MapUpdateClause",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.BaseCQLStatement"
        ],
        "class_docstring": " a cql select statement ",
        "klass": "cassandra.cqlengine.statements.SelectStatement",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.ContainerUpdateClause"
        ],
        "class_docstring": " updates a set collection ",
        "klass": "cassandra.cqlengine.statements.SetUpdateClause",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.AssignmentStatement"
        ],
        "class_docstring": " an cql update select statement ",
        "klass": "cassandra.cqlengine.statements.UpdateStatement",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "cassandra.cqlengine.statements.BaseClause"
        ],
        "class_docstring": " a single where statement used in queries ",
        "klass": "cassandra.cqlengine.statements.WhereClause",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Handles the batching of queries\n\n    http://docs.datastax.com/en/cql/3.0/cql/cql_reference/batch_r.html\n\n    See :doc:`/cqlengine/batches` for more details.\n    ",
        "klass": "cassandra.cqlengine.query.BatchQuery",
        "module": "cassandra"
    },
    {
        "base_classes": [
            "pydot.Graph"
        ],
        "class_docstring": "Class representing a cluster in Graphviz's dot language.\n\n    This class implements the methods to work on a representation\n    of a cluster in Graphviz's dot language.\n\n    cluster(graph_name='subG',\n            suppress_disconnected=False,\n            attribute=value,\n            ...)\n\n    graph_name:\n        the cluster's name\n        (the string 'cluster' will be always prepended)\n    suppress_disconnected:\n        defaults to false, which will remove from the\n        cluster any disconnected nodes.\n    All the attributes defined in the Graphviz dot language should\n    be supported.\n\n    Attributes can be set through the dynamically generated methods:\n\n     set_[attribute name], i.e. set_color, set_fontname\n\n    or using the instance's attributes:\n\n     Cluster.obj_dict['attributes'][attribute name], i.e.\n\n        cluster_instance.obj_dict['attributes']['label']\n        cluster_instance.obj_dict['attributes']['fontname']\n    ",
        "klass": "pydot.Cluster",
        "module": "pydot"
    },
    {
        "base_classes": [
            "pydot.Graph"
        ],
        "class_docstring": "A container for handling a dot language file.\n\n    This class implements methods to write and process\n    a dot language file. It is a derived class of\n    the base class 'Graph'.\n    ",
        "klass": "pydot.Dot",
        "module": "pydot"
    },
    {
        "base_classes": [
            "pydot.Graph"
        ],
        "class_docstring": "Class representing a subgraph in Graphviz's dot language.\n\n    This class implements the methods to work on a representation\n    of a subgraph in Graphviz's dot language.\n\n    subgraph(graph_name='subG',\n             suppress_disconnected=False,\n             attribute=value,\n             ...)\n\n    graph_name:\n        the subgraph's name\n    suppress_disconnected:\n        defaults to false, which will remove from the\n        subgraph any disconnected nodes.\n    All the attributes defined in the Graphviz dot language should\n    be supported.\n\n    Attributes can be set through the dynamically generated methods:\n\n     set_[attribute name], i.e. set_size, set_fontname\n\n    or using the instance's attributes:\n\n     Subgraph.obj_dict['attributes'][attribute name], i.e.\n\n        subgraph_instance.obj_dict['attributes']['label']\n        subgraph_instance.obj_dict['attributes']['fontname']\n    ",
        "klass": "pydot.Subgraph",
        "module": "pydot"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Inversion Class",
        "klass": "SimPEG.Inversion.BaseInversion",
        "module": "SimPEG"
    },
    {
        "base_classes": [
            "SimPEG.Maps.IdentityMap"
        ],
        "class_docstring": "\n        Electrical conductivity varies over many orders of magnitude, so it is\n        a common technique when solving the inverse problem to parameterize and\n        optimize in terms of log conductivity. This makes sense not only\n        because it ensures all conductivities will be positive, but because\n        this is fundamentally the space where conductivity\n        lives (i.e. it varies logarithmically).\n\n        Changes the model into the physical property.\n\n        A common example of this is to invert for electrical conductivity\n        in log space. In this case, your model will be log(sigma) and to\n        get back to sigma, you can take the exponential:\n\n        .. math::\n\n            m = \\log{\\sigma}\n\n            \\exp{m} = \\exp{\\log{\\sigma}} = \\sigma\n    ",
        "klass": "SimPEG.Maps.ExpMap",
        "module": "SimPEG"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": " A dictionary that provides attribute-style access.\n\n        >>> b = Munch()\n        >>> b.hello = 'world'\n        >>> b.hello\n        'world'\n        >>> b['hello'] += \"!\"\n        >>> b.hello\n        'world!'\n        >>> b.foo = Munch(lol=True)\n        >>> b.foo.lol\n        True\n        >>> b.foo is b['foo']\n        True\n\n        A Munch is a subclass of dict; it supports all the methods a dict does...\n\n        >>> sorted(b.keys())\n        ['foo', 'hello']\n\n        Including update()...\n\n        >>> b.update({ 'ponies': 'are pretty!' }, hello=42)\n        >>> print (repr(b))\n        Munch({'ponies': 'are pretty!', 'foo': Munch({'lol': True}), 'hello': 42})\n\n        As well as iteration...\n\n        >>> sorted([ (k,b[k]) for k in b ])\n        [('foo', Munch({'lol': True})), ('hello', 42), ('ponies', 'are pretty!')]\n\n        And \"splats\".\n\n        >>> \"The {knights} who say {ni}!\".format(**Munch(knights='lolcats', ni='can haz'))\n        'The lolcats who say can haz!'\n\n        See unmunchify/Munch.toDict, munchify/Munch.fromDict for notes about conversion.\n    ",
        "klass": "munch.Munch",
        "module": "munch"
    },
    {
        "base_classes": [
            "requests.sessions.Session"
        ],
        "class_docstring": "A Requests Session class with credentials.\n\n    This class is used to perform requests to API endpoints that require\n    authorization::\n\n        from google.auth.transport.requests import AuthorizedSession\n\n        authed_session = AuthorizedSession(credentials)\n\n        response = authed_session.request(\n            'GET', 'https://www.googleapis.com/storage/v1/b')\n\n    The underlying :meth:`request` implementation handles adding the\n    credentials' headers to the request and refreshing credentials as needed.\n\n    Args:\n        credentials (google.auth.credentials.Credentials): The credentials to\n            add to the request.\n        refresh_status_codes (Sequence[int]): Which HTTP status codes indicate\n            that credentials should be refreshed and the request should be\n            retried.\n        max_refresh_attempts (int): The maximum number of times to attempt to\n            refresh the credentials and retry the request.\n        refresh_timeout (Optional[int]): The timeout value in seconds for\n            credential refresh HTTP requests.\n        auth_request (google.auth.transport.requests.Request):\n            (Optional) An instance of\n            :class:`~google.auth.transport.requests.Request` used when\n            refreshing credentials. If not passed,\n            an instance of :class:`~google.auth.transport.requests.Request`\n            is created.\n    ",
        "klass": "google.auth.transport.requests.AuthorizedSession",
        "module": "google"
    },
    {
        "base_classes": [
            "passlib.apache._CommonFile"
        ],
        "class_docstring": "class for reading & writing Htpasswd files.\n\n    The class constructor accepts the following arguments:\n\n    :type path: filepath\n    :param path:\n\n        Specifies path to htpasswd file, use to implicitly load from and save to.\n\n        This class has two modes of operation:\n\n        1. It can be \"bound\" to a local file by passing a ``path`` to the class\n           constructor. In this case it will load the contents of the file when\n           created, and the :meth:`load` and :meth:`save` methods will automatically\n           load from and save to that file if they are called without arguments.\n\n        2. Alternately, it can exist as an independant object, in which case\n           :meth:`load` and :meth:`save` will require an explicit path to be\n           provided whenever they are called. As well, ``autosave`` behavior\n           will not be available.\n\n           This feature is new in Passlib 1.6, and is the default if no\n           ``path`` value is provided to the constructor.\n\n        This is also exposed as a readonly instance attribute.\n\n    :type new: bool\n    :param new:\n\n        Normally, if *path* is specified, :class:`HtpasswdFile` will\n        immediately load the contents of the file. However, when creating\n        a new htpasswd file, applications can set ``new=True`` so that\n        the existing file (if any) will not be loaded.\n\n        .. versionadded:: 1.6\n            This feature was previously enabled by setting ``autoload=False``.\n            That alias has been deprecated, and will be removed in Passlib 1.8\n\n    :type autosave: bool\n    :param autosave:\n\n        Normally, any changes made to an :class:`HtpasswdFile` instance\n        will not be saved until :meth:`save` is explicitly called. However,\n        if ``autosave=True`` is specified, any changes made will be\n        saved to disk immediately (assuming *path* has been set).\n\n        This is also exposed as a writeable instance attribute.\n\n    :type encoding: str\n    :param encoding:\n\n        Optionally specify character encoding used to read/write file\n        and hash passwords. Defaults to ``utf-8``, though ``latin-1``\n        is the only other commonly encountered encoding.\n\n        This is also exposed as a readonly instance attribute.\n\n    :type default_scheme: str\n    :param default_scheme:\n        Optionally specify default scheme to use when encoding new passwords.\n\n        This can be any of the schemes with builtin Apache support,\n        OR natively supported by the host OS's :func:`crypt.crypt` function.\n\n        * Builtin schemes include ``\"bcrypt\"`` (apache 2.4+), ``\"apr_md5_crypt\"`,\n          and ``\"des_crypt\"``.\n\n        * Schemes commonly supported by Unix hosts\n          include ``\"bcrypt\"``, ``\"sha256_crypt\"``, and ``\"des_crypt\"``.\n\n        In order to not have to sort out what you should use,\n        passlib offers a number of aliases, that will resolve\n        to the most appropriate scheme based on your needs:\n\n        * ``\"portable\"``, ``\"portable_apache_24\"`` -- pick scheme that's portable across hosts\n          running apache >= 2.4. **This will be the default as of Passlib 2.0**.\n\n        * ``\"portable_apache_22\"`` -- pick scheme that's portable across hosts\n          running apache >= 2.4. **This is the default up to Passlib 1.9**.\n\n        * ``\"host\"``, ``\"host_apache_24\"`` -- pick strongest scheme supported by\n           apache >= 2.4 and/or host OS.\n\n        * ``\"host_apache_22\"`` -- pick strongest scheme supported by\n           apache >= 2.2 and/or host OS.\n\n        .. versionadded:: 1.6\n            This keyword was previously named ``default``. That alias\n            has been deprecated, and will be removed in Passlib 1.8.\n\n        .. versionchanged:: 1.6.3\n\n            Added support for ``\"bcrypt\"``, ``\"sha256_crypt\"``, and ``\"portable\"`` alias.\n\n        .. versionchanged:: 1.7\n\n            Added apache 2.4 semantics, and additional aliases.\n\n    :type context: :class:`~passlib.context.CryptContext`\n    :param context:\n        :class:`!CryptContext` instance used to create\n        and verify the hashes found in the htpasswd file.\n        The default value is a pre-built context which supports all\n        of the hashes officially allowed in an htpasswd file.\n\n        This is also exposed as a readonly instance attribute.\n\n        .. warning::\n\n            This option may be used to add support for non-standard hash\n            formats to an htpasswd file. However, the resulting file\n            will probably not be usable by another application,\n            and particularly not by Apache.\n\n    :param autoload:\n        Set to ``False`` to prevent the constructor from automatically\n        loaded the file from disk.\n\n        .. deprecated:: 1.6\n            This has been replaced by the *new* keyword.\n            Instead of setting ``autoload=False``, you should use\n            ``new=True``. Support for this keyword will be removed\n            in Passlib 1.8.\n\n    :param default:\n        Change the default algorithm used to hash new passwords.\n\n        .. deprecated:: 1.6\n            This has been renamed to *default_scheme* for clarity.\n            Support for this alias will be removed in Passlib 1.8.\n\n    Loading & Saving\n    ================\n    .. automethod:: load\n    .. automethod:: load_if_changed\n    .. automethod:: load_string\n    .. automethod:: save\n    .. automethod:: to_string\n\n    Inspection\n    ================\n    .. automethod:: users\n    .. automethod:: check_password\n    .. automethod:: get_hash\n\n    Modification\n    ================\n    .. automethod:: set_password\n    .. automethod:: delete\n\n    Alternate Constructors\n    ======================\n    .. automethod:: from_string\n\n    Attributes\n    ==========\n    .. attribute:: path\n\n        Path to local file that will be used as the default\n        for all :meth:`load` and :meth:`save` operations.\n        May be written to, initialized by the *path* constructor keyword.\n\n    .. attribute:: autosave\n\n        Writeable flag indicating whether changes will be automatically\n        written to *path*.\n\n    Errors\n    ======\n    :raises ValueError:\n        All of the methods in this class will raise a :exc:`ValueError` if\n        any user name contains a forbidden character (one of ``:\\r\\n\\t\\x00``),\n        or is longer than 255 characters.\n    ",
        "klass": "passlib.apache.HtpasswdFile",
        "module": "passlib"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An Apache Zookeeper Python client supporting alternate callback\n    handlers and high-level functionality.\n\n    Watch functions registered with this class will not get session\n    events, unlike the default Zookeeper watches. They will also be\n    called with a single argument, a\n    :class:`~kazoo.protocol.states.WatchedEvent` instance.\n\n    ",
        "klass": "kazoo.client.KazooClient",
        "module": "kazoo"
    },
    {
        "base_classes": [
            "urllib3.request.RequestMethods"
        ],
        "class_docstring": "A urllib3 HTTP class with credentials.\n\n    This class is used to perform requests to API endpoints that require\n    authorization::\n\n        from google.auth.transport.urllib3 import AuthorizedHttp\n\n        authed_http = AuthorizedHttp(credentials)\n\n        response = authed_http.request(\n            'GET', 'https://www.googleapis.com/storage/v1/b')\n\n    This class implements :class:`urllib3.request.RequestMethods` and can be\n    used just like any other :class:`urllib3.PoolManager`.\n\n    The underlying :meth:`urlopen` implementation handles adding the\n    credentials' headers to the request and refreshing credentials as needed.\n\n    Args:\n        credentials (google.auth.credentials.Credentials): The credentials to\n            add to the request.\n        http (urllib3.PoolManager): The underlying HTTP object to\n            use to make requests. If not specified, a\n            :class:`urllib3.PoolManager` instance will be constructed with\n            sane defaults.\n        refresh_status_codes (Sequence[int]): Which HTTP status codes indicate\n            that credentials should be refreshed and the request should be\n            retried.\n        max_refresh_attempts (int): The maximum number of times to attempt to\n            refresh the credentials and retry the request.\n    ",
        "klass": "google.auth.transport.urllib3.AuthorizedHttp",
        "module": "google"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Allows overriding configuration settings for the test.\n\n    `conf` will be reset on cleanup.\n\n    ",
        "klass": "oslo_config.fixture.Config",
        "module": "oslo_config"
    },
    {
        "base_classes": [
            "autobahn.twisted.websocket.WebSocketAdapterFactory",
            "autobahn.websocket.protocol.WebSocketClientFactory",
            "twisted.internet.protocol.ClientFactory"
        ],
        "class_docstring": "\n    Base class for Twisted-based WebSocket client factories.\n\n    Implements :class:`autobahn.websocket.interfaces.IWebSocketClientChannelFactory`\n    ",
        "klass": "autobahn.twisted.websocket.WebSocketClientFactory",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This is a factory which produces protocols.\n\n    By default, buildProtocol will create a protocol of the class given in\n    self.protocol.\n    ",
        "klass": "twisted.internet.protocol.Factory",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    UnixSocket client endpoint.\n    ",
        "klass": "twisted.internet.endpoints.UNIXClientEndpoint",
        "module": "twisted"
    },
    {
        "base_classes": [
            "autobahn.twisted.rawsocket.WampRawSocketProtocol"
        ],
        "class_docstring": "\n    Twisted-based WAMP-over-RawSocket client protocol.\n\n    Implements:\n\n        * :class:`autobahn.wamp.interfaces.ITransport`\n    ",
        "klass": "autobahn.twisted.rawsocket.WampRawSocketClientProtocol",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "autobahn.twisted.rawsocket.WampRawSocketProtocol"
        ],
        "class_docstring": "\n    Twisted-based WAMP-over-RawSocket server protocol.\n\n    Implements:\n\n        * :class:`autobahn.wamp.interfaces.ITransport`\n    ",
        "klass": "autobahn.twisted.rawsocket.WampRawSocketServerProtocol",
        "module": "autobahn"
    },
    {
        "base_classes": [
            "twisted.web.resource.ErrorPage"
        ],
        "class_docstring": "\n    L{NoResource} is a specialization of L{ErrorPage} which returns the HTTP\n    response code I{NOT FOUND}.\n    ",
        "klass": "twisted.web.resource.NoResource",
        "module": "twisted"
    },
    {
        "base_classes": [
            "nacl.encoding.Encodable",
            "nacl.utils.StringFixer",
            "object"
        ],
        "class_docstring": "\n    The Box class boxes and unboxes messages between a pair of keys\n\n    The ciphertexts generated by :class:`~nacl.public.Box` include a 16\n    byte authenticator which is checked as part of the decryption. An invalid\n    authenticator will cause the decrypt function to raise an exception. The\n    authenticator is not a signature. Once you've decrypted the message you've\n    demonstrated the ability to create arbitrary valid message, so messages you\n    send are repudiable. For non-repudiable messages, sign them after\n    encryption.\n\n    :param private_key: :class:`~nacl.public.PrivateKey` used to encrypt and\n        decrypt messages\n    :param public_key: :class:`~nacl.public.PublicKey` used to encrypt and\n        decrypt messages\n\n    :cvar NONCE_SIZE: The size that the nonce is required to be.\n    ",
        "klass": "nacl.public.Box",
        "module": "nacl"
    },
    {
        "base_classes": [
            "plaso.engine.zeromq_queue.ZeroMQBufferedReplyQueue"
        ],
        "class_docstring": "A Plaso queue backed by a ZeroMQ REP socket that binds to a port.\n\n  This queue may only be used to pop items, not to push.\n  ",
        "klass": "plaso.engine.zeromq_queue.ZeroMQBufferedReplyBindQueue",
        "module": "plaso"
    },
    {
        "base_classes": [
            "plaso.engine.zeromq_queue.ZeroMQPushQueue"
        ],
        "class_docstring": "A Plaso queue backed by a ZeroMQ PUSH socket that binds to a port.\n\n  This queue may only be used to push items, not to pop.\n  ",
        "klass": "plaso.engine.zeromq_queue.ZeroMQPushBindQueue",
        "module": "plaso"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Generalized Low Rank Modeling\n\n    Builds a generalized low rank model of a H2O dataset.\n    ",
        "klass": "h2o.estimators.glrm.H2OGeneralizedLowRankEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Principal Components Analysis\n\n    ",
        "klass": "h2o.estimators.pca.H2OPrincipalComponentAnalysisEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    PSVM\n\n    ",
        "klass": "h2o.estimators.psvm.H2OSupportVectorMachineEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Deep Learning\n\n    Build a Deep Neural Network model using CPUs\n    Builds a feed-forward multilayer artificial neural network on an H2OFrame\n\n    :examples:\n\n    >>> import h2o\n    >>> from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n    >>> h2o.connect()\n    >>> rows = [[1,2,3,4,0], [2,1,2,4,1], [2,1,4,2,1], [0,1,2,34,1], [2,3,4,1,0]] * 50\n    >>> fr = h2o.H2OFrame(rows)\n    >>> fr[4] = fr[4].asfactor()\n    >>> model = H2ODeepLearningEstimator()\n    >>> model.train(x=range(4), y=4, training_frame=fr)\n    ",
        "klass": "h2o.estimators.deeplearning.H2ODeepLearningEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Generalized Linear Modeling\n\n    Fits a generalized linear model, specified by a response variable, a set of predictors, and a\n    description of the error distribution.\n\n    A subclass of :class:`ModelBase` is returned. The specific subclass depends on the machine learning task\n    at hand (if it's binomial classification, then an H2OBinomialModel is returned, if it's regression then a\n    H2ORegressionModel is returned). The default print-out of the models is shown, but further GLM-specific\n    information can be queried out of the object. Upon completion of the GLM, the resulting object has\n    coefficients, normalized coefficients, residual/null deviance, aic, and a host of model metrics including\n    MSE, AUC (for logistic regression), degrees of freedom, and confusion matrices.\n    ",
        "klass": "h2o.estimators.glm.H2OGeneralizedLinearEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Gradient Boosting Machine\n\n    Builds gradient boosted trees on a parsed data set, for regression or classification.\n    The default distribution function will guess the model type based on the response column type.\n    Otherwise, the response column must be an enum for \"bernoulli\" or \"multinomial\", and numeric\n    for all other distributions.\n    ",
        "klass": "h2o.estimators.gbm.H2OGradientBoostingEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Isolation Forest\n\n    Builds an Isolation Forest model. Isolation Forest algorithm samples the training frame\n    and in each iteration builds a tree that partitions the space of the sample observations until\n    it isolates each observation. Length of the path from root to a leaf node of the resulting tree\n    is used to calculate the anomaly score. Anomalies are easier to isolate and their average\n    tree path is expected to be shorter than paths of regular observations.\n    ",
        "klass": "h2o.estimators.isolation_forest.H2OIsolationForestEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    K-means\n\n    Performs k-means clustering on an H2O dataset.\n    ",
        "klass": "h2o.estimators.kmeans.H2OKMeansEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Naive Bayes\n\n    The naive Bayes classifier assumes independence between predictor variables\n    conditional on the response, and a Gaussian distribution of numeric predictors with\n    mean and standard deviation computed from the training dataset. When building a naive\n    Bayes classifier, every row in the training dataset that contains at least one NA will\n    be skipped completely. If the test dataset has missing values, then those predictors\n    are omitted in the probability calculation during prediction.\n    ",
        "klass": "h2o.estimators.naive_bayes.H2ONaiveBayesEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Distributed Random Forest\n\n    ",
        "klass": "h2o.estimators.random_forest.H2ORandomForestEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Stacked Ensemble\n\n    Builds a stacked ensemble (aka \"super learner\") machine learning method that uses two\n    or more H2O learning algorithms to improve predictive performance. It is a loss-based\n    supervised learning method that finds the optimal combination of a collection of prediction\n    algorithms.This method supports regression and binary classification.\n\n    :examples:\n\n    >>> import h2o\n    >>> h2o.init()\n    >>> from h2o.estimators.random_forest import H2ORandomForestEstimator\n    >>> from h2o.estimators.gbm import H2OGradientBoostingEstimator\n    >>> from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n    >>> col_types = [\"numeric\", \"numeric\", \"numeric\", \"enum\", \"enum\", \"numeric\", \"numeric\", \"numeric\", \"numeric\"]\n    >>> data = h2o.import_file(\"http://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv\", col_types=col_types)\n    >>> train, test = data.split_frame(ratios=[.8], seed=1)\n    >>> x = [\"CAPSULE\",\"GLEASON\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\"]\n    >>> y = \"AGE\"\n    >>> nfolds = 5\n    >>> my_gbm = H2OGradientBoostingEstimator(nfolds=nfolds, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n    >>> my_gbm.train(x=x, y=y, training_frame=train)\n    >>> my_rf = H2ORandomForestEstimator(nfolds=nfolds, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n    >>> my_rf.train(x=x, y=y, training_frame=train)\n    >>> stack = H2OStackedEnsembleEstimator(model_id=\"my_ensemble\", training_frame=train, validation_frame=test, base_models=[my_gbm.model_id, my_rf.model_id])\n    >>> stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n    >>> stack.model_performance()\n    ",
        "klass": "h2o.estimators.stackedensemble.H2OStackedEnsembleEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "h2o.estimators.estimator_base.H2OEstimator"
        ],
        "class_docstring": "\n    Word2Vec\n\n    ",
        "klass": "h2o.estimators.word2vec.H2OWord2vecEstimator",
        "module": "h2o"
    },
    {
        "base_classes": [
            "pandas.core.frame.DataFrame"
        ],
        "class_docstring": "This class adds some convenience functions to the regular Datafram class",
        "klass": "PyFoam.Wrappers.Pandas.PyFoamDataFrame",
        "module": "PyFoam"
    },
    {
        "base_classes": [
            "psycopg2.sql.Composable"
        ],
        "class_docstring": "\n    A `Composable` object made of a sequence of `!Composable`.\n\n    The object is usually created using `!Composable` operators and methods.\n    However it is possible to create a `!Composed` directly specifying a\n    sequence of `!Composable` as arguments.\n\n    Example::\n\n        >>> comp = sql.Composed(\n        ...     [sql.SQL(\"insert into \"), sql.Identifier(\"table\")])\n        >>> print(comp.as_string(conn))\n        insert into \"table\"\n\n    `!Composed` objects are iterable (so they can be used in `SQL.join` for\n    instance).\n    ",
        "klass": "psycopg2.sql.Composed",
        "module": "psycopg2"
    },
    {
        "base_classes": [
            "psycopg2.sql.Composable"
        ],
        "class_docstring": "A `Composable` representing a placeholder for query parameters.\n\n    If the name is specified, generate a named placeholder (e.g. ``%(name)s``),\n    otherwise generate a positional placeholder (e.g. ``%s``).\n\n    The object is useful to generate SQL queries with a variable number of\n    arguments.\n\n    Examples::\n\n        >>> names = ['foo', 'bar', 'baz']\n\n        >>> q1 = sql.SQL(\"insert into table ({}) values ({})\").format(\n        ...     sql.SQL(', ').join(map(sql.Identifier, names)),\n        ...     sql.SQL(', ').join(sql.Placeholder() * len(names)))\n        >>> print(q1.as_string(conn))\n        insert into table (\"foo\", \"bar\", \"baz\") values (%s, %s, %s)\n\n        >>> q2 = sql.SQL(\"insert into table ({}) values ({})\").format(\n        ...     sql.SQL(', ').join(map(sql.Identifier, names)),\n        ...     sql.SQL(', ').join(map(sql.Placeholder, names)))\n        >>> print(q2.as_string(conn))\n        insert into table (\"foo\", \"bar\", \"baz\") values (%(foo)s, %(bar)s, %(baz)s)\n\n    ",
        "klass": "psycopg2.sql.Placeholder",
        "module": "psycopg2"
    },
    {
        "base_classes": [
            "psycopg2.sql.Composable"
        ],
        "class_docstring": "\n    A `Composable` representing a snippet of SQL statement.\n\n    `!SQL` exposes `join()` and `format()` methods useful to create a template\n    where to merge variable parts of a query (for instance field or table\n    names).\n\n    The *string* doesn't undergo any form of escaping, so it is not suitable to\n    represent variable identifiers or values: you should only use it to pass\n    constant strings representing templates or snippets of SQL statements; use\n    other objects such as `Identifier` or `Literal` to represent variable\n    parts.\n\n    Example::\n\n        >>> query = sql.SQL(\"select {0} from {1}\").format(\n        ...    sql.SQL(', ').join([sql.Identifier('foo'), sql.Identifier('bar')]),\n        ...    sql.Identifier('table'))\n        >>> print(query.as_string(conn))\n        select \"foo\", \"bar\" from \"table\"\n    ",
        "klass": "psycopg2.sql.SQL",
        "module": "psycopg2"
    },
    {
        "base_classes": [
            "pex.http.Context"
        ],
        "class_docstring": "A requests-based Context.",
        "klass": "pex.http.RequestsContext",
        "module": "pex"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A multi-page PDF file.\n\n    Examples\n    --------\n\n    >>> import matplotlib.pyplot as plt\n    >>> # Initialize:\n    >>> with PdfPages('foo.pdf') as pdf:\n    ...     # As many times as you like, create a figure fig and save it:\n    ...     fig = plt.figure()\n    ...     pdf.savefig(fig)\n    ...     # When no figure is specified the current figure is saved\n    ...     pdf.savefig()\n\n    Notes\n    -----\n\n    In reality :class:`PdfPages` is a thin wrapper around :class:`PdfFile`, in\n    order to avoid confusion when using :func:`~matplotlib.pyplot.savefig` and\n    forgetting the format argument.\n    ",
        "klass": "matplotlib.backends.backend_pdf.PdfPages",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "matplotlib.backends._backend_tk.NavigationToolbar2Tk"
        ],
        "class_docstring": "\n.. deprecated:: 2.2\n    The NavigationToolbar2TkAgg class was deprecated in version 2.2.\n\n\\ ",
        "klass": "matplotlib.backends.backend_tkagg.NavigationToolbar2TkAgg",
        "module": "matplotlib"
    },
    {
        "base_classes": [
            "Crypto.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER OBJECT ID.\n\n    An example of encoding is:\n\n    >>> from Crypto.Util.asn1 import DerObjectId\n    >>> from binascii import hexlify, unhexlify\n    >>> oid_der = DerObjectId(\"1.2\")\n    >>> oid_der.value += \".840.113549.1.1.1\"\n    >>> print hexlify(oid_der.encode())\n\n    which will show ``06092a864886f70d010101``, the DER encoding for the\n    RSA Object Identifier ``1.2.840.113549.1.1.1``.\n\n    For decoding:\n\n    >>> s = unhexlify(b'06092a864886f70d010101')\n    >>> try:\n    >>>   oid_der = DerObjectId()\n    >>>   oid_der.decode(s)\n    >>>   print oid_der.value\n    >>> except ValueError:\n    >>>   print \"Not a valid DER OBJECT ID\"\n\n    the output will be ``1.2.840.113549.1.1.1``.\n\n    :ivar value: The Object ID (OID), a dot separated list of integers\n    :vartype value: string\n    ",
        "klass": "Crypto.Util.asn1.DerObjectId",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "Crypto.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER SEQUENCE.\n\n        This object behaves like a dynamic Python sequence.\n\n        Sub-elements that are INTEGERs behave like Python integers.\n\n        Any other sub-element is a binary string encoded as a complete DER\n        sub-element (TLV).\n\n        An example of encoding is:\n\n          >>> from Crypto.Util.asn1 import DerSequence, DerInteger\n          >>> from binascii import hexlify, unhexlify\n          >>> obj_der = unhexlify('070102')\n          >>> seq_der = DerSequence([4])\n          >>> seq_der.append(9)\n          >>> seq_der.append(obj_der.encode())\n          >>> print hexlify(seq_der.encode())\n\n        which will show ``3009020104020109070102``, the DER encoding of the\n        sequence containing ``4``, ``9``, and the object with payload ``02``.\n\n        For decoding:\n\n          >>> s = unhexlify(b'3009020104020109070102')\n          >>> try:\n          >>>   seq_der = DerSequence()\n          >>>   seq_der.decode(s)\n          >>>   print len(seq_der)\n          >>>   print seq_der[0]\n          >>>   print seq_der[:]\n          >>> except ValueError:\n          >>>   print \"Not a valid DER SEQUENCE\"\n\n        the output will be::\n\n          3\n          4\n          [4, 9, b'\u0007\u0001\u0002']\n\n        ",
        "klass": "Crypto.Util.asn1.DerSequence",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Class defining an ECC key.\n    Do not instantiate directly.\n    Use :func:`generate`, :func:`construct` or :func:`import_key` instead.\n\n    :ivar curve: The name of the ECC as defined in :numref:`curve_names`.\n    :vartype curve: string\n\n    :ivar pointQ: an ECC point representating the public component\n    :vartype pointQ: :class:`EccPoint`\n\n    :ivar d: A scalar representating the private component\n    :vartype d: integer\n    ",
        "klass": "Crypto.PublicKey.ECC.EccKey",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class to abstract a point over an Elliptic Curve.\n\n    The class support special methods for:\n\n    * Adding two points: ``R = S + T``\n    * In-place addition: ``S += T``\n    * Negating a point: ``R = -T``\n    * Comparing two points: ``if S == T: ...``\n    * Multiplying a point by a scalar: ``R = S*k``\n    * In-place multiplication by a scalar: ``T *= k``\n\n    :ivar x: The affine X-coordinate of the ECC point\n    :vartype x: integer\n\n    :ivar y: The affine Y-coordinate of the ECC point\n    :vartype y: integer\n\n    :ivar xy: The tuple with X- and Y- coordinates\n    ",
        "klass": "Crypto.PublicKey.ECC.EccPoint",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Element of GF(2^128) field",
        "klass": "Crypto.Protocol.SecretSharing._Element",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Base class for defining a single DER object.\n\n        This class should never be directly instantiated.\n        ",
        "klass": "Crypto.Util.asn1.DerObject",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "Crypto.Util.asn1.DerObject"
        ],
        "class_docstring": "Class to model a DER NULL element.",
        "klass": "Crypto.Util.asn1.DerNull",
        "module": "Crypto"
    },
    {
        "base_classes": [
            "novaideo.core.VersionableEntity",
            "novaideo.core.DuplicableEntity",
            "novaideo.core.SearchableEntity",
            "novaideo.core.CorrelableEntity",
            "novaideo.core.PresentableEntity",
            "novaideo.core.ExaminableEntity",
            "novaideo.core.Node",
            "novaideo.core.Emojiable",
            "novaideo.core.SignalableEntity",
            "novaideo.core.Debatable",
            "novaideo.core.Tokenable"
        ],
        "class_docstring": "Idea class",
        "klass": "novaideo.content.idea.Idea",
        "module": "novaideo"
    },
    {
        "base_classes": [
            "pontus.core.VisualisableElement",
            "dace.objectofcollaboration.entity.Entity"
        ],
        "class_docstring": "Invitation class",
        "klass": "novaideo.content.invitation.Invitation",
        "module": "novaideo"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Acts as a context manager to allow mocking\n    ",
        "klass": "httmock.HTTMock",
        "module": "httmock"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "Test fixture for testing neutron-lib API definitions.\n\n    Extension API definition RESOURCE_ATTRIBUTE_MAP dicts get updated as\n    part of standard extension processing/handling. While this behavior is\n    fine for service runtime, it can impact testing scenarios whereby test1\n    updates the attribute map (globally) and test2 doesn't expect the\n    test1 updates.\n\n    This fixture saves and restores 1 or more neutron-lib API definitions\n    attribute maps. It should be used anywhere multiple tests can be run\n    that might update an extension attribute map.\n\n    In addition the fixture backs up and restores the global attribute\n    RESOURCES base on the boolean value of its backup_global_resources\n    attribute.\n    ",
        "klass": "neutron_lib.fixture.APIDefinitionFixture",
        "module": "neutron_lib"
    },
    {
        "base_classes": [
            "torch.nn.modules.module.Module",
            "allennlp.common.from_params.FromParams"
        ],
        "class_docstring": "\n    Openai transformer, as per https://blog.openai.com/language-unsupervised/.\n    Default parameters are the ones for their pretrained model.\n\n    Parameters\n    ----------\n    vocab_size: ``int`` (optional, default: 40478)\n        The size of the vocabulary (number of byte pair embeddings)\n        excluding the n_special embeddings (if any), and the positional embeddings.\n    n_ctx: ``int`` (optional, default: 512)\n        The number of positional encodings to use for evaluation.\n    embedding_dim: ``int`` (optional, default: 768)\n        The dimension of the output embeddings.\n    num_heads: ``int`` (optional, default: 12)\n        How many \"heads\" the attention has.\n    num_layers: ``int`` (optional, default: 12)\n        How many layers of \"blocks\" the transformer has.\n    embedding_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for the embedding.\n    attention_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for attention.\n    residual_dropout_probability: ``float`` (optional, default: 0.1)\n        Dropout for residual\n    activation_function: ``str`` (optional, default: ``'gelu'``)\n        Activation function for the multi-layer perceptron.\n    model_path: ``str`` (optional, default: ``None``)\n        A tar.gz file containing serialized model weights. If supplied,\n        the weights will be loaded from that file.\n    requires_grad: ``bool`` (optional, default: ``False``)\n        If true, the transformer will be fine-tuneable.\n    n_special: ``int`` (optional, default: ``-1``)\n        The number of special tokens added to the byte pair vocabulary\n        (via ``OpenaiTransformerBytePairIndexer``).\n    ",
        "klass": "allennlp.modules.openai_transformer.OpenaiTransformer",
        "module": "allennlp"
    },
    {
        "base_classes": [
            "SoftLayer.utils.IdentifierMixin",
            "object"
        ],
        "class_docstring": "Manages SoftLayer Reserved Capacity Groups.\n\n        Product Information\n\n        - https://console.test.cloud.ibm.com/docs/vsi/vsi_placegroup.html#placement-groups\n        - https://softlayer.github.io/reference/services/SoftLayer_Account/getPlacementGroups/\n        - https://softlayer.github.io/reference/services/SoftLayer_Virtual_PlacementGroup_Rule/\n\n        Existing instances cannot be added to a placement group.\n        You can only add a virtual server instance to a placement group at provisioning.\n        To remove an instance from a placement group, you must delete or reclaim the instance.\n\n    :param SoftLayer.API.BaseClient client: the client instance\n    ",
        "klass": "SoftLayer.managers.vs_placement.PlacementManager",
        "module": "SoftLayer"
    },
    {
        "base_classes": [
            "SoftLayer.utils.IdentifierMixin",
            "object"
        ],
        "class_docstring": "Manages SoftLayer Reserved Capacity Groups.\n\n        Product Information\n\n        - https://console.bluemix.net/docs/vsi/vsi_about_reserved.html\n        - https://softlayer.github.io/reference/services/SoftLayer_Virtual_ReservedCapacityGroup/\n        - https://softlayer.github.io/reference/services/SoftLayer_Virtual_ReservedCapacityGroup_Instance/\n\n\n    :param SoftLayer.API.BaseClient client: the client instance\n    :param SoftLayer.managers.OrderingManager ordering_manager: an optional manager to handle ordering.\n                                              If none is provided, one will be auto initialized.\n    ",
        "klass": "SoftLayer.managers.vs_capacity.CapacityManager",
        "module": "SoftLayer"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "This helps with accessing a heavily nested dictionary.\n\n    Dictionary where accessing keys that don't exist will return another\n    NestedDict object.\n\n    ",
        "klass": "SoftLayer.utils.NestedDict",
        "module": "SoftLayer"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This class represents an association between a server and a\n    consumer.  In general, users of this library will never see\n    instances of this object.  The only exception is if you implement\n    a custom C{L{OpenIDStore<openid.store.interface.OpenIDStore>}}.\n\n    If you do implement such a store, it will need to store the values\n    of the C{L{handle}}, C{L{secret}}, C{L{issued}}, C{L{lifetime}}, and\n    C{L{assoc_type}} instance variables.\n\n    @ivar handle: This is the handle the server gave this association.\n\n    @type handle: C{str}\n\n\n    @ivar secret: This is the shared secret the server generated for\n        this association.\n\n    @type secret: C{str}\n\n\n    @ivar issued: This is the time this association was issued, in\n        seconds since 00:00 GMT, January 1, 1970.  (ie, a unix\n        timestamp)\n\n    @type issued: C{int}\n\n\n    @ivar lifetime: This is the amount of time this association is\n        good for, measured in seconds since the association was\n        issued.\n\n    @type lifetime: C{int}\n\n\n    @ivar assoc_type: This is the type of association this instance\n        represents.  The only valid value of this field at this time\n        is C{'HMAC-SHA1'}, but new types may be defined in the future.\n\n    @type assoc_type: C{str}\n\n\n    @sort: __init__, fromExpiresIn, expiresIn, __eq__, __ne__,\n        handle, secret, issued, lifetime, assoc_type\n    ",
        "klass": "openid.association.Association",
        "module": "openid"
    },
    {
        "base_classes": [
            "openid.store.interface.OpenIDStore"
        ],
        "class_docstring": "\n    This is a filesystem-based store for OpenID associations and\n    nonces.  This store should be safe for use in concurrent systems\n    on both windows and unix (excluding NFS filesystems).  There are a\n    couple race conditions in the system, but those failure cases have\n    been set up in such a way that the worst-case behavior is someone\n    having to try to log in a second time.\n\n    Most of the methods of this class are implementation details.\n    People wishing to just use this store need only pay attention to\n    the C{L{__init__}} method.\n\n    Methods of this object can raise OSError if unexpected filesystem\n    conditions, such as bad permissions or missing directories, occur.\n    ",
        "klass": "openid.store.filestore.FileOpenIDStore",
        "module": "openid"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "In-process memory store.\n\n    Use for single long-running processes.  No persistence supplied.\n    ",
        "klass": "openid.store.memstore.MemoryStore",
        "module": "openid"
    },
    {
        "base_classes": [
            "pyspark.sql.types.DataType"
        ],
        "class_docstring": "A field in :class:`StructType`.\n\n    :param name: string, name of the field.\n    :param dataType: :class:`DataType` of the field.\n    :param nullable: boolean, whether the field can be null (None) or not.\n    :param metadata: a dict from string to simple type that can be toInternald to JSON automatically\n    ",
        "klass": "pyspark.sql.types.StructField",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.sql.types.DataType"
        ],
        "class_docstring": "Struct type, consisting of a list of :class:`StructField`.\n\n    This is the data type representing a :class:`Row`.\n\n    Iterating a :class:`StructType` will iterate its :class:`StructField`\\s.\n    A contained :class:`StructField` can be accessed by name or position.\n\n    >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n    >>> struct1[\"f1\"]\n    StructField(f1,StringType,true)\n    >>> struct1[0]\n    StructField(f1,StringType,true)\n    ",
        "klass": "pyspark.sql.types.StructType",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    This is a file-like object takes a stream of data, of unknown length, and breaks it into fixed\n    length frames.  The intended use case is serializing large data and sending it immediately over\n    a socket -- we do not want to buffer the entire data before sending it, but the receiving end\n    needs to know whether or not there is more data coming.\n\n    It works by buffering the incoming data in some fixed-size chunks.  If the buffer is full, it\n    first sends the buffer size, then the data.  This repeats as long as there is more data to send.\n    When this is closed, it sends the length of whatever data is in the buffer, then that data, and\n    finally a \"length\" of -1 to indicate the stream has completed.\n    ",
        "klass": "pyspark.serializers.ChunkedStream",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.serializers.FramedSerializer"
        ],
        "class_docstring": "\n    Serializes objects using Python's pickle serializer:\n\n        http://docs.python.org/2/library/pickle.html\n\n    This serializer supports nearly any Python object, but may\n    not be as fast as more specialized serializers.\n    ",
        "klass": "pyspark.serializers.PickleSerializer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Inverse document frequency (IDF).\n\n    The standard formulation is used: `idf = log((m + 1) / (d(t) + 1))`,\n    where `m` is the total number of documents and `d(t)` is the number\n    of documents that contain term `t`.\n\n    This implementation supports filtering out terms which do not appear\n    in a minimum number of documents (controlled by the variable\n    `minDocFreq`). For terms that are not in at least `minDocFreq`\n    documents, the IDF is found as 0, resulting in TF-IDFs of 0.\n\n    :param minDocFreq: minimum of documents in which a term\n                       should appear for filtering\n\n    >>> n = 4\n    >>> freqs = [Vectors.sparse(n, (1, 3), (1.0, 2.0)),\n    ...          Vectors.dense([0.0, 1.0, 2.0, 3.0]),\n    ...          Vectors.sparse(n, [1], [1.0])]\n    >>> data = sc.parallelize(freqs)\n    >>> idf = IDF()\n    >>> model = idf.fit(data)\n    >>> tfidf = model.transform(data)\n    >>> for r in tfidf.collect(): r\n    SparseVector(4, {1: 0.0, 3: 0.5754})\n    DenseVector([0.0, 0.0, 1.3863, 0.863])\n    SparseVector(4, {1: 0.0})\n    >>> model.transform(Vectors.dense([0.0, 1.0, 2.0, 3.0]))\n    DenseVector([0.0, 0.0, 1.3863, 0.863])\n    >>> model.transform([0.0, 1.0, 2.0, 3.0])\n    DenseVector([0.0, 0.0, 1.3863, 0.863])\n    >>> model.transform(Vectors.sparse(n, (1, 3), (1.0, 2.0)))\n    SparseVector(4, {1: 0.0, 3: 0.5754})\n\n    .. versionadded:: 1.2.0\n    ",
        "klass": "pyspark.mllib.feature.IDF",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Standardizes features by removing the mean and scaling to unit\n    variance using column summary statistics on the samples in the\n    training set.\n\n    :param withMean: False by default. Centers the data with mean\n                     before scaling. It will build a dense output, so take\n                     care when applying to sparse input.\n    :param withStd: True by default. Scales the data to unit\n                    standard deviation.\n\n    >>> vs = [Vectors.dense([-2.0, 2.3, 0]), Vectors.dense([3.8, 0.0, 1.9])]\n    >>> dataset = sc.parallelize(vs)\n    >>> standardizer = StandardScaler(True, True)\n    >>> model = standardizer.fit(dataset)\n    >>> result = model.transform(dataset)\n    >>> for r in result.collect(): r\n    DenseVector([-0.7071, 0.7071, -0.7071])\n    DenseVector([0.7071, -0.7071, 0.7071])\n    >>> int(model.std[0])\n    4\n    >>> int(model.mean[0]*10)\n    9\n    >>> model.withStd\n    True\n    >>> model.withMean\n    True\n\n    .. versionadded:: 1.2.0\n    ",
        "klass": "pyspark.mllib.feature.StandardScaler",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Binarize a column of continuous features given a threshold.\n\n    >>> df = spark.createDataFrame([(0.5,)], [\"values\"])\n    >>> binarizer = Binarizer(threshold=1.0, inputCol=\"values\", outputCol=\"features\")\n    >>> binarizer.transform(df).head().features\n    0.0\n    >>> binarizer.setParams(outputCol=\"freqs\").transform(df).head().freqs\n    0.0\n    >>> params = {binarizer.threshold: -0.5, binarizer.outputCol: \"vector\"}\n    >>> binarizer.transform(df, params).head().vector\n    1.0\n    >>> binarizerPath = temp_path + \"/binarizer\"\n    >>> binarizer.save(binarizerPath)\n    >>> loadedBinarizer = Binarizer.load(binarizerPath)\n    >>> loadedBinarizer.getThreshold() == binarizer.getThreshold()\n    True\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.Binarizer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.param.shared.HasHandleInvalid",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Maps a column of continuous features to a column of feature buckets.\n\n    >>> values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n    >>> df = spark.createDataFrame(values, [\"values\"])\n    >>> bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],\n    ...     inputCol=\"values\", outputCol=\"buckets\")\n    >>> bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df).collect()\n    >>> len(bucketed)\n    6\n    >>> bucketed[0].buckets\n    0.0\n    >>> bucketed[1].buckets\n    0.0\n    >>> bucketed[2].buckets\n    1.0\n    >>> bucketed[3].buckets\n    2.0\n    >>> bucketizer.setParams(outputCol=\"b\").transform(df).head().b\n    0.0\n    >>> bucketizerPath = temp_path + \"/bucketizer\"\n    >>> bucketizer.save(bucketizerPath)\n    >>> loadedBucketizer = Bucketizer.load(bucketizerPath)\n    >>> loadedBucketizer.getSplits() == bucketizer.getSplits()\n    True\n    >>> bucketed = bucketizer.setHandleInvalid(\"skip\").transform(df).collect()\n    >>> len(bucketed)\n    4\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.Bucketizer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.feature._CountVectorizerParams",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.\n\n    >>> df = spark.createDataFrame(\n    ...    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n    ...    [\"label\", \"raw\"])\n    >>> cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n    >>> model = cv.fit(df)\n    >>> model.transform(df).show(truncate=False)\n    +-----+---------------+-------------------------+\n    |label|raw            |vectors                  |\n    +-----+---------------+-------------------------+\n    |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n    |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n    +-----+---------------+-------------------------+\n    ...\n    >>> sorted(model.vocabulary) == ['a', 'b', 'c']\n    True\n    >>> countVectorizerPath = temp_path + \"/count-vectorizer\"\n    >>> cv.save(countVectorizerPath)\n    >>> loadedCv = CountVectorizer.load(countVectorizerPath)\n    >>> loadedCv.getMinDF() == cv.getMinDF()\n    True\n    >>> loadedCv.getMinTF() == cv.getMinTF()\n    True\n    >>> loadedCv.getVocabSize() == cv.getVocabSize()\n    True\n    >>> modelPath = temp_path + \"/count-vectorizer-model\"\n    >>> model.save(modelPath)\n    >>> loadedModel = CountVectorizerModel.load(modelPath)\n    >>> loadedModel.vocabulary == model.vocabulary\n    True\n    >>> fromVocabModel = CountVectorizerModel.from_vocabulary([\"a\", \"b\", \"c\"],\n    ...     inputCol=\"raw\", outputCol=\"vectors\")\n    >>> fromVocabModel.transform(df).show(truncate=False)\n    +-----+---------------+-------------------------+\n    |label|raw            |vectors                  |\n    +-----+---------------+-------------------------+\n    |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n    |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n    +-----+---------------+-------------------------+\n    ...\n\n    .. versionadded:: 1.6.0\n    ",
        "klass": "pyspark.ml.feature.CountVectorizer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.param.shared.HasNumFeatures",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Maps a sequence of terms to their term frequencies using the hashing trick.\n    Currently we use Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)\n    to calculate the hash code value for the term object.\n    Since a simple modulo is used to transform the hash function to a column index,\n    it is advisable to use a power of two as the numFeatures parameter;\n    otherwise the features will not be mapped evenly to the columns.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n    >>> hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\")\n    >>> hashingTF.transform(df).head().features\n    SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n    >>> hashingTF.setParams(outputCol=\"freqs\").transform(df).head().freqs\n    SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n    >>> params = {hashingTF.numFeatures: 5, hashingTF.outputCol: \"vector\"}\n    >>> hashingTF.transform(df, params).head().vector\n    SparseVector(5, {0: 1.0, 1: 1.0, 2: 1.0})\n    >>> hashingTFPath = temp_path + \"/hashing-tf\"\n    >>> hashingTF.save(hashingTFPath)\n    >>> loadedHashingTF = HashingTF.load(hashingTFPath)\n    >>> loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()\n    True\n\n    .. versionadded:: 1.3.0\n    ",
        "klass": "pyspark.ml.feature.HashingTF",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Compute the Inverse Document Frequency (IDF) given a collection of documents.\n\n    >>> from pyspark.ml.linalg import DenseVector\n    >>> df = spark.createDataFrame([(DenseVector([1.0, 2.0]),),\n    ...     (DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [\"tf\"])\n    >>> idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n    >>> model = idf.fit(df)\n    >>> model.idf\n    DenseVector([0.0, 0.0])\n    >>> model.transform(df).head().idf\n    DenseVector([0.0, 0.0])\n    >>> idf.setParams(outputCol=\"freqs\").fit(df).transform(df).collect()[1].freqs\n    DenseVector([0.0, 0.0])\n    >>> params = {idf.minDocFreq: 1, idf.outputCol: \"vector\"}\n    >>> idf.fit(df, params).transform(df).head().vector\n    DenseVector([0.2877, 0.0])\n    >>> idfPath = temp_path + \"/idf\"\n    >>> idf.save(idfPath)\n    >>> loadedIdf = IDF.load(idfPath)\n    >>> loadedIdf.getMinDocFreq() == idf.getMinDocFreq()\n    True\n    >>> modelPath = temp_path + \"/idf-model\"\n    >>> model.save(modelPath)\n    >>> loadedModel = IDFModel.load(modelPath)\n    >>> loadedModel.transform(df).head().idf == model.transform(df).head().idf\n    True\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.IDF",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A feature transformer that converts the input array of strings into an array of n-grams. Null\n    values in the input array are ignored.\n    It returns an array of n-grams where each n-gram is represented by a space-separated string of\n    words.\n    When the input is empty, an empty array is returned.\n    When the input array length is less than n (number of elements per n-gram), no n-grams are\n    returned.\n\n    >>> df = spark.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n    >>> ngram = NGram(n=2, inputCol=\"inputTokens\", outputCol=\"nGrams\")\n    >>> ngram.transform(df).head()\n    Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b', 'b c', 'c d', 'd e'])\n    >>> # Change n-gram length\n    >>> ngram.setParams(n=4).transform(df).head()\n    Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b c d', 'b c d e'])\n    >>> # Temporarily modify output column.\n    >>> ngram.transform(df, {ngram.outputCol: \"output\"}).head()\n    Row(inputTokens=['a', 'b', 'c', 'd', 'e'], output=['a b c d', 'b c d e'])\n    >>> ngram.transform(df).head()\n    Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b c d', 'b c d e'])\n    >>> # Must use keyword arguments to specify params.\n    >>> ngram.setParams(\"text\")\n    Traceback (most recent call last):\n        ...\n    TypeError: Method setParams forces keyword arguments.\n    >>> ngramPath = temp_path + \"/ngram\"\n    >>> ngram.save(ngramPath)\n    >>> loadedNGram = NGram.load(ngramPath)\n    >>> loadedNGram.getN() == ngram.getN()\n    True\n\n    .. versionadded:: 1.5.0\n    ",
        "klass": "pyspark.ml.feature.NGram",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasLabelCol",
            "pyspark.ml.param.shared.HasHandleInvalid",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    .. note:: Experimental\n\n    Implements the transforms required for fitting a dataset against an\n    R model formula. Currently we support a limited subset of the R\n    operators, including '~', '.', ':', '+', and '-'. Also see the `R formula docs\n    <http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html>`_.\n\n    >>> df = spark.createDataFrame([\n    ...     (1.0, 1.0, \"a\"),\n    ...     (0.0, 2.0, \"b\"),\n    ...     (0.0, 0.0, \"a\")\n    ... ], [\"y\", \"x\", \"s\"])\n    >>> rf = RFormula(formula=\"y ~ x + s\")\n    >>> model = rf.fit(df)\n    >>> model.transform(df).show()\n    +---+---+---+---------+-----+\n    |  y|  x|  s| features|label|\n    +---+---+---+---------+-----+\n    |1.0|1.0|  a|[1.0,1.0]|  1.0|\n    |0.0|2.0|  b|[2.0,0.0]|  0.0|\n    |0.0|0.0|  a|[0.0,1.0]|  0.0|\n    +---+---+---+---------+-----+\n    ...\n    >>> rf.fit(df, {rf.formula: \"y ~ . - s\"}).transform(df).show()\n    +---+---+---+--------+-----+\n    |  y|  x|  s|features|label|\n    +---+---+---+--------+-----+\n    |1.0|1.0|  a|   [1.0]|  1.0|\n    |0.0|2.0|  b|   [2.0]|  0.0|\n    |0.0|0.0|  a|   [0.0]|  0.0|\n    +---+---+---+--------+-----+\n    ...\n    >>> rFormulaPath = temp_path + \"/rFormula\"\n    >>> rf.save(rFormulaPath)\n    >>> loadedRF = RFormula.load(rFormulaPath)\n    >>> loadedRF.getFormula() == rf.getFormula()\n    True\n    >>> loadedRF.getFeaturesCol() == rf.getFeaturesCol()\n    True\n    >>> loadedRF.getLabelCol() == rf.getLabelCol()\n    True\n    >>> loadedRF.getHandleInvalid() == rf.getHandleInvalid()\n    True\n    >>> str(loadedRF)\n    'RFormula(y ~ x + s) (uid=...)'\n    >>> modelPath = temp_path + \"/rFormulaModel\"\n    >>> model.save(modelPath)\n    >>> loadedModel = RFormulaModel.load(modelPath)\n    >>> loadedModel.uid == model.uid\n    True\n    >>> loadedModel.transform(df).show()\n    +---+---+---+---------+-----+\n    |  y|  x|  s| features|label|\n    +---+---+---+---------+-----+\n    |1.0|1.0|  a|[1.0,1.0]|  1.0|\n    |0.0|2.0|  b|[2.0,0.0]|  0.0|\n    |0.0|0.0|  a|[0.0,1.0]|  0.0|\n    +---+---+---+---------+-----+\n    ...\n    >>> str(loadedModel)\n    'RFormulaModel(ResolvedRFormula(label=y, terms=[x,s], hasIntercept=true)) (uid=...)'\n\n    .. versionadded:: 1.5.0\n    ",
        "klass": "pyspark.ml.feature.RFormula",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A regex based tokenizer that extracts tokens either by using the\n    provided regex pattern (in Java dialect) to split the text\n    (default) or repeatedly matching the regex (if gaps is false).\n    Optional parameters also allow filtering tokens using a minimal\n    length.\n    It returns an array of strings that can be empty.\n\n    >>> df = spark.createDataFrame([(\"A B  c\",)], [\"text\"])\n    >>> reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n    >>> reTokenizer.transform(df).head()\n    Row(text='A B  c', words=['a', 'b', 'c'])\n    >>> # Change a parameter.\n    >>> reTokenizer.setParams(outputCol=\"tokens\").transform(df).head()\n    Row(text='A B  c', tokens=['a', 'b', 'c'])\n    >>> # Temporarily modify a parameter.\n    >>> reTokenizer.transform(df, {reTokenizer.outputCol: \"words\"}).head()\n    Row(text='A B  c', words=['a', 'b', 'c'])\n    >>> reTokenizer.transform(df).head()\n    Row(text='A B  c', tokens=['a', 'b', 'c'])\n    >>> # Must use keyword arguments to specify params.\n    >>> reTokenizer.setParams(\"text\")\n    Traceback (most recent call last):\n        ...\n    TypeError: Method setParams forces keyword arguments.\n    >>> regexTokenizerPath = temp_path + \"/regex-tokenizer\"\n    >>> reTokenizer.save(regexTokenizerPath)\n    >>> loadedReTokenizer = RegexTokenizer.load(regexTokenizerPath)\n    >>> loadedReTokenizer.getMinTokenLength() == reTokenizer.getMinTokenLength()\n    True\n    >>> loadedReTokenizer.getGaps() == reTokenizer.getGaps()\n    True\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.RegexTokenizer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    Standardizes features by removing the mean and scaling to unit variance using column summary\n    statistics on the samples in the training set.\n\n    The \"unit std\" is computed using the `corrected sample standard deviation     <https://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation>`_,\n    which is computed as the square root of the unbiased sample variance.\n\n    >>> from pyspark.ml.linalg import Vectors\n    >>> df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n    >>> standardScaler = StandardScaler(inputCol=\"a\", outputCol=\"scaled\")\n    >>> model = standardScaler.fit(df)\n    >>> model.mean\n    DenseVector([1.0])\n    >>> model.std\n    DenseVector([1.4142])\n    >>> model.transform(df).collect()[1].scaled\n    DenseVector([1.4142])\n    >>> standardScalerPath = temp_path + \"/standard-scaler\"\n    >>> standardScaler.save(standardScalerPath)\n    >>> loadedStandardScaler = StandardScaler.load(standardScalerPath)\n    >>> loadedStandardScaler.getWithMean() == standardScaler.getWithMean()\n    True\n    >>> loadedStandardScaler.getWithStd() == standardScaler.getWithStd()\n    True\n    >>> modelPath = temp_path + \"/standard-scaler-model\"\n    >>> model.save(modelPath)\n    >>> loadedModel = StandardScalerModel.load(modelPath)\n    >>> loadedModel.std == model.std\n    True\n    >>> loadedModel.mean == model.mean\n    True\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.StandardScaler",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A feature transformer that filters out stop words from input.\n\n    .. note:: null values from input array are preserved unless adding null to stopWords explicitly.\n\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n    >>> remover = StopWordsRemover(inputCol=\"text\", outputCol=\"words\", stopWords=[\"b\"])\n    >>> remover.transform(df).head().words == ['a', 'c']\n    True\n    >>> stopWordsRemoverPath = temp_path + \"/stopwords-remover\"\n    >>> remover.save(stopWordsRemoverPath)\n    >>> loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n    >>> loadedRemover.getStopWords() == remover.getStopWords()\n    True\n    >>> loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\n    True\n\n    .. versionadded:: 1.6.0\n    ",
        "klass": "pyspark.ml.feature.StopWordsRemover",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.feature._StringIndexerParams",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A label indexer that maps a string column of labels to an ML column of label indices.\n    If the input column is numeric, we cast it to string and index the string values.\n    The indices are in [0, numLabels). By default, this is ordered by label frequencies\n    so the most frequent label gets index 0. The ordering behavior is controlled by\n    setting :py:attr:`stringOrderType`. Its default value is 'frequencyDesc'.\n\n    >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n    ...     stringOrderType=\"frequencyDesc\")\n    >>> model = stringIndexer.fit(stringIndDf)\n    >>> td = model.transform(stringIndDf)\n    >>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n    ...     key=lambda x: x[0])\n    [(0, 0.0), (1, 2.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 1.0)]\n    >>> inverter = IndexToString(inputCol=\"indexed\", outputCol=\"label2\", labels=model.labels)\n    >>> itd = inverter.transform(td)\n    >>> sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),\n    ...     key=lambda x: x[0])\n    [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')]\n    >>> stringIndexerPath = temp_path + \"/string-indexer\"\n    >>> stringIndexer.save(stringIndexerPath)\n    >>> loadedIndexer = StringIndexer.load(stringIndexerPath)\n    >>> loadedIndexer.getHandleInvalid() == stringIndexer.getHandleInvalid()\n    True\n    >>> modelPath = temp_path + \"/string-indexer-model\"\n    >>> model.save(modelPath)\n    >>> loadedModel = StringIndexerModel.load(modelPath)\n    >>> loadedModel.labels == model.labels\n    True\n    >>> indexToStringPath = temp_path + \"/index-to-string\"\n    >>> inverter.save(indexToStringPath)\n    >>> loadedInverter = IndexToString.load(indexToStringPath)\n    >>> loadedInverter.getLabels() == inverter.getLabels()\n    True\n    >>> stringIndexer.getStringOrderType()\n    'frequencyDesc'\n    >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n    ...     stringOrderType=\"alphabetDesc\")\n    >>> model = stringIndexer.fit(stringIndDf)\n    >>> td = model.transform(stringIndDf)\n    >>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n    ...     key=lambda x: x[0])\n    [(0, 2.0), (1, 1.0), (2, 0.0), (3, 2.0), (4, 2.0), (5, 0.0)]\n    >>> fromlabelsModel = StringIndexerModel.from_labels([\"a\", \"b\", \"c\"],\n    ...     inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\")\n    >>> result = fromlabelsModel.transform(stringIndDf)\n    >>> sorted(set([(i[0], i[1]) for i in result.select(result.id, result.indexed).collect()]),\n    ...     key=lambda x: x[0])\n    [(0, 0.0), (1, 1.0), (2, 2.0), (3, 0.0), (4, 0.0), (5, 2.0)]\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.StringIndexer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A tokenizer that converts the input string to lowercase and then\n    splits it by white spaces.\n\n    >>> df = spark.createDataFrame([(\"a b c\",)], [\"text\"])\n    >>> tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n    >>> tokenizer.transform(df).head()\n    Row(text='a b c', words=['a', 'b', 'c'])\n    >>> # Change a parameter.\n    >>> tokenizer.setParams(outputCol=\"tokens\").transform(df).head()\n    Row(text='a b c', tokens=['a', 'b', 'c'])\n    >>> # Temporarily modify a parameter.\n    >>> tokenizer.transform(df, {tokenizer.outputCol: \"words\"}).head()\n    Row(text='a b c', words=['a', 'b', 'c'])\n    >>> tokenizer.transform(df).head()\n    Row(text='a b c', tokens=['a', 'b', 'c'])\n    >>> # Must use keyword arguments to specify params.\n    >>> tokenizer.setParams(\"text\")\n    Traceback (most recent call last):\n        ...\n    TypeError: Method setParams forces keyword arguments.\n    >>> tokenizerPath = temp_path + \"/tokenizer\"\n    >>> tokenizer.save(tokenizerPath)\n    >>> loadedTokenizer = Tokenizer.load(tokenizerPath)\n    >>> loadedTokenizer.transform(df).head().tokens == tokenizer.transform(df).head().tokens\n    True\n\n    .. versionadded:: 1.3.0\n    ",
        "klass": "pyspark.ml.feature.Tokenizer",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCols",
            "pyspark.ml.param.shared.HasOutputCol",
            "pyspark.ml.param.shared.HasHandleInvalid",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    A feature transformer that merges multiple columns into a vector column.\n\n    >>> df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n    >>> vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n    >>> vecAssembler.transform(df).head().features\n    DenseVector([1.0, 0.0, 3.0])\n    >>> vecAssembler.setParams(outputCol=\"freqs\").transform(df).head().freqs\n    DenseVector([1.0, 0.0, 3.0])\n    >>> params = {vecAssembler.inputCols: [\"b\", \"a\"], vecAssembler.outputCol: \"vector\"}\n    >>> vecAssembler.transform(df, params).head().vector\n    DenseVector([0.0, 1.0])\n    >>> vectorAssemblerPath = temp_path + \"/vector-assembler\"\n    >>> vecAssembler.save(vectorAssemblerPath)\n    >>> loadedAssembler = VectorAssembler.load(vectorAssemblerPath)\n    >>> loadedAssembler.transform(df).head().freqs == vecAssembler.transform(df).head().freqs\n    True\n    >>> dfWithNullsAndNaNs = spark.createDataFrame(\n    ...    [(1.0, 2.0, None), (3.0, float(\"nan\"), 4.0), (5.0, 6.0, 7.0)], [\"a\", \"b\", \"c\"])\n    >>> vecAssembler2 = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\",\n    ...    handleInvalid=\"keep\")\n    >>> vecAssembler2.transform(dfWithNullsAndNaNs).show()\n    +---+---+----+-------------+\n    |  a|  b|   c|     features|\n    +---+---+----+-------------+\n    |1.0|2.0|null|[1.0,2.0,NaN]|\n    |3.0|NaN| 4.0|[3.0,NaN,4.0]|\n    |5.0|6.0| 7.0|[5.0,6.0,7.0]|\n    +---+---+----+-------------+\n    ...\n    >>> vecAssembler2.setParams(handleInvalid=\"skip\").transform(dfWithNullsAndNaNs).show()\n    +---+---+---+-------------+\n    |  a|  b|  c|     features|\n    +---+---+---+-------------+\n    |5.0|6.0|7.0|[5.0,6.0,7.0]|\n    +---+---+---+-------------+\n    ...\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.feature.VectorAssembler",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaTransformer",
            "pyspark.ml.param.shared.HasInputCol",
            "pyspark.ml.param.shared.HasHandleInvalid",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.util.JavaMLWritable"
        ],
        "class_docstring": "\n    .. note:: Experimental\n\n    A feature transformer that adds size information to the metadata of a vector column.\n    VectorAssembler needs size information for its input columns and cannot be used on streaming\n    dataframes without this metadata.\n\n    .. note:: VectorSizeHint modifies `inputCol` to include size metadata and does not have an\n        outputCol.\n\n    >>> from pyspark.ml.linalg import Vectors\n    >>> from pyspark.ml import Pipeline, PipelineModel\n    >>> data = [(Vectors.dense([1., 2., 3.]), 4.)]\n    >>> df = spark.createDataFrame(data, [\"vector\", \"float\"])\n    >>>\n    >>> sizeHint = VectorSizeHint(inputCol=\"vector\", size=3, handleInvalid=\"skip\")\n    >>> vecAssembler = VectorAssembler(inputCols=[\"vector\", \"float\"], outputCol=\"assembled\")\n    >>> pipeline = Pipeline(stages=[sizeHint, vecAssembler])\n    >>>\n    >>> pipelineModel = pipeline.fit(df)\n    >>> pipelineModel.transform(df).head().assembled\n    DenseVector([1.0, 2.0, 3.0, 4.0])\n    >>> vectorSizeHintPath = temp_path + \"/vector-size-hint-pipeline\"\n    >>> pipelineModel.save(vectorSizeHintPath)\n    >>> loadedPipeline = PipelineModel.load(vectorSizeHintPath)\n    >>> loaded = loadedPipeline.transform(df).head().assembled\n    >>> expected = pipelineModel.transform(df).head().assembled\n    >>> loaded == expected\n    True\n\n    .. versionadded:: 2.3.0\n    ",
        "klass": "pyspark.ml.feature.VectorSizeHint",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.base.Estimator",
            "pyspark.ml.util.MLReadable",
            "pyspark.ml.util.MLWritable"
        ],
        "class_docstring": "\n    A simple pipeline, which acts as an estimator. A Pipeline consists\n    of a sequence of stages, each of which is either an\n    :py:class:`Estimator` or a :py:class:`Transformer`. When\n    :py:meth:`Pipeline.fit` is called, the stages are executed in\n    order. If a stage is an :py:class:`Estimator`, its\n    :py:meth:`Estimator.fit` method will be called on the input\n    dataset to fit a model. Then the model, which is a transformer,\n    will be used to transform the dataset as the input to the next\n    stage. If a stage is a :py:class:`Transformer`, its\n    :py:meth:`Transformer.transform` method will be called to produce\n    the dataset for the next stage. The fitted model from a\n    :py:class:`Pipeline` is a :py:class:`PipelineModel`, which\n    consists of fitted models and transformers, corresponding to the\n    pipeline stages. If stages is an empty list, the pipeline acts as an\n    identity transformer.\n\n    .. versionadded:: 1.3.0\n    ",
        "klass": "pyspark.ml.pipeline.Pipeline",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasDistanceMeasure",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasPredictionCol",
            "pyspark.ml.param.shared.HasMaxIter",
            "pyspark.ml.param.shared.HasTol",
            "pyspark.ml.param.shared.HasSeed",
            "pyspark.ml.util.JavaMLWritable",
            "pyspark.ml.util.JavaMLReadable"
        ],
        "class_docstring": "\n    K-means clustering with a k-means++ like initialization mode\n    (the k-means|| algorithm by Bahmani et al).\n\n    >>> from pyspark.ml.linalg import Vectors\n    >>> data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n    ...         (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n    >>> df = spark.createDataFrame(data, [\"features\"])\n    >>> kmeans = KMeans(k=2, seed=1)\n    >>> model = kmeans.fit(df)\n    >>> centers = model.clusterCenters()\n    >>> len(centers)\n    2\n    >>> model.computeCost(df)\n    2.000...\n    >>> transformed = model.transform(df).select(\"features\", \"prediction\")\n    >>> rows = transformed.collect()\n    >>> rows[0].prediction == rows[1].prediction\n    True\n    >>> rows[2].prediction == rows[3].prediction\n    True\n    >>> model.hasSummary\n    True\n    >>> summary = model.summary\n    >>> summary.k\n    2\n    >>> summary.clusterSizes\n    [2, 2]\n    >>> summary.trainingCost\n    2.000...\n    >>> kmeans_path = temp_path + \"/kmeans\"\n    >>> kmeans.save(kmeans_path)\n    >>> kmeans2 = KMeans.load(kmeans_path)\n    >>> kmeans2.getK()\n    2\n    >>> model_path = temp_path + \"/kmeans_model\"\n    >>> model.save(model_path)\n    >>> model2 = KMeansModel.load(model_path)\n    >>> model2.hasSummary\n    False\n    >>> model.clusterCenters()[0] == model2.clusterCenters()[0]\n    array([ True,  True], dtype=bool)\n    >>> model.clusterCenters()[1] == model2.clusterCenters()[1]\n    array([ True,  True], dtype=bool)\n\n    .. versionadded:: 1.5.0\n    ",
        "klass": "pyspark.ml.clustering.KMeans",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasLabelCol",
            "pyspark.ml.param.shared.HasPredictionCol",
            "pyspark.ml.param.shared.HasMaxIter",
            "pyspark.ml.regression.GBTParams",
            "pyspark.ml.param.shared.HasCheckpointInterval",
            "pyspark.ml.param.shared.HasStepSize",
            "pyspark.ml.param.shared.HasSeed",
            "pyspark.ml.util.JavaMLWritable",
            "pyspark.ml.util.JavaMLReadable",
            "pyspark.ml.regression.TreeRegressorParams"
        ],
        "class_docstring": "\n    `Gradient-Boosted Trees (GBTs) <http://en.wikipedia.org/wiki/Gradient_boosting>`_\n    learning algorithm for regression.\n    It supports both continuous and categorical features.\n\n    >>> from numpy import allclose\n    >>> from pyspark.ml.linalg import Vectors\n    >>> df = spark.createDataFrame([\n    ...     (1.0, Vectors.dense(1.0)),\n    ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n    >>> gbt = GBTRegressor(maxIter=5, maxDepth=2, seed=42)\n    >>> print(gbt.getImpurity())\n    variance\n    >>> print(gbt.getFeatureSubsetStrategy())\n    all\n    >>> model = gbt.fit(df)\n    >>> model.featureImportances\n    SparseVector(1, {0: 1.0})\n    >>> model.numFeatures\n    1\n    >>> allclose(model.treeWeights, [1.0, 0.1, 0.1, 0.1, 0.1])\n    True\n    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n    >>> model.transform(test0).head().prediction\n    0.0\n    >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n    >>> model.transform(test1).head().prediction\n    1.0\n    >>> gbtr_path = temp_path + \"gbtr\"\n    >>> gbt.save(gbtr_path)\n    >>> gbt2 = GBTRegressor.load(gbtr_path)\n    >>> gbt2.getMaxDepth()\n    2\n    >>> model_path = temp_path + \"gbtr_model\"\n    >>> model.save(model_path)\n    >>> model2 = GBTRegressionModel.load(model_path)\n    >>> model.featureImportances == model2.featureImportances\n    True\n    >>> model.treeWeights == model2.treeWeights\n    True\n    >>> model.trees\n    [DecisionTreeRegressionModel (uid=...) of depth..., DecisionTreeRegressionModel...]\n    >>> validation = spark.createDataFrame([(0.0, Vectors.dense(-1.0))],\n    ...              [\"label\", \"features\"])\n    >>> model.evaluateEachIteration(validation, \"squared\")\n    [0.0, 0.0, 0.0, 0.0, 0.0]\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.regression.GBTRegressor",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasLabelCol",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasPredictionCol",
            "pyspark.ml.param.shared.HasFitIntercept",
            "pyspark.ml.param.shared.HasMaxIter",
            "pyspark.ml.param.shared.HasTol",
            "pyspark.ml.param.shared.HasRegParam",
            "pyspark.ml.param.shared.HasWeightCol",
            "pyspark.ml.param.shared.HasSolver",
            "pyspark.ml.util.JavaMLWritable",
            "pyspark.ml.util.JavaMLReadable"
        ],
        "class_docstring": "\n    .. note:: Experimental\n\n    Generalized Linear Regression.\n\n    Fit a Generalized Linear Model specified by giving a symbolic description of the linear\n    predictor (link function) and a description of the error distribution (family). It supports\n    \"gaussian\", \"binomial\", \"poisson\", \"gamma\" and \"tweedie\" as family. Valid link functions for\n    each family is listed below. The first link function of each family is the default one.\n\n    * \"gaussian\" -> \"identity\", \"log\", \"inverse\"\n\n    * \"binomial\" -> \"logit\", \"probit\", \"cloglog\"\n\n    * \"poisson\"  -> \"log\", \"identity\", \"sqrt\"\n\n    * \"gamma\"    -> \"inverse\", \"identity\", \"log\"\n\n    * \"tweedie\"  -> power link function specified through \"linkPower\".                     The default link power in the tweedie family is 1 - variancePower.\n\n    .. seealso:: `GLM <https://en.wikipedia.org/wiki/Generalized_linear_model>`_\n\n    >>> from pyspark.ml.linalg import Vectors\n    >>> df = spark.createDataFrame([\n    ...     (1.0, Vectors.dense(0.0, 0.0)),\n    ...     (1.0, Vectors.dense(1.0, 2.0)),\n    ...     (2.0, Vectors.dense(0.0, 0.0)),\n    ...     (2.0, Vectors.dense(1.0, 1.0)),], [\"label\", \"features\"])\n    >>> glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", linkPredictionCol=\"p\")\n    >>> model = glr.fit(df)\n    >>> transformed = model.transform(df)\n    >>> abs(transformed.head().prediction - 1.5) < 0.001\n    True\n    >>> abs(transformed.head().p - 1.5) < 0.001\n    True\n    >>> model.coefficients\n    DenseVector([1.5..., -1.0...])\n    >>> model.numFeatures\n    2\n    >>> abs(model.intercept - 1.5) < 0.001\n    True\n    >>> glr_path = temp_path + \"/glr\"\n    >>> glr.save(glr_path)\n    >>> glr2 = GeneralizedLinearRegression.load(glr_path)\n    >>> glr.getFamily() == glr2.getFamily()\n    True\n    >>> model_path = temp_path + \"/glr_model\"\n    >>> model.save(model_path)\n    >>> model2 = GeneralizedLinearRegressionModel.load(model_path)\n    >>> model.intercept == model2.intercept\n    True\n    >>> model.coefficients[0] == model2.coefficients[0]\n    True\n\n    .. versionadded:: 2.0.0\n    ",
        "klass": "pyspark.ml.regression.GeneralizedLinearRegression",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasLabelCol",
            "pyspark.ml.param.shared.HasPredictionCol",
            "pyspark.ml.param.shared.HasMaxIter",
            "pyspark.ml.param.shared.HasRegParam",
            "pyspark.ml.param.shared.HasTol",
            "pyspark.ml.param.shared.HasElasticNetParam",
            "pyspark.ml.param.shared.HasFitIntercept",
            "pyspark.ml.param.shared.HasStandardization",
            "pyspark.ml.param.shared.HasSolver",
            "pyspark.ml.param.shared.HasWeightCol",
            "pyspark.ml.param.shared.HasAggregationDepth",
            "pyspark.ml.param.shared.HasLoss",
            "pyspark.ml.util.JavaMLWritable",
            "pyspark.ml.util.JavaMLReadable"
        ],
        "class_docstring": "\n    Linear regression.\n\n    The learning objective is to minimize the specified loss function, with regularization.\n    This supports two kinds of loss:\n\n    * squaredError (a.k.a squared loss)\n    * huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n\n    This supports multiple types of regularization:\n\n    * none (a.k.a. ordinary least squares)\n    * L2 (ridge regression)\n    * L1 (Lasso)\n    * L2 + L1 (elastic net)\n\n    Note: Fitting with huber loss only supports none and L2 regularization.\n\n    >>> from pyspark.ml.linalg import Vectors\n    >>> df = spark.createDataFrame([\n    ...     (1.0, 2.0, Vectors.dense(1.0)),\n    ...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n    >>> lr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n    >>> model = lr.fit(df)\n    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n    >>> abs(model.transform(test0).head().prediction - (-1.0)) < 0.001\n    True\n    >>> abs(model.coefficients[0] - 1.0) < 0.001\n    True\n    >>> abs(model.intercept - 0.0) < 0.001\n    True\n    >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n    >>> abs(model.transform(test1).head().prediction - 1.0) < 0.001\n    True\n    >>> lr.setParams(\"vector\")\n    Traceback (most recent call last):\n        ...\n    TypeError: Method setParams forces keyword arguments.\n    >>> lr_path = temp_path + \"/lr\"\n    >>> lr.save(lr_path)\n    >>> lr2 = LinearRegression.load(lr_path)\n    >>> lr2.getMaxIter()\n    5\n    >>> model_path = temp_path + \"/lr_model\"\n    >>> model.save(model_path)\n    >>> model2 = LinearRegressionModel.load(model_path)\n    >>> model.coefficients[0] == model2.coefficients[0]\n    True\n    >>> model.intercept == model2.intercept\n    True\n    >>> model.numFeatures\n    1\n    >>> model.write().format(\"pmml\").save(model_path + \"_2\")\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.regression.LinearRegression",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "pyspark.ml.wrapper.JavaEstimator",
            "pyspark.ml.param.shared.HasFeaturesCol",
            "pyspark.ml.param.shared.HasLabelCol",
            "pyspark.ml.param.shared.HasPredictionCol",
            "pyspark.ml.param.shared.HasSeed",
            "pyspark.ml.regression.RandomForestParams",
            "pyspark.ml.regression.TreeRegressorParams",
            "pyspark.ml.param.shared.HasCheckpointInterval",
            "pyspark.ml.util.JavaMLWritable",
            "pyspark.ml.util.JavaMLReadable"
        ],
        "class_docstring": "\n    `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n    learning algorithm for regression.\n    It supports both continuous and categorical features.\n\n    >>> from numpy import allclose\n    >>> from pyspark.ml.linalg import Vectors\n    >>> df = spark.createDataFrame([\n    ...     (1.0, Vectors.dense(1.0)),\n    ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n    >>> rf = RandomForestRegressor(numTrees=2, maxDepth=2, seed=42)\n    >>> model = rf.fit(df)\n    >>> model.featureImportances\n    SparseVector(1, {0: 1.0})\n    >>> allclose(model.treeWeights, [1.0, 1.0])\n    True\n    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n    >>> model.transform(test0).head().prediction\n    0.0\n    >>> model.numFeatures\n    1\n    >>> model.trees\n    [DecisionTreeRegressionModel (uid=...) of depth..., DecisionTreeRegressionModel...]\n    >>> model.getNumTrees\n    2\n    >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n    >>> model.transform(test1).head().prediction\n    0.5\n    >>> rfr_path = temp_path + \"/rfr\"\n    >>> rf.save(rfr_path)\n    >>> rf2 = RandomForestRegressor.load(rfr_path)\n    >>> rf2.getNumTrees()\n    2\n    >>> model_path = temp_path + \"/rfr_model\"\n    >>> model.save(model_path)\n    >>> model2 = RandomForestRegressionModel.load(model_path)\n    >>> model.featureImportances == model2.featureImportances\n    True\n\n    .. versionadded:: 1.4.0\n    ",
        "klass": "pyspark.ml.regression.RandomForestRegressor",
        "module": "pyspark"
    },
    {
        "base_classes": [
            "sklearn.tree.tree.BaseDecisionTree",
            "sklearn.base.ClassifierMixin"
        ],
        "class_docstring": "A decision tree classifier.\n\n    Read more in the :ref:`User Guide <tree>`.\n\n    Parameters\n    ----------\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n\n    splitter : string, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n            - If int, then consider `max_features` features at each split.\n            - If float, then `max_features` is a fraction and\n              `int(max_features * n_features)` features are considered at each\n              split.\n            - If \"auto\", then `max_features=sqrt(n_features)`.\n            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n            - If \"log2\", then `max_features=log2(n_features)`.\n            - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    presort : bool, optional (default=False)\n        Whether to presort the data to speed up the finding of best splits in\n        fitting. For the default settings of a decision tree on large\n        datasets, setting this to true may slow down the training process.\n        When using either a smaller dataset or a restricted depth, this may\n        speed up the training.\n\n    Attributes\n    ----------\n    classes_ : array of shape = [n_classes] or a list of such arrays\n        The classes labels (single output problem),\n        or a list of arrays of class labels (multi-output problem).\n\n    feature_importances_ : array of shape = [n_features]\n        The feature importances. The higher, the more important the\n        feature. The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance [4]_.\n\n    max_features_ : int,\n        The inferred value of max_features.\n\n    n_classes_ : int or list\n        The number of classes (for single output problems),\n        or a list containing the number of classes for each\n        output (for multi-output problems).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    tree_ : Tree object\n        The underlying Tree object. Please refer to\n        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n        for basic usage of these attributes.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    DecisionTreeRegressor\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n           Learning\", Springer, 2009.\n\n    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> clf = DecisionTreeClassifier(random_state=0)\n    >>> iris = load_iris()\n    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n    ...                             # doctest: +SKIP\n    ...\n    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n    ",
        "klass": "sklearn.tree.tree.DecisionTreeClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.linear_model.base.LinearClassifierMixin",
            "sklearn.linear_model.base.SparseCoefMixin"
        ],
        "class_docstring": "Logistic Regression (aka logit, MaxEnt) classifier.\n\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n    scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n    entropy loss if the 'multi_class' option is set to 'multinomial'.\n    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n    'sag' and 'newton-cg' solvers.)\n\n    This class implements regularized logistic regression using the\n    'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n    both dense and sparse input. Use C-ordered arrays or CSR matrices\n    containing 64-bit floats for optimal performance; any other input format\n    will be converted (and copied).\n\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n    with primal formulation. The 'liblinear' solver supports both L1 and L2\n    regularization, with a dual formulation only for the L2 penalty.\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    penalty : str, 'l1' or 'l2', default: 'l2'\n        Used to specify the norm used in the penalization. The 'newton-cg',\n        'sag' and 'lbfgs' solvers support only l2 penalties.\n\n        .. versionadded:: 0.19\n           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n    dual : bool, default: False\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    tol : float, default: 1e-4\n        Tolerance for stopping criteria.\n\n    C : float, default: 1.0\n        Inverse of regularization strength; must be a positive float.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, default: True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    intercept_scaling : float, default 1.\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    class_weight : dict or 'balanced', default: None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           *class_weight='balanced'*\n\n    random_state : int, RandomState instance or None, optional, default: None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag' or\n        'liblinear'.\n\n    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              default: 'liblinear'.\n\n        Algorithm to use in the optimization problem.\n\n        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n          'saga' are faster for large ones.\n        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n          schemes.\n        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n          'liblinear' and 'saga' handle L1 penalty.\n\n        Note that 'sag' and 'saga' fast convergence is only guaranteed on\n        features with approximately the same scale. You can\n        preprocess the data with a scaler from sklearn.preprocessing.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.20\n            Default will change from 'liblinear' to 'lbfgs' in 0.22.\n\n    max_iter : int, default: 100\n        Useful only for the newton-cg, sag and lbfgs solvers.\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.20\n            Default will change from 'ovr' to 'auto' in 0.22.\n\n    verbose : int, default: 0\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, default: False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    Attributes\n    ----------\n\n    classes_ : array, shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : array, shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : array, shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_iter_ : array, shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0, solver='lbfgs',\n    ...                          multi_class='multinomial').fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n\n    See also\n    --------\n    SGDClassifier : incrementally trained logistic regression (when given\n        the parameter ``loss=\"log\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n        SAGA: A Fast Incremental Gradient Method With Support\n        for Non-Strongly Convex Composite Objectives\n        https://arxiv.org/abs/1407.0202\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n    ",
        "klass": "sklearn.linear_model.logistic.LogisticRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearModel",
            "sklearn.base.RegressorMixin",
            "sklearn.base.MultiOutputMixin"
        ],
        "class_docstring": "\n    Ordinary least squares Linear Regression.\n\n    Parameters\n    ----------\n    fit_intercept : boolean, optional, default True\n        whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n        an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This will only provide\n        speedup for n_targets > 1 and sufficient large problems.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.\n        If multiple targets are passed during the fit (y 2D), this\n        is a 2D array of shape (n_targets, n_features), while if only\n        one target is passed, this is a 1D array of length n_features.\n\n    intercept_ : array\n        Independent term in the linear model.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n    >>> # y = 1 * x_0 + 2 * x_1 + 3\n    >>> y = np.dot(X, np.array([1, 2])) + 3\n    >>> reg = LinearRegression().fit(X, y)\n    >>> reg.score(X, y)\n    1.0\n    >>> reg.coef_\n    array([1., 2.])\n    >>> reg.intercept_ # doctest: +ELLIPSIS\n    3.0000...\n    >>> reg.predict(np.array([[3, 5]]))\n    array([16.])\n\n    Notes\n    -----\n    From the implementation point of view, this is just plain Ordinary\n    Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n\n    ",
        "klass": "sklearn.linear_model.base.LinearRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.TransformerMixin"
        ],
        "class_docstring": "Imputation transformer for completing missing values.\n\n    Read more in the :ref:`User Guide <imputation>`.\n\n    Parameters\n    ----------\n    missing_values : integer or \"NaN\", optional (default=\"NaN\")\n        The placeholder for the missing values. All occurrences of\n        `missing_values` will be imputed. For missing values encoded as np.nan,\n        use the string value \"NaN\".\n\n    strategy : string, optional (default=\"mean\")\n        The imputation strategy.\n\n        - If \"mean\", then replace missing values using the mean along\n          the axis.\n        - If \"median\", then replace missing values using the median along\n          the axis.\n        - If \"most_frequent\", then replace missing using the most frequent\n          value along the axis.\n\n    axis : integer, optional (default=0)\n        The axis along which to impute.\n\n        - If `axis=0`, then impute along columns.\n        - If `axis=1`, then impute along rows.\n\n    verbose : integer, optional (default=0)\n        Controls the verbosity of the imputer.\n\n    copy : boolean, optional (default=True)\n        If True, a copy of X will be created. If False, imputation will\n        be done in-place whenever possible. Note that, in the following cases,\n        a new copy will always be made, even if `copy=False`:\n\n        - If X is not an array of floating values;\n        - If X is sparse and `missing_values=0`;\n        - If `axis=0` and X is encoded as a CSR matrix;\n        - If `axis=1` and X is encoded as a CSC matrix.\n\n    Attributes\n    ----------\n    statistics_ : array of shape (n_features,)\n        The imputation fill value for each feature if axis == 0.\n\n    Notes\n    -----\n    - When ``axis=0``, columns which only contained missing values at `fit`\n      are discarded upon `transform`.\n    - When ``axis=1``, an exception is raised if there are rows for which it is\n      not possible to fill in the missing values (e.g., because they only\n      contain missing values).\n    ",
        "klass": "sklearn.preprocessing.imputation.Imputer",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.tree.tree.BaseDecisionTree",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "A decision tree regressor.\n\n    Read more in the :ref:`User Guide <tree>`.\n\n    Parameters\n    ----------\n    criterion : string, optional (default=\"mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"mse\" for the mean squared error, which is equal to variance\n        reduction as feature selection criterion and minimizes the L2 loss\n        using the mean of each terminal node, \"friedman_mse\", which uses mean\n        squared error with Friedman's improvement score for potential splits,\n        and \"mae\" for the mean absolute error, which minimizes the L1 loss\n        using the median of each terminal node.\n\n        .. versionadded:: 0.18\n           Mean Absolute Error (MAE) criterion.\n\n    splitter : string, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    presort : bool, optional (default=False)\n        Whether to presort the data to speed up the finding of best splits in\n        fitting. For the default settings of a decision tree on large\n        datasets, setting this to true may slow down the training process.\n        When using either a smaller dataset or a restricted depth, this may\n        speed up the training.\n\n    Attributes\n    ----------\n    feature_importances_ : array of shape = [n_features]\n        The feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the\n        (normalized) total reduction of the criterion brought\n        by that feature. It is also known as the Gini importance [4]_.\n\n    max_features_ : int,\n        The inferred value of max_features.\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    tree_ : Tree object\n        The underlying Tree object. Please refer to\n        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n        for basic usage of these attributes.\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    DecisionTreeClassifier\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n           Learning\", Springer, 2009.\n\n    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_boston\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.tree import DecisionTreeRegressor\n    >>> boston = load_boston()\n    >>> regressor = DecisionTreeRegressor(random_state=0)\n    >>> cross_val_score(regressor, boston.data, boston.target, cv=10)\n    ...                    # doctest: +SKIP\n    ...\n    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\n            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\n    ",
        "klass": "sklearn.tree.tree.DecisionTreeRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.ridge._BaseRidge",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Linear least squares with l2 regularization.\n\n    Minimizes the objective function::\n\n    ||y - Xw||^2_2 + alpha * ||w||^2_2\n\n    This model solves a regression model where the loss function is\n    the linear least squares function and regularization is given by\n    the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n    This estimator has built-in support for multi-variate regression\n    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : {float, array-like}, shape (n_targets)\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC. If an array is passed, penalties are\n        assumed to be specific to the targets. Hence they must correspond in\n        number.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        Maximum number of iterations for conjugate gradient solver.\n        For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n        by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n\n    tol : float\n        Precision of the solution.\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. More stable for singular matrices than\n          'cholesky'.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its improved, unbiased version named SAGA. Both methods also use an\n          iterative procedure, and are often faster than other solvers when\n          both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n        All last five solvers support both dense and sparse data. However, only\n        'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n        True.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag'.\n\n        .. versionadded:: 0.17\n           *random_state* to support Stochastic Average Gradient.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : array or None, shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n        .. versionadded:: 0.17\n\n    See also\n    --------\n    RidgeClassifier : Ridge classifier\n    RidgeCV : Ridge regression with built-in cross validation\n    :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n        combines ridge regression with the kernel trick\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import Ridge\n    >>> import numpy as np\n    >>> n_samples, n_features = 10, 5\n    >>> rng = np.random.RandomState(0)\n    >>> y = rng.randn(n_samples)\n    >>> X = rng.randn(n_samples, n_features)\n    >>> clf = Ridge(alpha=1.0)\n    >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n    Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n          normalize=False, random_state=None, solver='auto', tol=0.001)\n\n    ",
        "klass": "sklearn.linear_model.ridge.Ridge",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.ridge._BaseRidgeCV",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Ridge regression with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n        If using generalized cross-validation, alphas must be positive.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n        If None, the negative mean squared error if cv is 'auto' or None\n        (i.e. when using generalized cross-validation), and r2 score otherwise.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n          (also known as Generalized Cross-Validation).\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n        Flag indicating which strategy to use when performing\n        Generalized Cross-Validation. Options are::\n\n            'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n            'svd' : force use of singular value decomposition of X when X is\n                dense, eigenvalue decomposition of X^T.X when X is sparse.\n            'eigen' : force computation via eigendecomposition of X.X^T\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending on the shape of the training data.\n\n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if ``store_cv_values=True``        and ``cv=None``). After ``fit()`` has been called, this attribute         will contain the mean squared errors (by default) or the values         of the ``{loss,score}_func`` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y) # doctest: +ELLIPSIS\n    0.5166...\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeClassifierCV : Ridge classifier with built-in cross validation\n    ",
        "klass": "sklearn.linear_model.ridge.RidgeCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearClassifierMixin",
            "sklearn.linear_model.ridge._BaseRidge"
        ],
        "class_docstring": "Classifier using Ridge regression.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alpha : float\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set to false, no\n        intercept will be used in calculations (e.g. data is expected to be\n        already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        Maximum number of iterations for conjugate gradient solver.\n        The default value is determined by scipy.sparse.linalg.\n\n    tol : float\n        Precision of the solution.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. More stable for singular matrices than\n          'cholesky'.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution.\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its unbiased and more flexible version named SAGA. Both methods\n          use an iterative procedure, and are often faster than other solvers\n          when both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n          .. versionadded:: 0.17\n             Stochastic Average Gradient descent solver.\n          .. versionadded:: 0.19\n           SAGA solver.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag'.\n\n    Attributes\n    ----------\n    coef_ : array, shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    n_iter_ : array or None, shape (n_targets,)\n        Actual number of iterations for each target. Available only for\n        sag and lsqr solvers. Other solvers will return None.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifier\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifier().fit(X, y)\n    >>> clf.score(X, y) # doctest: +ELLIPSIS\n    0.9595...\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifierCV :  Ridge classifier with built-in cross validation\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n    ",
        "klass": "sklearn.linear_model.ridge.RidgeClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearClassifierMixin",
            "sklearn.linear_model.ridge._BaseRidgeCV"
        ],
        "class_docstring": "Ridge classifier with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation. Currently, only the n_features >\n    n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if ``store_cv_values=True`` and\n        ``cv=None``). After ``fit()`` has been called, this attribute will\n        contain the mean squared errors (by default) or the values of the\n        ``{loss,score}_func`` function (if provided in the constructor).\n\n    coef_ : array, shape (1, n_features) or (n_targets, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y) # doctest: +ELLIPSIS\n    0.9630...\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeCV : Ridge regression with built-in cross validation\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n    ",
        "klass": "sklearn.linear_model.ridge.RidgeClassifierCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearModel"
        ],
        "class_docstring": "Ridge regression with built-in Generalized Cross-Validation\n\n    It allows efficient Leave-One-Out cross-validation.\n\n    This class is not intended to be used directly. Use RidgeCV instead.\n\n    Notes\n    -----\n\n    We want to solve (K + alpha*Id)c = y,\n    where K = X X^T is the kernel matrix.\n\n    Let G = (K + alpha*Id)^-1.\n\n    Dual solution: c = Gy\n    Primal solution: w = X^T c\n\n    Compute eigendecomposition K = Q V Q^T.\n    Then G = Q (V + alpha*Id)^-1 Q^T,\n    where (V + alpha*Id) is diagonal.\n    It is thus inexpensive to inverse for many alphas.\n\n    Let loov be the vector of prediction values for each example\n    when the model was fitted with all examples but this example.\n\n    loov = (KGY - diag(KG)Y) / diag(I-KG)\n\n    Let looe be the vector of prediction errors for each example\n    when the model was fitted with all examples but this example.\n\n    looe = y - loov = c / diag(G)\n\n    References\n    ----------\n    http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf\n    https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf\n    ",
        "klass": "sklearn.linear_model.ridge._RidgeGCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors.ball_tree.BinaryTree"
        ],
        "class_docstring": "BallTree for fast generalized N-point problems\n\nBallTree(X, leaf_size=40, metric='minkowski', \\**kwargs)\n\nParameters\n----------\nX : array-like, shape = [n_samples, n_features]\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive integer (default = 40)\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : string or DistanceMetric object\n    the distance metric to use for the tree.  Default='minkowski'\n    with p=2 (that is, a euclidean metric). See the documentation\n    of the DistanceMetric class for a list of available metrics.\n    ball_tree.valid_metrics gives a list of the metrics which\n    are valid for BallTree.\n\nAdditional keywords are passed to the distance metric class.\n\nAttributes\n----------\ndata : memory view\n    The training data\n\nExamples\n--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = BallTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])\n\n",
        "klass": "sklearn.neighbors.ball_tree.BallTree",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A max-heap structure to keep track of distances/indices of neighbors\n\n    This implements an efficient pre-allocated set of fixed-size heaps\n    for chasing neighbors, holding both an index and a distance.\n    When any row of the heap is full, adding an additional point will push\n    the furthest point off the heap.\n\n    Parameters\n    ----------\n    n_pts : int\n        the number of heaps to use\n    n_nbrs : int\n        the size of each heap.\n    ",
        "klass": "sklearn.neighbors.ball_tree.NeighborsHeap",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.TransformerMixin"
        ],
        "class_docstring": "Non-Negative Matrix Factorization (NMF)\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is::\n\n        0.5 * ||X - WH||_Fro^2\n        + alpha * l1_ratio * ||vec(W)||_1\n        + alpha * l1_ratio * ||vec(H)||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\n    Where::\n\n        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)\n        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\n    For multiplicative-update ('mu') solver, the Frobenius norm\n    (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,\n    by changing the beta_loss parameter.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or None\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init : None | 'random' | 'nndsvd' |  'nndsvda' | 'nndsvdar' | 'custom'\n        Method used to initialize the procedure.\n        Default: None.\n        Valid options:\n\n        - None: 'nndsvd' if n_components <= min(n_samples, n_features),\n            otherwise random.\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H\n\n    solver : 'cd' | 'mu'\n        Numerical solver to use:\n        'cd' is a Coordinate Descent solver.\n        'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or string, default 'frobenius'\n        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default: 1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : integer, default: 200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms. Set it to zero to\n        have no regularization.\n\n        .. versionadded:: 0.17\n           *alpha* used in the Coordinate Descent solver.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    shuffle : boolean, default: False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Factorization matrix, sometimes called 'dictionary'.\n\n    reconstruction_err_ : number\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    ",
        "klass": "sklearn.decomposition.nmf.NMF",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Multidimensional scaling\n\n    Read more in the :ref:`User Guide <multidimensional_scaling>`.\n\n    Parameters\n    ----------\n    n_components : int, optional, default: 2\n        Number of dimensions in which to immerse the dissimilarities.\n\n    metric : boolean, optional, default: True\n        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.\n\n    n_init : int, optional, default: 4\n        Number of times the SMACOF algorithm will be run with different\n        initializations. The final results will be the best output of the runs,\n        determined by the run with the smallest final stress.\n\n    max_iter : int, optional, default: 300\n        Maximum number of iterations of the SMACOF algorithm for a single run.\n\n    verbose : int, optional, default: 0\n        Level of verbosity.\n\n    eps : float, optional, default: 1e-3\n        Relative tolerance with respect to stress at which to declare\n        convergence.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. If multiple\n        initializations are used (``n_init``), each run of the algorithm is\n        computed in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        The generator used to initialize the centers.  If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    dissimilarity : 'euclidean' | 'precomputed', optional, default: 'euclidean'\n        Dissimilarity measure to use:\n\n        - 'euclidean':\n            Pairwise Euclidean distances between points in the dataset.\n\n        - 'precomputed':\n            Pre-computed dissimilarities are passed directly to ``fit`` and\n            ``fit_transform``.\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape (n_samples, n_components)\n        Stores the position of the dataset in the embedding space.\n\n    stress_ : float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points).\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import MDS\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = MDS(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n    \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\n    Groenen P. Springer Series in Statistics (1997)\n\n    \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\n    Psychometrika, 29 (1964)\n\n    \"Multidimensional scaling by optimizing goodness of fit to a nonmetric\n    hypothesis\" Kruskal, J. Psychometrika, 29, (1964)\n\n    ",
        "klass": "sklearn.manifold.mds.MDS",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator"
        ],
        "class_docstring": "Spectral embedding for non-linear dimensionality reduction.\n\n    Forms an affinity matrix given by the specified function and\n    applies spectral decomposition to the corresponding graph laplacian.\n    The resulting transformation is given by the value of the\n    eigenvectors for each data point.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    ----------\n    n_components : integer, default: 2\n        The dimension of the projected subspace.\n\n    affinity : string or callable, default : \"nearest_neighbors\"\n        How to construct the affinity matrix.\n         - 'nearest_neighbors' : construct affinity matrix by knn graph\n         - 'rbf' : construct affinity matrix by rbf kernel\n         - 'precomputed' : interpret X as precomputed affinity matrix\n         - callable : use passed in function as affinity\n           the function takes in data matrix (n_samples, n_features)\n           and return affinity matrix (n_samples, n_samples).\n\n    gamma : float, optional, default : 1/n_features\n        Kernel coefficient for rbf kernel.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigenvectors.  If int, random_state is the seed used by the\n        random number generator; If RandomState instance, random_state is the\n        random number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``solver`` ==\n        'amg'.\n\n    eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities.\n\n    n_neighbors : int, default : max(n_samples/10 , 1)\n        Number of nearest neighbors for nearest_neighbors graph building.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n\n    embedding_ : array, shape = (n_samples, n_components)\n        Spectral embedding of the training matrix.\n\n    affinity_matrix_ : array, shape = (n_samples, n_samples)\n        Affinity_matrix constructed from samples or precomputed.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import SpectralEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = SpectralEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    References\n    ----------\n\n    - A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n    - On Spectral Clustering: Analysis and an algorithm, 2001\n      Andrew Y. Ng, Michael I. Jordan, Yair Weiss\n      http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100\n\n    - Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n    ",
        "klass": "sklearn.manifold.spectral_embedding_.SpectralEmbedding",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors.kd_tree.BinaryTree"
        ],
        "class_docstring": "KDTree for fast generalized N-point problems\n\nKDTree(X, leaf_size=40, metric='minkowski', \\**kwargs)\n\nParameters\n----------\nX : array-like, shape = [n_samples, n_features]\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive integer (default = 40)\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : string or DistanceMetric object\n    the distance metric to use for the tree.  Default='minkowski'\n    with p=2 (that is, a euclidean metric). See the documentation\n    of the DistanceMetric class for a list of available metrics.\n    kd_tree.valid_metrics gives a list of the metrics which\n    are valid for KDTree.\n\nAdditional keywords are passed to the distance metric class.\n\nAttributes\n----------\ndata : memory view\n    The training data\n\nExamples\n--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])\n\n",
        "klass": "sklearn.neighbors.kd_tree.KDTree",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.neighbors.base.NeighborsBase",
            "sklearn.neighbors.base.KNeighborsMixin",
            "sklearn.neighbors.base.RadiusNeighborsMixin",
            "sklearn.neighbors.base.UnsupervisedMixin"
        ],
        "class_docstring": "Unsupervised learner for implementing neighbor searches.\n\n    Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n\n    Parameters\n    ----------\n    n_neighbors : int, optional (default = 5)\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n\n    radius : float, optional (default = 1.0)\n        Range of parameter space to use by default for :meth:`radius_neighbors`\n        queries.\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        Algorithm used to compute the nearest neighbors:\n\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n\n    metric : string or callable, default 'minkowski'\n        metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n          'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n          'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    p : integer, optional (default = 2)\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n    metric_params : dict, optional (default = None)\n        Additional keyword arguments for the metric function.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Examples\n    --------\n      >>> import numpy as np\n      >>> from sklearn.neighbors import NearestNeighbors\n      >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n\n      >>> neigh = NearestNeighbors(2, 0.4)\n      >>> neigh.fit(samples)  #doctest: +ELLIPSIS\n      NearestNeighbors(...)\n\n      >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n      ... #doctest: +ELLIPSIS\n      array([[2, 0]]...)\n\n      >>> nbrs = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False)\n      >>> np.asarray(nbrs[0][0])\n      array(2)\n\n    See also\n    --------\n    KNeighborsClassifier\n    RadiusNeighborsClassifier\n    KNeighborsRegressor\n    RadiusNeighborsRegressor\n    BallTree\n\n    Notes\n    -----\n    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n\n    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n    ",
        "klass": "sklearn.neighbors.unsupervised.NearestNeighbors",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cluster.bicluster.BaseSpectral"
        ],
        "class_docstring": "Spectral biclustering (Kluger, 2003).\n\n    Partitions rows and columns under the assumption that the data has\n    an underlying checkerboard structure. For instance, if there are\n    two row partitions and three column partitions, each row will\n    belong to three biclusters, and each column will belong to two\n    biclusters. The outer product of the corresponding row and column\n    label vectors gives this checkerboard structure.\n\n    Read more in the :ref:`User Guide <spectral_biclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : integer or tuple (n_row_clusters, n_column_clusters)\n        The number of row and column clusters in the checkerboard\n        structure.\n\n    method : string, optional, default: 'bistochastic'\n        Method of normalizing and converting singular vectors into\n        biclusters. May be one of 'scale', 'bistochastic', or 'log'.\n        The authors recommend using 'log'. If the data is sparse,\n        however, log normalization will not work, which is why the\n        default is 'bistochastic'. CAUTION: if `method='log'`, the\n        data must not be sparse.\n\n    n_components : integer, optional, default: 6\n        Number of singular vectors to check.\n\n    n_best : integer, optional, default: 3\n        Number of best singular vectors to which to project the data\n        for clustering.\n\n    svd_method : string, optional, default: 'randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', uses\n        `sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', uses\n        `scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, optional, default: None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, optional, default: False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random' or an ndarray}\n         Method for initialization of k-means algorithm; defaults to\n         'k-means++'.\n\n    n_init : int, optional, default: 10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None (default)\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like, shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like, shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like, shape (n_rows,)\n        Row partition labels.\n\n    column_labels_ : array-like, shape (n_cols,)\n        Column partition labels.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralBiclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralBiclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([1, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 1], dtype=int32)\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    SpectralBiclustering(init='k-means++', method='bistochastic',\n               mini_batch=False, n_best=3, n_clusters=2, n_components=6,\n               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,\n               svd_method='randomized')\n\n    References\n    ----------\n\n    * Kluger, Yuval, et. al., 2003. `Spectral biclustering of microarray\n      data: coclustering genes and conditions\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608>`__.\n\n    ",
        "klass": "sklearn.cluster.bicluster.SpectralBiclustering",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cluster.bicluster.BaseSpectral"
        ],
        "class_docstring": "Spectral Co-Clustering algorithm (Dhillon, 2001).\n\n    Clusters rows and columns of an array `X` to solve the relaxed\n    normalized cut of the bipartite graph created from `X` as follows:\n    the edge between row vertex `i` and column vertex `j` has weight\n    `X[i, j]`.\n\n    The resulting bicluster structure is block-diagonal, since each\n    row and each column belongs to exactly one bicluster.\n\n    Supports sparse matrices, as long as they are nonnegative.\n\n    Read more in the :ref:`User Guide <spectral_coclustering>`.\n\n    Parameters\n    ----------\n    n_clusters : integer, optional, default: 3\n        The number of biclusters to find.\n\n    svd_method : string, optional, default: 'randomized'\n        Selects the algorithm for finding singular vectors. May be\n        'randomized' or 'arpack'. If 'randomized', use\n        :func:`sklearn.utils.extmath.randomized_svd`, which may be faster\n        for large matrices. If 'arpack', use\n        :func:`scipy.sparse.linalg.svds`, which is more accurate, but\n        possibly slower in some cases.\n\n    n_svd_vecs : int, optional, default: None\n        Number of vectors to use in calculating the SVD. Corresponds\n        to `ncv` when `svd_method=arpack` and `n_oversamples` when\n        `svd_method` is 'randomized`.\n\n    mini_batch : bool, optional, default: False\n        Whether to use mini-batch k-means, which is faster but may get\n        different results.\n\n    init : {'k-means++', 'random' or an ndarray}\n         Method for initialization of k-means algorithm; defaults to\n         'k-means++'.\n\n    n_init : int, optional, default: 10\n        Number of random initializations that are tried with the\n        k-means algorithm.\n\n        If mini-batch k-means is used, the best initialization is\n        chosen and the algorithm runs once. Otherwise, the algorithm\n        is run for each initialization and the best solution chosen.\n\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None (default)\n        Used for randomizing the singular value decomposition and the k-means\n        initialization. Use an int to make the randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    rows_ : array-like, shape (n_row_clusters, n_rows)\n        Results of the clustering. `rows[i, r]` is True if\n        cluster `i` contains row `r`. Available only after calling ``fit``.\n\n    columns_ : array-like, shape (n_column_clusters, n_columns)\n        Results of the clustering, like `rows`.\n\n    row_labels_ : array-like, shape (n_rows,)\n        The bicluster label of each row.\n\n    column_labels_ : array-like, shape (n_cols,)\n        The bicluster label of each column.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import SpectralCoclustering\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [1, 0],\n    ...               [4, 7], [3, 5], [3, 6]])\n    >>> clustering = SpectralCoclustering(n_clusters=2, random_state=0).fit(X)\n    >>> clustering.row_labels_\n    array([0, 1, 1, 0, 0, 0], dtype=int32)\n    >>> clustering.column_labels_\n    array([0, 0], dtype=int32)\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    SpectralCoclustering(init='k-means++', mini_batch=False, n_clusters=2,\n               n_init=10, n_jobs=None, n_svd_vecs=None, random_state=0,\n               svd_method='randomized')\n\n    References\n    ----------\n\n    * Dhillon, Inderjit S, 2001. `Co-clustering documents and words using\n      bipartite spectral graph partitioning\n      <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011>`__.\n\n    ",
        "klass": "sklearn.cluster.bicluster.SpectralCoclustering",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A max-heap structure to keep track of distances/indices of neighbors\n\n    This implements an efficient pre-allocated set of fixed-size heaps\n    for chasing neighbors, holding both an index and a distance.\n    When any row of the heap is full, adding an additional point will push\n    the furthest point off the heap.\n\n    Parameters\n    ----------\n    n_pts : int\n        the number of heaps to use\n    n_nbrs : int\n        the size of each heap.\n    ",
        "klass": "sklearn.neighbors.kd_tree.NeighborsHeap",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.ClusterMixin"
        ],
        "class_docstring": "Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, optional\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, optional\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a sparse matrix, in which case only \"nonzero\"\n        elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, optional\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, optional\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : array, shape = [n_core_samples]\n        Indices of core samples.\n\n    components_ : array, shape = [n_core_samples, n_features]\n        Copy of each core sample found by training.\n\n    labels_ : array, shape = [n_samples]\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    DBSCAN(algorithm='auto', eps=3, leaf_size=30, metric='euclidean',\n        metric_params=None, min_samples=2, n_jobs=None, p=None)\n\n    See also\n    --------\n    OPTICS\n        A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    ",
        "klass": "sklearn.cluster.dbscan_.DBSCAN",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearModel",
            "sklearn.base.RegressorMixin",
            "sklearn.base.MultiOutputMixin"
        ],
        "class_docstring": "Linear regression with combined L1 and L2 priors as regularizer.\n\n    Minimizes the objective function::\n\n            1 / (2 * n_samples) * ||y - Xw||^2_2\n            + alpha * l1_ratio * ||w||_1\n            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n            a * L1 + b * L2\n\n    where::\n\n            alpha = a + b and l1_ratio = a / (a + b)\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package while\n    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n    = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n    unless you supply your own sequence of alpha.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Constant that multiplies the penalty terms. Defaults to 1.0.\n        See the notes for the exact mathematical meaning of this\n        parameter.``alpha = 0`` is equivalent to an ordinary least square,\n        solved by the :class:`LinearRegression` object. For numerical\n        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n        Given this, you should use the :class:`LinearRegression` object.\n\n    l1_ratio : float\n        The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n        is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n        combination of L1 and L2.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If ``False``, the\n        data is assumed to be already centered.\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. The Gram matrix can also be passed as argument.\n        For sparse input this option is always ``True`` to preserve sparsity.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, optional\n        When set to ``True``, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n        ``sparse_coef_`` is a readonly property derived from ``coef_``\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    n_iter_ : array-like, shape (n_targets,)\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNet\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNet(random_state=0)\n    >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n    ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n          max_iter=1000, normalize=False, positive=False, precompute=False,\n          random_state=0, selection='cyclic', tol=0.0001, warm_start=False)\n    >>> print(regr.coef_) # doctest: +ELLIPSIS\n    [18.83816048 64.55968825]\n    >>> print(regr.intercept_) # doctest: +ELLIPSIS\n    1.451...\n    >>> print(regr.predict([[0, 0]])) # doctest: +ELLIPSIS\n    [1.451...]\n\n\n    Notes\n    -----\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    See also\n    --------\n    ElasticNetCV : Elastic net model with best model selection by\n        cross-validation.\n    SGDRegressor: implements elastic net regression with incremental training.\n    SGDClassifier: implements logistic regression with elastic net penalty\n        (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n    ",
        "klass": "sklearn.linear_model.coordinate_descent.ElasticNet",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.coordinate_descent.LinearModelCV",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Elastic Net model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    l1_ratio : float or array of floats, optional\n        float between 0 and 1 passed to ElasticNet (scaling between\n        l1 and l2 penalties). For ``l1_ratio = 0``\n        the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n        For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n        This parameter can be a list, in which case the different\n        values are tested by cross-validation and the one giving the best\n        prediction score is used. Note that a good choice of list of\n        values for l1_ratio is often to put more values close to 1\n        (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n        .9, .95, .99, 1]``\n\n    eps : float, optional\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path, used for each l1_ratio.\n\n    alphas : numpy array, optional\n        List of alphas where to compute the models.\n        If None alphas are set automatically\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation\n\n    l1_ratio_ : float\n        The compromise between l1 and l2 penalization chosen by\n        cross validation\n\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        Parameter vector (w in the cost function formula),\n\n    intercept_ : float | array, shape (n_targets, n_features)\n        Independent term in the decision function.\n\n    mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)\n        Mean square error for the test set on each fold, varying l1_ratio and\n        alpha.\n\n    alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n        The grid of alphas used for fitting, for each l1_ratio.\n\n    n_iter_ : int\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import ElasticNetCV\n    >>> from sklearn.datasets import make_regression\n\n    >>> X, y = make_regression(n_features=2, random_state=0)\n    >>> regr = ElasticNetCV(cv=5, random_state=0)\n    >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n    ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n           l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,\n           normalize=False, positive=False, precompute='auto', random_state=0,\n           selection='cyclic', tol=0.0001, verbose=0)\n    >>> print(regr.alpha_) # doctest: +ELLIPSIS\n    0.199...\n    >>> print(regr.intercept_) # doctest: +ELLIPSIS\n    0.398...\n    >>> print(regr.predict([[0, 0]])) # doctest: +ELLIPSIS\n    [0.398...]\n\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package\n    while alpha corresponds to the lambda parameter in glmnet.\n    More specifically, the optimization objective is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n        a * L1 + b * L2\n\n    for::\n\n        alpha = a + b and l1_ratio = a / (a + b).\n\n    See also\n    --------\n    enet_path\n    ElasticNet\n\n    ",
        "klass": "sklearn.linear_model.coordinate_descent.ElasticNetCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.coordinate_descent.ElasticNet"
        ],
        "class_docstring": "Linear Model trained with L1 prior as regularizer (aka the Lasso)\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Technically the Lasso model is optimizing the same objective function as\n    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Constant that multiplies the L1 term. Defaults to 1.0.\n        ``alpha = 0`` is equivalent to an ordinary least square, solved\n        by the :class:`LinearRegression` object. For numerical\n        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n        Given this, you should use the :class:`LinearRegression` object.\n\n    fit_intercept : boolean, optional, default True\n        Whether to calculate the intercept for this model. If set\n        to False, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | array-like, default=False\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument. For sparse input\n        this option is always ``True`` to preserve sparsity.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    warm_start : bool, optional\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n    positive : bool, optional\n        When set to ``True``, forces the coefficients to be positive.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n        ``sparse_coef_`` is a readonly property derived from ``coef_``\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    n_iter_ : int | array-like, shape (n_targets,)\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ... # doctest: +NORMALIZE_WHITESPACE\n    Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n       normalize=False, positive=False, precompute=False, random_state=None,\n       selection='cyclic', tol=0.0001, warm_start=False)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)  # doctest: +ELLIPSIS\n    0.15...\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    LassoLars\n    LassoCV\n    LassoLarsCV\n    sklearn.decomposition.sparse_encode\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n    ",
        "klass": "sklearn.linear_model.coordinate_descent.Lasso",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.coordinate_descent.LinearModelCV",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Lasso linear model with iterative fitting along a regularization path.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The best model is selected by cross-validation.\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    eps : float, optional\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``.\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path\n\n    alphas : numpy array, optional\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically\n\n    fit_intercept : boolean, default True\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : int, optional\n        The maximum number of iterations\n\n    tol : float, optional\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        dual gap for optimality and continues until it is smaller\n        than ``tol``.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive : bool, optional\n        If positive, restrict regression coefficients to be positive\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator that selects a random\n        feature to update.  If int, random_state is the seed used by the random\n        number generator; If RandomState instance, random_state is the random\n        number generator; If None, the random number generator is the\n        RandomState instance used by `np.random`. Used when ``selection`` ==\n        'random'.\n\n    selection : str, default 'cyclic'\n        If set to 'random', a random coefficient is updated every iteration\n        rather than looping over features sequentially by default. This\n        (setting to 'random') often leads to significantly faster convergence\n        especially when tol is higher than 1e-4.\n\n    Attributes\n    ----------\n    alpha_ : float\n        The amount of penalization chosen by cross validation\n\n    coef_ : array, shape (n_features,) | (n_targets, n_features)\n        parameter vector (w in the cost function formula)\n\n    intercept_ : float | array, shape (n_targets,)\n        independent term in decision function.\n\n    mse_path_ : array, shape (n_alphas, n_folds)\n        mean square error for the test set on each fold, varying alpha\n\n    alphas_ : numpy array, shape (n_alphas,)\n        The grid of alphas used for fitting\n\n    dual_gap_ : ndarray, shape ()\n        The dual gap at the end of the optimization for the optimal alpha\n        (``alpha_``).\n\n    n_iter_ : int\n        number of iterations run by the coordinate descent solver to reach\n        the specified tolerance for the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4, random_state=0)\n    >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n    >>> reg.score(X, y) # doctest: +ELLIPSIS\n    0.9993...\n    >>> reg.predict(X[:1,])\n    array([-78.4951...])\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_model_selection.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    LassoLars\n    Lasso\n    LassoLarsCV\n    ",
        "klass": "sklearn.linear_model.coordinate_descent.LassoCV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.TransformerMixin",
            "sklearn.base.ClusterMixin"
        ],
        "class_docstring": "Implements the Birch clustering algorithm.\n\n    It is a memory-efficient, online-learning algorithm provided as an\n    alternative to :class:`MiniBatchKMeans`. It constructs a tree\n    data structure with the cluster centroids being read off the leaf.\n    These can be either the final cluster centroids or can be provided as input\n    to another clustering algorithm such as :class:`AgglomerativeClustering`.\n\n    Read more in the :ref:`User Guide <birch>`.\n\n    Parameters\n    ----------\n    threshold : float, default 0.5\n        The radius of the subcluster obtained by merging a new sample and the\n        closest subcluster should be lesser than the threshold. Otherwise a new\n        subcluster is started. Setting this value to be very low promotes\n        splitting and vice-versa.\n\n    branching_factor : int, default 50\n        Maximum number of CF subclusters in each node. If a new samples enters\n        such that the number of subclusters exceed the branching_factor then\n        that node is split into two nodes with the subclusters redistributed\n        in each. The parent subcluster of that node is removed and two new\n        subclusters are added as parents of the 2 split nodes.\n\n    n_clusters : int, instance of sklearn.cluster model, default 3\n        Number of clusters after the final clustering step, which treats the\n        subclusters from the leaves as new samples.\n\n        - `None` : the final clustering step is not performed and the\n          subclusters are returned as they are.\n\n        - `sklearn.cluster` Estimator : If a model is provided, the model is\n          fit treating the subclusters as new samples and the initial data is\n          mapped to the label of the closest subcluster.\n\n        - `int` : the model fit is :class:`AgglomerativeClustering` with\n          `n_clusters` set to be equal to the int.\n\n    compute_labels : bool, default True\n        Whether or not to compute labels for each fit.\n\n    copy : bool, default True\n        Whether or not to make a copy of the given data. If set to False,\n        the initial data will be overwritten.\n\n    Attributes\n    ----------\n    root_ : _CFNode\n        Root of the CFTree.\n\n    dummy_leaf_ : _CFNode\n        Start pointer to all the leaves.\n\n    subcluster_centers_ : ndarray,\n        Centroids of all subclusters read directly from the leaves.\n\n    subcluster_labels_ : ndarray,\n        Labels assigned to the centroids of the subclusters after\n        they are clustered globally.\n\n    labels_ : ndarray, shape (n_samples,)\n        Array of labels assigned to the input data.\n        if partial_fit is used instead of fit, they are assigned to the\n        last batch of data.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import Birch\n    >>> X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n    >>> brc = Birch(branching_factor=50, n_clusters=None, threshold=0.5,\n    ... compute_labels=True)\n    >>> brc.fit(X) # doctest: +NORMALIZE_WHITESPACE\n    Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,\n       threshold=0.5)\n    >>> brc.predict(X)\n    array([0, 0, 0, 1, 1, 1])\n\n    References\n    ----------\n    * Tian Zhang, Raghu Ramakrishnan, Maron Livny\n      BIRCH: An efficient data clustering method for large databases.\n      https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n    * Roberto Perdisci\n      JBirch - Java implementation of BIRCH clustering algorithm\n      https://code.google.com/archive/p/jbirch\n\n    Notes\n    -----\n    The tree data structure consists of nodes with each node consisting of\n    a number of subclusters. The maximum number of subclusters in a node\n    is determined by the branching factor. Each subcluster maintains a\n    linear sum, squared sum and the number of samples in that subcluster.\n    In addition, each subcluster can also have a node as its child, if the\n    subcluster is not a member of a leaf node.\n\n    For a new point entering the root, it is merged with the subcluster closest\n    to it and the linear sum, squared sum and the number of samples of that\n    subcluster are updated. This is done recursively till the properties of\n    the leaf node are updated.\n    ",
        "klass": "sklearn.cluster.birch.Birch",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.cross_decomposition.pls_._PLS"
        ],
        "class_docstring": " PLSCanonical implements the 2 blocks canonical PLS of the original Wold\n    algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\n\n    This class inherits from PLS with mode=\"A\" and deflation_mode=\"canonical\",\n    norm_y_weights=True and algorithm=\"nipals\", but svd should provide similar\n    results up to numerical errors.\n\n    Read more in the :ref:`User Guide <cross_decomposition>`.\n\n    Parameters\n    ----------\n    n_components : int, (default 2).\n        Number of components to keep\n\n    scale : boolean, (default True)\n        Option to scale data\n\n    algorithm : string, \"nipals\" or \"svd\"\n        The algorithm used to estimate the weights. It will be called\n        n_components times, i.e. once for each iteration of the outer loop.\n\n    max_iter : an integer, (default 500)\n        the maximum number of iterations of the NIPALS inner loop (used\n        only if algorithm=\"nipals\")\n\n    tol : non-negative real, default 1e-06\n        the tolerance used in the iterative algorithm\n\n    copy : boolean, default True\n        Whether the deflation should be done on a copy. Let the default\n        value to True unless you don't care about side effect\n\n    Attributes\n    ----------\n    x_weights_ : array, shape = [p, n_components]\n        X block weights vectors.\n\n    y_weights_ : array, shape = [q, n_components]\n        Y block weights vectors.\n\n    x_loadings_ : array, shape = [p, n_components]\n        X block loadings vectors.\n\n    y_loadings_ : array, shape = [q, n_components]\n        Y block loadings vectors.\n\n    x_scores_ : array, shape = [n_samples, n_components]\n        X scores.\n\n    y_scores_ : array, shape = [n_samples, n_components]\n        Y scores.\n\n    x_rotations_ : array, shape = [p, n_components]\n        X block to latents rotations.\n\n    y_rotations_ : array, shape = [q, n_components]\n        Y block to latents rotations.\n\n    n_iter_ : array-like\n        Number of iterations of the NIPALS inner loop for each\n        component. Not useful if the algorithm provided is \"svd\".\n\n    Notes\n    -----\n    Matrices::\n\n        T: x_scores_\n        U: y_scores_\n        W: x_weights_\n        C: y_weights_\n        P: x_loadings_\n        Q: y_loadings__\n\n    Are computed such that::\n\n        X = T P.T + Err and Y = U Q.T + Err\n        T[:, k] = Xk W[:, k] for k in range(n_components)\n        U[:, k] = Yk C[:, k] for k in range(n_components)\n        x_rotations_ = W (P.T W)^(-1)\n        y_rotations_ = C (Q.T C)^(-1)\n\n    where Xk and Yk are residual matrices at iteration k.\n\n    `Slides explaining PLS\n    <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`_\n\n    For each component k, find weights u, v that optimize::\n\n        max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``\n\n    Note that it maximizes both the correlations between the scores and the\n    intra-block variances.\n\n    The residual matrix of X (Xk+1) block is obtained by the deflation on the\n    current X score: x_score.\n\n    The residual matrix of Y (Yk+1) block is obtained by deflation on the\n    current Y score. This performs a canonical symmetric version of the PLS\n    regression. But slightly different than the CCA. This is mostly used\n    for modeling.\n\n    This implementation provides the same results that the \"plspm\" package\n    provided in the R language (R-project), using the function plsca(X, Y).\n    Results are equal or collinear with the function\n    ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The difference\n    relies in the fact that mixOmics implementation does not exactly implement\n    the Wold algorithm since it does not normalize y_weights to one.\n\n    Examples\n    --------\n    >>> from sklearn.cross_decomposition import PLSCanonical\n    >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    >>> plsca = PLSCanonical(n_components=2)\n    >>> plsca.fit(X, Y)\n    ... # doctest: +NORMALIZE_WHITESPACE\n    PLSCanonical(algorithm='nipals', copy=True, max_iter=500, n_components=2,\n                 scale=True, tol=1e-06)\n    >>> X_c, Y_c = plsca.transform(X, Y)\n\n    References\n    ----------\n\n    Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with\n    emphasis on the two-block case. Technical Report 371, Department of\n    Statistics, University of Washington, Seattle, 2000.\n\n    Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n    Editions Technic.\n\n    See also\n    --------\n    CCA\n    PLSSVD\n    ",
        "klass": "sklearn.cross_decomposition.pls_.PLSCanonical",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble.gradient_boosting.BaseGradientBoosting",
            "sklearn.base.ClassifierMixin"
        ],
        "class_docstring": "Gradient Boosting for classification.\n\n    GB builds an additive model in a\n    forward stage-wise fashion; it allows for the optimization of\n    arbitrary differentiable loss functions. In each stage ``n_classes_``\n    regression trees are fit on the negative gradient of the\n    binomial or multinomial deviance loss function. Binary classification\n    is a special case where only a single regression tree is induced.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'deviance', 'exponential'}, optional (default='deviance')\n        loss function to be optimized. 'deviance' refers to\n        deviance (= logistic regression) for classification\n        with probabilistic outputs. For loss 'exponential' gradient\n        boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, optional (default=0.1)\n        learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int (default=100)\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, optional (default=1.0)\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : string, optional (default=\"friedman_mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"friedman_mse\" for the mean squared error with improvement\n        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n        the mean absolute error. The default value of \"friedman_mse\" is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : integer, optional (default=3)\n        maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', optional (default=None)\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the\n        initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default: 0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default: False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    presort : bool or 'auto', optional (default='auto')\n        Whether to presort the data to speed up the finding of best splits in\n        fitting. Auto mode by default will use presorting on dense data and\n        default to normal sorting on sparse data. Setting presort to true on\n        sparse data will raise an error.\n\n        .. versionadded:: 0.17\n           *presort* parameter.\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n\n        .. versionadded:: 0.20\n\n    tol : float, optional, default 1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : array, shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    oob_improvement_ : array, shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n\n    train_score_ : array, shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, ``loss_.K``)\n        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n        classification, otherwise n_classes.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    sklearn.ensemble.HistGradientBoostingClassifier,\n    sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n    AdaBoostClassifier\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n    ",
        "klass": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.ensemble.gradient_boosting.BaseGradientBoosting",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Gradient Boosting for regression.\n\n    GB builds an additive model in a forward stage-wise fashion;\n    it allows for the optimization of arbitrary differentiable loss functions.\n    In each stage a regression tree is fit on the negative gradient of the\n    given loss function.\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n        loss function to be optimized. 'ls' refers to least squares\n        regression. 'lad' (least absolute deviation) is a highly robust\n        loss function solely based on order information of the input\n        variables. 'huber' is a combination of the two. 'quantile'\n        allows quantile regression (use `alpha` to specify the quantile).\n\n    learning_rate : float, optional (default=0.1)\n        learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n\n    n_estimators : int (default=100)\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n\n    subsample : float, optional (default=1.0)\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n\n    criterion : string, optional (default=\"friedman_mse\")\n        The function to measure the quality of a split. Supported criteria\n        are \"friedman_mse\" for the mean squared error with improvement\n        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n        the mean absolute error. The default value of \"friedman_mse\" is\n        generally the best as it can provide a better approximation in\n        some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_depth : integer, optional (default=3)\n        maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n\n    init : estimator or 'zero', optional (default=None)\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n        raw predictions are set to zero. By default a ``DummyEstimator`` is\n        used, predicting either the average target value (for loss='ls'), or\n        a quantile for the other losses.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=n_features`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    alpha : float (default=0.9)\n        The alpha-quantile of the huber loss function and the quantile\n        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n\n    verbose : int, default: 0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    warm_start : bool, default: False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    presort : bool or 'auto', optional (default='auto')\n        Whether to presort the data to speed up the finding of best splits in\n        fitting. Auto mode by default will use presorting on dense data and\n        default to normal sorting on sparse data. Setting presort to true on\n        sparse data will raise an error.\n\n        .. versionadded:: 0.17\n           optional parameter *presort*.\n\n    validation_fraction : float, optional, default 0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations.\n\n        .. versionadded:: 0.20\n\n    tol : float, optional, default 1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n\n        .. versionadded:: 0.20\n\n\n    Attributes\n    ----------\n    feature_importances_ : array, shape (n_features,)\n        The feature importances (the higher, the more important the feature).\n\n    oob_improvement_ : array, shape (n_estimators,)\n        The improvement in loss (= deviance) on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n\n    train_score_ : array, shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the deviance on the training data.\n\n    loss_ : LossFunction\n        The concrete ``LossFunction`` object.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n        The collection of fitted sub-estimators.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    See also\n    --------\n    sklearn.ensemble.HistGradientBoostingRegressor,\n    sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n    ",
        "klass": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.ClusterMixin"
        ],
        "class_docstring": "Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : boolean, optional, default: True\n        Make a copy of input data.\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : string, optional, default=``euclidean``\n        Which affinity to use. At the moment ``precomputed`` and\n        ``euclidean`` are supported. ``euclidean`` uses the\n        negative squared euclidean distance between points.\n\n    verbose : boolean, optional, default: False\n        Whether to be verbose.\n\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : array, shape (n_clusters,)\n        Indices of cluster centers\n\n    cluster_centers_ : array, shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    affinity_matrix_ : array, shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation().fit(X)\n    >>> clustering # doctest: +NORMALIZE_WHITESPACE\n    AffinityPropagation(affinity='euclidean', convergence_iter=15, copy=True,\n              damping=0.5, max_iter=200, preference=None, verbose=False)\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    ",
        "klass": "sklearn.cluster.affinity_propagation_.AffinityPropagation",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearModel",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Bayesian ARD regression.\n\n    Fit the weights of a regression model, using an ARD prior. The weights of\n    the regression model are assumed to be in Gaussian distributions.\n    Also estimate the parameters lambda (precisions of the distributions of the\n    weights) and alpha (precision of the distribution of the noise).\n    The estimation is done by an iterative procedures (Evidence Maximization)\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    n_iter : int, optional\n        Maximum number of iterations. Default is 300\n\n    tol : float, optional\n        Stop the algorithm if w has converged. Default is 1.e-3.\n\n    alpha_1 : float, optional\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter. Default is 1.e-6.\n\n    alpha_2 : float, optional\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter. Default is 1.e-6.\n\n    lambda_1 : float, optional\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter. Default is 1.e-6.\n\n    lambda_2 : float, optional\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter. Default is 1.e-6.\n\n    compute_score : boolean, optional\n        If True, compute the objective function at each step of the model.\n        Default is False.\n\n    threshold_lambda : float, optional\n        threshold for removing (pruning) weights with high precision from\n        the computation. Default is 1.e+4.\n\n    fit_intercept : boolean, optional\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n        Default is True.\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True.\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : boolean, optional, default False\n        Verbose mode when fitting the model.\n\n    Attributes\n    ----------\n    coef_ : array, shape = (n_features)\n        Coefficients of the regression model (mean of distribution)\n\n    alpha_ : float\n       estimated precision of the noise.\n\n    lambda_ : array, shape = (n_features)\n       estimated precisions of the weights.\n\n    sigma_ : array, shape = (n_features, n_features)\n        estimated variance-covariance matrix of the weights\n\n    scores_ : float\n        if computed, value of the objective function (to be maximized)\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ... # doctest: +NORMALIZE_WHITESPACE\n    ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n            copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n            n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,\n            verbose=False)\n    >>> clf.predict([[1, 1]])\n    array([1.])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n    ",
        "klass": "sklearn.linear_model.bayes.ARDRegression",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.base.LinearModel",
            "sklearn.base.RegressorMixin"
        ],
        "class_docstring": "Bayesian ridge regression.\n\n    Fit a Bayesian ridge model. See the Notes section for details on this\n    implementation and the optimization of the regularization parameters\n    lambda (precision of the weights) and alpha (precision of the noise).\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    n_iter : int, optional\n        Maximum number of iterations.  Default is 300. Should be greater than\n        or equal to 1.\n\n    tol : float, optional\n        Stop the algorithm if w has converged. Default is 1.e-3.\n\n    alpha_1 : float, optional\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter. Default is 1.e-6\n\n    alpha_2 : float, optional\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n        Default is 1.e-6.\n\n    lambda_1 : float, optional\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter. Default is 1.e-6.\n\n    lambda_2 : float, optional\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n        Default is 1.e-6\n\n    compute_score : boolean, optional\n        If True, compute the log marginal likelihood at each iteration of the\n        optimization. Default is False.\n\n    fit_intercept : boolean, optional, default True\n        Whether to calculate the intercept for this model.\n        The intercept is not treated as a probabilistic parameter\n        and thus has no associated variance. If set\n        to False, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : boolean, optional, default False\n        Verbose mode when fitting the model.\n\n\n    Attributes\n    ----------\n    coef_ : array, shape = (n_features,)\n        Coefficients of the regression model (mean of distribution).\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n       Estimated precision of the noise.\n\n    lambda_ : float\n       Estimated precision of the weights.\n\n    sigma_ : array, shape = (n_features, n_features)\n        Estimated variance-covariance matrix of the weights.\n\n    scores_ : array, shape = (n_iter_ + 1,)\n        If computed_score is True, value of the log marginal likelihood (to be\n        maximized) at each iteration of the optimization. The array starts\n        with the value of the log marginal likelihood obtained for the initial\n        values of alpha and lambda and ends with the value obtained for the\n        estimated alpha and lambda.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ... # doctest: +NORMALIZE_WHITESPACE\n    BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n            copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n            n_iter=300, normalize=False, tol=0.001, verbose=False)\n    >>> clf.predict([[1, 1]])\n    array([1.])\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n    ",
        "klass": "sklearn.linear_model.bayes.BayesianRidge",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier"
        ],
        "class_docstring": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n\n    This estimator implements regularized linear models with stochastic\n    gradient descent (SGD) learning: the gradient of the loss is estimated\n    each sample at a time and the model is updated along the way with a\n    decreasing strength schedule (aka learning rate). SGD allows minibatch\n    (online/out-of-core) learning, see the partial_fit method.\n    For best results using the default learning rate schedule, the data should\n    have zero mean and unit variance.\n\n    This implementation works with data represented as dense or sparse arrays\n    of floating point values for the features. The model it fits can be\n    controlled with the loss parameter; by default, it fits a linear support\n    vector machine (SVM).\n\n    The regularizer is a penalty added to the loss function that shrinks model\n    parameters towards the zero vector using either the squared euclidean norm\n    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n    parameter update crosses the 0.0 value because of the regularizer, the\n    update is truncated to 0.0 to allow for learning sparse models and achieve\n    online feature selection.\n\n    Read more in the :ref:`User Guide <sgd>`.\n\n    Parameters\n    ----------\n    loss : str, default: 'hinge'\n        The loss function to be used. Defaults to 'hinge', which gives a\n        linear SVM.\n\n        The possible options are 'hinge', 'log', 'modified_huber',\n        'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n\n        The 'log' loss gives logistic regression, a probabilistic classifier.\n        'modified_huber' is another smooth loss that brings tolerance to\n        outliers as well as probability estimates.\n        'squared_hinge' is like hinge but is quadratically penalized.\n        'perceptron' is the linear loss used by the perceptron algorithm.\n        The other losses are designed for regression but can be useful in\n        classification as well; see SGDRegressor for a description.\n\n    penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n        The penalty (aka regularization term) to be used. Defaults to 'l2'\n        which is the standard regularizer for linear SVM models. 'l1' and\n        'elasticnet' might bring sparsity to the model (feature selection)\n        not achievable with 'l2'.\n\n    alpha : float\n        Constant that multiplies the regularization term. Defaults to 0.0001\n        Also used to compute learning_rate when set to 'optimal'.\n\n    l1_ratio : float\n        The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n        Defaults to 0.15.\n\n    fit_intercept : bool\n        Whether the intercept should be estimated or not. If False, the\n        data is assumed to be already centered. Defaults to True.\n\n    max_iter : int, optional (default=1000)\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the ``fit`` method, and not the\n        `partial_fit`.\n\n        .. versionadded:: 0.19\n\n    tol : float or None, optional (default=1e-3)\n        The stopping criterion. If it is not None, the iterations will stop\n        when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n        epochs.\n\n        .. versionadded:: 0.19\n\n    shuffle : bool, optional\n        Whether or not the training data should be shuffled after each epoch.\n        Defaults to True.\n\n    verbose : integer, optional\n        The verbosity level\n\n    epsilon : float\n        Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n        'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n        For 'huber', determines the threshold at which it becomes less\n        important to get the prediction exactly right.\n        For epsilon-insensitive, any differences between the current prediction\n        and the correct label are ignored if they are less than this threshold.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the OVA (One Versus All, for\n        multi-class problems) computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    learning_rate : string, optional\n        The learning rate schedule:\n\n        'constant':\n            eta = eta0\n        'optimal': [default]\n            eta = 1.0 / (alpha * (t + t0))\n            where t0 is chosen by a heuristic proposed by Leon Bottou.\n        'invscaling':\n            eta = eta0 / pow(t, power_t)\n        'adaptive':\n            eta = eta0, as long as the training keeps decreasing.\n            Each time n_iter_no_change consecutive epochs fail to decrease the\n            training loss by tol or fail to increase validation score by tol if\n            early_stopping is True, the current learning rate is divided by 5.\n\n    eta0 : double\n        The initial learning rate for the 'constant', 'invscaling' or\n        'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n        the default schedule 'optimal'.\n\n    power_t : double\n        The exponent for inverse scaling learning rate [default 0.5].\n\n    early_stopping : bool, default=False\n        Whether to use early stopping to terminate training when validation\n        score is not improving. If set to True, it will automatically set aside\n        a stratified fraction of training data as validation and terminate\n        training when validation score is not improving by at least tol for\n        n_iter_no_change consecutive epochs.\n\n        .. versionadded:: 0.20\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Must be between 0 and 1.\n        Only used if early_stopping is True.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=5\n        Number of iterations with no improvement to wait before early stopping.\n\n        .. versionadded:: 0.20\n\n    class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n        Preset for the class_weight fit parameter.\n\n        Weights associated with classes. If not given, all classes\n        are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    warm_start : bool, optional\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        See :term:`the Glossary <warm_start>`.\n\n        Repeatedly calling fit or partial_fit when warm_start is True can\n        result in a different solution than when calling fit a single time\n        because of the way the data is shuffled.\n        If a dynamic learning rate is used, the learning rate is adapted\n        depending on the number of samples already seen. Calling ``fit`` resets\n        this counter, while ``partial_fit`` will result in increasing the\n        existing counter.\n\n    average : bool or int, optional\n        When set to True, computes the averaged SGD weights and stores the\n        result in the ``coef_`` attribute. If set to an int greater than 1,\n        averaging will begin once the total number of samples seen reaches\n        average. So ``average=10`` will begin averaging after seeing 10\n        samples.\n\n    Attributes\n    ----------\n    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)\n        Weights assigned to the features.\n\n    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)\n        Constants in decision function.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n        For multiclass fits, it is the maximum over every binary fit.\n\n    loss_function_ : concrete ``LossFunction``\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> Y = np.array([1, 1, 2, 2])\n    >>> clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)\n    >>> clf.fit(X, Y)\n    ... #doctest: +NORMALIZE_WHITESPACE\n    SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n           random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,\n           verbose=0, warm_start=False)\n\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    sklearn.svm.LinearSVC, LogisticRegression, Perceptron\n\n    ",
        "klass": "sklearn.linear_model.stochastic_gradient.SGDClassifier",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.base.BaseEstimator",
            "sklearn.base.MetaEstimatorMixin",
            "sklearn.feature_selection.base.SelectorMixin"
        ],
        "class_docstring": "Feature ranking with recursive feature elimination.\n\n    Given an external estimator that assigns weights to features (e.g., the\n    coefficients of a linear model), the goal of recursive feature elimination\n    (RFE) is to select features by recursively considering smaller and smaller\n    sets of features. First, the estimator is trained on the initial set of\n    features and the importance of each feature is obtained either through a\n    ``coef_`` attribute or through a ``feature_importances_`` attribute.\n    Then, the least important features are pruned from current set of features.\n    That procedure is recursively repeated on the pruned set until the desired\n    number of features to select is eventually reached.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : object\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance either through a ``coef_``\n        attribute or through a ``feature_importances_`` attribute.\n\n    n_features_to_select : int or None (default=None)\n        The number of features to select. If `None`, half of the features\n        are selected.\n\n    step : int or float, optional (default=1)\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n\n    verbose : int, (default=0)\n        Controls verbosity of output.\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of selected features.\n\n    support_ : array of shape [n_features]\n        The mask of selected features.\n\n    ranking_ : array of shape [n_features]\n        The feature ranking, such that ``ranking_[i]`` corresponds to the\n        ranking position of the i-th feature. Selected (i.e., estimated\n        best) features are assigned rank 1.\n\n    estimator_ : object\n        The external estimator fit on the reduced dataset.\n\n    Examples\n    --------\n    The following example shows how to retrieve the 5 right informative\n    features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFE\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFE(estimator, 5, step=1)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_ # doctest: +NORMALIZE_WHITESPACE\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    See also\n    --------\n    RFECV : Recursive feature elimination with built-in cross-validated\n        selection of the best number of features\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    ",
        "klass": "sklearn.feature_selection.rfe.RFE",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.feature_selection.rfe.RFE",
            "sklearn.base.MetaEstimatorMixin"
        ],
        "class_docstring": "Feature ranking with recursive feature elimination and cross-validated\n    selection of the best number of features.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <rfe>`.\n\n    Parameters\n    ----------\n    estimator : object\n        A supervised learning estimator with a ``fit`` method that provides\n        information about feature importance either through a ``coef_``\n        attribute or through a ``feature_importances_`` attribute.\n\n    step : int or float, optional (default=1)\n        If greater than or equal to 1, then ``step`` corresponds to the\n        (integer) number of features to remove at each iteration.\n        If within (0.0, 1.0), then ``step`` corresponds to the percentage\n        (rounded down) of features to remove at each iteration.\n        Note that the last iteration may remove fewer than ``step`` features in\n        order to reach ``min_features_to_select``.\n\n    min_features_to_select : int, (default=1)\n        The minimum number of features to be selected. This number of features\n        will always be scored, even if the difference between the original\n        feature count and ``min_features_to_select`` isn't divisible by\n        ``step``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used. If the\n        estimator is a classifier or if ``y`` is neither binary nor multiclass,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value of None will change from 3-fold to 5-fold\n            in v0.22.\n\n    scoring : string, callable or None, optional, (default=None)\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    verbose : int, (default=0)\n        Controls verbosity of output.\n\n    n_jobs : int or None, optional (default=None)\n        Number of cores to run in parallel while fitting across folds.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    n_features_ : int\n        The number of selected features with cross-validation.\n\n    support_ : array of shape [n_features]\n        The mask of selected features.\n\n    ranking_ : array of shape [n_features]\n        The feature ranking, such that `ranking_[i]`\n        corresponds to the ranking\n        position of the i-th feature.\n        Selected (i.e., estimated best)\n        features are assigned rank 1.\n\n    grid_scores_ : array of shape [n_subsets_of_features]\n        The cross-validation scores such that\n        ``grid_scores_[i]`` corresponds to\n        the CV score of the i-th subset of features.\n\n    estimator_ : object\n        The external estimator fit on the reduced dataset.\n\n    Notes\n    -----\n    The size of ``grid_scores_`` is equal to\n    ``ceil((n_features - min_features_to_select) / step) + 1``,\n    where step is the number of features removed at each iteration.\n\n    Examples\n    --------\n    The following example shows how to retrieve the a-priori not known 5\n    informative features in the Friedman #1 dataset.\n\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.feature_selection import RFECV\n    >>> from sklearn.svm import SVR\n    >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n    >>> estimator = SVR(kernel=\"linear\")\n    >>> selector = RFECV(estimator, step=1, cv=5)\n    >>> selector = selector.fit(X, y)\n    >>> selector.support_ # doctest: +NORMALIZE_WHITESPACE\n    array([ True,  True,  True,  True,  True, False, False, False, False,\n           False])\n    >>> selector.ranking_\n    array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n\n    See also\n    --------\n    RFE : Recursive feature elimination\n\n    References\n    ----------\n\n    .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n           for cancer classification using support vector machines\",\n           Mach. Learn., 46(1-3), 389--422, 2002.\n    ",
        "klass": "sklearn.feature_selection.rfe.RFECV",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "sklearn.semi_supervised.label_propagation.BaseLabelPropagation"
        ],
        "class_docstring": "LabelSpreading model for semi-supervised learning\n\n    This model is similar to the basic Label Propagation algorithm,\n    but uses affinity matrix based on the normalized graph Laplacian\n    and soft clamping across the labels.\n\n    Read more in the :ref:`User Guide <label_propagation>`.\n\n    Parameters\n    ----------\n    kernel : {'knn', 'rbf', callable}\n        String identifier for kernel function to use or the kernel function\n        itself. Only 'rbf' and 'knn' strings are valid inputs. The function\n        passed should take two inputs, each of shape [n_samples, n_features],\n        and return a [n_samples, n_samples] shaped weight matrix\n\n    gamma : float\n      parameter for rbf kernel\n\n    n_neighbors : integer > 0\n      parameter for knn kernel\n\n    alpha : float\n      Clamping factor. A value in (0, 1) that specifies the relative amount\n      that an instance should adopt the information from its neighbors as\n      opposed to its initial label.\n      alpha=0 means keeping the initial label information; alpha=1 means\n      replacing all initial information.\n\n    max_iter : integer\n      maximum number of iterations allowed\n\n    tol : float\n      Convergence tolerance: threshold to consider the system at steady\n      state\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    X_ : array, shape = [n_samples, n_features]\n        Input array.\n\n    classes_ : array, shape = [n_classes]\n        The distinct labels used in classifying instances.\n\n    label_distributions_ : array, shape = [n_samples, n_classes]\n        Categorical distribution for each item.\n\n    transduction_ : array, shape = [n_samples]\n        Label assigned to each item via the transduction.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets\n    >>> from sklearn.semi_supervised import LabelSpreading\n    >>> label_prop_model = LabelSpreading()\n    >>> iris = datasets.load_iris()\n    >>> rng = np.random.RandomState(42)\n    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3\n    >>> labels = np.copy(iris.target)\n    >>> labels[random_unlabeled_points] = -1\n    >>> label_prop_model.fit(iris.data, labels)\n    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    LabelSpreading(...)\n\n    References\n    ----------\n    Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,\n    Bernhard Schoelkopf. Learning with local and global consistency (2004)\n    http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\n\n    See Also\n    --------\n    LabelPropagation : Unregularized graph based semi-supervised learning\n    ",
        "klass": "sklearn.semi_supervised.label_propagation.LabelSpreading",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "An estimator predicting the log odds ratio.",
        "klass": "sklearn.ensemble.gradient_boosting.LogOddsEstimator",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Parameters\n    ----------\n    data\n    mmap_mode\n    ",
        "klass": "sklearn.utils.testing.TempMemmap",
        "module": "sklearn"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Kqueue syscall wrapper.\n\nFor example, to start watching a socket for input:\n>>> kq = kqueue()\n>>> sock = socket()\n>>> sock.connect((host, port))\n>>> kq.control([kevent(sock, KQ_FILTER_WRITE, KQ_EV_ADD)], 0)\n\nTo wait one second for it to become writeable:\n>>> kq.control(None, 1, 1000)\n\nTo stop listening:\n>>> kq.control([kevent(sock, KQ_FILTER_WRITE, KQ_EV_DELETE)], 0)",
        "klass": "select.kqueue",
        "module": "select"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Pool objects",
        "klass": "essentia.Pool",
        "module": "essentia"
    },
    {
        "base_classes": [
            "django.db.models.fields.related.ForeignObject"
        ],
        "class_docstring": "\n    Provide a many-to-one relation by adding a column to the local model\n    to hold the remote value.\n\n    By default ForeignKey will target the pk of the remote model but this\n    behavior can be changed by using the ``to_field`` argument.\n    ",
        "klass": "django.db.models.fields.related.ForeignKey",
        "module": "django"
    },
    {
        "base_classes": [
            "fixtures.fixture.Fixture"
        ],
        "class_docstring": "A fixture for overriding the time returned by timeutils.utcnow().\n\n    :param override_time: datetime instance or list thereof. If not given,\n                          defaults to the current UTC time.\n\n    ",
        "klass": "oslo_utils.fixture.TimeFixture",
        "module": "oslo_utils"
    },
    {
        "base_classes": [
            "networkx.es.graph.Graph"
        ],
        "class_docstring": "\n    The Region Adjacency Graph (RAG) of an image, subclasses\n    `networx.Graph <http://networkx.github.io/documentation/latest/reference/classes/graph.html>`_\n\n    Parameters\n    ----------\n    label_image : array of int\n        An initial segmentation, with each region labeled as a different\n        integer. Every unique value in ``label_image`` will correspond to\n        a node in the graph.\n    connectivity : int in {1, ..., ``label_image.ndim``}, optional\n        The connectivity between pixels in ``label_image``. For a 2D image,\n        a connectivity of 1 corresponds to immediate neighbors up, down,\n        left, and right, while a connectivity of 2 also includes diagonal\n        neighbors. See `scipy.ndimage.generate_binary_structure`.\n    data : networkx Graph specification, optional\n        Initial or additional edges to pass to the NetworkX Graph\n        constructor. See `networkx.Graph`. Valid edge specifications\n        include edge list (list of tuples), NumPy arrays, and SciPy\n        sparse matrices.\n    **attr : keyword arguments, optional\n        Additional attributes to add to the graph.\n    ",
        "klass": "skimage.future.graph.rag.RAG",
        "module": "skimage"
    },
    {
        "base_classes": [
            "skimage.transform._geometric.EuclideanTransform"
        ],
        "class_docstring": "2D similarity transformation.\n\n    Has the following form::\n\n        X = a0 * x - b0 * y + a1 =\n          = s * x * cos(rotation) - s * y * sin(rotation) + a1\n\n        Y = b0 * x + a0 * y + b1 =\n          = s * x * sin(rotation) + s * y * cos(rotation) + b1\n\n    where ``s`` is a scale factor and the homogeneous transformation matrix is::\n\n        [[a0  b0  a1]\n         [b0  a0  b1]\n         [0   0    1]]\n\n    The similarity transformation extends the Euclidean transformation with a\n    single scaling factor in addition to the rotation and translation\n    parameters.\n\n    Parameters\n    ----------\n    matrix : (3, 3) array, optional\n        Homogeneous transformation matrix.\n    scale : float, optional\n        Scale factor.\n    rotation : float, optional\n        Rotation angle in counter-clockwise direction as radians.\n    translation : (tx, ty) as array, list or tuple, optional\n        x, y translation parameters.\n\n    Attributes\n    ----------\n    params : (3, 3) array\n        Homogeneous transformation matrix.\n\n    ",
        "klass": "skimage.transform._geometric.SimilarityTransform",
        "module": "skimage"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "BinaryHeap(initial_capacity=128)\n\n    A binary heap class that can store values and an integer reference.\n\n    A binary heap is an object to store values in, optimized in such a way\n    that the minimum (or maximum, but a minimum in this implementation)\n    value can be found in O(log2(N)) time. In this implementation, a reference\n    value (a single integer) can also be stored with each value.\n\n    Use the methods push() and pop() to put in or extract values.\n    In C, use the corresponding push_fast() and pop_fast().\n\n    Parameters\n    ----------\n    initial_capacity : int\n        Estimate of the size of the heap, if known in advance. (In any case,\n        the heap will dynamically grow and shrink as required, though never\n        below the `initial_capacity`.)\n\n    Attributes\n    ----------\n    count : int\n        The number of values in the heap\n    levels : int\n        The number of levels in the binary heap (see Notes below). The values\n        are stored in the last level, so 2**levels is the capacity of the\n        heap before another resize is required.\n    min_levels : int\n        The minimum number of levels in the heap (relates to the\n        `initial_capacity` parameter.)\n\n    Notes\n    -----\n    This implementation stores the binary heap in an array twice as long as\n    the number of elements in the heap. The array is structured in levels,\n    starting at level 0 with a single value, doubling the amount of values in\n    each level. The final level contains the actual values, the level before\n    it contains the pairwise minimum values. The level before that contains\n    the pairwise minimum values of that level, etc. Take a look at this\n    illustration:\n\n    level: 0 11 2222 33333333 4444444444444444\n    index: 0 12 3456 78901234 5678901234567890\n                        1          2         3\n\n     The actual values are stored in level 4. The minimum value of position 15\n    and 16 is stored in position 7. min(17,18)->8, min(7,8)->3, min(3,4)->1.\n    When adding a value, only the path to the top has to be updated, which\n    takesO(log2(N)) time.\n\n     The advantage of this implementation relative to more common\n    implementations that swap values when pushing to the array is that data\n    only needs to be swapped once when an element is removed. This means that\n    keeping an array of references along with the values is very inexpensive.\n    Th disadvantage is that if you pop the minimum value, the tree has to be\n    traced from top to bottom and back. So if you only want values and no\n    references, this implementation will probably be slower. If you need\n    references (and maybe cross references to be kept up to date) this\n    implementation will be faster.\n\n    ",
        "klass": "skimage.graph.heap.BinaryHeap",
        "module": "skimage"
    },
    {
        "base_classes": [
            "skimage.graph.heap.BinaryHeap"
        ],
        "class_docstring": "FastUpdateBinaryHeap(initial_capacity=128, max_reference=None)\n\n    Binary heap that allows the value of a reference to be updated quickly.\n\n    This heap class keeps cross-references so that the value associated with a\n    given reference can be quickly queried (O(1) time) or updated (O(log2(N))\n    time). This is ideal for pathfinding algorithms that implement some\n    variant of Dijkstra's algorithm.\n\n    Parameters\n    ----------\n    initial_capacity : int\n        Estimate of the size of the heap, if known in advance. (In any case,\n        the heap will dynamically grow and shrink as required, though never\n        below the `initial_capacity`.)\n\n    max_reference : int, optional\n        Largest reference value that might be pushed to the heap. (Pushing a\n        larger value will result in an error.) If no value is provided,\n        `1-initial_capacity` will be used. For the cross-reference index to\n        work, all references must be in the range [0, max_reference];\n        references pushed outside of that range will not be added to the heap.\n        The cross-references are kept as a 1-d array of length\n        `max_reference+1', so memory use of this heap is effectively\n        O(max_reference)\n\n    Attributes\n    ----------\n    count : int\n        The number of values in the heap\n    levels : int\n        The number of levels in the binary heap (see Notes below). The values\n        are stored in the last level, so 2**levels is the capacity of the\n        heap before another resize is required.\n    min_levels : int\n        The minimum number of levels in the heap (relates to the\n        `initial_capacity` parameter.)\n    max_reference : int\n        The provided or calculated maximum allowed reference value.\n\n    Notes\n    -----\n    The cross-references map data[reference]->internalindex, such that the\n    value corresponding to a given reference can be found efficiently. This\n    can be queried with the value_of() method.\n\n    A special method, push_if_lower() is provided that will update the heap if\n    the given reference is not in the heap, or if it is and the provided value\n    is lower than the current value in the heap. This is again useful for\n    pathfinding algorithms.\n\n    ",
        "klass": "skimage.graph.heap.FastUpdateBinaryHeap",
        "module": "skimage"
    },
    {
        "base_classes": [
            "argparse.ArgumentParser"
        ],
        "class_docstring": "Custom ArgumentParser class to support special absl flags.",
        "klass": "absl.flags.argparse_flags.ArgumentParser",
        "module": "absl"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Context manager for dynamic management of a stack of exit callbacks\n\n    For example:\n\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception\n\n    ",
        "klass": "contextlib2.ExitStack",
        "module": "contextlib2"
    },
    {
        "base_classes": [
            "nnabla.utils.data_source.DataSource"
        ],
        "class_docstring": "\n    ",
        "klass": "nnabla.utils.data_source_implements.CsvDataSource",
        "module": "nnabla"
    },
    {
        "base_classes": [
            "nnabla.utils.data_source.DataSource"
        ],
        "class_docstring": "\n    This class contains properties and methods for data source that can be read from cache files, which are utilized by data iterator.\n\n    Args:\n        data_source (:py:class:`DataSource <nnabla.utils.data_source.DataSource>`):\n             Instance of DataSource class which provides data.\n        cache_dir (str):\n            Location of file_cache.\n            If this value is None, :py:class:`.data_source.DataSourceWithFileCache`\n            creates file caches implicitly on temporary directory and erases them all\n            when data_iterator is finished.\n            Otherwise, :py:class:`.data_source.DataSourceWithFileCache` keeps created cache.\n            Default is None.\n        cache_file_name_prefix (str):\n            Beginning of the filenames of cache files.\n            Default is 'cache'. \n        shuffle (bool):\n             Indicates whether the dataset is shuffled or not.\n        rng (None or :obj:`numpy.random.RandomState`): Numpy random number\n            generator.\n    ",
        "klass": "nnabla.utils.data_source.DataSourceWithFileCache",
        "module": "nnabla"
    },
    {
        "base_classes": [
            "nnabla.utils.data_source.DataSource"
        ],
        "class_docstring": "\n    Get data from file cache directly.\n    ",
        "klass": "nnabla.utils.data_source_implements.CacheDataSource",
        "module": "nnabla"
    },
    {
        "base_classes": [
            "tensorflow_federated.python.core.impl.executor_base.Executor"
        ],
        "class_docstring": "The caching executor only performs caching.",
        "klass": "tensorflow_federated.python.core.impl.caching_executor.CachingExecutor",
        "module": "tensorflow_federated"
    },
    {
        "base_classes": [
            "tensorflow_federated.python.core.impl.executor_base.Executor"
        ],
        "class_docstring": "The lambda executor handles lambda expressions and related abstractions.\n\n  NOTE: This component is only available in Python 3.\n\n  This executor understands TFF computation compositional constructs, including\n  lambdas, blocks, references, calls, tuples, and selections, and orchestrates\n  the execution of these constructs, while delegating all the non-compositional\n  constructs (tensorflow, intrinsics, data, or placement) to a target executor.\n\n  NOTE: Not all lambda expressions are executed by this lambda executor. If the\n  computation contains a call to an instrinsic that takes a functional argument,\n  that functional argument is fed in its entirety to the target executor rather\n  than being parsed by the lambda executor (since its execution needs to happen\n  elsewhere).\n\n  The arguments to be ingested can be either federated computations (those are\n  natively interpreted), or whatever other form of arguments are understood by\n  the target executor.\n  ",
        "klass": "tensorflow_federated.python.core.impl.lambda_executor.LambdaExecutor",
        "module": "tensorflow_federated"
    },
    {
        "base_classes": [
            "PySide2.QtCore.QIODevice"
        ],
        "class_docstring": "QBuffer(self, buf: PySide2.QtCore.QByteArray, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQBuffer(self, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)",
        "klass": "PySide2.QtCore.QBuffer",
        "module": "PySide2"
    },
    {
        "base_classes": [
            "PySide2.QtCore.QObject"
        ],
        "class_docstring": "QCoreApplication(self)\nQCoreApplication(self, arg__1: typing.List[str])",
        "klass": "PySide2.QtCore.QCoreApplication",
        "module": "PySide2"
    },
    {
        "base_classes": [
            "Shiboken.Object"
        ],
        "class_docstring": "QRegExp(self)\nQRegExp(self, pattern: str, cs: PySide2.QtCore.Qt.CaseSensitivity = PySide2.QtCore.Qt.CaseSensitivity.CaseSensitive, syntax: PySide2.QtCore.QRegExp.PatternSyntax = PySide2.QtCore.QRegExp.PatternSyntax.RegExp)\nQRegExp(self, rx: PySide2.QtCore.QRegExp)",
        "klass": "PySide2.QtCore.QRegExp",
        "module": "PySide2"
    },
    {
        "base_classes": [
            "PySide2.QtCore.QObject"
        ],
        "class_docstring": "QSettings(self, fileName: str, format: PySide2.QtCore.QSettings.Format, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQSettings(self, format: PySide2.QtCore.QSettings.Format, scope: PySide2.QtCore.QSettings.Scope, organization: str, application: str = '', parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQSettings(self, organization: str, application: str = '', parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQSettings(self, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQSettings(self, scope: PySide2.QtCore.QSettings.Scope, organization: str, application: str = '', parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)\nQSettings(self, scope: PySide2.QtCore.QSettings.Scope, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)",
        "klass": "PySide2.QtCore.QSettings",
        "module": "PySide2"
    },
    {
        "base_classes": [
            "PySide2.QtCore.QObject"
        ],
        "class_docstring": "QTimer(self, parent: typing.Union[PySide2.QtCore.QObject, NoneType] = None)",
        "klass": "PySide2.QtCore.QTimer",
        "module": "PySide2"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " Each Bottle object represents a single, distinct web application and\n        consists of routes, callbacks, plugins, resources and configuration.\n        Instances are callable WSGI applications.\n\n        :param catchall: If true (default), handle all exceptions. Turn off to\n                         let debugging middleware handle exceptions.\n    ",
        "klass": "bottle.Bottle",
        "module": "bottle"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A CloudSearch document service.\n\n    The DocumentServiceConection is used to add, remove and update documents in\n    CloudSearch. Commands are uploaded to CloudSearch in SDF (Search Document Format).\n\n    To generate an appropriate SDF, use :func:`add` to add or update documents,\n    as well as :func:`delete` to remove documents.\n\n    Once the set of documents is ready to be index, use :func:`commit` to send the\n    commands to CloudSearch.\n\n    If there are a lot of documents to index, it may be preferable to split the\n    generation of SDF data and the actual uploading into CloudSearch. Retrieve\n    the current SDF with :func:`get_sdf`. If this file is the uploaded into S3,\n    it can be retrieved back afterwards for upload into CloudSearch using\n    :func:`add_sdf_from_s3`.\n\n    The SDF is not cleared after a :func:`commit`. If you wish to continue\n    using the DocumentServiceConnection for another batch upload of commands,\n    you will need to :func:`clear_sdf` first to stop the previous batch of\n    commands from being uploaded again.\n\n    ",
        "klass": "boto.cloudsearch.document.DocumentServiceConnection",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    A CloudSearch document service.\n\n    The DocumentServiceConection is used to add, remove and update documents in\n    CloudSearch. Commands are uploaded to CloudSearch in SDF (Search Document\n    Format).\n\n    To generate an appropriate SDF, use :func:`add` to add or update documents,\n    as well as :func:`delete` to remove documents.\n\n    Once the set of documents is ready to be index, use :func:`commit` to send\n    the commands to CloudSearch.\n\n    If there are a lot of documents to index, it may be preferable to split the\n    generation of SDF data and the actual uploading into CloudSearch. Retrieve\n    the current SDF with :func:`get_sdf`. If this file is the uploaded into S3,\n    it can be retrieved back afterwards for upload into CloudSearch using\n    :func:`add_sdf_from_s3`.\n\n    The SDF is not cleared after a :func:`commit`. If you wish to continue\n    using the DocumentServiceConnection for another batch upload of commands,\n    you will need to :func:`clear_sdf` first to stop the previous batch of\n    commands from being uploaded again.\n\n    ",
        "klass": "boto.cloudsearch2.document.DocumentServiceConnection",
        "module": "boto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Trainer will load the data and train all components.\n\n    Requires a pipeline specification and configuration to use for\n    the training.",
        "klass": "rasa.nlu.model.Trainer",
        "module": "rasa"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "The Agent class provides a convenient interface for the most important\n     Rasa functionality.\n\n     This includes training, handling messages, loading a dialogue model,\n     getting the next action, and handling a channel.",
        "klass": "rasa.core.agent.Agent",
        "module": "rasa"
    },
    {
        "base_classes": [
            "apscheduler.schedulers.base.BaseScheduler"
        ],
        "class_docstring": "\n    A scheduler that runs on an asyncio (:pep:`3156`) event loop.\n\n    The default executor can run jobs based on native coroutines (``async def``).\n\n    Extra options:\n\n    ============== =============================================================\n    ``event_loop`` AsyncIO event loop to use (defaults to the global event loop)\n    ============== =============================================================\n    ",
        "klass": "apscheduler.schedulers.asyncio.AsyncIOScheduler",
        "module": "apscheduler"
    },
    {
        "base_classes": [
            "itsdangerous.URLSafeSerializerMixin",
            "itsdangerous.TimedSerializer"
        ],
        "class_docstring": "Works like :class:`TimedSerializer` but dumps and loads into a URL\n    safe string consisting of the upper and lowercase character of the\n    alphabet as well as ``'_'``, ``'-'`` and ``'.'``.\n    ",
        "klass": "itsdangerous.URLSafeTimedSerializer",
        "module": "itsdangerous"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": " This class manages a list of search paths and helps to find and open\n        application-bound resources (files).\n\n        :param base: default value for :meth:`add_path` calls.\n        :param opener: callable used to open resources.\n        :param cachemode: controls which lookups are cached. One of 'all',\n                         'found' or 'none'.\n    ",
        "klass": "bottle.ResourceManager",
        "module": "bottle"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Rect(left, top, width, height) -> Rect\nRect((left, top), (width, height)) -> Rect\nRect(object) -> Rect\npygame object for storing rectangular coordinates",
        "klass": "pygame.Rect",
        "module": "pygame"
    },
    {
        "base_classes": [
            "pygame.sprite.AbstractGroup"
        ],
        "class_docstring": "container class for many Sprites\n\n    pygame.sprite.Group(*sprites): return Group\n\n    A simple container for Sprite objects. This class can be subclassed to\n    create containers with more specific behaviors. The constructor takes any\n    number of Sprite arguments to add to the Group. The group supports the\n    following standard Python operations:\n\n        in      test if a Sprite is contained\n        len     the number of Sprites contained\n        bool    test if any Sprites are contained\n        iter    iterate through all the Sprites\n\n    The Sprites in the Group are not ordered, so the Sprites are drawn and\n    iterated over in no particular order.\n\n    ",
        "klass": "pygame.sprite.Group",
        "module": "pygame"
    },
    {
        "base_classes": [
            "pygame.sprite.LayeredUpdates"
        ],
        "class_docstring": "LayeredDirty Group is for DirtySprites; subclasses LayeredUpdates\n\n    pygame.sprite.LayeredDirty(*spites, **kwargs): return LayeredDirty\n\n    This group requires pygame.sprite.DirtySprite or any sprite that\n    has the following attributes:\n        image, rect, dirty, visible, blendmode (see doc of DirtySprite).\n\n    It uses the dirty flag technique and is therefore faster than\n    pygame.sprite.RenderUpdates if you have many static sprites.  It\n    also switches automatically between dirty rect updating and full\n    screen drawing, so you do no have to worry which would be faster.\n\n    As with the pygame.sprite.Group, you can specify some additional attributes\n    through kwargs:\n        _use_update: True/False   (default is False)\n        _default_layer: default layer where the sprites without a layer are\n            added\n        _time_threshold: treshold time for switching between dirty rect mode\n            and fullscreen mode; defaults to updating at 80 frames per second,\n            which is equal to 1000.0 / 80.0\n\n    New in pygame 1.8.0\n\n    ",
        "klass": "pygame.sprite.LayeredDirty",
        "module": "pygame"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Event(type, dict) -> EventType instance\nEvent(type, **attributes) -> EventType instance\ncreate a new event object",
        "klass": "Event",
        "module": "Event"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Surface((width, height), flags=0, depth=0, masks=None) -> Surface\nSurface((width, height), flags=0, Surface) -> Surface\npygame object for representing images",
        "klass": "pygame.Surface",
        "module": "pygame"
    },
    {
        "base_classes": [
            "keras_preprocessing.image.image_data_generator.ImageDataGenerator"
        ],
        "class_docstring": "Generate batches of tensor image data with real-time data augmentation.\n     The data will be looped over (in batches).\n\n    # Arguments\n        featurewise_center: Boolean.\n            Set input mean to 0 over the dataset, feature-wise.\n        samplewise_center: Boolean. Set each sample mean to 0.\n        featurewise_std_normalization: Boolean.\n            Divide inputs by std of the dataset, feature-wise.\n        samplewise_std_normalization: Boolean. Divide each input by its std.\n        zca_whitening: Boolean. Apply ZCA whitening.\n        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n        rotation_range: Int. Degree range for random rotations.\n        width_shift_range: Float, 1-D array-like or int\n            - float: fraction of total width, if < 1, or pixels if >= 1.\n            - 1-D array-like: random elements from the array.\n            - int: integer number of pixels from interval\n                `(-width_shift_range, +width_shift_range)`\n            - With `width_shift_range=2` possible values\n                are integers `[-1, 0, +1]`,\n                same as with `width_shift_range=[-1, 0, +1]`,\n                while with `width_shift_range=1.0` possible values are floats\n                in the interval [-1.0, +1.0).\n        height_shift_range: Float, 1-D array-like or int\n            - float: fraction of total height, if < 1, or pixels if >= 1.\n            - 1-D array-like: random elements from the array.\n            - int: integer number of pixels from interval\n                `(-height_shift_range, +height_shift_range)`\n            - With `height_shift_range=2` possible values\n                are integers `[-1, 0, +1]`,\n                same as with `height_shift_range=[-1, 0, +1]`,\n                while with `height_shift_range=1.0` possible values are floats\n                in the interval [-1.0, +1.0).\n        brightness_range: Tuple or list of two floats. Range for picking\n            a brightness shift value from.\n        shear_range: Float. Shear Intensity\n            (Shear angle in counter-clockwise direction in degrees)\n        zoom_range: Float or [lower, upper]. Range for random zoom.\n            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n        channel_shift_range: Float. Range for random channel shifts.\n        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n            Default is 'nearest'.\n            Points outside the boundaries of the input are filled\n            according to the given mode:\n            - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n            - 'nearest':  aaaaaaaa|abcd|dddddddd\n            - 'reflect':  abcddcba|abcd|dcbaabcd\n            - 'wrap':  abcdabcd|abcd|abcdabcd\n        cval: Float or Int.\n            Value used for points outside the boundaries\n            when `fill_mode = \"constant\"`.\n        horizontal_flip: Boolean. Randomly flip inputs horizontally.\n        vertical_flip: Boolean. Randomly flip inputs vertically.\n        rescale: rescaling factor. Defaults to None.\n            If None or 0, no rescaling is applied,\n            otherwise we multiply the data by the value provided\n            (after applying all other transformations).\n        preprocessing_function: function that will be applied on each input.\n            The function will run after the image is resized and augmented.\n            The function should take one argument:\n            one image (Numpy tensor with rank 3),\n            and should output a Numpy tensor with the same shape.\n        data_format: Image data format,\n            either \"channels_first\" or \"channels_last\".\n            \"channels_last\" mode means that the images should have shape\n            `(samples, height, width, channels)`,\n            \"channels_first\" mode means that the images should have shape\n            `(samples, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be \"channels_last\".\n        validation_split: Float. Fraction of images reserved for validation\n            (strictly between 0 and 1).\n        interpolation_order: int, order to use for\n            the spline interpolation. Higher is slower.\n        dtype: Dtype to use for the generated arrays.\n\n    # Examples\n    Example of using `.flow(x, y)`:\n\n    ```python\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = np_utils.to_categorical(y_train, num_classes)\n    y_test = np_utils.to_categorical(y_test, num_classes)\n\n    datagen = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True)\n\n    # compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied)\n    datagen.fit(x_train)\n\n    # fits the model on batches with real-time data augmentation:\n    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n                        steps_per_epoch=len(x_train) / 32, epochs=epochs)\n\n    # here's a more \"manual\" example\n    for e in range(epochs):\n        print('Epoch', e)\n        batches = 0\n        for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n            model.fit(x_batch, y_batch)\n            batches += 1\n            if batches >= len(x_train) / 32:\n                # we need to break the loop by hand because\n                # the generator loops indefinitely\n                break\n    ```\n    Example of using `.flow_from_directory(directory)`:\n\n    ```python\n    train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True)\n\n    test_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_directory(\n            'data/train',\n            target_size=(150, 150),\n            batch_size=32,\n            class_mode='binary')\n\n    validation_generator = test_datagen.flow_from_directory(\n            'data/validation',\n            target_size=(150, 150),\n            batch_size=32,\n            class_mode='binary')\n\n    model.fit_generator(\n            train_generator,\n            steps_per_epoch=2000,\n            epochs=50,\n            validation_data=validation_generator,\n            validation_steps=800)\n    ```\n\n    Example of transforming images and masks together.\n\n    ```python\n    # we create two instances with the same arguments\n    data_gen_args = dict(featurewise_center=True,\n                         featurewise_std_normalization=True,\n                         rotation_range=90,\n                         width_shift_range=0.1,\n                         height_shift_range=0.1,\n                         zoom_range=0.2)\n    image_datagen = ImageDataGenerator(**data_gen_args)\n    mask_datagen = ImageDataGenerator(**data_gen_args)\n\n    # Provide the same seed and keyword arguments to the fit and flow methods\n    seed = 1\n    image_datagen.fit(images, augment=True, seed=seed)\n    mask_datagen.fit(masks, augment=True, seed=seed)\n\n    image_generator = image_datagen.flow_from_directory(\n        'data/images',\n        class_mode=None,\n        seed=seed)\n\n    mask_generator = mask_datagen.flow_from_directory(\n        'data/masks',\n        class_mode=None,\n        seed=seed)\n\n    # combine generators into one which yields image and masks\n    train_generator = zip(image_generator, mask_generator)\n\n    model.fit_generator(\n        train_generator,\n        steps_per_epoch=2000,\n        epochs=50)\n    ```\n\n    Example of using ```.flow_from_dataframe(dataframe, directory,\n                                            x_col, y_col)```:\n\n    ```python\n\n    train_df = pandas.read_csv(\"./train.csv\")\n    valid_df = pandas.read_csv(\"./valid.csv\")\n\n    train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True)\n\n    test_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_dataframe(\n            dataframe=train_df,\n            directory='data/train',\n            x_col=\"filename\",\n            y_col=\"class\",\n            target_size=(150, 150),\n            batch_size=32,\n            class_mode='binary')\n\n    validation_generator = test_datagen.flow_from_dataframe(\n            dataframe=valid_df,\n            directory='data/validation',\n            x_col=\"filename\",\n            y_col=\"class\",\n            target_size=(150, 150),\n            batch_size=32,\n            class_mode='binary')\n\n    model.fit_generator(\n            train_generator,\n            steps_per_epoch=2000,\n            epochs=50,\n            validation_data=validation_generator,\n            validation_steps=800)\n    ```\n    ",
        "klass": "keras.preprocessing.image.ImageDataGenerator",
        "module": "keras"
    },
    {
        "base_classes": [
            "keras.engine.network.Network"
        ],
        "class_docstring": "The `Model` class adds training & evaluation routines to a `Network`.\n    ",
        "klass": "keras.engine.training.Model",
        "module": "keras"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Displays a progress bar.\n\n    # Arguments\n        target: Total number of steps expected, None if unknown.\n        width: Progress bar width on screen.\n        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n        stateful_metrics: Iterable of string names of metrics that\n            should *not* be averaged over time. Metrics in this list\n            will be displayed as-is. All others will be averaged\n            by the progbar before display.\n        interval: Minimum visual progress update interval (in seconds).\n    ",
        "klass": "keras.utils.generic_utils.Progbar",
        "module": "keras"
    },
    {
        "base_classes": [
            "keras.engine.base_layer.Layer"
        ],
        "class_docstring": "Parametric Rectified Linear Unit.\n\n    It follows:\n    `f(x) = alpha * x for x < 0`,\n    `f(x) = x for x >= 0`,\n    where `alpha` is a learned array with the same shape as x.\n\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    # Output shape\n        Same shape as the input.\n\n    # Arguments\n        alpha_initializer: initializer function for the weights.\n        alpha_regularizer: regularizer for the weights.\n        alpha_constraint: constraint for the weights.\n        shared_axes: the axes along which to share learnable\n            parameters for the activation function.\n            For example, if the incoming feature maps\n            are from a 2D convolution\n            with output shape `(batch, height, width, channels)`,\n            and you wish to share parameters across space\n            so that each filter only has one set of parameters,\n            set `shared_axes=[1, 2]`.\n\n    # References\n        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n           ImageNet Classification](https://arxiv.org/abs/1502.01852)\n    ",
        "klass": "keras.layers.advanced_activations.PReLU",
        "module": "keras"
    },
    {
        "base_classes": [
            "keras.wrappers.scikit_learn.BaseWrapper"
        ],
        "class_docstring": "Implementation of the scikit-learn regressor API for Keras.\n    ",
        "klass": "keras.wrappers.scikit_learn.KerasRegressor",
        "module": "keras"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A proxy processes messages from/to the named exchange.\n\n    For **internal** usage only (not for public consumption).\n    ",
        "klass": "taskflow.engines.worker_based.proxy.Proxy",
        "module": "taskflow"
    },
    {
        "base_classes": [
            "oauth2client.client.Storage"
        ],
        "class_docstring": "Store and retrieve a single credential to and from the keyring.\n\n    To use this module you must have the keyring module installed. See\n    <http://pypi.python.org/pypi/keyring/>. This is an optional module and is\n    not installed with oauth2client by default because it does not work on all\n    the platforms that oauth2client supports, such as Google App Engine.\n\n    The keyring module <http://pypi.python.org/pypi/keyring/> is a\n    cross-platform library for access the keyring capabilities of the local\n    system. The user will be prompted for their keyring password when this\n    module is used, and the manner in which the user is prompted will vary per\n    platform.\n\n    Usage::\n\n        from oauth2client import keyring_storage\n\n        s = keyring_storage.Storage('name_of_application', 'user1')\n        credentials = s.get()\n\n    ",
        "klass": "oauth2client.contrib.keyring_storage.Storage",
        "module": "oauth2client"
    },
    {
        "base_classes": [
            "pyclustering.nnet.sync.sync_network"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm SyncNet. \n    @details SyncNet is bio-inspired algorithm that is based on oscillatory network that uses modified Kuramoto model. Each attribute of a data object\n             is considered as a phase oscillator.\n    \n    Example:\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.syncnet import syncnet, solve_type\n        from pyclustering.samples.definitions import SIMPLE_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Read sample for clustering from some file.\n        sample = read_sample(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)\n\n        # Create oscillatory network with connectivity radius 1.0.\n        network = syncnet(sample, 1.0)\n\n        # Run cluster analysis and collect output dynamic of the oscillatory network.\n        # Network simulation is performed by Runge Kutta 4.\n        analyser = network.process(0.998, solve_type.RK4)\n\n        # Show oscillatory network.\n        network.show_network()\n\n        # Obtain clustering results.\n        clusters = analyser.allocate_clusters()\n\n        # Visualize clustering results.\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.show()\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.syncnet.syncnet",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Common visualizer of clusters on 1D, 2D or 3D surface.\n    @details Use 'cluster_visualizer_multidim' visualizer in case of data dimension is greater than 3.\n\n    @see cluster_visualizer_multidim\n    \n    ",
        "klass": "pyclustering.cluster.cluster_visualizer",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class implements K-Means clustering algorithm.\n    @details K-Means clustering aims to partition n observations into k clusters in which each observation belongs to\n              the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning\n              of the data space into Voronoi cells.\n\n    K-Means clustering results depend on initial centers. Algorithm K-Means++ can used for initialization of\n    initial centers - see module 'pyclustering.cluster.center_initializer'.\n\n    CCORE implementation (C/C++ part of the library) of the algorithm performs parallel processing to ensure maximum\n    performance.\n\n    Implementation based on the paper @cite inproceedings::kmeans::1.\n\n    @image html kmeans_example_clustering.png \"K-Means clustering results. At the left - 'Simple03.data' sample, at the right - 'Lsun.data' sample.\"\n\n    Example #1 - Clustering using K-Means++ for center initialization:\n    @code\n        from pyclustering.cluster.kmeans import kmeans, kmeans_visualizer\n        from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Load list of points for cluster analysis.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS)\n\n        # Prepare initial centers using K-Means++ method.\n        initial_centers = kmeans_plusplus_initializer(sample, 2).initialize()\n\n        # Create instance of K-Means algorithm with prepared centers.\n        kmeans_instance = kmeans(sample, initial_centers)\n\n        # Run cluster analysis and obtain results.\n        kmeans_instance.process()\n        clusters = kmeans_instance.get_clusters()\n        final_centers = kmeans_instance.get_centers()\n\n        # Visualize obtained results\n        kmeans_visualizer.show_clusters(sample, clusters, final_centers)\n    @endcode\n\n    Example #2 - Clustering using specific distance metric, for example, Manhattan distance:\n    @code\n        # prepare input data and initial centers for cluster analysis using K-Means\n\n        # create metric that will be used for clustering\n        manhattan_metric = distance_metric(type_metric.MANHATTAN)\n\n        # create instance of K-Means using specific distance metric:\n        kmeans_instance = kmeans(sample, initial_centers, metric=manhattan_metric)\n\n        # run cluster analysis and obtain results\n        kmeans_instance.process()\n        clusters = kmeans_instance.get_clusters()\n    @endcode\n\n    @see center_initializer\n    \n    ",
        "klass": "pyclustering.cluster.kmeans.kmeans",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents agglomerative algorithm for cluster analysis.\n    @details Agglomerative algorithm considers each data point (object) as a separate cluster at the beggining and\n              step by step finds the best pair of clusters for merge until required amount of clusters is obtained.\n    \n    Example of agglomerative algorithm where centroid link is used:\n    @code\n        from pyclustering.cluster.agglomerative import agglomerative, type_link\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Sample for cluster analysis (represented by list)\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_TARGET)\n\n        # Create object that uses python code only\n        agglomerative_instance = agglomerative(sample, 6, type_link.SINGLE_LINK, ccore=True)\n\n        # Cluster analysis\n        agglomerative_instance.process()\n\n        # Obtain results of clustering\n        clusters = agglomerative_instance.get_clusters()\n\n        # Visualize clustering results\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.show()\n    @endcode\n    \n    There is example of clustering 'LSUN' sample:\n    @code\n        from pyclustering.cluster.agglomerative import agglomerative, type_link\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # sample Lsun for cluster analysis\n        lsun_sample = read_sample(FCPS_SAMPLES.SAMPLE_LSUN)\n\n        # create instance of the algorithm that will use ccore library (the last argument)\n        agglomerative_instance = agglomerative(lsun_sample, 3, type_link.SINGLE_LINK, True)\n\n        # start processing\n        agglomerative_instance.process()\n\n        # get result and visualize it\n        lsun_clusters = agglomerative_instance.get_clusters()\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(lsun_clusters, lsun_sample)\n        visualizer.show()\n    @endcode\n    \n    Example of agglomerative clustering using different links:\n    @image html agglomerative_lsun_clustering_single_link.png\n    \n    ",
        "klass": "pyclustering.cluster.agglomerative.agglomerative",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents Fuzzy C-means (FCM) clustering algorithm.\n    @details Fuzzy clustering is a form of clustering in which each data point can belong to more than one cluster.\n\n    Fuzzy C-Means algorithm uses two general formulas for cluster analysis. The first is to updated membership of each\n    point:\n    \f[w_{ij}=\frac{1}{\\sum_{k=0}^{c}\\left ( \frac{\\left \\| x_{i}-c_{j} \right \\|}{\\left \\| x_{i}-c_{k} \right \\|} \right )^{\frac{2}{m-1}}}\f]\n\n    The second formula is used to update centers in line with obtained centers:\n    \f[c_{k}=\frac{\\sum_{i=0}^{N}w_{k}\\left ( x_{i} \right )^{m}x_{i}}{\\sum_{i=0}^{N}w_{k}\\left ( x_{i} \right )^{m}}\f]\n\n    Fuzzy C-Means clustering results depend on initial centers. Algorithm K-Means++ can used for center initialization\n    from module 'pyclustering.cluster.center_initializer'.\n\n    CCORE implementation of the algorithm uses thread pool to parallelize the clustering process.\n\n    Here is an example how to perform cluster analysis using Fuzzy C-Means algorithm:\n    @code\n        from pyclustering.samples.definitions import FAMOUS_SAMPLES\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n        from pyclustering.cluster.fcm import fcm\n        from pyclustering.utils import read_sample\n\n        # load list of points for cluster analysis\n        sample = read_sample(FAMOUS_SAMPLES.SAMPLE_OLD_FAITHFUL)\n\n        # initialize\n        initial_centers = kmeans_plusplus_initializer(sample, 2, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n\n        # create instance of Fuzzy C-Means algorithm\n        fcm_instance = fcm(sample, initial_centers)\n\n        # run cluster analysis and obtain results\n        fcm_instance.process()\n        clusters = fcm_instance.get_clusters()\n        centers = fcm_instance.get_centers()\n\n        # visualize clustering results\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.append_cluster(centers, marker='*', markersize=10)\n        visualizer.show()\n    @endcode\n\n    The next example shows how to perform image segmentation using Fuzzy C-Means algorithm:\n    @code\n        from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n        from pyclustering.cluster.fcm import fcm\n        from pyclustering.utils import read_image, draw_image_mask_segments\n\n        # load list of points for cluster analysis\n        data = read_image(\"stpetersburg_admiral.jpg\")\n\n        # initialize\n        initial_centers = kmeans_plusplus_initializer(data, 3, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n\n        # create instance of Fuzzy C-Means algorithm\n        fcm_instance = fcm(data, initial_centers)\n\n        # run cluster analysis and obtain results\n        fcm_instance.process()\n        clusters = fcm_instance.get_clusters()\n\n        # visualize segmentation results\n        draw_image_mask_segments(\"stpetersburg_admiral.jpg\", clusters)\n    @endcode\n\n    @image html fcm_segmentation_stpetersburg.png \"Image segmentation using Fuzzy C-Means algorithm.\"\n\n    ",
        "klass": "pyclustering.cluster.fcm.fcm",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm K-Medians.\n    @details The algorithm is less sensitive to outliers than K-Means. Medians are calculated instead of centroids.\n    \n    Example:\n    @code\n        from pyclustering.cluster.kmedians import kmedians\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n\n        # Load list of points for cluster analysis.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS)\n\n        # Create instance of K-Medians algorithm.\n        initial_medians = [[0.0, 0.1], [2.5, 0.7]]\n        kmedians_instance = kmedians(sample, initial_medians)\n\n        # Run cluster analysis and obtain results.\n        kmedians_instance.process()\n        clusters = kmedians_instance.get_clusters()\n        medians = kmedians_instance.get_medians()\n\n        # Visualize clustering results.\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.append_cluster(initial_medians, marker='*', markersize=10)\n        visualizer.append_cluster(medians, marker='*', markersize=10)\n        visualizer.show()\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.kmedians.kmedians",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "pyclustering.nnet.network"
        ],
        "class_docstring": "!\n    @brief Hysteresis oscillatory network that uses relaxation oscillators that are represented by objective hysteresis neurons whose output in range [-1, +1].\n    \n    Examples:\n    @code\n        # create hysteresis oscillatory network with 5 oscillators.\n        network = hysteresis_network(5);\n        \n        # set initial states (from range [-1, +1]).\n        network.states = [1 0 1 0 1];\n        \n        # set initial outputs.\n        network.outputs = [1 1 1 1 1];\n        \n        # perform simulation of the network during 1000 steps in 10 time units.\n        output_dynamic = network.simulate(1000, 10);\n        \n        # show output dynamic of the network.\n        hysteresis_visualizer.show_output_dynamic(output_dynamic);\n        \n        # obtain synchronous ensembles.\n        ensembles = output_dynamic.allocate_sync_ensembles(tolerance = 0.5, threshold_steps = 5);\n        print(ensembles);\n    @endcode\n    \n    ",
        "klass": "pyclustering.nnet.hysteresis.hysteresis_network",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm CURE with KD-tree optimization.\n    @details CCORE option can be used to use the pyclustering core - C/C++ shared library for processing that significantly increases performance.\n    \n    Here is an example how to perform cluster analysis of sample 'Lsun':\n    @code\n        from pyclustering.cluster import cluster_visualizer;\n        from pyclustering.cluster.cure import cure;\n        from pyclustering.utils import read_sample;\n        from pyclustering.samples.definitions import FCPS_SAMPLES;\n\n        # Input data in following format [ [0.1, 0.5], [0.3, 0.1], ... ].\n        input_data = read_sample(FCPS_SAMPLES.SAMPLE_LSUN);\n\n        # Allocate three clusters.\n        cure_instance = cure(input_data, 3);\n        cure_instance.process();\n        clusters = cure_instance.get_clusters();\n\n        # Visualize allocated clusters.\n        visualizer = cluster_visualizer();\n        visualizer.append_clusters(clusters, input_data);\n        visualizer.show();\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.cure.cure",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class implements CLIQUE grid based clustering algorithm.\n    @details CLIQUE automatically finnds subspaces with high-density clusters. It produces identical results\n              irrespective of the order in which the input records are presented and it does not presume any canonical\n              distribution for input data @cite article::clique::1.\n\n    Here is an example where data in two-dimensional space is clustered using CLIQUE algorithm:\n    @code\n        from pyclustering.cluster.clique import clique, clique_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n\n        # read two-dimensional input data 'Target'\n        data = read_sample(FCPS_SAMPLES.SAMPLE_TARGET)\n\n        # create CLIQUE algorithm for processing\n        intervals = 10  # defines amount of cells in grid in each dimension\n        threshold = 0   # lets consider each point as non-outlier\n        clique_instance = clique(data, intervals, threshold)\n\n        # start clustering process and obtain results\n        clique_instance.process()\n        clusters = clique_instance.get_clusters()  # allocated clusters\n        noise = clique_instance.get_noise()     # points that are considered as outliers (in this example should be empty)\n        cells = clique_instance.get_cells()     # CLIQUE blocks that forms grid\n\n        print(\"Amount of clusters:\", len(clusters))\n\n        # visualize clustering results\n        clique_visualizer.show_grid(cells, data)    # show grid that has been formed by the algorithm\n        clique_visualizer.show_clusters(data, clusters, noise)  # show clustering results\n    @endcode\n\n    In this example 6 clusters are allocated including four small cluster where each such small cluster consists of\n    three points. There are visualized clustering results - grid that has been formed by CLIQUE algorithm with\n    density and clusters itself:\n    @image html clique_clustering_target.png \"Fig. 1. CLIQUE clustering results (grid and clusters itself).\"\n\n    Sometimes such small clusters should be considered as outliers taking into account fact that two clusters in the\n    central are relatively huge. To treat them as a noise threshold value should be increased:\n    @code\n        intervals = 10\n        threshold = 3   # block that contains 3 or less points is considered as a outlier as well as its points\n        clique_instance = clique(data, intervals, threshold)\n    @endcode\n\n    Two clusters are allocated, but in this case some points in cluster-\"circle\" are also considered as outliers,\n    because CLIQUE operates with blocks, not with points:\n    @image html clique_clustering_with_noise.png \"Fig. 2. Noise allocation by CLIQUE.\"\n\n    ",
        "klass": "pyclustering.cluster.clique.clique",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Visualizer for cluster in multi-dimensional data.\n    @details This cluster visualizer is useful for clusters in data whose dimension is greater than 3. The\n              multidimensional visualizer helps to overcome 'cluster_visualizer' shortcoming - ability to display\n              clusters in 1D, 2D or 3D dimensional data space.\n\n        Example of clustering results visualization where 'Iris' is used:\n        @code\n            from pyclustering.utils import read_sample\n            from pyclustering.samples.definitions import FAMOUS_SAMPLES\n            from pyclustering.cluster import cluster_visualizer_multidim\n\n            # load 4D data sample 'Iris'\n            sample_4d = read_sample(FAMOUS_SAMPLES.SAMPLE_IRIS)\n\n            # initialize 3 initial centers using K-Means++ algorithm\n            centers = kmeans_plusplus_initializer(sample_4d, 3).initialize()\n\n            # performs cluster analysis using X-Means\n            xmeans_instance = xmeans(sample_4d, centers)\n            xmeans_instance.process()\n            clusters = xmeans_instance.get_clusters()\n\n            # visualize obtained clusters in multi-dimensional space\n            visualizer = cluster_visualizer_multidim()\n            visualizer.append_clusters(clusters, sample_4d)\n            visualizer.show(max_row_size=3)\n        @endcode\n\n        Visualized clustering results of 'Iris' data (multi-dimensional data):\n        @image html xmeans_clustering_famous_iris.png \"Fig. 1. X-Means clustering results (data 'Iris').\"\n\n        Sometimes no need to display results in all dimensions. Parameter 'filter' can be used to display only\n        interesting coordinate pairs. Here is an example of visualization of pair coordinates (x0, x1) and (x0, x2) for\n        previous clustering results:\n        @code\n            visualizer = cluster_visualizer_multidim()\n            visualizer.append_clusters(clusters, sample_4d)\n            visualizer.show(pair_filter=[[0, 1], [0, 2]])\n        @endcode\n\n        Visualized results of specified coordinate pairs:\n        @image html xmeans_clustering_famous_iris_filtered.png \"Fig. 2. X-Means clustering results (x0, x1) and (x0, x2) (data 'Iris').\"\n\n    ",
        "klass": "pyclustering.cluster.cluster_visualizer_multidim",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm CLARANS (a method for clustering objects for spatial data mining).\n    \n    ",
        "klass": "pyclustering.cluster.clarans.clarans",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class implements BANG grid based clustering algorithm.\n    @details BANG clustering algorithms uses a multidimensional grid structure to organize the value space surrounding\n              the pattern values. The patterns are grouped into blocks and clustered with respect to the blocks by\n              a topological neighbor search algorithm @cite inproceedings::bang::1.\n\n    Code example of BANG usage:\n    @code\n        from pyclustering.cluster.bang import bang, bang_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n\n        # Read data three dimensional data.\n        data = read_sample(FCPS_SAMPLES.SAMPLE_CHAINLINK)\n\n        # Prepare algorithm's parameters.\n        levels = 11\n\n        # Create instance of BANG algorithm.\n        bang_instance = bang(data, levels)\n        bang_instance.process()\n\n        # Obtain clustering results.\n        clusters = bang_instance.get_clusters()\n        noise = bang_instance.get_noise()\n        directory = bang_instance.get_directory()\n        dendrogram = bang_instance.get_dendrogram()\n\n        # Visualize BANG clustering results.\n        bang_visualizer.show_blocks(directory)\n        bang_visualizer.show_dendrogram(dendrogram)\n        bang_visualizer.show_clusters(data, clusters, noise)\n    @endcode\n\n    There is visualization of BANG-clustering of three-dimensional data 'chainlink'. BANG-blocks that were formed during\n    processing are shown on following figure. The darkest color means highest density, blocks that does not cover points\n    are transparent:\n    @image html bang_blocks_chainlink.png \"Fig. 1. BANG-blocks that cover input data.\"\n\n    Here is obtained dendrogram that can be used for further analysis to improve clustering results:\n    @image html bang_dendrogram_chainlink.png \"Fig. 2. BANG dendrogram where the X-axis contains BANG-blocks, the Y-axis contains density.\"\n\n    BANG clustering result of 'chainlink' data:\n    @image html bang_clustering_chainlink.png \"Fig. 3. BANG clustering result. Data: 'chainlink'.\"\n\n    ",
        "klass": "pyclustering.cluster.bang.bang",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm DBSCAN.\n    @details This DBSCAN algorithm is KD-tree optimized.\n             \n             CCORE option can be used to use the pyclustering core - C/C++ shared library for processing that significantly increases performance.\n    \n    Example:\n    @code\n        from pyclustering.cluster.dbscan import dbscan\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n\n        # Sample for cluster analysis.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_CHAINLINK)\n\n        # Create DBSCAN algorithm.\n        dbscan_instance = dbscan(sample, 0.7, 3)\n\n        # Start processing by DBSCAN.\n        dbscan_instance.process()\n\n        # Obtain results of clustering.\n        clusters = dbscan_instance.get_clusters()\n        noise = dbscan_instance.get_noise()\n\n        # Visualize clustering results\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.append_cluster(noise, sample, marker='x')\n        visualizer.show()\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.dbscan.dbscan",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm BIRCH.\n    \n    Example how to extract clusters from 'OldFaithful' sample using BIRCH algorithm:\n    @code\n        from pyclustering.cluster.birch import birch, measurement_type\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FAMOUS_SAMPLES\n\n        # Sample for cluster analysis (represented by list)\n        sample = read_sample(FAMOUS_SAMPLES.SAMPLE_OLD_FAITHFUL)\n\n        # Create BIRCH algorithm\n        birch_instance = birch(sample, 2)\n\n        # Cluster analysis\n        birch_instance.process()\n\n        # Obtain results of clustering\n        clusters = birch_instance.get_clusters()\n\n        # Visualize allocated clusters\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.show()\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.birch.birch",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm SYNC-SOM. SYNC-SOM is bio-inspired algorithm that is based on oscillatory network \n           that uses self-organized feature map as the first layer.\n           \n    Example:\n    @code\n        # read sample for clustering\n        sample = read_sample(file);\n        \n        # create oscillatory network for cluster analysis where the first layer has \n        # size 10x10 and connectivity radius for objects 1.0.\n        network = syncsom(sample, 10, 10, 1.0);\n        \n        # simulate network (perform cluster analysis) and collect output dynamic\n        (dyn_time, dyn_phase) = network.process(True, 0.998);\n        \n        # obtain encoded clusters\n        encoded_clusters = network.get_som_clusters();\n        \n        # obtain real clusters\n        clusters = network.get_clusters();\n        \n        # show the first layer of the network\n        network.show_som_layer();\n        \n        # show the second layer of the network\n        network.show_sync_layer();\n    @endcode\n    \n    ",
        "klass": "pyclustering.cluster.syncsom.syncsom",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm OPTICS (Ordering Points To Identify Clustering Structure) with KD-tree optimization (ccore options is supported).\n    @details OPTICS is a density-based algorithm. Purpose of the algorithm is to provide explicit clusters, but create clustering-ordering representation of the input data. \n             Clustering-ordering information contains information about internal structures of data set in terms of density and proper connectivity radius can be obtained\n             for allocation required amount of clusters using this diagram. In case of usage additional input parameter 'amount of clusters' connectivity radius should be\n             bigger than real - because it will be calculated by the algorithms if requested amount of clusters is not allocated.\n\n    @image html optics_example_clustering.png \"Scheme how does OPTICS works. At the beginning only one cluster is allocated, but two is requested. At the second step OPTICS calculates connectivity radius using cluster-ordering and performs final cluster allocation.\"\n\n    Clustering example using sample 'Chainlink':\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.optics import optics, ordering_analyser, ordering_visualizer\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Read sample for clustering from some file.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_CHAINLINK)\n\n        # Run cluster analysis where connectivity radius is bigger than real.\n        radius = 0.5\n        neighbors = 3\n        optics_instance = optics(sample, radius, neighbors)\n\n        # Performs cluster analysis.\n        optics_instance.process()\n\n        # Obtain results of clustering.\n        clusters = optics_instance.get_clusters()\n        noise = optics_instance.get_noise()\n        ordering = optics_instance.get_ordering()\n\n        # Visualize clustering results.\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.show()\n\n        # Display ordering.\n        analyser = ordering_analyser(ordering)\n        ordering_visualizer.show_ordering_diagram(analyser, 2)\n    @endcode\n\n    Amount of clusters that should be allocated can be also specified. In this case connectivity radius should be greater than real, for example:\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.optics import optics, ordering_analyser, ordering_visualizer\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Read sample for clustering from some file\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_LSUN)\n\n        # Run cluster analysis where connectivity radius is bigger than real\n        radius = 2.0\n        neighbors = 3\n        amount_of_clusters = 3\n        optics_instance = optics(sample, radius, neighbors, amount_of_clusters)\n\n        # Performs cluster analysis\n        optics_instance.process()\n\n        # Obtain results of clustering\n        clusters = optics_instance.get_clusters()\n        noise = optics_instance.get_noise()\n        ordering = optics_instance.get_ordering()\n\n        # Visualize ordering diagram\n        analyser = ordering_analyser(ordering)\n        ordering_visualizer.show_ordering_diagram(analyser, amount_of_clusters)\n\n        # Visualize clustering results\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.show()\n    @endcode\n\n    Here is an example where OPTICS extracts outliers from sample 'Tetra':\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.optics import optics\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n        from pyclustering.utils import read_sample\n\n        # Read sample for clustering from some file.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_TETRA)\n\n        # Run cluster analysis where connectivity radius is bigger than real.\n        radius = 0.4\n        neighbors = 3\n        optics_instance = optics(sample, radius, neighbors)\n\n        # Performs cluster analysis.\n        optics_instance.process()\n\n        # Obtain results of clustering.\n        clusters = optics_instance.get_clusters()\n        noise = optics_instance.get_noise()\n\n        # Visualize clustering results (clusters and outliers).\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.append_cluster(noise, sample, marker='x')\n        visualizer.show()\n    @endcode\n\n    Visualization result of allocated clusters and outliers is presented on the image below:\n    @image html optics_noise_tetra.png \"Clusters and outliers extracted by OPTICS algorithm from sample 'Tetra'.\"\n\n    ",
        "klass": "pyclustering.cluster.optics.optics",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Chaotic neural network based on system of logistic map where clustering phenomenon can be observed.\n    @details Here is an example how to perform cluster analysis using chaotic neural network:\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.samples.definitions import SIMPLE_SAMPLES\n        from pyclustering.utils import read_sample\n        from pyclustering.nnet.cnn import cnn_network, cnn_visualizer\n\n        # Load stimulus from file.\n        stimulus = read_sample(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)\n\n        # Create chaotic neural network, amount of neurons should be equal to amount of stimulus.\n        network_instance = cnn_network(len(stimulus))\n\n        # Perform simulation during 100 steps.\n        steps = 100\n        output_dynamic = network_instance.simulate(steps, stimulus)\n\n        # Display output dynamic of the network.\n        cnn_visualizer.show_output_dynamic(output_dynamic)\n\n        # Display dynamic matrix and observation matrix to show clustering phenomenon.\n        cnn_visualizer.show_dynamic_matrix(output_dynamic)\n        cnn_visualizer.show_observation_matrix(output_dynamic)\n\n        # Visualize clustering results.\n        clusters = output_dynamic.allocate_sync_ensembles(10)\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, stimulus)\n        visualizer.show()\n    @endcode\n    \n    ",
        "klass": "pyclustering.nnet.cnn.cnn_network",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Expectation-Maximization clustering algorithm for Gaussian Mixture Model (GMM).\n    @details The algorithm provides only clustering services (unsupervised learning).\n              Here an example of data clustering process:\n    @code\n        from pyclustering.cluster.ema import ema, ema_visualizer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import FCPS_SAMPLES\n\n        # Read data from text file.\n        sample = read_sample(FCPS_SAMPLES.SAMPLE_LSUN)\n\n        # Create EM algorithm to allocated four clusters.\n        ema_instance = ema(sample, 3)\n\n        # Run clustering process.\n        ema_instance.process()\n\n        # Get clustering results.\n        clusters = ema_instance.get_clusters()\n        covariances = ema_instance.get_covariances()\n        means = ema_instance.get_centers()\n\n        # Visualize obtained clustering results.\n        ema_visualizer.show_clusters(clusters, sample, covariances, means)\n    @endcode\n    \n    Here is clustering results of the Expectation-Maximization clustering algorithm where popular sample 'OldFaithful' was used.\n    Initial random means and covariances were used in the example. The first step is presented on the left side of the figure and\n    final result (the last step) is on the right side:\n    @image html ema_old_faithful_clustering.png\n    \n    @see ema_visualizer\n    @see ema_observer\n    \n    ",
        "klass": "pyclustering.cluster.ema.ema",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "!\n    @brief Class represents clustering algorithm X-Means.\n    @details X-means clustering method starts with the assumption of having a minimum number of clusters, \n             and then dynamically increases them. X-means uses specified splitting criterion to control \n             the process of splitting clusters. Method K-Means++ can be used for calculation of initial centers.\n             \n             CCORE implementation of the algorithm uses thread pool to parallelize the clustering process.\n    \n    Here example how to perform cluster analysis using X-Means algorithm:\n    @code\n        from pyclustering.cluster import cluster_visualizer\n        from pyclustering.cluster.xmeans import xmeans\n        from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n        from pyclustering.utils import read_sample\n        from pyclustering.samples.definitions import SIMPLE_SAMPLES\n\n        # Read sample 'simple3' from file.\n        sample = read_sample(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)\n\n        # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will\n        # start analysis.\n        amount_initial_centers = 2\n        initial_centers = kmeans_plusplus_initializer(sample, amount_initial_centers).initialize()\n\n        # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum\n        # number of clusters that can be allocated is 20.\n        xmeans_instance = xmeans(sample, initial_centers, 20)\n        xmeans_instance.process()\n\n        # Extract clustering results: clusters and their centers\n        clusters = xmeans_instance.get_clusters()\n        centers = xmeans_instance.get_centers()\n\n        # Print total sum of metric errors\n        print(\"Total WCE:\", xmeans_instance.get_total_wce())\n\n        # Visualize clustering results\n        visualizer = cluster_visualizer()\n        visualizer.append_clusters(clusters, sample)\n        visualizer.append_cluster(centers, None, marker='*', markersize=10)\n        visualizer.show()\n    @endcode\n\n    Visualization of clustering results that were obtained using code above and where X-Means algorithm allocates four clusters.\n    @image html xmeans_clustering_simple3.png \"Fig. 1. X-Means clustering results (data 'Simple3').\"\n    \n    @see center_initializer\n    \n    ",
        "klass": "pyclustering.cluster.xmeans.xmeans",
        "module": "pyclustering"
    },
    {
        "base_classes": [
            "tensor2tensor.data_generators.text_encoder.ClassLabelEncoder"
        ],
        "class_docstring": "One-hot encoder for class labels.",
        "klass": "tensor2tensor.data_generators.text_encoder.OneHotClassLabelEncoder",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "tensor2tensor.data_generators.text_encoder.TextEncoder"
        ],
        "class_docstring": "Class for invertibly encoding text using a limited vocabulary.\n\n  Invertibly encodes a native string as a sequence of subtokens from a limited\n  vocabulary.\n\n  A SubwordTextEncoder is built from a corpus (so it is tailored to the text in\n  the corpus), and stored to a file. See text_encoder_build_subword.py.\n\n  It can then be loaded and used to encode/decode any text.\n\n  Encoding has four phases:\n\n  1. Tokenize into a list of tokens.  Each token is a unicode string of either\n     all alphanumeric characters or all non-alphanumeric characters.  We drop\n     tokens consisting of a single space that are between two alphanumeric\n     tokens.\n\n  2. Escape each token.  This escapes away special and out-of-vocabulary\n     characters, and makes sure that each token ends with an underscore, and\n     has no other underscores.\n\n  3. Represent each escaped token as a the concatenation of a list of subtokens\n     from the limited vocabulary.  Subtoken selection is done greedily from\n     beginning to end.  That is, we construct the list in order, always picking\n     the longest subtoken in our vocabulary that matches a prefix of the\n     remaining portion of the encoded token.\n\n  4. Concatenate these lists.  This concatenation is invertible due to the\n     fact that the trailing underscores indicate when one list is finished.\n\n  ",
        "klass": "tensor2tensor.data_generators.text_encoder.SubwordTextEncoder",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "tensor2tensor.data_generators.text_encoder.TextEncoder"
        ],
        "class_docstring": "Encoder based on a user-supplied vocabulary (file or list).",
        "klass": "tensor2tensor.data_generators.text_encoder.TokenTextEncoder",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "tensor2tensor.data_generators.text_encoder.TextEncoder"
        ],
        "class_docstring": "ACTG strings to ints and back. Optionally chunks bases into single ids.\n\n  To use a different character set, subclass and set BASES to the char set. UNK\n  and PAD must not appear in the char set, but can also be reset.\n\n  Uses 'N' as an unknown base.\n  ",
        "klass": "tensor2tensor.data_generators.dna_encoder.DNAEncoder",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "tensor2tensor.data_generators.dna_encoder.DNAEncoder"
        ],
        "class_docstring": "DNAEncoder for delimiter separated subsequences.\n\n  Uses ',' as default delimiter.\n  ",
        "klass": "tensor2tensor.data_generators.dna_encoder.DelimitedDNAEncoder",
        "module": "tensor2tensor"
    },
    {
        "base_classes": [
            "ray.tune.suggest.suggestion.SuggestionAlgorithm"
        ],
        "class_docstring": "A wrapper around HyperOpt to provide trial suggestions.\n\n    Requires HyperOpt to be installed from source.\n    Uses the Tree-structured Parzen Estimators algorithm, although can be\n    trivially extended to support any algorithm HyperOpt uses. Externally\n    added trials will not be tracked by HyperOpt. Trials of the current run\n    can be saved using save method, trials of a previous run can be loaded\n    using restore method, thus enabling a warm start feature.\n\n    Parameters:\n        space (dict): HyperOpt configuration. Parameters will be sampled\n            from this configuration and will be used to override\n            parameters generated in the variant generation process.\n        max_concurrent (int): Number of maximum concurrent trials. Defaults\n            to 10.\n        metric (str): The training result objective value attribute.\n        mode (str): One of {min, max}. Determines whether objective is\n            minimizing or maximizing the metric attribute.\n        points_to_evaluate (list): Initial parameter suggestions to be run\n            first. This is for when you already have some good parameters\n            you want hyperopt to run first to help the TPE algorithm\n            make better suggestions for future parameters. Needs to be\n            a list of dict of hyperopt-named variables.\n            Choice variables should be indicated by their index in the\n            list (see example)\n        n_initial_points (int): number of random evaluations of the\n            objective function before starting to aproximate it with\n            tree parzen estimators. Defaults to 20.\n        random_state_seed (int, array_like, None): seed for reproducible\n            results. Defaults to None.\n        gamma (float in range (0,1)): parameter governing the tree parzen\n            estimators suggestion algorithm. Defaults to 0.25.\n\n    Example:\n        >>> space = {\n        >>>     'width': hp.uniform('width', 0, 20),\n        >>>     'height': hp.uniform('height', -100, 100),\n        >>>     'activation': hp.choice(\"activation\", [\"relu\", \"tanh\"])\n        >>> }\n        >>> current_best_params = [{\n        >>>     'width': 10,\n        >>>     'height': 0,\n        >>>     'activation': 0, # The index of \"relu\"\n        >>> }]\n        >>> algo = HyperOptSearch(\n        >>>     space, max_concurrent=4, metric=\"mean_loss\", mode=\"min\",\n        >>>     points_to_evaluate=current_best_params)\n    ",
        "klass": "ray.tune.suggest.hyperopt.HyperOptSearch",
        "module": "ray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A streaming environment.\n\n    This class is responsible for constructing the logical and the\n    physical dataflow.\n\n    Attributes:\n         logical_topo (DiGraph): The user-defined logical topology in\n         NetworkX DiGRaph format.\n         (See: https://networkx.github.io)\n         physical_topo (DiGraph): The physical topology in NetworkX\n         DiGRaph format. The physical dataflow is constructed by the\n         environment based on logical_topo.\n         operators (dict): A mapping from operator ids to operator metadata\n         (See: Operator in operator.py).\n         config (Config): The environment's configuration.\n         topo_cleaned (bool): A flag that indicates whether the logical\n         topology is garbage collected (True) or not (False).\n         actor_handles (list): A list of all Ray actor handles that execute\n         the streaming dataflow.\n    ",
        "klass": "ray.experimental.streaming.streaming.Environment",
        "module": "ray"
    },
    {
        "base_classes": [
            "dict"
        ],
        "class_docstring": "Dict that tracks which keys have been accessed.\n\n    It can also intercept gets and allow an arbitrary callback to be applied\n    (i.e., to lazily convert numpy arrays to Tensors).\n\n    We make the simplifying assumption only __getitem__ is used to access\n    values.\n    ",
        "klass": "ray.rllib.utils.tracking_dict.UsageTrackingDict",
        "module": "ray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "A class used to set and get weights for Tensorflow networks.\n\n    Attributes:\n        sess (tf.Session): The tensorflow session used to run assignment.\n        variables (Dict[str, tf.Variable]): Extracted variables from the loss\n            or additional variables that are passed in.\n        placeholders (Dict[str, tf.placeholders]): Placeholders for weights.\n        assignment_nodes (Dict[str, tf.Tensor]): Nodes that assign weights.\n    ",
        "klass": "ray.experimental.tf_utils.TensorFlowVariables",
        "module": "ray"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "Queue implementation on Ray.\n\n    Args:\n        maxsize (int): maximum size of the queue. If zero, size is unboundend.\n    ",
        "klass": "ray.experimental.queue.Queue",
        "module": "ray"
    },
    {
        "base_classes": [
            "asn1crypto.core.Sequence"
        ],
        "class_docstring": "\n    An ASN.1 class for translating between the OS crypto library's\n    representation of an (EC)DSA signature and the ASN.1 structure that is part\n    of various RFCs.\n\n    Original Name: DSS-Sig-Value\n    Source: https://tools.ietf.org/html/rfc3279#section-2.2.2\n    ",
        "klass": "asn1crypto.algos.DSASignature",
        "module": "asn1crypto"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    Context manager to temporarily set options in the `with` statement context.\n\n    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.\n\n    Examples\n    --------\n\n    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n            ...\n\n    ",
        "klass": "pandas.core.config.option_context",
        "module": "pandas"
    },
    {
        "base_classes": [
            "angr.vaults.Vault"
        ],
        "class_docstring": "\n    A Vault that uses a dictionary for storage.\n    ",
        "klass": "angr.vaults.VaultDict",
        "module": "angr"
    },
    {
        "base_classes": [
            "angr.vaults.Vault"
        ],
        "class_docstring": "\n    A Vault that uses a directory for storage.\n    ",
        "klass": "angr.vaults.VaultDir",
        "module": "angr"
    },
    {
        "base_classes": [
            "twisted.internet.protocol.Factory"
        ],
        "class_docstring": "\n    A Protocol factory for clients.\n\n    This can be used together with the various connectXXX methods in\n    reactors.\n    ",
        "klass": "twisted.internet.protocol.ClientFactory",
        "module": "twisted"
    },
    {
        "base_classes": [
            "object"
        ],
        "class_docstring": "\n    An event driven queue.\n\n    Objects may be added as usual to this queue.  When an attempt is\n    made to retrieve an object when the queue is empty, a L{Deferred} is\n    returned which will fire when an object becomes available.\n\n    @ivar size: The maximum number of objects to allow into the queue\n    at a time.  When an attempt to add a new object would exceed this\n    limit, L{QueueOverflow} is raised synchronously.  L{None} for no limit.\n\n    @ivar backlog: The maximum number of L{Deferred} gets to allow at\n    one time.  When an attempt is made to get an object which would\n    exceed this limit, L{QueueUnderflow} is raised synchronously.  L{None}\n    for no limit.\n    ",
        "klass": "twisted.internet.defer.DeferredQueue",
        "module": "twisted"
    }
]