{
    "module": "sklearn",
    "function": "sklearn.metrics.regression.mean_squared_error",
    "stackoverflow": [
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "9608",
            "_score": 23.82178,
            "_source": {
                "title": "Mean error (not squared) in scikit-learn cross_val_score",
                "content": "Mean error (not squared) in scikit-learn cross_val_score <p>I need to know if the values generated by each fold of <code>cross_val_score</code> have a distribution which is centered on zero.  Something as simple as the median or mean of <code>y_true - y_predicted</code> would suffice.  All I see in the available options are absolute and squared.  I've looked into make scorer but can't see how to code the simple mean error and then call it as the scoring argument in <code>cross_val_score</code>.</p>\n <scikit-learn><cross-validation><pre><code>from sklearn.datasets import load_diabetes\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef mean_error(y, y_pred):\n    # assuming y and y_pred are numpy arrays\n    return np.mean(y_pred - y)\n\nX, y = load_diabetes(return_X_y=True)\nmean_error_scorer = make_scorer(mean_error, greater_is_better=False)\n\nregr = LinearRegression()\ncross_val_score(regr, X, y, scoring=mean_error_scorer)\n</code></pre>\n",
                "codes": [
                    [
                        "from sklearn.datasets import load_diabetes\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef mean_error(y, y_pred):\n    # assuming y and y_pred are numpy arrays\n    return np.mean(y_pred - y)\n\nX, y = load_diabetes(return_X_y=True)\nmean_error_scorer = make_scorer(mean_error, greater_is_better=False)\n\nregr = LinearRegression()\ncross_val_score(regr, X, y, scoring=mean_error_scorer)\n"
                    ]
                ],
                "question_id:": "34141",
                "question_votes:": "1",
                "question_text:": "<p>I need to know if the values generated by each fold of <code>cross_val_score</code> have a distribution which is centered on zero.  Something as simple as the median or mean of <code>y_true - y_predicted</code> would suffice.  All I see in the available options are absolute and squared.  I've looked into make scorer but can't see how to code the simple mean error and then call it as the scoring argument in <code>cross_val_score</code>.</p>\n",
                "tags": "<scikit-learn><cross-validation>",
                "answers": [
                    [
                        "34147",
                        "2",
                        "34141",
                        "",
                        "",
                        "<pre><code>from sklearn.datasets import load_diabetes\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef mean_error(y, y_pred):\n    # assuming y and y_pred are numpy arrays\n    return np.mean(y_pred - y)\n\nX, y = load_diabetes(return_X_y=True)\nmean_error_scorer = make_scorer(mean_error, greater_is_better=False)\n\nregr = LinearRegression()\ncross_val_score(regr, X, y, scoring=mean_error_scorer)\n</code></pre>\n",
                        "",
                        "3"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "16366",
            "_score": 23.792849,
            "_source": {
                "title": "Accuracy of machine learning models",
                "content": "Accuracy of machine learning models <p>I started learning ML and I have some problems with evaluating / finding the accuracy of regression and classification models.\nTill now I used <code>.score()</code> in both cases but people told me that Its not the accuracy.\nThen I tried to use this:</p>\n\n<pre><code>from sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n\nvar_train, var_test, res_train, res_test = train_test_split(variables, results, test_size = 0.2, random_state = 4)\n\nregression = linear_model.LinearRegression()\nregression.fit(var_train, res_train)\n\ninput_values = [14, 2]\n\nprediction = regression.predict([input_values])\naccuracy_regression = mean_squared_error(var_test, prediction)\n</code></pre>\n\n<p>But I always get this error:</p>\n\n<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [2, 1]\n</code></pre>\n\n<p>I have looked all over the udemy and youtube, and a lot of people are calculating accuracy like <code>.score()</code>. Then I looked all over <code>scikit-learn</code> website and  stackoverflow and I saw the other solution with metrics but I keep getting the same error.\nWhat am I doing wrong?</p>\n\n<p>More about the problem:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/56622349/accuracy-of-multivariate-classification-and-regression-models-with-scikit-learn\">https://stackoverflow.com/questions/56622349/accuracy-of-multivariate-classification-and-regression-models-with-scikit-learn</a></p>\n <machine-learning><python><scikit-learn><p>Linear regressions are incompatible with accuracy measures. Accuracy is a metric for <strong><em>classification tasks only</em></strong> - it represents the percentage of observations that your model was able to classify correctly.</p>\n\n<p>In case of Linear regression instead, you are predicting a <strong>continuous output</strong>. No accuracy can be computed on this. You need other metrics, such as MSE (the one you used), that can be interpreted as \"<em>how distant you are from perfect prediction</em>\". Sometimes statisticians use the <strong>R-squared</strong> metric, which represents the percentage of the dependent variable's variance that your model is able to explain (i.e.: when all your data are on a straight line, the R-squared is = 1).</p>\n\n<p>The fact that you are getting an error at <code>mean_squared_error()</code> is suggesting me that your input objects (<code>input_values</code> and <code>var_test</code>) must have either: different shapes, and/or contain missing values. In particular, you are only feeding two observations as <code>input_values</code>: <code>[14, 2]</code>. Is <code>var_test</code> a vector of length 2 ?</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "53931",
                "question_votes:": "1",
                "question_text:": "<p>I started learning ML and I have some problems with evaluating / finding the accuracy of regression and classification models.\nTill now I used <code>.score()</code> in both cases but people told me that Its not the accuracy.\nThen I tried to use this:</p>\n\n<pre><code>from sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n\nvar_train, var_test, res_train, res_test = train_test_split(variables, results, test_size = 0.2, random_state = 4)\n\nregression = linear_model.LinearRegression()\nregression.fit(var_train, res_train)\n\ninput_values = [14, 2]\n\nprediction = regression.predict([input_values])\naccuracy_regression = mean_squared_error(var_test, prediction)\n</code></pre>\n\n<p>But I always get this error:</p>\n\n<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [2, 1]\n</code></pre>\n\n<p>I have looked all over the udemy and youtube, and a lot of people are calculating accuracy like <code>.score()</code>. Then I looked all over <code>scikit-learn</code> website and  stackoverflow and I saw the other solution with metrics but I keep getting the same error.\nWhat am I doing wrong?</p>\n\n<p>More about the problem:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/56622349/accuracy-of-multivariate-classification-and-regression-models-with-scikit-learn\">https://stackoverflow.com/questions/56622349/accuracy-of-multivariate-classification-and-regression-models-with-scikit-learn</a></p>\n",
                "tags": "<machine-learning><python><scikit-learn>",
                "answers": [
                    [
                        "53939",
                        "2",
                        "53931",
                        "",
                        "",
                        "<p>Linear regressions are incompatible with accuracy measures. Accuracy is a metric for <strong><em>classification tasks only</em></strong> - it represents the percentage of observations that your model was able to classify correctly.</p>\n\n<p>In case of Linear regression instead, you are predicting a <strong>continuous output</strong>. No accuracy can be computed on this. You need other metrics, such as MSE (the one you used), that can be interpreted as \"<em>how distant you are from perfect prediction</em>\". Sometimes statisticians use the <strong>R-squared</strong> metric, which represents the percentage of the dependent variable's variance that your model is able to explain (i.e.: when all your data are on a straight line, the R-squared is = 1).</p>\n\n<p>The fact that you are getting an error at <code>mean_squared_error()</code> is suggesting me that your input objects (<code>input_values</code> and <code>var_test</code>) must have either: different shapes, and/or contain missing values. In particular, you are only feeding two observations as <code>input_values</code>: <code>[14, 2]</code>. Is <code>var_test</code> a vector of length 2 ?</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "3052",
            "_score": 23.317892,
            "_source": {
                "title": "Regression in Keras",
                "content": "Regression in Keras <p>I was trying to implement a regression model in Keras. But I am unable to figure out how to calculate the score of my model i.e. how well it performed on my dataset.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.cross_validation import cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\n## Load the dataset\ndataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True,header=None)\ndataset = dataframe.values\nX_train = dataset[:400,0:13]\nY_train = dataset[:400,13]\nX_test = dataset[401:,0:13]\nY_test = dataset[401:,13]\n\n##define base model\ndef base_model():\n     model = Sequential()\n     model.add(Dense(14, input_dim=13, init='normal', activation='relu'))\n     model.add(Dense(7, init='normal', activation='relu'))\n     model.add(Dense(1, init='normal'))\n     model.compile(loss='mean_squared_error', optimizer = 'adam')\n     return model\n\nseed = 7\nnp.random.seed(seed)\n\nscale = StandardScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.fit_transform(X_test)\n\nclf = KerasRegressor(build_fn=base_model, nb_epoch=100, batch_size=5,verbose=0)\n\nclf.fit(X_test,Y_test)\nres = clf.predict(X_test)\n\n## line below throws an error\nclf.score(Y_test,res)\n</code></pre>\n\n<p>Please tell me how can I get the score for my model and what mistake am I doing in the above code.</p>\n <machine-learning><python><deep-learning><regression><keras><p>The syntax is not exact, you should pass the features <code>X_test</code> and the true labels <code>Y_test</code> to <code>clt.score</code> (the method performs the prediction on itself, no need to do it explicitly).</p>\n\n<pre><code>score = clf.score(X_test, Y_test)\n</code></pre>\n\n<p>You can also use other metrics available in the <code>metrics</code> module of sklearn. For example,</p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\nscore = mean_squared_error(Y_test, clf.predict(X_test))\n\nfrom sklearn.metrics import mean_absolute_error\nscore = mean_absolute_error(Y_test, clf.predict(X_test))\n</code></pre>\n\n<p>Just some other remarks on your code that are not directly related to the question:</p>\n\n<ul>\n<li><p>you should not call <code>clf.fit</code> on the test data, you should instead fit on the training data and use the test set to compute the score to check the generalization of your model</p></li>\n<li><p>you should fit <code>StandardScaler</code> only on the training data and use <code>X_test = scale.transform(X_test)</code> to apply the same transformation on the test set</p></li>\n</ul>\n",
                "codes": [
                    [
                        "score = clf.score(X_test, Y_test)\n",
                        "from sklearn.metrics import mean_squared_error\nscore = mean_squared_error(Y_test, clf.predict(X_test))\n\nfrom sklearn.metrics import mean_absolute_error\nscore = mean_absolute_error(Y_test, clf.predict(X_test))\n"
                    ]
                ],
                "question_id:": "13350",
                "question_votes:": "5",
                "question_text:": "<p>I was trying to implement a regression model in Keras. But I am unable to figure out how to calculate the score of my model i.e. how well it performed on my dataset.</p>\n\n<pre><code>import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.cross_validation import cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\n## Load the dataset\ndataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True,header=None)\ndataset = dataframe.values\nX_train = dataset[:400,0:13]\nY_train = dataset[:400,13]\nX_test = dataset[401:,0:13]\nY_test = dataset[401:,13]\n\n##define base model\ndef base_model():\n     model = Sequential()\n     model.add(Dense(14, input_dim=13, init='normal', activation='relu'))\n     model.add(Dense(7, init='normal', activation='relu'))\n     model.add(Dense(1, init='normal'))\n     model.compile(loss='mean_squared_error', optimizer = 'adam')\n     return model\n\nseed = 7\nnp.random.seed(seed)\n\nscale = StandardScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.fit_transform(X_test)\n\nclf = KerasRegressor(build_fn=base_model, nb_epoch=100, batch_size=5,verbose=0)\n\nclf.fit(X_test,Y_test)\nres = clf.predict(X_test)\n\n## line below throws an error\nclf.score(Y_test,res)\n</code></pre>\n\n<p>Please tell me how can I get the score for my model and what mistake am I doing in the above code.</p>\n",
                "tags": "<machine-learning><python><deep-learning><regression><keras>",
                "answers": [
                    [
                        "13351",
                        "2",
                        "13350",
                        "",
                        "",
                        "<p>The syntax is not exact, you should pass the features <code>X_test</code> and the true labels <code>Y_test</code> to <code>clt.score</code> (the method performs the prediction on itself, no need to do it explicitly).</p>\n\n<pre><code>score = clf.score(X_test, Y_test)\n</code></pre>\n\n<p>You can also use other metrics available in the <code>metrics</code> module of sklearn. For example,</p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\nscore = mean_squared_error(Y_test, clf.predict(X_test))\n\nfrom sklearn.metrics import mean_absolute_error\nscore = mean_absolute_error(Y_test, clf.predict(X_test))\n</code></pre>\n\n<p>Just some other remarks on your code that are not directly related to the question:</p>\n\n<ul>\n<li><p>you should not call <code>clf.fit</code> on the test data, you should instead fit on the training data and use the test set to compute the score to check the generalization of your model</p></li>\n<li><p>you should fit <code>StandardScaler</code> only on the training data and use <code>X_test = scale.transform(X_test)</code> to apply the same transformation on the test set</p></li>\n</ul>\n",
                        "",
                        "7"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "15744",
            "_score": 23.274563,
            "_source": {
                "title": "Normalized metric for comparing regression models performance",
                "content": "Normalized metric for comparing regression models performance <p>I was recently trying to explain to someone whether performance of my estimation approach is good or bad. For instance, whether a model with <em>Mean Absolute Error</em> (MAE) of 17000 is a bad solution. It was also hard for me to explain whether performance loss by 225 (in terms of MAE), when switching from one model to another, is significant or not.</p>\n\n<p>To me it was clear that both are little because I knew the context: we we're talking about predicting house prices ranging from \\$34,900 (min) to \\$755,000 (max), so that</p>\n\n<ol>\n<li>MAE=17,000 is just 2.5% of the difference between max and min</li>\n<li>Change in MAE by \\$225 is just 0.03% of the difference between max and min.</li>\n</ol>\n\n<p>Are there some normalized metrics for comparing performance of regression models without the need to know the context? </p>\n\n<p>Which of those metrics are available in <a href=\"https://scikit-learn.org/\" rel=\"nofollow noreferrer\">scikit-learn</a>? For instance, it provides <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\" rel=\"nofollow noreferrer\"><code>mean_absolute_error</code></a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\" rel=\"nofollow noreferrer\"><code>mean_squared_error</code></a> but they are not normalized.</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>After Joe B suggestions, I've plotted a graph to see deviations between predicted and expected price. In fact, that's gives more insight than a single-number metric:</p>\n\n<p><a href=\"https://i.stack.imgur.com/S8lUd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/S8lUd.png\" alt=\"Plot\"></a></p>\n <scikit-learn><evaluation><performance><h1>A couple suggestions</h1>\n\n<ol>\n<li><p>MAE represents your <em>mean</em> error. This is essential to recognize as having an error of even half of your MAE on the low end of your spectrum ($34,900) is huge. However, to your point, at the high end of your spectrum it is quite small.</p></li>\n<li><p>To solve the above, you should plot a graph to identify the patterns in your error. You'll plot your <code>predicted y value</code> vs. your <code>actual y value</code> and see where your predictions deviate from the <code>y=x</code> line.</p></li>\n</ol>\n\n<h1>Normalized metrics:</h1>\n\n<p>you may use an r-squared value. This might be ideal because its looking at the percent of variance explained by your regression and thus is a relative measure of fit. Your r-squared value will always be between 0 and 1 (assuming your model is not worse than always guessing your mean value) where 1 is perfect. Implement as follows:</p>\n\n<pre><code>from sklearn.metrics import r2_score\ny_pred = model.predict(X_train)\nprint(r2_score(y_true, y_pred))\n</code></pre>\n",
                "codes": [
                    [
                        "from sklearn.metrics import r2_score\ny_pred = model.predict(X_train)\nprint(r2_score(y_true, y_pred))\n"
                    ]
                ],
                "question_id:": "52554",
                "question_votes:": "",
                "question_text:": "<p>I was recently trying to explain to someone whether performance of my estimation approach is good or bad. For instance, whether a model with <em>Mean Absolute Error</em> (MAE) of 17000 is a bad solution. It was also hard for me to explain whether performance loss by 225 (in terms of MAE), when switching from one model to another, is significant or not.</p>\n\n<p>To me it was clear that both are little because I knew the context: we we're talking about predicting house prices ranging from \\$34,900 (min) to \\$755,000 (max), so that</p>\n\n<ol>\n<li>MAE=17,000 is just 2.5% of the difference between max and min</li>\n<li>Change in MAE by \\$225 is just 0.03% of the difference between max and min.</li>\n</ol>\n\n<p>Are there some normalized metrics for comparing performance of regression models without the need to know the context? </p>\n\n<p>Which of those metrics are available in <a href=\"https://scikit-learn.org/\" rel=\"nofollow noreferrer\">scikit-learn</a>? For instance, it provides <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\" rel=\"nofollow noreferrer\"><code>mean_absolute_error</code></a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\" rel=\"nofollow noreferrer\"><code>mean_squared_error</code></a> but they are not normalized.</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>After Joe B suggestions, I've plotted a graph to see deviations between predicted and expected price. In fact, that's gives more insight than a single-number metric:</p>\n\n<p><a href=\"https://i.stack.imgur.com/S8lUd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/S8lUd.png\" alt=\"Plot\"></a></p>\n",
                "tags": "<scikit-learn><evaluation><performance>",
                "answers": [
                    [
                        "52558",
                        "2",
                        "52554",
                        "",
                        "",
                        "<h1>A couple suggestions</h1>\n\n<ol>\n<li><p>MAE represents your <em>mean</em> error. This is essential to recognize as having an error of even half of your MAE on the low end of your spectrum ($34,900) is huge. However, to your point, at the high end of your spectrum it is quite small.</p></li>\n<li><p>To solve the above, you should plot a graph to identify the patterns in your error. You'll plot your <code>predicted y value</code> vs. your <code>actual y value</code> and see where your predictions deviate from the <code>y=x</code> line.</p></li>\n</ol>\n\n<h1>Normalized metrics:</h1>\n\n<p>you may use an r-squared value. This might be ideal because its looking at the percent of variance explained by your regression and thus is a relative measure of fit. Your r-squared value will always be between 0 and 1 (assuming your model is not worse than always guessing your mean value) where 1 is perfect. Implement as follows:</p>\n\n<pre><code>from sklearn.metrics import r2_score\ny_pred = model.predict(X_train)\nprint(r2_score(y_true, y_pred))\n</code></pre>\n",
                        "",
                        "3"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "2994",
            "_score": 22.316982,
            "_source": {
                "title": "Nested cross-validation and selecting the best regression model - is this the right SKLearn process?",
                "content": "Nested cross-validation and selecting the best regression model - is this the right SKLearn process? <p>If I understand correctly, nested-CV can help me evaluate what model and hyperparameter tuning process is best. The inner loop (<code>GridSearchCV</code>) finds the best hyperparameters, and the outter loop (<code>cross_val_score</code>) evaluates the hyperparameter tuning algorithm. I then choose which tuning/model combo from the outer loop that minimizes <code>mse</code> (I'm looking at regression classifier) for my final model test. </p>\n\n<p>I've read the questions/answers on nested-cross-validation, but haven't seen an example of a full pipeline that utilizes this. So, does my code below (please ignore the actual hyperparameter ranges - this is just for example) and thought process make sense?</p>\n\n<pre><code>from sklearn.cross_validation import cross_val_score, train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# create some regression data\nX, y = make_regression(n_samples=1000, n_features=10)\nparams = [{'C':[0.01,0.05,0.1,1]},{'n_estimators':[10,100,1000]}]\n\n# setup models, variables\nmean_score = []\nmodels = [SVR(), RandomForestRegressor()]\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3)\n\n# estimate performance of hyperparameter tuning and model algorithm pipeline\nfor idx, model in enumerate(models):\n    clf = GridSearchCV(model, params[idx], scoring='mean_squared_error')\n\n    # this performs a nested CV in SKLearn\n    score = cross_val_score(clf, X_train, y_train, scoring='mean_squared_error')\n\n    # get the mean MSE across each fold\n    mean_score.append(np.mean(score))\n    print('Model:', model, 'MSE:', mean_score[-1])\n\n# estimate generalization performance of the best model selection technique\nbest_idx = mean_score.index(max(mean_score)) # because SKLearn flips MSE signs, max works OK here\nbest_model = models[best_idx]\n\nclf_final = GridSearchCV(best_model, params[best_idx])\nclf_final.fit(X_train, y_train)\n\ny_pred = clf_final.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint('Final Model': best_model, 'Final model RMSE:', rmse)\n</code></pre>\n <python><scikit-learn><cross-validation><model-selection><p>Nested cross validation estimates the generalization error of a model, so it is a good way to choose the best model from a list of candidate models and their associated parameter grids. The original post is close to doing nested CV: rather than doing a single train\u2013test split, one should instead use a second cross-validation splitter. That is, one \"nests\" an \"inner\" cross-validation splitter inside an \"outer\" cross validation splitter. </p>\n\n<p>The inner cross-validation splitter is used to choose hyperparameters. The outer cross-validation splitter averages the test error over multiple train\u2013test splits. Averaging the generalization error over multiple train\u2013test splits provides a more reliable estimate of the accuracy of the model on unseen data.</p>\n\n<p>I modified the original post's code to update it to the latest version of <code>sklearn</code> (with <code>sklearn.cross_validation</code> superseded by <code>sklearn.model_selection</code> and with <code>'mean_squared_error'</code> replaced by <code>'neg_mean_squared_error'</code>), and I used two <code>KFold</code> cross-validation splitters to select the best model. To learn more about nested cross validation, see the <code>sklearn</code>'s <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\" rel=\"nofollow noreferrer\">example on nested cross-validation</a>. </p>\n\n<pre><code>from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\n# `outer_cv` creates 3 folds for estimating generalization error\nouter_cv = KFold(3)\n\n# when we train on a certain fold, we use a second cross-validation\n# split in order to choose hyperparameters\ninner_cv = KFold(3)\n\n# create some regression data\nX, y = make_regression(n_samples=1000, n_features=10)\n\n# give shorthand names to models and use those as dictionary keys mapping\n# to models and parameter grids for that model\nmodels_and_parameters = {\n    'svr': (SVR(),\n            {'C': [0.01, 0.05, 0.1, 1]}),\n    'rf': (RandomForestRegressor(),\n           {'max_depth': [5, 10, 50, 100, 200, 500]})}\n\n# we will collect the average of the scores on the 3 outer folds in this dictionary\n# with keys given by the names of the models in `models_and_parameters`\naverage_scores_across_outer_folds_for_each_model = dict()\n\n# find the model with the best generalization error\nfor name, (model, params) in models_and_parameters.items():\n    # this object is a regressor that also happens to choose\n    # its hyperparameters automatically using `inner_cv`\n    regressor_that_optimizes_its_hyperparams = GridSearchCV(\n        estimator=model, param_grid=params,\n        cv=inner_cv, scoring='neg_mean_squared_error')\n\n    # estimate generalization error on the 3-fold splits of the data\n    scores_across_outer_folds = cross_val_score(\n        regressor_that_optimizes_its_hyperparams,\n        X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n\n    # get the mean MSE across each of outer_cv's 3 folds\n    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n    error_summary = 'Model: {name}\\nMSE in the 3 outer folds: {scores}.\\nAverage error: {avg}'\n    print(error_summary.format(\n        name=name, scores=scores_across_outer_folds,\n        avg=np.mean(scores_across_outer_folds)))\n    print()\n\nprint('Average score across the outer folds: ',\n      average_scores_across_outer_folds_for_each_model)\n\nmany_stars = '\\n' + '*' * 100 + '\\n'\nprint(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n\nbest_model_name, best_model_avg_score = max(\n    average_scores_across_outer_folds_for_each_model.items(),\n    key=(lambda name_averagescore: name_averagescore[1]))\n\n# get the best model and its associated parameter grid\nbest_model, best_model_params = models_and_parameters[best_model_name]\n\n# now we refit this best model on the whole dataset so that we can start\n# making predictions on other data, and now we have a reliable estimate of\n# this model's generalization error and we are confident this is the best model\n# among the ones we have tried\nfinal_regressor = GridSearchCV(best_model, best_model_params, cv=inner_cv)\nfinal_regressor.fit(X, y)\n\nprint('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\nprint('Estimation of its generalization error (negative mean squared error):\\n\\t{}'.format(\n    best_model_avg_score), end='\\n\\n')\nprint('Best parameter choice for this model: \\n\\t{params}'\n      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n      params=final_regressor.best_params_, cv=inner_cv))\n</code></pre>\n<p>You do not need</p>\n\n<pre><code># this performs a nested CV in SKLearn\nscore = cross_val_score(clf, X_train, y_train, scoring='mean_squared_error')\n</code></pre>\n\n<p><code>GridSearchCV</code> does this for you. To get intuition of the grid-search process, try to use  <code>GridSearchCV(... , verbose=3)</code></p>\n\n<p>To extract scores for each fold see <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\" rel=\"nofollow\">this example in scikit-learn documentation</a></p>\n<p>Yours is not an example of nested cross-validation.</p>\n\n<p>Nested cross-validation is useful to figure out whether, say, a random forest or a SVM is better suited for your problem. Nested CV only outputs a score, it does not output a model like in your code.</p>\n\n<p>This would be an example of nested cross validation:</p>\n\n<pre><code>from sklearn.datasets import load_boston\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\nparams = [{'C': [0.01, 0.05, 0.1, 1]}, {'n_estimators': [10, 100, 1000]}]\nmodels = [SVR(), RandomForestRegressor()]\n\ndf = load_boston()\nX = df['data']\ny = df['target']\n\ncv = [[] for _ in range(len(models))]\nfor tr, ts in KFold(len(X)):\n    for i, (model, param) in enumerate(zip(models, params)):\n        best_m = GridSearchCV(model, param)\n        best_m.fit(X[tr], y[tr])\n        s = mean_squared_error(y[ts], best_m.predict(X[ts]))\n        cv[i].append(s)\nprint(np.mean(cv, 1))\n</code></pre>\n\n<p>By the way, a couple of thoughts:</p>\n\n<ul>\n<li>I see no purpose to grid search for <code>n_estimators</code> for your random forest. Obviously, the more, the merrier. Things like <code>max_depth</code> is the kind of regularization that you want to optimize. The error for the nested CV of <code>RandomForest</code> was much higher because you did not optimize for the right hyperparameters, not necessarily because it is a worse model.</li>\n<li>You might also want to try gradient boosting trees.</li>\n</ul>\n",
                "codes": [
                    [
                        "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\n# `outer_cv` creates 3 folds for estimating generalization error\nouter_cv = KFold(3)\n\n# when we train on a certain fold, we use a second cross-validation\n# split in order to choose hyperparameters\ninner_cv = KFold(3)\n\n# create some regression data\nX, y = make_regression(n_samples=1000, n_features=10)\n\n# give shorthand names to models and use those as dictionary keys mapping\n# to models and parameter grids for that model\nmodels_and_parameters = {\n    'svr': (SVR(),\n            {'C': [0.01, 0.05, 0.1, 1]}),\n    'rf': (RandomForestRegressor(),\n           {'max_depth': [5, 10, 50, 100, 200, 500]})}\n\n# we will collect the average of the scores on the 3 outer folds in this dictionary\n# with keys given by the names of the models in `models_and_parameters`\naverage_scores_across_outer_folds_for_each_model = dict()\n\n# find the model with the best generalization error\nfor name, (model, params) in models_and_parameters.items():\n    # this object is a regressor that also happens to choose\n    # its hyperparameters automatically using `inner_cv`\n    regressor_that_optimizes_its_hyperparams = GridSearchCV(\n        estimator=model, param_grid=params,\n        cv=inner_cv, scoring='neg_mean_squared_error')\n\n    # estimate generalization error on the 3-fold splits of the data\n    scores_across_outer_folds = cross_val_score(\n        regressor_that_optimizes_its_hyperparams,\n        X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n\n    # get the mean MSE across each of outer_cv's 3 folds\n    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n    error_summary = 'Model: {name}\\nMSE in the 3 outer folds: {scores}.\\nAverage error: {avg}'\n    print(error_summary.format(\n        name=name, scores=scores_across_outer_folds,\n        avg=np.mean(scores_across_outer_folds)))\n    print()\n\nprint('Average score across the outer folds: ',\n      average_scores_across_outer_folds_for_each_model)\n\nmany_stars = '\\n' + '*' * 100 + '\\n'\nprint(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n\nbest_model_name, best_model_avg_score = max(\n    average_scores_across_outer_folds_for_each_model.items(),\n    key=(lambda name_averagescore: name_averagescore[1]))\n\n# get the best model and its associated parameter grid\nbest_model, best_model_params = models_and_parameters[best_model_name]\n\n# now we refit this best model on the whole dataset so that we can start\n# making predictions on other data, and now we have a reliable estimate of\n# this model's generalization error and we are confident this is the best model\n# among the ones we have tried\nfinal_regressor = GridSearchCV(best_model, best_model_params, cv=inner_cv)\nfinal_regressor.fit(X, y)\n\nprint('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\nprint('Estimation of its generalization error (negative mean squared error):\\n\\t{}'.format(\n    best_model_avg_score), end='\\n\\n')\nprint('Best parameter choice for this model: \\n\\t{params}'\n      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n      params=final_regressor.best_params_, cv=inner_cv))\n"
                    ],
                    [
                        "# this performs a nested CV in SKLearn\nscore = cross_val_score(clf, X_train, y_train, scoring='mean_squared_error')\n"
                    ],
                    [
                        "from sklearn.datasets import load_boston\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\nparams = [{'C': [0.01, 0.05, 0.1, 1]}, {'n_estimators': [10, 100, 1000]}]\nmodels = [SVR(), RandomForestRegressor()]\n\ndf = load_boston()\nX = df['data']\ny = df['target']\n\ncv = [[] for _ in range(len(models))]\nfor tr, ts in KFold(len(X)):\n    for i, (model, param) in enumerate(zip(models, params)):\n        best_m = GridSearchCV(model, param)\n        best_m.fit(X[tr], y[tr])\n        s = mean_squared_error(y[ts], best_m.predict(X[ts]))\n        cv[i].append(s)\nprint(np.mean(cv, 1))\n"
                    ]
                ],
                "question_id:": "13185",
                "question_votes:": "8",
                "question_text:": "<p>If I understand correctly, nested-CV can help me evaluate what model and hyperparameter tuning process is best. The inner loop (<code>GridSearchCV</code>) finds the best hyperparameters, and the outter loop (<code>cross_val_score</code>) evaluates the hyperparameter tuning algorithm. I then choose which tuning/model combo from the outer loop that minimizes <code>mse</code> (I'm looking at regression classifier) for my final model test. </p>\n\n<p>I've read the questions/answers on nested-cross-validation, but haven't seen an example of a full pipeline that utilizes this. So, does my code below (please ignore the actual hyperparameter ranges - this is just for example) and thought process make sense?</p>\n\n<pre><code>from sklearn.cross_validation import cross_val_score, train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\n# create some regression data\nX, y = make_regression(n_samples=1000, n_features=10)\nparams = [{'C':[0.01,0.05,0.1,1]},{'n_estimators':[10,100,1000]}]\n\n# setup models, variables\nmean_score = []\nmodels = [SVR(), RandomForestRegressor()]\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3)\n\n# estimate performance of hyperparameter tuning and model algorithm pipeline\nfor idx, model in enumerate(models):\n    clf = GridSearchCV(model, params[idx], scoring='mean_squared_error')\n\n    # this performs a nested CV in SKLearn\n    score = cross_val_score(clf, X_train, y_train, scoring='mean_squared_error')\n\n    # get the mean MSE across each fold\n    mean_score.append(np.mean(score))\n    print('Model:', model, 'MSE:', mean_score[-1])\n\n# estimate generalization performance of the best model selection technique\nbest_idx = mean_score.index(max(mean_score)) # because SKLearn flips MSE signs, max works OK here\nbest_model = models[best_idx]\n\nclf_final = GridSearchCV(best_model, params[best_idx])\nclf_final.fit(X_train, y_train)\n\ny_pred = clf_final.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint('Final Model': best_model, 'Final model RMSE:', rmse)\n</code></pre>\n",
                "tags": "<python><scikit-learn><cross-validation><model-selection>",
                "answers": [
                    [
                        "16856",
                        "2",
                        "13185",
                        "",
                        "",
                        "<p>Nested cross validation estimates the generalization error of a model, so it is a good way to choose the best model from a list of candidate models and their associated parameter grids. The original post is close to doing nested CV: rather than doing a single train\u2013test split, one should instead use a second cross-validation splitter. That is, one \"nests\" an \"inner\" cross-validation splitter inside an \"outer\" cross validation splitter. </p>\n\n<p>The inner cross-validation splitter is used to choose hyperparameters. The outer cross-validation splitter averages the test error over multiple train\u2013test splits. Averaging the generalization error over multiple train\u2013test splits provides a more reliable estimate of the accuracy of the model on unseen data.</p>\n\n<p>I modified the original post's code to update it to the latest version of <code>sklearn</code> (with <code>sklearn.cross_validation</code> superseded by <code>sklearn.model_selection</code> and with <code>'mean_squared_error'</code> replaced by <code>'neg_mean_squared_error'</code>), and I used two <code>KFold</code> cross-validation splitters to select the best model. To learn more about nested cross validation, see the <code>sklearn</code>'s <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html\" rel=\"nofollow noreferrer\">example on nested cross-validation</a>. </p>\n\n<pre><code>from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\n# `outer_cv` creates 3 folds for estimating generalization error\nouter_cv = KFold(3)\n\n# when we train on a certain fold, we use a second cross-validation\n# split in order to choose hyperparameters\ninner_cv = KFold(3)\n\n# create some regression data\nX, y = make_regression(n_samples=1000, n_features=10)\n\n# give shorthand names to models and use those as dictionary keys mapping\n# to models and parameter grids for that model\nmodels_and_parameters = {\n    'svr': (SVR(),\n            {'C': [0.01, 0.05, 0.1, 1]}),\n    'rf': (RandomForestRegressor(),\n           {'max_depth': [5, 10, 50, 100, 200, 500]})}\n\n# we will collect the average of the scores on the 3 outer folds in this dictionary\n# with keys given by the names of the models in `models_and_parameters`\naverage_scores_across_outer_folds_for_each_model = dict()\n\n# find the model with the best generalization error\nfor name, (model, params) in models_and_parameters.items():\n    # this object is a regressor that also happens to choose\n    # its hyperparameters automatically using `inner_cv`\n    regressor_that_optimizes_its_hyperparams = GridSearchCV(\n        estimator=model, param_grid=params,\n        cv=inner_cv, scoring='neg_mean_squared_error')\n\n    # estimate generalization error on the 3-fold splits of the data\n    scores_across_outer_folds = cross_val_score(\n        regressor_that_optimizes_its_hyperparams,\n        X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n\n    # get the mean MSE across each of outer_cv's 3 folds\n    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n    error_summary = 'Model: {name}\\nMSE in the 3 outer folds: {scores}.\\nAverage error: {avg}'\n    print(error_summary.format(\n        name=name, scores=scores_across_outer_folds,\n        avg=np.mean(scores_across_outer_folds)))\n    print()\n\nprint('Average score across the outer folds: ',\n      average_scores_across_outer_folds_for_each_model)\n\nmany_stars = '\\n' + '*' * 100 + '\\n'\nprint(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n\nbest_model_name, best_model_avg_score = max(\n    average_scores_across_outer_folds_for_each_model.items(),\n    key=(lambda name_averagescore: name_averagescore[1]))\n\n# get the best model and its associated parameter grid\nbest_model, best_model_params = models_and_parameters[best_model_name]\n\n# now we refit this best model on the whole dataset so that we can start\n# making predictions on other data, and now we have a reliable estimate of\n# this model's generalization error and we are confident this is the best model\n# among the ones we have tried\nfinal_regressor = GridSearchCV(best_model, best_model_params, cv=inner_cv)\nfinal_regressor.fit(X, y)\n\nprint('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\nprint('Estimation of its generalization error (negative mean squared error):\\n\\t{}'.format(\n    best_model_avg_score), end='\\n\\n')\nprint('Best parameter choice for this model: \\n\\t{params}'\n      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n      params=final_regressor.best_params_, cv=inner_cv))\n</code></pre>\n",
                        "",
                        "7"
                    ],
                    [
                        "13226",
                        "2",
                        "13185",
                        "",
                        "",
                        "<p>You do not need</p>\n\n<pre><code># this performs a nested CV in SKLearn\nscore = cross_val_score(clf, X_train, y_train, scoring='mean_squared_error')\n</code></pre>\n\n<p><code>GridSearchCV</code> does this for you. To get intuition of the grid-search process, try to use  <code>GridSearchCV(... , verbose=3)</code></p>\n\n<p>To extract scores for each fold see <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\" rel=\"nofollow\">this example in scikit-learn documentation</a></p>\n",
                        "",
                        "1"
                    ],
                    [
                        "13232",
                        "2",
                        "13185",
                        "",
                        "",
                        "<p>Yours is not an example of nested cross-validation.</p>\n\n<p>Nested cross-validation is useful to figure out whether, say, a random forest or a SVM is better suited for your problem. Nested CV only outputs a score, it does not output a model like in your code.</p>\n\n<p>This would be an example of nested cross validation:</p>\n\n<pre><code>from sklearn.datasets import load_boston\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport numpy as np\n\nparams = [{'C': [0.01, 0.05, 0.1, 1]}, {'n_estimators': [10, 100, 1000]}]\nmodels = [SVR(), RandomForestRegressor()]\n\ndf = load_boston()\nX = df['data']\ny = df['target']\n\ncv = [[] for _ in range(len(models))]\nfor tr, ts in KFold(len(X)):\n    for i, (model, param) in enumerate(zip(models, params)):\n        best_m = GridSearchCV(model, param)\n        best_m.fit(X[tr], y[tr])\n        s = mean_squared_error(y[ts], best_m.predict(X[ts]))\n        cv[i].append(s)\nprint(np.mean(cv, 1))\n</code></pre>\n\n<p>By the way, a couple of thoughts:</p>\n\n<ul>\n<li>I see no purpose to grid search for <code>n_estimators</code> for your random forest. Obviously, the more, the merrier. Things like <code>max_depth</code> is the kind of regularization that you want to optimize. The error for the nested CV of <code>RandomForest</code> was much higher because you did not optimize for the right hyperparameters, not necessarily because it is a worse model.</li>\n<li>You might also want to try gradient boosting trees.</li>\n</ul>\n",
                        "",
                        "9"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "7917",
            "_score": 20.808617,
            "_source": {
                "title": "high root mean square error in regression model",
                "content": "high root mean square error in regression model <p>I am applying regression to a data of 110 rows and 7 columns ,each having targets. When I applied Lasso to the data and calculated the RMSE value ,the RMSE value is coming to be 13.11.I think the RMSE value should be close to zero.What is the permissible values of RMSE in a regression model.What could have gone wrong in the computation.</p>\n\n<pre><code>from sklearn import linear_model\nreg = linear_model.Lasso(alpha = .00001)\nreg.fit(Xt,Yt)\nans=reg.predict(Xts)\nprint(ans)\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(Yts, ans))\n</code></pre>\n\n<p>whereas when I try cross validation the MSE scores are way below 0.35</p>\n\n<pre><code>kfold = KFold(n_splits=10)\nresults = cross_val_score(reg, full_data, target, cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\nresults\nResults: -0.13 (0.45) MSE\n</code></pre>\n <machine-learning><scikit-learn><regression><p>RMSE doesn't work that way. An RMSE of 13 might actually great, it completely depends on how your target variable is scaled. For example, if your target variable was in the range [0,1e9], than an RMSE of 13 is spectacular. On the other hand, if your target is in the range [0,1], an RMSE of 0.5 is terrible. If you want to try a metric that can be more readily interpretable as having a \"good\" or \"bad\" score, try Mean Average Percent Error (MAPE).</p>\n\n<p>As far as why you get a lower MSE when you cross validate: you don't show us how you constructed your training and test sets, but my guess is that you basically just got unlucky and ended up with a training/test split that performed poorly on your holdout set. Your CV-MSE is clearly better than your single holdout MSE, but you should also check the spread of CV scores as well. In any event, for a dataset as small as yours I'd recommend using bootstrap cross validation instead of k-fold.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "29293",
                "question_votes:": "2",
                "question_text:": "<p>I am applying regression to a data of 110 rows and 7 columns ,each having targets. When I applied Lasso to the data and calculated the RMSE value ,the RMSE value is coming to be 13.11.I think the RMSE value should be close to zero.What is the permissible values of RMSE in a regression model.What could have gone wrong in the computation.</p>\n\n<pre><code>from sklearn import linear_model\nreg = linear_model.Lasso(alpha = .00001)\nreg.fit(Xt,Yt)\nans=reg.predict(Xts)\nprint(ans)\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(Yts, ans))\n</code></pre>\n\n<p>whereas when I try cross validation the MSE scores are way below 0.35</p>\n\n<pre><code>kfold = KFold(n_splits=10)\nresults = cross_val_score(reg, full_data, target, cv=kfold)\nprint(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\nresults\nResults: -0.13 (0.45) MSE\n</code></pre>\n",
                "tags": "<machine-learning><scikit-learn><regression>",
                "answers": [
                    [
                        "29294",
                        "2",
                        "29293",
                        "",
                        "",
                        "<p>RMSE doesn't work that way. An RMSE of 13 might actually great, it completely depends on how your target variable is scaled. For example, if your target variable was in the range [0,1e9], than an RMSE of 13 is spectacular. On the other hand, if your target is in the range [0,1], an RMSE of 0.5 is terrible. If you want to try a metric that can be more readily interpretable as having a \"good\" or \"bad\" score, try Mean Average Percent Error (MAPE).</p>\n\n<p>As far as why you get a lower MSE when you cross validate: you don't show us how you constructed your training and test sets, but my guess is that you basically just got unlucky and ended up with a training/test split that performed poorly on your holdout set. Your CV-MSE is clearly better than your single holdout MSE, but you should also check the spread of CV scores as well. In any event, for a dataset as small as yours I'd recommend using bootstrap cross validation instead of k-fold.</p>\n",
                        "",
                        "5"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "13383",
            "_score": 20.80204,
            "_source": {
                "title": "MAE,MSE and MAPE aren't comparable?",
                "content": "MAE,MSE and MAPE aren't comparable? <p>I'm a newbie in data science. I'm working on a regression problem. I'm getting 2.5 MAPE. 400 MAE 437000 MSE. As my MAPE is quite low but why I'm getting high MSE and MAE?  <a href=\"https://drive.google.com/open?id=1o8VPhpHZ17io5mTD2sj4GchK4HeMwGvY\" rel=\"nofollow noreferrer\">This</a> is the link to my data</p>\n\n<pre><code>from sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\nimport pandas as pd\nfrom sklearn import preprocessing\n\nfeatures=pd.read_csv('selectedData.csv')\nimport numpy as np\nfrom scipy import stats\nprint(features.shape)\nfeatures=features[(np.abs(stats.zscore(features)) &lt; 3).all(axis=1)]\ntarget = features['SYSLoad']\nfeatures= features.drop('SYSLoad', axis = 1)\nnames=list(features)\n\nfor i in names:\n    x=features[[i]].values.astype(float)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    features[i]=x_scaled\n</code></pre>\n\n<h1>Selecting the target Variable which want to predict and for which we are finding feature imps</h1>\n\n<pre><code>import numpy as np\nprint(features.shape)\nprint(features.describe())\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = \ntrain_test_split(features, target, test_size = 0.25, random_state = 42)\ntrans=Normalizer().fit(train_input);\ntrain_input=Normalizer().fit_transform(train_input);\ntest_input=trans.fit_transform(test_input);\n\nn=test_target.values;\ntest_targ=pd.DataFrame(n);\n\nfrom sklearn.svm import SVR\nsvr_rbf = SVR(kernel='poly', C=10, epsilon=10,gamma=10)\ny_rbf = svr_rbf.fit(train_input, train_target);\npredicted=y_rbf.predict(test_input);\nplt.figure\nplt.xlim(300,500);\nprint('Total Days For training',len(train_input)); print('Total Days For \nTesting',len(test_input))\nplt.ylabel('Load(MW) Prediction 3 '); plt.xlabel('Days'); \nplt.plot(test_targ,'-b',label='Actual'); plt.plot(predicted,'-r',label='POLY \nkernel ');\nplt.gca().legend(('Actual','RBF'))\nplt.title('SVM')\nplt.show();\n\n\ntest_target=np.array(test_target)\nprint(test_target)\nMAPE=mean_absolute_percentage_error(test_target,predicted);\nprint(MAPE);\nmae=mean_absolute_error(test_target,predicted)\nmse=mean_squared_error(test_target, predicted)\nprint(mae);\nprint(mse);\nprint(test_target);\nprint(predicted);\n</code></pre>\n <data-mining><pandas><svm><numpy><p>You are stating something that is by definition the case.  A Mean Absolute Percentage Error (MAPE) is typically a small number expressed in percentage, hopefully in the single digit.  Meanwhile the Mean Squared Error (MSE) and Mean Absolute Error) are expressed in square of units and units respectively.  If your units are > 1, the MSE could get easily very large on a relative basis compared to the MAE.  And, the MAE could also be a lot larger than the MAPE.  This is just like saying a nominal number is a lot larger than its log or natural logs.  Your three error measures are measured on pretty different scale.</p>\n\n<p>They just bring some perspective to how well your model fit your data.  Depending on the circumstances or context one error measure may be more relevant than the other.  </p>\n<p>I'll be honest, I haven't thoroughly checked your code. However, I can see that the range of values of your dataset is approx <strong>[0,12000]</strong>. As an engineer, I see that:</p>\n\n<ol>\n<li>sqrt(MSE) = sqrt(437000) = 661 units.</li>\n<li>MAE = 400 units.</li>\n<li>MAPE = 2.5 which means that MAE can be up to 0.025*12000= 250 units.</li>\n</ol>\n\n<p>All three cases show similar magnitude of error, so I wouldn't say that \"MAPE is quite low but you're getting high mse and MAE\". </p>\n\n<p>Those 3 values explain the results from similar yet different perspectives. Keep in mind, if the values were all the same, there would have been no need for all 3 of those metrics to exist :)</p>\n",
                "codes": [
                    [],
                    []
                ],
                "question_id:": "45821",
                "question_votes:": "2",
                "question_text:": "<p>I'm a newbie in data science. I'm working on a regression problem. I'm getting 2.5 MAPE. 400 MAE 437000 MSE. As my MAPE is quite low but why I'm getting high MSE and MAE?  <a href=\"https://drive.google.com/open?id=1o8VPhpHZ17io5mTD2sj4GchK4HeMwGvY\" rel=\"nofollow noreferrer\">This</a> is the link to my data</p>\n\n<pre><code>from sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\nimport pandas as pd\nfrom sklearn import preprocessing\n\nfeatures=pd.read_csv('selectedData.csv')\nimport numpy as np\nfrom scipy import stats\nprint(features.shape)\nfeatures=features[(np.abs(stats.zscore(features)) &lt; 3).all(axis=1)]\ntarget = features['SYSLoad']\nfeatures= features.drop('SYSLoad', axis = 1)\nnames=list(features)\n\nfor i in names:\n    x=features[[i]].values.astype(float)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    features[i]=x_scaled\n</code></pre>\n\n<h1>Selecting the target Variable which want to predict and for which we are finding feature imps</h1>\n\n<pre><code>import numpy as np\nprint(features.shape)\nprint(features.describe())\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = \ntrain_test_split(features, target, test_size = 0.25, random_state = 42)\ntrans=Normalizer().fit(train_input);\ntrain_input=Normalizer().fit_transform(train_input);\ntest_input=trans.fit_transform(test_input);\n\nn=test_target.values;\ntest_targ=pd.DataFrame(n);\n\nfrom sklearn.svm import SVR\nsvr_rbf = SVR(kernel='poly', C=10, epsilon=10,gamma=10)\ny_rbf = svr_rbf.fit(train_input, train_target);\npredicted=y_rbf.predict(test_input);\nplt.figure\nplt.xlim(300,500);\nprint('Total Days For training',len(train_input)); print('Total Days For \nTesting',len(test_input))\nplt.ylabel('Load(MW) Prediction 3 '); plt.xlabel('Days'); \nplt.plot(test_targ,'-b',label='Actual'); plt.plot(predicted,'-r',label='POLY \nkernel ');\nplt.gca().legend(('Actual','RBF'))\nplt.title('SVM')\nplt.show();\n\n\ntest_target=np.array(test_target)\nprint(test_target)\nMAPE=mean_absolute_percentage_error(test_target,predicted);\nprint(MAPE);\nmae=mean_absolute_error(test_target,predicted)\nmse=mean_squared_error(test_target, predicted)\nprint(mae);\nprint(mse);\nprint(test_target);\nprint(predicted);\n</code></pre>\n",
                "tags": "<data-mining><pandas><svm><numpy>",
                "answers": [
                    [
                        "47763",
                        "2",
                        "45821",
                        "",
                        "",
                        "<p>You are stating something that is by definition the case.  A Mean Absolute Percentage Error (MAPE) is typically a small number expressed in percentage, hopefully in the single digit.  Meanwhile the Mean Squared Error (MSE) and Mean Absolute Error) are expressed in square of units and units respectively.  If your units are > 1, the MSE could get easily very large on a relative basis compared to the MAE.  And, the MAE could also be a lot larger than the MAPE.  This is just like saying a nominal number is a lot larger than its log or natural logs.  Your three error measures are measured on pretty different scale.</p>\n\n<p>They just bring some perspective to how well your model fit your data.  Depending on the circumstances or context one error measure may be more relevant than the other.  </p>\n",
                        "",
                        "1"
                    ],
                    [
                        "45825",
                        "2",
                        "45821",
                        "",
                        "",
                        "<p>I'll be honest, I haven't thoroughly checked your code. However, I can see that the range of values of your dataset is approx <strong>[0,12000]</strong>. As an engineer, I see that:</p>\n\n<ol>\n<li>sqrt(MSE) = sqrt(437000) = 661 units.</li>\n<li>MAE = 400 units.</li>\n<li>MAPE = 2.5 which means that MAE can be up to 0.025*12000= 250 units.</li>\n</ol>\n\n<p>All three cases show similar magnitude of error, so I wouldn't say that \"MAPE is quite low but you're getting high mse and MAE\". </p>\n\n<p>Those 3 values explain the results from similar yet different perspectives. Keep in mind, if the values were all the same, there would have been no need for all 3 of those metrics to exist :)</p>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "7465",
            "_score": 20.59703,
            "_source": {
                "title": "Ridge regression - varying alpha and observing the residual",
                "content": "Ridge regression - varying alpha and observing the residual <p>I am trying to reproduce this figure from Bishop:</p>\n\n<p><a href=\"https://i.stack.imgur.com/xkJUH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xkJUH.png\" alt=\"enter image description here\"></a></p>\n\n<p>Residual vs. Alpha (lambda in figure)</p>\n\n<p>The code is pasted below:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\nx_train = np.linspace(0, 1, 10)\nnoise_10 = np.random.normal(0, 0.3, 10)\ny_train = np.sin(2*np.pi*x_train) + noise_10\nx_train = x_train[:, np.newaxis]\n\nx_test = np.linspace(0, 1, 100)\nnoise_100 = np.random.normal(0, 0.3, 100)\ny_test = np.sin(2*np.pi*x_test) + noise_100\nx_test = x_test[:, np.newaxis]\n\n\nn_alphas = 200\nalphas = np.logspace(-40, -18, n_alphas)\n\nerrors = []\nfor a in alphas:\n    ridge = make_pipeline(PolynomialFeatures(degree = 9), \n              Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    mse = mean_squared_error(y_test, ridge.predict(x_test))\n    errors.append(np.sqrt(mse))    \n\nprint(errors)\n</code></pre>\n\n<p>However, the errors array has the same value for all values of alpha. It's taking the first value of <code>alpha = np.exp(-40)</code> and all the other values seem to be the same for all future iterations of the for loop. How can I correct this error?</p>\n <scikit-learn><regression><p>I still haven't figured out what the previously posted code is doing wrong. However, manually populating the alphas array gives me results that are close to the original figure.</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(12344)\nx_1000 = np.linspace(0, 1, 1000)\nx_train = np.linspace(0, 1, 10)\nnoise_10 = np.random.normal(0, 0.3, 10)\ny_train = np.sin(2*np.pi*x_train) + noise_10\nx_train = x_train[:, np.newaxis]\nx_test = np.linspace(0, 1, 100)\nnoise_100 = np.random.normal(0, 0.3, 100)\ny_test = np.sin(2*np.pi*x_test) + noise_100\nx_test = x_test[:, np.newaxis]\nlow_alpha = make_pipeline(PolynomialFeatures(degree = 9), Ridge(alpha=np.exp(-18)))\nlow_alpha.fit(x_test, y_test)\nplt.figure(1)\nplt.plot(x_train, low_alpha.predict(x_train), label = 'alpha = ln(-18)')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nhigh_alpha = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=np.exp(0)))\nhigh_alpha.fit(x_test, y_test)\nplt.figure(2)\nplt.plot(x_train, high_alpha.predict(x_train), label = 'alpha = 1')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nalphas = np.array([np.exp(-30), np.exp(-29), np.exp(-28), np.exp(-27), np.exp(-26), np.exp(-25), np.exp(-24), np.exp(-23), np.exp(-22), np.exp(-21), np.exp(-20), np.exp(-19), np.exp(-18), np.exp(-17), np.exp(-16), np.exp(-15), np.exp(-14), np.exp(-13), np.exp(-12), np.exp(-11), np.exp(-10), np.exp(-9), np.exp(-8), np.exp(-7), np.exp(-6), np.exp(-5), np.exp(-4), np.exp(-3), np.exp(-2), np.exp(-1), np.exp(-1), np.exp(0), np.exp(1)])\n\ntest_errors = []\ntrain_errors = []\n\nfor a in np.nditer(alphas):\n    ridge = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    mse_train = mean_squared_error(y_train, ridge.predict(x_train))\n    mse_test = mean_squared_error(y_test, ridge.predict(x_test))\n    train_errors.append(np.sqrt(mse_train))\n    test_errors.append(np.sqrt(mse_test))\nplt.figure(3)\nplt.plot(alphas, test_errors, 'g^', label = 'Test Error')\nplt.plot(alphas, train_errors, 'bs', label = 'Train Error')\nplt.xscale('log')\nplt.xlabel('Regression coefficient Lambda')\nplt.ylabel('Residuals')\nplt.legend()\nplt.show()\n</code></pre>\n\n<p>The output (only for the third figure) is shown below: <a href=\"https://i.stack.imgur.com/dBKRb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/dBKRb.png\" alt=\"Reproduction of Figure from Bishop\"></a></p>\n<p>I have taken a look on your code. You obtain same errors results for each alpha value because your regularization strength is too small. Replacing : </p>\n\n<pre><code>alphas = np.logspace(-40, -18, n_alphas)\n</code></pre>\n\n<p>with : </p>\n\n<pre><code>alphas = np.logspace(-40, -1, n_alphas)\n</code></pre>\n\n<p>will yields different errors values for alpha values large enough. Are you sure about figure alpha values? Do you have a link to this hands-on?</p>\n\n<p>Also, I would like to highlight the fact that you have to standardize your features before using regularization. Reason is, by creating polynomial features, polynomial features will have different magnitudes. Therefore when fitting model, coefficients to be estimated won't have the same magnitudes neither and so regularization will highly penalize coefficients with large values. Standardization / Normalization is a strong prerequisite to regularization.</p>\n",
                "codes": [
                    [
                        "import numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(12344)\nx_1000 = np.linspace(0, 1, 1000)\nx_train = np.linspace(0, 1, 10)\nnoise_10 = np.random.normal(0, 0.3, 10)\ny_train = np.sin(2*np.pi*x_train) + noise_10\nx_train = x_train[:, np.newaxis]\nx_test = np.linspace(0, 1, 100)\nnoise_100 = np.random.normal(0, 0.3, 100)\ny_test = np.sin(2*np.pi*x_test) + noise_100\nx_test = x_test[:, np.newaxis]\nlow_alpha = make_pipeline(PolynomialFeatures(degree = 9), Ridge(alpha=np.exp(-18)))\nlow_alpha.fit(x_test, y_test)\nplt.figure(1)\nplt.plot(x_train, low_alpha.predict(x_train), label = 'alpha = ln(-18)')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nhigh_alpha = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=np.exp(0)))\nhigh_alpha.fit(x_test, y_test)\nplt.figure(2)\nplt.plot(x_train, high_alpha.predict(x_train), label = 'alpha = 1')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nalphas = np.array([np.exp(-30), np.exp(-29), np.exp(-28), np.exp(-27), np.exp(-26), np.exp(-25), np.exp(-24), np.exp(-23), np.exp(-22), np.exp(-21), np.exp(-20), np.exp(-19), np.exp(-18), np.exp(-17), np.exp(-16), np.exp(-15), np.exp(-14), np.exp(-13), np.exp(-12), np.exp(-11), np.exp(-10), np.exp(-9), np.exp(-8), np.exp(-7), np.exp(-6), np.exp(-5), np.exp(-4), np.exp(-3), np.exp(-2), np.exp(-1), np.exp(-1), np.exp(0), np.exp(1)])\n\ntest_errors = []\ntrain_errors = []\n\nfor a in np.nditer(alphas):\n    ridge = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    mse_train = mean_squared_error(y_train, ridge.predict(x_train))\n    mse_test = mean_squared_error(y_test, ridge.predict(x_test))\n    train_errors.append(np.sqrt(mse_train))\n    test_errors.append(np.sqrt(mse_test))\nplt.figure(3)\nplt.plot(alphas, test_errors, 'g^', label = 'Test Error')\nplt.plot(alphas, train_errors, 'bs', label = 'Train Error')\nplt.xscale('log')\nplt.xlabel('Regression coefficient Lambda')\nplt.ylabel('Residuals')\nplt.legend()\nplt.show()\n"
                    ],
                    [
                        "alphas = np.logspace(-40, -18, n_alphas)\n",
                        "alphas = np.logspace(-40, -1, n_alphas)\n"
                    ]
                ],
                "question_id:": "27918",
                "question_votes:": "2",
                "question_text:": "<p>I am trying to reproduce this figure from Bishop:</p>\n\n<p><a href=\"https://i.stack.imgur.com/xkJUH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xkJUH.png\" alt=\"enter image description here\"></a></p>\n\n<p>Residual vs. Alpha (lambda in figure)</p>\n\n<p>The code is pasted below:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\nx_train = np.linspace(0, 1, 10)\nnoise_10 = np.random.normal(0, 0.3, 10)\ny_train = np.sin(2*np.pi*x_train) + noise_10\nx_train = x_train[:, np.newaxis]\n\nx_test = np.linspace(0, 1, 100)\nnoise_100 = np.random.normal(0, 0.3, 100)\ny_test = np.sin(2*np.pi*x_test) + noise_100\nx_test = x_test[:, np.newaxis]\n\n\nn_alphas = 200\nalphas = np.logspace(-40, -18, n_alphas)\n\nerrors = []\nfor a in alphas:\n    ridge = make_pipeline(PolynomialFeatures(degree = 9), \n              Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    mse = mean_squared_error(y_test, ridge.predict(x_test))\n    errors.append(np.sqrt(mse))    \n\nprint(errors)\n</code></pre>\n\n<p>However, the errors array has the same value for all values of alpha. It's taking the first value of <code>alpha = np.exp(-40)</code> and all the other values seem to be the same for all future iterations of the for loop. How can I correct this error?</p>\n",
                "tags": "<scikit-learn><regression>",
                "answers": [
                    [
                        "28012",
                        "2",
                        "27918",
                        "",
                        "",
                        "<p>I still haven't figured out what the previously posted code is doing wrong. However, manually populating the alphas array gives me results that are close to the original figure.</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(12344)\nx_1000 = np.linspace(0, 1, 1000)\nx_train = np.linspace(0, 1, 10)\nnoise_10 = np.random.normal(0, 0.3, 10)\ny_train = np.sin(2*np.pi*x_train) + noise_10\nx_train = x_train[:, np.newaxis]\nx_test = np.linspace(0, 1, 100)\nnoise_100 = np.random.normal(0, 0.3, 100)\ny_test = np.sin(2*np.pi*x_test) + noise_100\nx_test = x_test[:, np.newaxis]\nlow_alpha = make_pipeline(PolynomialFeatures(degree = 9), Ridge(alpha=np.exp(-18)))\nlow_alpha.fit(x_test, y_test)\nplt.figure(1)\nplt.plot(x_train, low_alpha.predict(x_train), label = 'alpha = ln(-18)')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nhigh_alpha = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=np.exp(0)))\nhigh_alpha.fit(x_test, y_test)\nplt.figure(2)\nplt.plot(x_train, high_alpha.predict(x_train), label = 'alpha = 1')\nplt.plot(x_1000, np.sin(2*np.pi*x_1000))\nplt.legend()\nplt.show\nalphas = np.array([np.exp(-30), np.exp(-29), np.exp(-28), np.exp(-27), np.exp(-26), np.exp(-25), np.exp(-24), np.exp(-23), np.exp(-22), np.exp(-21), np.exp(-20), np.exp(-19), np.exp(-18), np.exp(-17), np.exp(-16), np.exp(-15), np.exp(-14), np.exp(-13), np.exp(-12), np.exp(-11), np.exp(-10), np.exp(-9), np.exp(-8), np.exp(-7), np.exp(-6), np.exp(-5), np.exp(-4), np.exp(-3), np.exp(-2), np.exp(-1), np.exp(-1), np.exp(0), np.exp(1)])\n\ntest_errors = []\ntrain_errors = []\n\nfor a in np.nditer(alphas):\n    ridge = make_pipeline(PolynomialFeatures(degree = 9),\n              Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    mse_train = mean_squared_error(y_train, ridge.predict(x_train))\n    mse_test = mean_squared_error(y_test, ridge.predict(x_test))\n    train_errors.append(np.sqrt(mse_train))\n    test_errors.append(np.sqrt(mse_test))\nplt.figure(3)\nplt.plot(alphas, test_errors, 'g^', label = 'Test Error')\nplt.plot(alphas, train_errors, 'bs', label = 'Train Error')\nplt.xscale('log')\nplt.xlabel('Regression coefficient Lambda')\nplt.ylabel('Residuals')\nplt.legend()\nplt.show()\n</code></pre>\n\n<p>The output (only for the third figure) is shown below: <a href=\"https://i.stack.imgur.com/dBKRb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/dBKRb.png\" alt=\"Reproduction of Figure from Bishop\"></a></p>\n",
                        "",
                        "1"
                    ],
                    [
                        "27951",
                        "2",
                        "27918",
                        "",
                        "",
                        "<p>I have taken a look on your code. You obtain same errors results for each alpha value because your regularization strength is too small. Replacing : </p>\n\n<pre><code>alphas = np.logspace(-40, -18, n_alphas)\n</code></pre>\n\n<p>with : </p>\n\n<pre><code>alphas = np.logspace(-40, -1, n_alphas)\n</code></pre>\n\n<p>will yields different errors values for alpha values large enough. Are you sure about figure alpha values? Do you have a link to this hands-on?</p>\n\n<p>Also, I would like to highlight the fact that you have to standardize your features before using regularization. Reason is, by creating polynomial features, polynomial features will have different magnitudes. Therefore when fitting model, coefficients to be estimated won't have the same magnitudes neither and so regularization will highly penalize coefficients with large values. Standardization / Normalization is a strong prerequisite to regularization.</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "9900",
            "_score": 20.469336,
            "_source": {
                "title": "Predict the accuracy of Linear Regression",
                "content": "Predict the accuracy of Linear Regression <p>How do I test if the predicted values in Linear Regression model are matching with the actuals?</p>\n\n<p>I tried using - Confusion matrix, but I get this error -</p>\n\n<pre><code>#==============================================================================\n# Create confusion matrix to evaluate performance of data\n#==============================================================================\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix (dv_test, y_pred)\n\nprint(confusionMatrix)\n</code></pre>\n\n<p><code>ValueError: Can't handle mix of multiclass and continuous</code></p>\n\n<p>When I execute the below code -</p>\n\n<pre><code>##Performing Linear Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import model_selection\nregressor=LinearRegression()\n##Fit train\nregressor.fit(iv_train,dv_train)\ny_pred=regressor.predict(iv_test)\nprint('Accuracy of LR',mean_squared_error(y_pred,dv_test))\n</code></pre>\n\n<p>It results - Accuracy of LR 7837176694.18</p>\n\n<p>Which is incorrect.</p>\n\n<p>Below is my sample data set -</p>\n\n<pre><code>longitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   ocean_proximity median_house_value\n-122.23 37.88   41  880 129 322 126 8.3252  NEAR BAY    452600\n-122.22 37.86   21  7099    1106    2401    1138    8.3014  NEAR BAY    358500\n-122.24 37.85   52  1467    190 496 177 7.2574  NEAR BAY    352100\n-122.25 37.85   52  1274    235 558 219 5.6431  NEAR BAY    341300\n-122.25 37.85   52  1627    280 565 259 3.8462  NEAR BAY    342200\n-122.25 37.85   52  919 213 413 193 4.0368  NEAR BAY    269700\n-122.25 37.84   52  2535    489 1094    514 3.6591  NEAR BAY    299200\n</code></pre>\n <scikit-learn><linear-regression><p>There are several ways to check your Linear Regression model accuracy. Usually, you may use <strong>Root mean squared error</strong>. You may train several Linear Regression models, adding or removing features to your dataset, and see which one has the lowest <strong>RMSE</strong> - the best one in your case. Also try to normalize your data before fitting into Linear Regression model. </p>\n\n<p>The confusion matrix is used to check discrete results, but Linear Regression model returns predicted result as a continuous values. That is why you get the error: your <code>dv_test</code> data likely is integer, but <code>y_pred</code> is float.</p>\n\n<p>You may try using classification model if it is suitable for the problem you try to solve - depends on what you try to predict. But for regression problem it would be better to use metric mentioned above. </p>\n",
                "codes": [
                    []
                ],
                "question_id:": "36083",
                "question_votes:": "2",
                "question_text:": "<p>How do I test if the predicted values in Linear Regression model are matching with the actuals?</p>\n\n<p>I tried using - Confusion matrix, but I get this error -</p>\n\n<pre><code>#==============================================================================\n# Create confusion matrix to evaluate performance of data\n#==============================================================================\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix (dv_test, y_pred)\n\nprint(confusionMatrix)\n</code></pre>\n\n<p><code>ValueError: Can't handle mix of multiclass and continuous</code></p>\n\n<p>When I execute the below code -</p>\n\n<pre><code>##Performing Linear Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import model_selection\nregressor=LinearRegression()\n##Fit train\nregressor.fit(iv_train,dv_train)\ny_pred=regressor.predict(iv_test)\nprint('Accuracy of LR',mean_squared_error(y_pred,dv_test))\n</code></pre>\n\n<p>It results - Accuracy of LR 7837176694.18</p>\n\n<p>Which is incorrect.</p>\n\n<p>Below is my sample data set -</p>\n\n<pre><code>longitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   ocean_proximity median_house_value\n-122.23 37.88   41  880 129 322 126 8.3252  NEAR BAY    452600\n-122.22 37.86   21  7099    1106    2401    1138    8.3014  NEAR BAY    358500\n-122.24 37.85   52  1467    190 496 177 7.2574  NEAR BAY    352100\n-122.25 37.85   52  1274    235 558 219 5.6431  NEAR BAY    341300\n-122.25 37.85   52  1627    280 565 259 3.8462  NEAR BAY    342200\n-122.25 37.85   52  919 213 413 193 4.0368  NEAR BAY    269700\n-122.25 37.84   52  2535    489 1094    514 3.6591  NEAR BAY    299200\n</code></pre>\n",
                "tags": "<scikit-learn><linear-regression>",
                "answers": [
                    [
                        "36086",
                        "2",
                        "36083",
                        "",
                        "",
                        "<p>There are several ways to check your Linear Regression model accuracy. Usually, you may use <strong>Root mean squared error</strong>. You may train several Linear Regression models, adding or removing features to your dataset, and see which one has the lowest <strong>RMSE</strong> - the best one in your case. Also try to normalize your data before fitting into Linear Regression model. </p>\n\n<p>The confusion matrix is used to check discrete results, but Linear Regression model returns predicted result as a continuous values. That is why you get the error: your <code>dv_test</code> data likely is integer, but <code>y_pred</code> is float.</p>\n\n<p>You may try using classification model if it is suitable for the problem you try to solve - depends on what you try to predict. But for regression problem it would be better to use metric mentioned above. </p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "3728",
            "_score": 20.140503,
            "_source": {
                "title": "Calculating RMSE AND R-squared from the confusion matrix",
                "content": "Calculating RMSE AND R-squared from the confusion matrix <p>I have my confusion matrix as C.mat</p>\n\n<pre><code>8263    20    39\n2       3826  14\n43       7    4431 \n</code></pre>\n\n<p>My predicted class labels are Ypred and actual labels are Ytest. Ypred size is 16000*1 and Ytest 16000*1.</p>\n\n<p>I am trying to calculate the <strong>R-squared</strong> and <strong>RMSE</strong>. Is there a way to directly calculate <strong>RMSE</strong> and <strong>R-squared</strong> from the confusion matrix?</p>\n\n<p>I tried this:</p>\n\n<pre><code>RMSE = sqrt(immse(Ypred, Ytest))\n</code></pre>\n\n<p>However, it didn't work.</p>\n\n<p>I can use  either R or Matlab.</p>\n\n<p>Any advice will be appreciated!</p>\n <multiclass-classification><confusion-matrix><p>In addition to what the other respondents said, I would like to add that using RMSE and MSE as metrics to evaluate a classifier can actually be a good idea if the classes are <em>ordinal</em>. In this case, there is a natural order between the categories, i.e. <em>good</em> > <em>moderate</em> > <em>poor</em>. Because missing by one class is less bad than missing by two or more, you want to use a metric that takes this into account. If you want to use only a single metric, MSE and MAE are your best choices according to <a href=\"https://link.springer.com/chapter/10.1007/978-3-642-01818-3_25\" rel=\"nofollow noreferrer\">Gaudette and Japkowicz (2009)</a>. Judging from their approach, RMSE could also be a good choice.</p>\n\n<p>Here is one way to calculate MSE and RMSE from a confusion matrix in MATLAB:</p>\n\n<pre><code>cm = [8263   20   39\n         2 3826   14\n        43    7 4431];\n\nse = 0;\nfor i = 1:3\n    for j = 1:3\n        se = se + cm(i,j) * (i-j)^2;\n    end\nend\n\nmse = se / sum(sum(cm));\nrmse = sqrt(mse);\n</code></pre>\n<p>The confusion matrix suggests that you are performing classification rather than regression. RMSE and R-square are measures associated with continuous variables; For categorical variables, I'd suggest using Accuracy / Recall / Precision / F1 score to measure the performance of the model.</p>\n\n<p><a href=\"https://www.quora.com/How-is-root-mean-square-error-RMSE-and-classification-related\" rel=\"nofollow noreferrer\">https://www.quora.com/How-is-root-mean-square-error-RMSE-and-classification-related</a></p>\n<p>If you use RMSE for classification, then effectively every squared error will be a <code>1</code>. The mean squared error will be your misclassification rate and the RMSE the square root of that.</p>\n\n<p><strong>Other Helpful Information</strong></p>\n\n<p>The square root of the mean/average of the square of all of the error.</p>\n\n<p>The use of RMSE is very common and it makes an excellent general purpose error metric for numerical predictions.</p>\n\n<p>Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors.</p>\n\n<p>$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}((y_{i} -  \\hat{y}_{i})^2)}$$</p>\n\n<p><strong>MATLAB Code</strong></p>\n\n<pre><code>RMSE = sqrt(mean((y-y_pred).^2));\n</code></pre>\n\n<p><strong>R Code</strong></p>\n\n<pre><code>RMSE &lt;- sqrt(mean((y-y_pred)^2))\n</code></pre>\n\n<p><strong>Python (Scikit Learn)</strong></p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(y, y_pred)**0.5\n</code></pre>\n",
                "codes": [
                    [
                        "cm = [8263   20   39\n         2 3826   14\n        43    7 4431];\n\nse = 0;\nfor i = 1:3\n    for j = 1:3\n        se = se + cm(i,j) * (i-j)^2;\n    end\nend\n\nmse = se / sum(sum(cm));\nrmse = sqrt(mse);\n"
                    ],
                    [],
                    [
                        "RMSE = sqrt(mean((y-y_pred).^2));\n",
                        "RMSE <- sqrt(mean((y-y_pred)^2))\n",
                        "from sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(y, y_pred)**0.5\n"
                    ]
                ],
                "question_id:": "15512",
                "question_votes:": "2",
                "question_text:": "<p>I have my confusion matrix as C.mat</p>\n\n<pre><code>8263    20    39\n2       3826  14\n43       7    4431 \n</code></pre>\n\n<p>My predicted class labels are Ypred and actual labels are Ytest. Ypred size is 16000*1 and Ytest 16000*1.</p>\n\n<p>I am trying to calculate the <strong>R-squared</strong> and <strong>RMSE</strong>. Is there a way to directly calculate <strong>RMSE</strong> and <strong>R-squared</strong> from the confusion matrix?</p>\n\n<p>I tried this:</p>\n\n<pre><code>RMSE = sqrt(immse(Ypred, Ytest))\n</code></pre>\n\n<p>However, it didn't work.</p>\n\n<p>I can use  either R or Matlab.</p>\n\n<p>Any advice will be appreciated!</p>\n",
                "tags": "<multiclass-classification><confusion-matrix>",
                "answers": [
                    [
                        "54827",
                        "2",
                        "15512",
                        "",
                        "",
                        "<p>In addition to what the other respondents said, I would like to add that using RMSE and MSE as metrics to evaluate a classifier can actually be a good idea if the classes are <em>ordinal</em>. In this case, there is a natural order between the categories, i.e. <em>good</em> > <em>moderate</em> > <em>poor</em>. Because missing by one class is less bad than missing by two or more, you want to use a metric that takes this into account. If you want to use only a single metric, MSE and MAE are your best choices according to <a href=\"https://link.springer.com/chapter/10.1007/978-3-642-01818-3_25\" rel=\"nofollow noreferrer\">Gaudette and Japkowicz (2009)</a>. Judging from their approach, RMSE could also be a good choice.</p>\n\n<p>Here is one way to calculate MSE and RMSE from a confusion matrix in MATLAB:</p>\n\n<pre><code>cm = [8263   20   39\n         2 3826   14\n        43    7 4431];\n\nse = 0;\nfor i = 1:3\n    for j = 1:3\n        se = se + cm(i,j) * (i-j)^2;\n    end\nend\n\nmse = se / sum(sum(cm));\nrmse = sqrt(mse);\n</code></pre>\n",
                        "",
                        "1"
                    ],
                    [
                        "15514",
                        "2",
                        "15512",
                        "",
                        "",
                        "<p>The confusion matrix suggests that you are performing classification rather than regression. RMSE and R-square are measures associated with continuous variables; For categorical variables, I'd suggest using Accuracy / Recall / Precision / F1 score to measure the performance of the model.</p>\n\n<p><a href=\"https://www.quora.com/How-is-root-mean-square-error-RMSE-and-classification-related\" rel=\"nofollow noreferrer\">https://www.quora.com/How-is-root-mean-square-error-RMSE-and-classification-related</a></p>\n",
                        "",
                        "1"
                    ],
                    [
                        "15515",
                        "2",
                        "15512",
                        "",
                        "",
                        "<p>If you use RMSE for classification, then effectively every squared error will be a <code>1</code>. The mean squared error will be your misclassification rate and the RMSE the square root of that.</p>\n\n<p><strong>Other Helpful Information</strong></p>\n\n<p>The square root of the mean/average of the square of all of the error.</p>\n\n<p>The use of RMSE is very common and it makes an excellent general purpose error metric for numerical predictions.</p>\n\n<p>Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors.</p>\n\n<p>$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}((y_{i} -  \\hat{y}_{i})^2)}$$</p>\n\n<p><strong>MATLAB Code</strong></p>\n\n<pre><code>RMSE = sqrt(mean((y-y_pred).^2));\n</code></pre>\n\n<p><strong>R Code</strong></p>\n\n<pre><code>RMSE &lt;- sqrt(mean((y-y_pred)^2))\n</code></pre>\n\n<p><strong>Python (Scikit Learn)</strong></p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\nRMSE = mean_squared_error(y, y_pred)**0.5\n</code></pre>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "7344",
            "_score": 20.006475,
            "_source": {
                "title": "Unable to Use The K-Fold Validation Sklearn Python",
                "content": "Unable to Use The K-Fold Validation Sklearn Python <p>I have an <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" rel=\"nofollow noreferrer\">dataset</a>.</p>\n\n<p>I am unable to use the K-Fold Validation. I am getting the error raised:</p>\n\n<blockquote>\n  <p>ValueError(\"{0} is not supported\".format(y_type))</p>\n  \n  <p>ValueError: continuous is not supported .</p>\n</blockquote>\n\n<p>I do not want to do encoding to int, since it may affect the data, and also I want to understand why K-fold is not working.</p>\n\n<p>Below is my python code.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import cross_validation, metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn import svm\nfrom sklearn import preprocessing\n\n - `List item`\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\nnames=['Relative Compactness','Surface Area','Wall Area','Roof Area','Overall Height','Orientation','Glazing Area','Glazing Area Distribution','Heating Load','Cooling Load']\ndf = pd.read_excel(url,names=names)\n\n#Feature selection\ntrain=df.sample(frac=0.8,random_state=150)\ntest=df.drop(train.index)\n\n#save the original values in a dataframe so we can compare later\ntest_loads=test[[\"Cooling Load\"]]\n\n#Create 2 lists of response values to train our model\nY1=np.array(train['Heating Load'])\nY2=np.array(train['Cooling Load'])\n\n#Select the features\ntrain_corr=train[['Overall Height','Relative Compactness','Roof Area','Surface Area']]\ntest_corr=test[['Overall Height','Relative Compactness','Roof Area','Surface Area']]\nseed = 7\nscoring = 'accuracy'\nX_train,X_test,y_train,y_test=cross_validation.train_test_split(train_corr,Y1,test_size=0.2)\nkfold = model_selection.KFold(n_splits=10, random_state=seed) \ncv_results = model_selection.cross_val_score(RandomForestRegressor(), X_train, y_train, cv=kfold, scoring=scoring)\nprint (cv_results.mean())\n</code></pre>\n <machine-learning><python><deep-learning><scikit-learn><regression><p>Based on the answer <a href=\"https://stackoverflow.com/a/48551629/5120235\">here</a>, <em>Since you are doing a classification task, you should be using the metric R-squared (co-effecient of determination) instead of accuracy score (accuracy score is used for classification purposes).</em> You should use something like <code>score</code> for evaluation because your task is regression.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "27581",
                "question_votes:": "3",
                "question_text:": "<p>I have an <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" rel=\"nofollow noreferrer\">dataset</a>.</p>\n\n<p>I am unable to use the K-Fold Validation. I am getting the error raised:</p>\n\n<blockquote>\n  <p>ValueError(\"{0} is not supported\".format(y_type))</p>\n  \n  <p>ValueError: continuous is not supported .</p>\n</blockquote>\n\n<p>I do not want to do encoding to int, since it may affect the data, and also I want to understand why K-fold is not working.</p>\n\n<p>Below is my python code.</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import cross_validation, metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn import svm\nfrom sklearn import preprocessing\n\n - `List item`\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\nnames=['Relative Compactness','Surface Area','Wall Area','Roof Area','Overall Height','Orientation','Glazing Area','Glazing Area Distribution','Heating Load','Cooling Load']\ndf = pd.read_excel(url,names=names)\n\n#Feature selection\ntrain=df.sample(frac=0.8,random_state=150)\ntest=df.drop(train.index)\n\n#save the original values in a dataframe so we can compare later\ntest_loads=test[[\"Cooling Load\"]]\n\n#Create 2 lists of response values to train our model\nY1=np.array(train['Heating Load'])\nY2=np.array(train['Cooling Load'])\n\n#Select the features\ntrain_corr=train[['Overall Height','Relative Compactness','Roof Area','Surface Area']]\ntest_corr=test[['Overall Height','Relative Compactness','Roof Area','Surface Area']]\nseed = 7\nscoring = 'accuracy'\nX_train,X_test,y_train,y_test=cross_validation.train_test_split(train_corr,Y1,test_size=0.2)\nkfold = model_selection.KFold(n_splits=10, random_state=seed) \ncv_results = model_selection.cross_val_score(RandomForestRegressor(), X_train, y_train, cv=kfold, scoring=scoring)\nprint (cv_results.mean())\n</code></pre>\n",
                "tags": "<machine-learning><python><deep-learning><scikit-learn><regression>",
                "answers": [
                    [
                        "27582",
                        "2",
                        "27581",
                        "",
                        "",
                        "<p>Based on the answer <a href=\"https://stackoverflow.com/a/48551629/5120235\">here</a>, <em>Since you are doing a classification task, you should be using the metric R-squared (co-effecient of determination) instead of accuracy score (accuracy score is used for classification purposes).</em> You should use something like <code>score</code> for evaluation because your task is regression.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "11879",
            "_score": 19.984013,
            "_source": {
                "title": "During a regression task, I am getting low R^2 values, but elementwise difference between test set and prediction values is huge",
                "content": "During a regression task, I am getting low R^2 values, but elementwise difference between test set and prediction values is huge <p>I am doing a random forest regression on my dataset (which has abut 15 input features and 1 target feature). I am getting a decently low R^2 of &lt;1 for both the train and test sets (please do let me know if &lt;1 is not a good-enough R^2 score).</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# load dataset\ndf = pd.read_csv('Dataset.csv')\n\n# split into input (X) and output (Y) variables\nX = df.drop(['ID_COLUMN', 'TARGET_COLUMN'], axis=1)\nY = df.TARGET_COLUMN\n\n# Split the data into 67% for training and 33% for testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n\n# Fitting the regression model to the dataset\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 50)\nregressor.fit(X_train, Y_train.ravel()) # Using ravel() to avoid getting 'DataConversionWarning' warning message\n\n\nprint(\"Predicting Values:\")\ny_pred = regressor.predict(X_test)\n\nprint(\"Getting Model Performance...\")\n\n# Get regression scores\nprint(\"R^2 train = \", regressor.score(X_train, Y_train))\nprint(\"R^2 test = \", regressor.score(X_test, Y_test))\n</code></pre>\n\n<p>This outputs the following:</p>\n\n<pre><code>Predicting Values:\nGetting Model Performance...\nR^2 train =  0.9791000275450427\nR^2 test = 0.8577464692386905\n</code></pre>\n\n<p>Then, I checked the difference between the actual target column values in the test dataset versus the predicted values, like so:</p>\n\n<pre><code>diff = []\nfor i in range(len(y_pred)):\n    if Y_test.values[i]!=0: # a few values were 0 which was causing the corresponding diff value to become inf\n        diff.append(100*np.abs(y_pred[i]-Y_test.values[i])/Y_test.values[i]) # element-wise percentage error\n</code></pre>\n\n<p>I found that <strong>the majority of the element-wise differences were between 40-60% and their mean was almost 50%!</strong></p>\n\n<pre><code>np.mean(diff)\n&gt;&gt;&gt; 49.07580695857447\n</code></pre>\n\n<p>So, which one is correct? Is the regression score correct and my model is good for this data, or is the element-wise error I calculated correct and the model didn't do well for this data? If its the latter, please advise on how to increase the prediction accuracy.</p>\n\n<hr>\n\n<p>I also checked the rmse score:</p>\n\n<pre><code>import math\nrmse = math.sqrt(np.mean((np.array(Y_test) - y_pred)**2))\nrmse\n&gt;&gt;&gt; 3.67328471827293\n</code></pre>\n\n<p>This seems quite high for the model to have done a good job, but please correct me if I'm wrong.</p>\n\n<p>And I also checked the R^2 scores for different number of estimators:</p>\n\n<pre><code>import matplotlib.pyplot as plt\nmodel = RandomForestRegressor(n_jobs=-1)\n# Try different numbers of n_estimators\nestimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, Y_train)\n    scores.append(model.score(X_test, Y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/MbQtn.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MbQtn.png\" alt=\"enter image description here\"></a></p>\n\n<p>Please advise.</p>\n\n<hr>\n\n<p>I tried using linear regression first, and got a very high MSE (which is why I was trying out random forest):</p>\n\n<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n# The coefficients\nprint('Coefficients: \\n', lr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\n\n\nCoefficients: \n [ 1.93829229e-01 -4.68738825e-01  2.01635420e-01  6.35902010e-01\n  6.57354434e-03  5.13180293e-03  2.84015810e-01 -1.31469084e-06\n  1.95335035e+00]\nMean squared error: 86.92\nVariance score: 0.08\n</code></pre>\n <machine-learning><python><predictive-modeling><regression><random-forest><p>This line looks wrong to me:</p>\n\n<pre><code>diff.append(100*np.abs(y_pred[i]-Y_test.values[i])/Y_test.values[i])\n</code></pre>\n\n<p>Shouldn't the abs be around the entire calculation?</p>\n\n<pre><code>diff.append(100*np.abs((y_pred[i]-Y_test.values[i])/Y_test.values[i]))\n</code></pre>\n\n<p>That aside, the RMSE calculation looks accurate and is in the scale of the error, and the <span class=\"math-container\">$R^2$</span> is great, so all things being equal, I would lean towards looking for something you did wrong in assessing the errors.  That's why I was focused on your calculation.</p>\n\n<p>One other thought, have you checked for outliers?  This could affect some measures and not others as drastically.</p>\n",
                "codes": [
                    [
                        "diff.append(100*np.abs(y_pred[i]-Y_test.values[i])/Y_test.values[i])\n",
                        "diff.append(100*np.abs((y_pred[i]-Y_test.values[i])/Y_test.values[i]))\n"
                    ]
                ],
                "question_id:": "41842",
                "question_votes:": "",
                "question_text:": "<p>I am doing a random forest regression on my dataset (which has abut 15 input features and 1 target feature). I am getting a decently low R^2 of &lt;1 for both the train and test sets (please do let me know if &lt;1 is not a good-enough R^2 score).</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# load dataset\ndf = pd.read_csv('Dataset.csv')\n\n# split into input (X) and output (Y) variables\nX = df.drop(['ID_COLUMN', 'TARGET_COLUMN'], axis=1)\nY = df.TARGET_COLUMN\n\n# Split the data into 67% for training and 33% for testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n\n# Fitting the regression model to the dataset\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 50)\nregressor.fit(X_train, Y_train.ravel()) # Using ravel() to avoid getting 'DataConversionWarning' warning message\n\n\nprint(\"Predicting Values:\")\ny_pred = regressor.predict(X_test)\n\nprint(\"Getting Model Performance...\")\n\n# Get regression scores\nprint(\"R^2 train = \", regressor.score(X_train, Y_train))\nprint(\"R^2 test = \", regressor.score(X_test, Y_test))\n</code></pre>\n\n<p>This outputs the following:</p>\n\n<pre><code>Predicting Values:\nGetting Model Performance...\nR^2 train =  0.9791000275450427\nR^2 test = 0.8577464692386905\n</code></pre>\n\n<p>Then, I checked the difference between the actual target column values in the test dataset versus the predicted values, like so:</p>\n\n<pre><code>diff = []\nfor i in range(len(y_pred)):\n    if Y_test.values[i]!=0: # a few values were 0 which was causing the corresponding diff value to become inf\n        diff.append(100*np.abs(y_pred[i]-Y_test.values[i])/Y_test.values[i]) # element-wise percentage error\n</code></pre>\n\n<p>I found that <strong>the majority of the element-wise differences were between 40-60% and their mean was almost 50%!</strong></p>\n\n<pre><code>np.mean(diff)\n&gt;&gt;&gt; 49.07580695857447\n</code></pre>\n\n<p>So, which one is correct? Is the regression score correct and my model is good for this data, or is the element-wise error I calculated correct and the model didn't do well for this data? If its the latter, please advise on how to increase the prediction accuracy.</p>\n\n<hr>\n\n<p>I also checked the rmse score:</p>\n\n<pre><code>import math\nrmse = math.sqrt(np.mean((np.array(Y_test) - y_pred)**2))\nrmse\n&gt;&gt;&gt; 3.67328471827293\n</code></pre>\n\n<p>This seems quite high for the model to have done a good job, but please correct me if I'm wrong.</p>\n\n<p>And I also checked the R^2 scores for different number of estimators:</p>\n\n<pre><code>import matplotlib.pyplot as plt\nmodel = RandomForestRegressor(n_jobs=-1)\n# Try different numbers of n_estimators\nestimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, Y_train)\n    scores.append(model.score(X_test, Y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/MbQtn.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MbQtn.png\" alt=\"enter image description here\"></a></p>\n\n<p>Please advise.</p>\n\n<hr>\n\n<p>I tried using linear regression first, and got a very high MSE (which is why I was trying out random forest):</p>\n\n<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n# The coefficients\nprint('Coefficients: \\n', lr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))\n\n\nCoefficients: \n [ 1.93829229e-01 -4.68738825e-01  2.01635420e-01  6.35902010e-01\n  6.57354434e-03  5.13180293e-03  2.84015810e-01 -1.31469084e-06\n  1.95335035e+00]\nMean squared error: 86.92\nVariance score: 0.08\n</code></pre>\n",
                "tags": "<machine-learning><python><predictive-modeling><regression><random-forest>",
                "answers": [
                    [
                        "41870",
                        "2",
                        "41842",
                        "",
                        "",
                        "<p>This line looks wrong to me:</p>\n\n<pre><code>diff.append(100*np.abs(y_pred[i]-Y_test.values[i])/Y_test.values[i])\n</code></pre>\n\n<p>Shouldn't the abs be around the entire calculation?</p>\n\n<pre><code>diff.append(100*np.abs((y_pred[i]-Y_test.values[i])/Y_test.values[i]))\n</code></pre>\n\n<p>That aside, the RMSE calculation looks accurate and is in the scale of the error, and the <span class=\"math-container\">$R^2$</span> is great, so all things being equal, I would lean towards looking for something you did wrong in assessing the errors.  That's why I was focused on your calculation.</p>\n\n<p>One other thought, have you checked for outliers?  This could affect some measures and not others as drastically.</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "9720",
            "_score": 19.440939,
            "_source": {
                "title": "Inference of root mean square value in terms of house prediction",
                "content": "Inference of root mean square value in terms of house prediction <p>The objective of the task is to predict the housing prices. A model is created based on California housing dataset to predict housing prices and is subjected to evaluation using the below code.</p>\n\n<pre><code>    from sklearn.metrics import mean_squared_error\n\n    housing_predictions = lin_reg.predict(housing_prepared)\n    error = mean_squared_error(housing_labels,housing_predictions)\n    rmse = np.sqrt(error) --&gt; prediction error\n\n    // value I got for rmse was - 68628.1981\n</code></pre>\n\n<p>The average housing prices range from 120,000 - 265,000. But the rmse value is \n68628.1981. </p>\n\n<blockquote>\n  <p>Questions</p>\n</blockquote>\n\n<ol>\n<li>By looking at rmse value, What should I infer?. I know the model is underfitting. I wanted to know the intuition behind the rmse value.</li>\n</ol>\n <predictive-modeling><prediction><model-selection><p>The RMSE is a measure for how \"wrong\" a regression model's predictions are on average, and is mostly useful as a relative metric for determining which of a set of models is best. If you want an absolute baseline against which to compare any given model you can always compute the standard deviation of your target, which is the RMSE of a model which predicts the average value of $y$ for every observation.</p>\n\n<p>I would also argue that for something like housing prices a more meaningful error measure is root mean square <em>logarithmic</em> error, which is the RMSE after taking the log of $y$ (or $y + 1$ if $y$ contains zeros) and the predicted values. The reason for this is that we're interested in errors as proportions of our target (an error of \\$50,000 is much more serious for a \\$100,000 house compared to a \\$1,000,000 house), and taking logs is the way to adjust for this. Also be sure to focus on test or cross validation error, else you will always choose your most overfit model.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "35476",
                "question_votes:": "",
                "question_text:": "<p>The objective of the task is to predict the housing prices. A model is created based on California housing dataset to predict housing prices and is subjected to evaluation using the below code.</p>\n\n<pre><code>    from sklearn.metrics import mean_squared_error\n\n    housing_predictions = lin_reg.predict(housing_prepared)\n    error = mean_squared_error(housing_labels,housing_predictions)\n    rmse = np.sqrt(error) --&gt; prediction error\n\n    // value I got for rmse was - 68628.1981\n</code></pre>\n\n<p>The average housing prices range from 120,000 - 265,000. But the rmse value is \n68628.1981. </p>\n\n<blockquote>\n  <p>Questions</p>\n</blockquote>\n\n<ol>\n<li>By looking at rmse value, What should I infer?. I know the model is underfitting. I wanted to know the intuition behind the rmse value.</li>\n</ol>\n",
                "tags": "<predictive-modeling><prediction><model-selection>",
                "answers": [
                    [
                        "35478",
                        "2",
                        "35476",
                        "",
                        "",
                        "<p>The RMSE is a measure for how \"wrong\" a regression model's predictions are on average, and is mostly useful as a relative metric for determining which of a set of models is best. If you want an absolute baseline against which to compare any given model you can always compute the standard deviation of your target, which is the RMSE of a model which predicts the average value of $y$ for every observation.</p>\n\n<p>I would also argue that for something like housing prices a more meaningful error measure is root mean square <em>logarithmic</em> error, which is the RMSE after taking the log of $y$ (or $y + 1$ if $y$ contains zeros) and the predicted values. The reason for this is that we're interested in errors as proportions of our target (an error of \\$50,000 is much more serious for a \\$100,000 house compared to a \\$1,000,000 house), and taking logs is the way to adjust for this. Also be sure to focus on test or cross validation error, else you will always choose your most overfit model.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "13443",
            "_score": 18.363495,
            "_source": {
                "title": "keras plotting loss and MSE",
                "content": "keras plotting loss and MSE <p>Can someone give me a tip on how I could incorporate MSE &amp; loss plots? I have been following some <a href=\"https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\" rel=\"nofollow noreferrer\">machinelearningmastery</a> posts to plot this but the application is classification and I am attempting regression. Also what is different in my script is I am defining the model thru calling a function, so I am curious if my script could be re-written without the function <code>def wider_model()</code> that defines the model.</p>\n\n<p>This script below works except what is commented out on the bottom for the <code>plt</code> plots. In the machinelearningmastery post, someone does ask this question how to do for regression, and supposedly if you print <code>print(history.history.keys())</code> two values are returned for <code>dict_keys([\u2018mean_absolute_error\u2019, \u2018loss\u2019])</code>...  </p>\n\n<p>Any tips help, there isn't a lot of wisdom here... Thank you</p>\n\n<pre><code>import numpy\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport math\n\n\n\n# load dataset\ndataset = pandas.read_csv(\"prepdSPdata.csv\", index_col='Date', parse_dates=True)\n\n# shuffle dataset\ndataset = dataset.sample(frac=1.0)\n\n\n# split into input (X) and output (Y) variables\nX = numpy.array(dataset.drop(['Demand'],1))\nY = numpy.array(dataset['Demand'])\n\nprint(dataset.shape)\nprint(dataset.dtypes)\nprint(dataset.columns)\n\n\ndef wider_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(20, input_dim=11, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=1, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\nprint(\"RMSE\", math.sqrt(results.std()))\n\n\n\n# list all data in history\n#print(wider_model.wider_model.keys())\n\n# summarize history for MSE\n#plt.plot(history.history['acc'])\n#plt.plot(history.history['val_acc'])\n#plt.title('model MSE')\n#plt.ylabel('MSE')\n#plt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\n#plt.show()\n\n# summarize history for loss\n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('model loss')\n#plt.ylabel('loss')\n#plt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\n#plt.show()\n</code></pre>\n <python><deep-learning><keras><regression><matplotlib><p>You're only training your model for 1 epoch so you're only giving it one data point to work from. If you want to plot a line of loss or accuracy you need to train for more epochs.</p>\n<p><code>cross_val_score</code> does not return the history of the training. You can use <code>fit</code> instead: </p>\n\n<pre><code>history = model.fit( ...\n</code></pre>\n\n<p>See this <a href=\"https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\" rel=\"nofollow noreferrer\">example</a>.</p>\n\n<p>As you mentioned, the <code>history</code> object holds the results of the training for each epoch.</p>\n\n<p>Here is the relevant bit:</p>\n\n<pre><code>history = model.fit(X, X, epochs=500, batch_size=len(X), verbose=2)\npyplot.plot(history.history['mean_squared_error'])\npyplot.plot(history.history['mean_absolute_error'])\npyplot.plot(history.history['mean_absolute_percentage_error'])\npyplot.plot(history.history['cosine_proximity'])\npyplot.show()\n</code></pre>\n\n<p>I was actually working on the same example that you referenced yesterday. I think it is hard to understand because it introduces many functions and concepts: estimators, <code>StandardScaler</code>, <code>KerasRegressor</code>, <code>Pipeline</code>, <code>KFold</code> and <code>cross_val_score</code>.</p>\n\n<p>However, I did like the approach to creating and testing models, and Cross Validation would produce more robust models.</p>\n",
                "codes": [
                    [],
                    [
                        "history = model.fit( ...\n",
                        "history = model.fit(X, X, epochs=500, batch_size=len(X), verbose=2)\npyplot.plot(history.history['mean_squared_error'])\npyplot.plot(history.history['mean_absolute_error'])\npyplot.plot(history.history['mean_absolute_percentage_error'])\npyplot.plot(history.history['cosine_proximity'])\npyplot.show()\n"
                    ]
                ],
                "question_id:": "45954",
                "question_votes:": "2",
                "question_text:": "<p>Can someone give me a tip on how I could incorporate MSE &amp; loss plots? I have been following some <a href=\"https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\" rel=\"nofollow noreferrer\">machinelearningmastery</a> posts to plot this but the application is classification and I am attempting regression. Also what is different in my script is I am defining the model thru calling a function, so I am curious if my script could be re-written without the function <code>def wider_model()</code> that defines the model.</p>\n\n<p>This script below works except what is commented out on the bottom for the <code>plt</code> plots. In the machinelearningmastery post, someone does ask this question how to do for regression, and supposedly if you print <code>print(history.history.keys())</code> two values are returned for <code>dict_keys([\u2018mean_absolute_error\u2019, \u2018loss\u2019])</code>...  </p>\n\n<p>Any tips help, there isn't a lot of wisdom here... Thank you</p>\n\n<pre><code>import numpy\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport math\n\n\n\n# load dataset\ndataset = pandas.read_csv(\"prepdSPdata.csv\", index_col='Date', parse_dates=True)\n\n# shuffle dataset\ndataset = dataset.sample(frac=1.0)\n\n\n# split into input (X) and output (Y) variables\nX = numpy.array(dataset.drop(['Demand'],1))\nY = numpy.array(dataset['Demand'])\n\nprint(dataset.shape)\nprint(dataset.dtypes)\nprint(dataset.columns)\n\n\ndef wider_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(20, input_dim=11, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=1, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\nprint(\"RMSE\", math.sqrt(results.std()))\n\n\n\n# list all data in history\n#print(wider_model.wider_model.keys())\n\n# summarize history for MSE\n#plt.plot(history.history['acc'])\n#plt.plot(history.history['val_acc'])\n#plt.title('model MSE')\n#plt.ylabel('MSE')\n#plt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\n#plt.show()\n\n# summarize history for loss\n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('model loss')\n#plt.ylabel('loss')\n#plt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\n#plt.show()\n</code></pre>\n",
                "tags": "<python><deep-learning><keras><regression><matplotlib>",
                "answers": [
                    [
                        "45961",
                        "2",
                        "45954",
                        "",
                        "",
                        "<p>You're only training your model for 1 epoch so you're only giving it one data point to work from. If you want to plot a line of loss or accuracy you need to train for more epochs.</p>\n",
                        "",
                        "1"
                    ],
                    [
                        "45985",
                        "2",
                        "45954",
                        "",
                        "",
                        "<p><code>cross_val_score</code> does not return the history of the training. You can use <code>fit</code> instead: </p>\n\n<pre><code>history = model.fit( ...\n</code></pre>\n\n<p>See this <a href=\"https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\" rel=\"nofollow noreferrer\">example</a>.</p>\n\n<p>As you mentioned, the <code>history</code> object holds the results of the training for each epoch.</p>\n\n<p>Here is the relevant bit:</p>\n\n<pre><code>history = model.fit(X, X, epochs=500, batch_size=len(X), verbose=2)\npyplot.plot(history.history['mean_squared_error'])\npyplot.plot(history.history['mean_absolute_error'])\npyplot.plot(history.history['mean_absolute_percentage_error'])\npyplot.plot(history.history['cosine_proximity'])\npyplot.show()\n</code></pre>\n\n<p>I was actually working on the same example that you referenced yesterday. I think it is hard to understand because it introduces many functions and concepts: estimators, <code>StandardScaler</code>, <code>KerasRegressor</code>, <code>Pipeline</code>, <code>KFold</code> and <code>cross_val_score</code>.</p>\n\n<p>However, I did like the approach to creating and testing models, and Cross Validation would produce more robust models.</p>\n",
                        "",
                        "3"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "11401",
            "_score": 18.059093,
            "_source": {
                "title": "Evaluation of linear regression model",
                "content": "Evaluation of linear regression model <p>I want to evaluate the performance of my linear regression model. I have the true values of y (y-true). I am thinking of two way for evaluation but not sure which one is correct. </p>\n\n<p>Let's assume that we have 2 samples and each sample has two outputs as following: </p>\n\n<pre><code>y_true = [[0.5, 0.5],[0.6, 0.3]]\ny_pred = [[0.3, 0.7],[0.9, 0.1]]\n</code></pre>\n\n<p><strong>- Approach#1 :</strong> </p>\n\n<p>One way to calculate the sum of the difference between the actual and predicted for each vector and then average all, as follows: </p>\n\n<p>sum_diff_Vector(1) = abs( 0.5 - 0.3 ) + abs( 0.5 - 0.7 ) =  0.4</p>\n\n<p>sum_diff_Vector(2) = abs( 0.6 - 0.9 ) + abs( 0.3 - 0.1 ) = 0.5</p>\n\n<p>Then avg ( sum_diff_Vector(1) , sum_diff_Vector(2) ) = 0.45</p>\n\n<p><strong>- Approach#2 :</strong> </p>\n\n<p>Another way to use the mean absolute error provided by sklearn.metrics in python.  The thing with this metric, as opposed to the previous method, it calculates the mean absolute error for each output over all samples independently and then average all of them, as follows: </p>\n\n<p>MAE_OUTPUT(1) = abs(( 0.5 - 0.3 ) + ( 0.6 - 0.9 )) / 2 = 0.25</p>\n\n<p>MAE_OUTPUT(1) = abs(( 0.5 - 0.7 ) + ( 0.3 - 0.1 )) /2 = 0.2 </p>\n\n<p>Then avg ( MAE_OUTPUT(1) , MAE_OUTPUT(1) ) =  0.225</p>\n\n<p>Which way is correct and I should use ? please advise? </p>\n <linear-regression><evaluation><p>The only difference is in your example is that you divide by an additional two, because you take the mean per vector instead of the sum. Correctness does not play here because for comparison between different models the only difference is a constant factor and for interpretability it depends on the problem you are solving.</p>\n\n<p>The mean absolute error punishes mistakes linearly while the mean squared error punishes larger mistakes more heavily. This means this depends a bit on what you want to measure, based on the problem you are solving. Next to proper evaluation you could use this same measure to change the KPI you are optimizing directly with a different loss function.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "40265",
                "question_votes:": "1",
                "question_text:": "<p>I want to evaluate the performance of my linear regression model. I have the true values of y (y-true). I am thinking of two way for evaluation but not sure which one is correct. </p>\n\n<p>Let's assume that we have 2 samples and each sample has two outputs as following: </p>\n\n<pre><code>y_true = [[0.5, 0.5],[0.6, 0.3]]\ny_pred = [[0.3, 0.7],[0.9, 0.1]]\n</code></pre>\n\n<p><strong>- Approach#1 :</strong> </p>\n\n<p>One way to calculate the sum of the difference between the actual and predicted for each vector and then average all, as follows: </p>\n\n<p>sum_diff_Vector(1) = abs( 0.5 - 0.3 ) + abs( 0.5 - 0.7 ) =  0.4</p>\n\n<p>sum_diff_Vector(2) = abs( 0.6 - 0.9 ) + abs( 0.3 - 0.1 ) = 0.5</p>\n\n<p>Then avg ( sum_diff_Vector(1) , sum_diff_Vector(2) ) = 0.45</p>\n\n<p><strong>- Approach#2 :</strong> </p>\n\n<p>Another way to use the mean absolute error provided by sklearn.metrics in python.  The thing with this metric, as opposed to the previous method, it calculates the mean absolute error for each output over all samples independently and then average all of them, as follows: </p>\n\n<p>MAE_OUTPUT(1) = abs(( 0.5 - 0.3 ) + ( 0.6 - 0.9 )) / 2 = 0.25</p>\n\n<p>MAE_OUTPUT(1) = abs(( 0.5 - 0.7 ) + ( 0.3 - 0.1 )) /2 = 0.2 </p>\n\n<p>Then avg ( MAE_OUTPUT(1) , MAE_OUTPUT(1) ) =  0.225</p>\n\n<p>Which way is correct and I should use ? please advise? </p>\n",
                "tags": "<linear-regression><evaluation>",
                "answers": [
                    [
                        "40266",
                        "2",
                        "40265",
                        "",
                        "",
                        "<p>The only difference is in your example is that you divide by an additional two, because you take the mean per vector instead of the sum. Correctness does not play here because for comparison between different models the only difference is a constant factor and for interpretability it depends on the problem you are solving.</p>\n\n<p>The mean absolute error punishes mistakes linearly while the mean squared error punishes larger mistakes more heavily. This means this depends a bit on what you want to measure, based on the problem you are solving. Next to proper evaluation you could use this same measure to change the KPI you are optimizing directly with a different loss function.</p>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "9207",
            "_score": 18.022823,
            "_source": {
                "title": "Making predictions from keras with SciKit",
                "content": "Making predictions from keras with SciKit <p>Ive been thinking about combining some processes between keras and Sci-kit Learn and am looking to the this group to either validate my process or tell Im crazy. Im creating a simple Regression problem using 17 inputs like this:</p>\n\n<p>Creating test/train here:</p>\n\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=7)\n</code></pre>\n\n<p>Building the network here:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Dense(34, input_dim=17, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(17, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='normal'))\nmodel.add(Dense(1, kernel_initializer='normal'))\n</code></pre>\n\n<p>compiling the model with:</p>\n\n<pre><code>model.compile(loss='mean_squared_error', optimizer='adam')\n</code></pre>\n\n<p>fitting the model here:</p>\n\n<pre><code>model.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=100, batch_size=10)\n</code></pre>\n\n<p>Now that Ive fit the model is the any reason I can use some of the SciKit functions and do the following?</p>\n\n<p>Make predictions</p>\n\n<pre><code>y_pred = model.predict(X_test)\n</code></pre>\n\n<p>Assess the model results:</p>\n\n<pre><code>mse = mean_squared_error(y_test, y_pred)\nrmse = sqrt(mse)\nr2score = r2_score(y_test,y_pred)\n</code></pre>\n\n<p>Am I way off-base here?</p>\n <python><deep-learning><p>This seems about right.</p>\n\n<p>You can use SciKit learn quite easily, as the predictions and test results you have should all be in NumPy arrays anyway. Take a look at <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\" rel=\"nofollow noreferrer\">the regression metrics</a>. The metrics you have named are shown in the documentation to take lists, but here is an example showing they work just as well with NumPy arrays:</p>\n\n<pre><code>In [1]: from sklearn.metrics import r2_score\n\nIn [2]: y_true = [3, -0.5, 2, 7]\n\nIn [3]: y_pred = [2.5, 0.0, 2, 8]\n\nIn [4]: r2_score(y_true, y_pred)\nOut[4]: 0.94860813704496794\n\nIn [6]: import numpy as np\n\nIn [13]: y_true = np.array([3, -0.5, 2, 7])\n\nIn [14]: y_pred = np.array([2.5, 0.0, 2, 8])\n\nIn [15]: r2_score(y_true, y_pred)\nOut[15]: 0.94860813704496794            # identical result\n</code></pre>\n\n<hr>\n\n<p><strong>One last comment:</strong></p>\n\n<p>17 examples doesn't sound like a lot. Look at model.summary() after compilation to see how many parameters your model has (<em>spoiler - it's 1360</em>). I would expect that you model (with your number of epochs and batch size etc.) will overfit, just memorising the dataset, and probably score 100%.</p>\n\n<p>While this is a good sanity check to make sure your model can indeed learn, it might be a good idea to split a larger dataset (if available) into <code>train</code>, <code>validation</code> and <code>test</code> datasets. The simply use train and val as you have above, but in the prediction line, us the test set, which the model has never seen before. Unless your data is extremely homogenous, I wouldn't expect a accuracy metric near 100%.</p>\n",
                "codes": [
                    [
                        "In [1]: from sklearn.metrics import r2_score\n\nIn [2]: y_true = [3, -0.5, 2, 7]\n\nIn [3]: y_pred = [2.5, 0.0, 2, 8]\n\nIn [4]: r2_score(y_true, y_pred)\nOut[4]: 0.94860813704496794\n\nIn [6]: import numpy as np\n\nIn [13]: y_true = np.array([3, -0.5, 2, 7])\n\nIn [14]: y_pred = np.array([2.5, 0.0, 2, 8])\n\nIn [15]: r2_score(y_true, y_pred)\nOut[15]: 0.94860813704496794            # identical result\n"
                    ]
                ],
                "question_id:": "32908",
                "question_votes:": "1",
                "question_text:": "<p>Ive been thinking about combining some processes between keras and Sci-kit Learn and am looking to the this group to either validate my process or tell Im crazy. Im creating a simple Regression problem using 17 inputs like this:</p>\n\n<p>Creating test/train here:</p>\n\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=7)\n</code></pre>\n\n<p>Building the network here:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Dense(34, input_dim=17, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(17, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='normal'))\nmodel.add(Dense(1, kernel_initializer='normal'))\n</code></pre>\n\n<p>compiling the model with:</p>\n\n<pre><code>model.compile(loss='mean_squared_error', optimizer='adam')\n</code></pre>\n\n<p>fitting the model here:</p>\n\n<pre><code>model.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=100, batch_size=10)\n</code></pre>\n\n<p>Now that Ive fit the model is the any reason I can use some of the SciKit functions and do the following?</p>\n\n<p>Make predictions</p>\n\n<pre><code>y_pred = model.predict(X_test)\n</code></pre>\n\n<p>Assess the model results:</p>\n\n<pre><code>mse = mean_squared_error(y_test, y_pred)\nrmse = sqrt(mse)\nr2score = r2_score(y_test,y_pred)\n</code></pre>\n\n<p>Am I way off-base here?</p>\n",
                "tags": "<python><deep-learning>",
                "answers": [
                    [
                        "32930",
                        "2",
                        "32908",
                        "",
                        "",
                        "<p>This seems about right.</p>\n\n<p>You can use SciKit learn quite easily, as the predictions and test results you have should all be in NumPy arrays anyway. Take a look at <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\" rel=\"nofollow noreferrer\">the regression metrics</a>. The metrics you have named are shown in the documentation to take lists, but here is an example showing they work just as well with NumPy arrays:</p>\n\n<pre><code>In [1]: from sklearn.metrics import r2_score\n\nIn [2]: y_true = [3, -0.5, 2, 7]\n\nIn [3]: y_pred = [2.5, 0.0, 2, 8]\n\nIn [4]: r2_score(y_true, y_pred)\nOut[4]: 0.94860813704496794\n\nIn [6]: import numpy as np\n\nIn [13]: y_true = np.array([3, -0.5, 2, 7])\n\nIn [14]: y_pred = np.array([2.5, 0.0, 2, 8])\n\nIn [15]: r2_score(y_true, y_pred)\nOut[15]: 0.94860813704496794            # identical result\n</code></pre>\n\n<hr>\n\n<p><strong>One last comment:</strong></p>\n\n<p>17 examples doesn't sound like a lot. Look at model.summary() after compilation to see how many parameters your model has (<em>spoiler - it's 1360</em>). I would expect that you model (with your number of epochs and batch size etc.) will overfit, just memorising the dataset, and probably score 100%.</p>\n\n<p>While this is a good sanity check to make sure your model can indeed learn, it might be a good idea to split a larger dataset (if available) into <code>train</code>, <code>validation</code> and <code>test</code> datasets. The simply use train and val as you have above, but in the prediction line, us the test set, which the model has never seen before. Unless your data is extremely homogenous, I wouldn't expect a accuracy metric near 100%.</p>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "7414",
            "_score": 17.087399,
            "_source": {
                "title": "Is linear regression fit for this data",
                "content": "Is linear regression fit for this data <p>I am predicting number of vehicles in 4 traffic junctions.</p>\n\n<p>So, I have following columns in my dataset :</p>\n\n<ol>\n<li>DateTime</li>\n<li>Junction_ID</li>\n<li>Number_of_vehicles</li>\n</ol>\n\n<p>At the first glance , this problem may look like Time series regression. But, the data given seems like Linear Regression problem.</p>\n\n<p>So, I have applied linear regression in the following manner :</p>\n\n<ul>\n<li>Used <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\" rel=\"nofollow noreferrer\">get_dummies</a> extensively for all the columns. I used dummy variables for 31 days,24 hours ,7 days of weeks and 4 Junction Ids.</li>\n<li><p>Then applied Linear Regression model </p>\n\n<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_data,train_vehicles)\n\nclf.fit(x_train,y_train)\n\nimport math\n\npred=clf.predict(x_test)\n\npred.shape #got result as (12030,)\n\nresult = []\nfor x in pred:\nresult.append(math.ceil(x))\n\nfrom sklearn.metrics import mean_squared_error\n\nscore=mean_squared_error(y_test, result)\nrmse=math.sqrt(score)\nprint('RMSE is :', rmse)\n</code></pre></li>\n</ul>\n\n<p>I am getting RMSE value as 10.636853077462394</p>\n\n<p>My questions are :</p>\n\n<ul>\n<li>Since RMSE value is on lower side , can I say this model is decent ?</li>\n<li>Is there any other approach which I can use on this dataset ?</li>\n<li>Do I need to check for colinearity ?</li>\n<li>How can I check if multiple variables are interrelated ?</li>\n<li>Should I go for non-linear regression on this dataset ?</li>\n</ul>\n <machine-learning><regression><linear-regression><p>There is a misconception about RMSE and other measurements for prediction quality, if you look at them standalone. In statistics you actually work with different models to compare RMSE's (using other approaches or other input variables) to have insights about prediction quality.</p>\n\n<p>Moreover, to conclude if a model is appropiate given your data you test the model assumptions. In linear Regression:</p>\n\n<ul>\n<li>Estimation Error follows a Normal distribution with E(mean) = 0, and sigma\u00b2</li>\n<li>Errors and input data are not autocorrelated (Find Beusch Godfrey - test, or ACF-Plot)</li>\n<li>No multicollinearity (Your dependent variables are not too strongly correlated - Pearson Correlation)</li>\n<li>Homoscedasticity (implied by normal distribution and independent error - Find White's test)</li>\n</ul>\n\n<p>However, good estimation doesn't necessarily lead to better predictions</p>\n<p>Welcome to the site! You could also ask yourself, \"Is <em>data science</em> fit for this data?\"</p>\n\n<p>Not all datasets require some sort of algorithmic approach. Depending on what you're going after, this may not be a data science problem. In traffic studies, a good number of problems are solved with \"plain\" statistics. For example, you could use the Poisson Distribution to solve any number of issues with your current dataset and that can be very effective even though it has relatively little to do with data science.</p>\n<p>For the first question, it is important to recall that RMSE has the same unit as the dependent variable. It means that there is no absolute good or bad threshold, however you can define it based on your DV. For a datum which ranges from 0 to 1000, an RMSE of 0.7 is small, but if the range goes from 0 to 1, it is not small.</p>\n\n<p>I would do some feature engineering (Create more variables: Time of Day, Day of Week, Month etc...) and run it through a Neural Network and then check for accuracy.  You may want to check if there is any correlation between the 4 junctions as well but if you run a NN, you don't have to.</p>\n<ul>\n<li>Is the target variable likely to be linearly, or additively dependent on the inputs? This means Monday will always have, say 10 more vehicles than Tuesday. If it is more intuitive to say that Monday will have 10% more vehicles than Tuesday, you can consider a log-linear model: transform the target variable by taking a log.</li>\n<li>In addition to measuring RMSE, you may want to visualize the data and predictions. Plot the actual and predicted vehicles on y-axis and date on x-axis, separately for each junction. This should tell you something about how good your model is, and potentially where it is going wrong.</li>\n<li>Based on the method of defining the time variables, the features are not likely to correlated to each other.</li>\n</ul>\n",
                "codes": [
                    [],
                    [],
                    [],
                    []
                ],
                "question_id:": "27744",
                "question_votes:": "1",
                "question_text:": "<p>I am predicting number of vehicles in 4 traffic junctions.</p>\n\n<p>So, I have following columns in my dataset :</p>\n\n<ol>\n<li>DateTime</li>\n<li>Junction_ID</li>\n<li>Number_of_vehicles</li>\n</ol>\n\n<p>At the first glance , this problem may look like Time series regression. But, the data given seems like Linear Regression problem.</p>\n\n<p>So, I have applied linear regression in the following manner :</p>\n\n<ul>\n<li>Used <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\" rel=\"nofollow noreferrer\">get_dummies</a> extensively for all the columns. I used dummy variables for 31 days,24 hours ,7 days of weeks and 4 Junction Ids.</li>\n<li><p>Then applied Linear Regression model </p>\n\n<pre><code>from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_data,train_vehicles)\n\nclf.fit(x_train,y_train)\n\nimport math\n\npred=clf.predict(x_test)\n\npred.shape #got result as (12030,)\n\nresult = []\nfor x in pred:\nresult.append(math.ceil(x))\n\nfrom sklearn.metrics import mean_squared_error\n\nscore=mean_squared_error(y_test, result)\nrmse=math.sqrt(score)\nprint('RMSE is :', rmse)\n</code></pre></li>\n</ul>\n\n<p>I am getting RMSE value as 10.636853077462394</p>\n\n<p>My questions are :</p>\n\n<ul>\n<li>Since RMSE value is on lower side , can I say this model is decent ?</li>\n<li>Is there any other approach which I can use on this dataset ?</li>\n<li>Do I need to check for colinearity ?</li>\n<li>How can I check if multiple variables are interrelated ?</li>\n<li>Should I go for non-linear regression on this dataset ?</li>\n</ul>\n",
                "tags": "<machine-learning><regression><linear-regression>",
                "answers": [
                    [
                        "57862",
                        "2",
                        "27744",
                        "",
                        "",
                        "<p>There is a misconception about RMSE and other measurements for prediction quality, if you look at them standalone. In statistics you actually work with different models to compare RMSE's (using other approaches or other input variables) to have insights about prediction quality.</p>\n\n<p>Moreover, to conclude if a model is appropiate given your data you test the model assumptions. In linear Regression:</p>\n\n<ul>\n<li>Estimation Error follows a Normal distribution with E(mean) = 0, and sigma\u00b2</li>\n<li>Errors and input data are not autocorrelated (Find Beusch Godfrey - test, or ACF-Plot)</li>\n<li>No multicollinearity (Your dependent variables are not too strongly correlated - Pearson Correlation)</li>\n<li>Homoscedasticity (implied by normal distribution and independent error - Find White's test)</li>\n</ul>\n\n<p>However, good estimation doesn't necessarily lead to better predictions</p>\n",
                        "",
                        "1"
                    ],
                    [
                        "57869",
                        "2",
                        "27744",
                        "",
                        "",
                        "<p>Welcome to the site! You could also ask yourself, \"Is <em>data science</em> fit for this data?\"</p>\n\n<p>Not all datasets require some sort of algorithmic approach. Depending on what you're going after, this may not be a data science problem. In traffic studies, a good number of problems are solved with \"plain\" statistics. For example, you could use the Poisson Distribution to solve any number of issues with your current dataset and that can be very effective even though it has relatively little to do with data science.</p>\n",
                        "",
                        ""
                    ],
                    [
                        "27746",
                        "2",
                        "27744",
                        "",
                        "",
                        "<p>For the first question, it is important to recall that RMSE has the same unit as the dependent variable. It means that there is no absolute good or bad threshold, however you can define it based on your DV. For a datum which ranges from 0 to 1000, an RMSE of 0.7 is small, but if the range goes from 0 to 1, it is not small.</p>\n\n<p>I would do some feature engineering (Create more variables: Time of Day, Day of Week, Month etc...) and run it through a Neural Network and then check for accuracy.  You may want to check if there is any correlation between the 4 junctions as well but if you run a NN, you don't have to.</p>\n",
                        "",
                        "2"
                    ],
                    [
                        "27773",
                        "2",
                        "27744",
                        "",
                        "",
                        "<ul>\n<li>Is the target variable likely to be linearly, or additively dependent on the inputs? This means Monday will always have, say 10 more vehicles than Tuesday. If it is more intuitive to say that Monday will have 10% more vehicles than Tuesday, you can consider a log-linear model: transform the target variable by taking a log.</li>\n<li>In addition to measuring RMSE, you may want to visualize the data and predictions. Plot the actual and predicted vehicles on y-axis and date on x-axis, separately for each junction. This should tell you something about how good your model is, and potentially where it is going wrong.</li>\n<li>Based on the method of defining the time variables, the features are not likely to correlated to each other.</li>\n</ul>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "16312",
            "_score": 15.91946,
            "_source": {
                "title": "Why is r squared lowered when adding polynomial features?",
                "content": "Why is r squared lowered when adding polynomial features? <p>I am trying to find a best fit line <code>f(x) = ?</code> for a random set of <em>x,y</em> coordinates.</p>\n\n<p>Linear Regression with polynomial features works well for around 10 different polynomials but beyond 10 the r squared actually starts to drop!</p>\n\n<p>If the new features are not useful to the Linear Regression I would assume that they would be given a coefficient of 0 and therefore adding features should not hurt the overall r squared.</p>\n\n<p>I reproduced this problem when housing price predictions when creating a large amount of interaction features.</p>\n\n<p>I have my python code below:</p>\n\n<p><strong>Create Random Data</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef pol(x):\n    return x * np.cos(x)\n\nx = np.linspace(0, 12, 100)\nrng = np.random.RandomState(1234)\nrng.shuffle(x)\nx = np.sort(x[:25])\ny = pol(x) + np.random.randn(25)*2\n\n\nplt.scatter(x, y, color='green', s=50, marker='.')\n\nplt.show()\n</code></pre>\n\n<p><strong>Regress and Check Each R Squared</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfor p in range(1,30):\n    plot_range = [i/10 for i in range(0,120)]\n    poly = PolynomialFeatures(p)\n    X_fin = poly.fit_transform([[samp] for samp in x])\n    X_fin_plot = poly.fit_transform([[samp] for samp in plot_range])\n    reg = LinearRegression().fit(X_fin, y)\n\n    from sklearn.metrics import mean_squared_error, r2_score\n    print(p,r2_score(y, reg.predict(X_fin)))\n</code></pre>\n\n<p><strong>Display Last Regression Line</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>plt.scatter(x, y, color='green', s=50, marker='.')\nplt.plot(plot_range,reg.predict(X_fin_plot))\nplt.show()\n</code></pre>\n\n<p>I also have two plots to compare. The first is with 10 polynomial features and the second is with 40. Notice how the second misses the majority of the first points.\n<a href=\"https://i.stack.imgur.com/8yQku.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8yQku.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/tpJEW.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tpJEW.png\" alt=\"enter image description here\"></a></p>\n <scikit-learn><predictive-modeling><linear-regression><p>You've got 25 points, so there is a perfect fitting polynomial of degree 24.  That doesn't happen, so something is breaking in the OLS solver, but I'm not sure of what exactly or how to detect that.  It's not too surprising though that you may have numerical issues when <code>p</code> gets large: you've got an x-value near 0.1 and others past 10; raising them to the 24th power pushes them very far apart, and probably generates many more significant digits than python is keeping around.</p>\n\n<p>I've put together a demonstration:<br>\n<a href=\"https://github.com/bmreiniger/datascience.stackexchange/blob/master/53818.ipynb\" rel=\"nofollow noreferrer\">https://github.com/bmreiniger/datascience.stackexchange/blob/master/53818.ipynb</a><br>\nScaling the x-values helps, though we still don't find something visually matching the perfect polynomial fit.</p>\n\n<p>See also <a href=\"https://stats.stackexchange.com/questions/350130/why-is-gradient-descent-so-bad-at-optimizing-polynomial-regression\">https://stats.stackexchange.com/questions/350130/why-is-gradient-descent-so-bad-at-optimizing-polynomial-regression</a></p>\n<p>My original answer was not correct, so here is a corrected answer:</p>\n\n<p>When you use <code>PolynomialFeatures()</code>, you don't get the intended polynomials. Instead you get polynomials plus an interaction term:</p>\n\n<pre><code>from sklearn.preprocessing import PolynomialFeatures import numpy as \nnp    z = np.array([[0, 1],\n                    [2, 3],\n                    [4, 5]]) \npoly = PolynomialFeatures(2)\nprint(poly.fit_transform(z))\n</code></pre>\n\n<p>Output is:</p>\n\n<pre><code>[[ 1.  0.  1.  0.  0.  1.]\n [ 1.  2.  3.  4.  6.  9.]\n [ 1.  4.  5. 16. 20. 25.]]\n</code></pre>\n\n<p>A raw polynomial should look like:</p>\n\n<pre><code>new_z = np.hstack((z**(i+1) for i in range(2)))\nprint(new_z)\n</code></pre>\n\n<p>Output is:</p>\n\n<pre><code>[[ 0  1  0  1]\n [ 2  3  4  9]\n [ 4  5 16 25]]\n</code></pre>\n\n<p>Here is a quick R implementation of your problem with raw polynomials:</p>\n\n<pre><code>x = c(0.12121212, 1.09090909, 3.27272727, 3.51515152, 4, 4.24242424,\n  4.72727273, 4.84848485, 5.09090909, 6.18181818, 6.78787879, 7.15151515,\n  7.39393939, 7.63636364, 8.24242424, 8.60606061, 9.09090909, 9.81818182,\n  9.93939394, 10.3030303, 10.54545455, 10.66666667, 11.39393939, 11.63636364,\n  11.87878788)\n\ny = c(-2.87011136,1.77132943,-1.23698978,-3.09768628,-2.11919042,-4.11234626,\n  -1.1684339, 1.34601699, -2.37623758,4.20290438, 6.16349341, 3.60661197,\n  2.58898819, 3.80785471, -2.96359566, -5.672873, -9.71694313, -7.62778351,\n  -8.95730409, -8.04664475, -5.18464423, -6.54562138, 3.45527603, 6.11936457,\n  9.30106747)\n\nregdata = data.frame(x,y)\ncolnames(regdata) &lt;- c(\"x\",\"y\")\n\nr2list = list()\nr2adjlist = list()\nplist = list()\n\nfor (p in seq(1:29)){\n  reg = lm(y~poly(x,p, raw=T), data=regdata)\n  print(paste0(\"Poly: \", p))\n  print(paste0(\"  R2      \", summary(reg)<span class=\"math-container\">$r.squared))\n  print(paste0(\"  R2_adj. \", summary(reg)$</span>adj.r.squared))\n  r2list[[p]] &lt;-  summary(reg)<span class=\"math-container\">$r.squared\n  r2adjlist[[p]] &lt;- summary(reg)$</span>adj.r.squared\n  plist[[p]] &lt;- p\n}\n\nplot(plist, r2list,xlab=\"Polynomial\", ylab=\"R2\")\nlines(plist, r2list)\n</code></pre>\n\n<p>The R2 contingent on the degree of the polynomial is shown below:\n<a href=\"https://i.stack.imgur.com/ZYuYB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZYuYB.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>So your initial intuition was (of course) correct, but your treatment of data was not correct.</p>\n",
                "codes": [
                    [],
                    [
                        "from sklearn.preprocessing import PolynomialFeatures import numpy as \nnp    z = np.array([[0, 1],\n                    [2, 3],\n                    [4, 5]]) \npoly = PolynomialFeatures(2)\nprint(poly.fit_transform(z))\n",
                        "[[ 1.  0.  1.  0.  0.  1.]\n [ 1.  2.  3.  4.  6.  9.]\n [ 1.  4.  5. 16. 20. 25.]]\n",
                        "new_z = np.hstack((z**(i+1) for i in range(2)))\nprint(new_z)\n",
                        "[[ 0  1  0  1]\n [ 2  3  4  9]\n [ 4  5 16 25]]\n"
                    ]
                ],
                "question_id:": "53818",
                "question_votes:": "1",
                "question_text:": "<p>I am trying to find a best fit line <code>f(x) = ?</code> for a random set of <em>x,y</em> coordinates.</p>\n\n<p>Linear Regression with polynomial features works well for around 10 different polynomials but beyond 10 the r squared actually starts to drop!</p>\n\n<p>If the new features are not useful to the Linear Regression I would assume that they would be given a coefficient of 0 and therefore adding features should not hurt the overall r squared.</p>\n\n<p>I reproduced this problem when housing price predictions when creating a large amount of interaction features.</p>\n\n<p>I have my python code below:</p>\n\n<p><strong>Create Random Data</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef pol(x):\n    return x * np.cos(x)\n\nx = np.linspace(0, 12, 100)\nrng = np.random.RandomState(1234)\nrng.shuffle(x)\nx = np.sort(x[:25])\ny = pol(x) + np.random.randn(25)*2\n\n\nplt.scatter(x, y, color='green', s=50, marker='.')\n\nplt.show()\n</code></pre>\n\n<p><strong>Regress and Check Each R Squared</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfor p in range(1,30):\n    plot_range = [i/10 for i in range(0,120)]\n    poly = PolynomialFeatures(p)\n    X_fin = poly.fit_transform([[samp] for samp in x])\n    X_fin_plot = poly.fit_transform([[samp] for samp in plot_range])\n    reg = LinearRegression().fit(X_fin, y)\n\n    from sklearn.metrics import mean_squared_error, r2_score\n    print(p,r2_score(y, reg.predict(X_fin)))\n</code></pre>\n\n<p><strong>Display Last Regression Line</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>plt.scatter(x, y, color='green', s=50, marker='.')\nplt.plot(plot_range,reg.predict(X_fin_plot))\nplt.show()\n</code></pre>\n\n<p>I also have two plots to compare. The first is with 10 polynomial features and the second is with 40. Notice how the second misses the majority of the first points.\n<a href=\"https://i.stack.imgur.com/8yQku.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8yQku.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.stack.imgur.com/tpJEW.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tpJEW.png\" alt=\"enter image description here\"></a></p>\n",
                "tags": "<scikit-learn><predictive-modeling><linear-regression>",
                "answers": [
                    [
                        "54107",
                        "2",
                        "53818",
                        "",
                        "",
                        "<p>You've got 25 points, so there is a perfect fitting polynomial of degree 24.  That doesn't happen, so something is breaking in the OLS solver, but I'm not sure of what exactly or how to detect that.  It's not too surprising though that you may have numerical issues when <code>p</code> gets large: you've got an x-value near 0.1 and others past 10; raising them to the 24th power pushes them very far apart, and probably generates many more significant digits than python is keeping around.</p>\n\n<p>I've put together a demonstration:<br>\n<a href=\"https://github.com/bmreiniger/datascience.stackexchange/blob/master/53818.ipynb\" rel=\"nofollow noreferrer\">https://github.com/bmreiniger/datascience.stackexchange/blob/master/53818.ipynb</a><br>\nScaling the x-values helps, though we still don't find something visually matching the perfect polynomial fit.</p>\n\n<p>See also <a href=\"https://stats.stackexchange.com/questions/350130/why-is-gradient-descent-so-bad-at-optimizing-polynomial-regression\">https://stats.stackexchange.com/questions/350130/why-is-gradient-descent-so-bad-at-optimizing-polynomial-regression</a></p>\n",
                        "",
                        "2"
                    ],
                    [
                        "53822",
                        "2",
                        "53818",
                        "",
                        "",
                        "<p>My original answer was not correct, so here is a corrected answer:</p>\n\n<p>When you use <code>PolynomialFeatures()</code>, you don't get the intended polynomials. Instead you get polynomials plus an interaction term:</p>\n\n<pre><code>from sklearn.preprocessing import PolynomialFeatures import numpy as \nnp    z = np.array([[0, 1],\n                    [2, 3],\n                    [4, 5]]) \npoly = PolynomialFeatures(2)\nprint(poly.fit_transform(z))\n</code></pre>\n\n<p>Output is:</p>\n\n<pre><code>[[ 1.  0.  1.  0.  0.  1.]\n [ 1.  2.  3.  4.  6.  9.]\n [ 1.  4.  5. 16. 20. 25.]]\n</code></pre>\n\n<p>A raw polynomial should look like:</p>\n\n<pre><code>new_z = np.hstack((z**(i+1) for i in range(2)))\nprint(new_z)\n</code></pre>\n\n<p>Output is:</p>\n\n<pre><code>[[ 0  1  0  1]\n [ 2  3  4  9]\n [ 4  5 16 25]]\n</code></pre>\n\n<p>Here is a quick R implementation of your problem with raw polynomials:</p>\n\n<pre><code>x = c(0.12121212, 1.09090909, 3.27272727, 3.51515152, 4, 4.24242424,\n  4.72727273, 4.84848485, 5.09090909, 6.18181818, 6.78787879, 7.15151515,\n  7.39393939, 7.63636364, 8.24242424, 8.60606061, 9.09090909, 9.81818182,\n  9.93939394, 10.3030303, 10.54545455, 10.66666667, 11.39393939, 11.63636364,\n  11.87878788)\n\ny = c(-2.87011136,1.77132943,-1.23698978,-3.09768628,-2.11919042,-4.11234626,\n  -1.1684339, 1.34601699, -2.37623758,4.20290438, 6.16349341, 3.60661197,\n  2.58898819, 3.80785471, -2.96359566, -5.672873, -9.71694313, -7.62778351,\n  -8.95730409, -8.04664475, -5.18464423, -6.54562138, 3.45527603, 6.11936457,\n  9.30106747)\n\nregdata = data.frame(x,y)\ncolnames(regdata) &lt;- c(\"x\",\"y\")\n\nr2list = list()\nr2adjlist = list()\nplist = list()\n\nfor (p in seq(1:29)){\n  reg = lm(y~poly(x,p, raw=T), data=regdata)\n  print(paste0(\"Poly: \", p))\n  print(paste0(\"  R2      \", summary(reg)<span class=\"math-container\">$r.squared))\n  print(paste0(\"  R2_adj. \", summary(reg)$</span>adj.r.squared))\n  r2list[[p]] &lt;-  summary(reg)<span class=\"math-container\">$r.squared\n  r2adjlist[[p]] &lt;- summary(reg)$</span>adj.r.squared\n  plist[[p]] &lt;- p\n}\n\nplot(plist, r2list,xlab=\"Polynomial\", ylab=\"R2\")\nlines(plist, r2list)\n</code></pre>\n\n<p>The R2 contingent on the degree of the polynomial is shown below:\n<a href=\"https://i.stack.imgur.com/ZYuYB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZYuYB.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>So your initial intuition was (of course) correct, but your treatment of data was not correct.</p>\n",
                        "",
                        "3"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "12435",
            "_score": 15.885702,
            "_source": {
                "title": "How can someone avoid over fitting or data leak in ridge and lasso regression when the training score is high and test score is low?",
                "content": "How can someone avoid over fitting or data leak in ridge and lasso regression when the training score is high and test score is low? <p>I used the code provided here:\n<a href=\"https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b</a></p>\n\n<p>The only difference is that i used StandardScalar on my data given below:</p>\n\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\nprint len(X_test), len(y_test)\n</code></pre>\n\n<p>Here are my ridge regression results:\n<code>linear regression train score: 1.0\nlinear regression test score: -0.07550729376673715\nridge regression train score low alpha: 0.9999999970240117\nridge regression test score low alpha: -0.07532716978805554\nridge regression train score high alpha: 0.8659167364307487\nridge regression test score high alpha: 0.013702748149851396</code></p>\n\n<p>My Lasso results:\n<code>training score: 0.48725444995774625\ntest score:  -0.3393210376146986\nnumber of features used:  4\ntraining score for alpha=0.01: 0.9998352085084429\ntest score for alpha =0.01:  -0.6995903332119675\nnumber of features used: for alpha =0.01: 24\ntraining score for alpha=0.0001: 0.9999999830932269\ntest score for alpha =0.0001:  -0.7189894474663594\nnumber of features used: for alpha =0.0001: 25\nLR training score: 1.0\nLR test score:  -0.7217224228737649</code></p>\n\n<p>I am not able to understand why am i getting such results!\nAny help is highly appreciated.</p>\n\n<p>Edit: The code is below</p>\n\n<pre><code>    #Importing modules\n\n        import sys\n        import math \n        import itertools\n        import numpy as np\n        import pandas as pd\n        from numpy import genfromtxt\n        from matplotlib import style\n        import matplotlib.pyplot as plt\n        from sklearn import linear_model\n        from matplotlib import style, figure\n        from sklearn.linear_model import Lasso\n        from sklearn.linear_model import Ridge\n        from sklearn.linear_model import LinearRegression\n        from sklearn.cross_validation import train_test_split\n\n    #Importing data\n    df = np.genfromtxt('/Users/pfc.csv', delimiter=',')\n\n    X = df[0:,1:298]\n    y = df[0:,0]\n    print (X).shape\n    print (y).shape\n    display (X)\n    display (y)\n    print (y)\n\n\n\n#print type(newY)# pandas core frame\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=4)\n\n  #Apply StandardScaler for feature scaling\n        from sklearn.preprocessing import StandardScaler\n        sc = StandardScaler()\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.transform (X_test)\n        print len(X_test), len(y_test)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization, coefficients are barely restricted and in this case linear and ridge regression resembles\n\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    rr.fit(X_train, y_train)\n    rr100 = Ridge(alpha=115.5) #  comparison with alpha value\n    rr100.fit(X_train, y_train)\n    train_score=lr.score(X_train, y_train)\n    test_score=lr.score(X_test, y_test)\n    Ridge_train_score = rr.score(X_train,y_train)\n    Ridge_test_score = rr.score(X_test, y_test)\n    Ridge_train_score100 = rr100.score(X_train,y_train)\n    Ridge_test_score100 = rr100.score(X_test, y_test)\n\n    print \"linear regression train score:\", train_score\n    print \"linear regression test score:\", test_score\n    print \"ridge regression train score low alpha:\", Ridge_train_score\n    print \"ridge regression test score low alpha:\", Ridge_test_score\n    print \"ridge regression train score high alpha:\", Ridge_train_score100\n    print \"ridge regression test score high alpha:\", Ridge_test_score100\n    plt.figure (figsize= (12.8,9.6), dpi =100)\n    plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; <span class=\"math-container\">$\\alpha = 0.01$</span>',zorder=7) # zorder for ordering the markers\n    plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; <span class=\"math-container\">$\\alpha = 100$</span>') # alpha here is for transparency\n    plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.show()\n\n    # difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are \n    # completely neglected\n    lasso = Lasso()\n    lasso.fit(X_train,y_train)\n    train_score=lasso.score(X_train,y_train)\n    test_score=lasso.score(X_test,y_test)\n    coeff_used = np.sum(lasso.coef_!=0)\n    print \"training score:\", train_score \n    print \"test score: \", test_score\n    print \"number of features used: \", coeff_used\n    lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n    lasso001.fit(X_train,y_train)\n    train_score001=lasso001.score(X_train,y_train)\n    test_score001=lasso001.score(X_test,y_test)\n    coeff_used001 = np.sum(lasso001.coef_!=0)\n    print \"training score for alpha=0.01:\", train_score001 \n    print \"test score for alpha =0.01: \", test_score001\n    print \"number of features used: for alpha =0.01:\", coeff_used001\n    lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n    lasso00001.fit(X_train,y_train)\n    train_score00001=lasso00001.score(X_train,y_train)\n    test_score00001=lasso00001.score(X_test,y_test)\n    coeff_used00001 = np.sum(lasso00001.coef_!=0)\n    print \"training score for alpha=0.0001:\", train_score00001 \n    print \"test score for alpha =0.0001: \", test_score00001\n    print \"number of features used: for alpha =0.0001:\", coeff_used00001\n    lr = LinearRegression()\n    lr.fit(X_train,y_train)\n    lr_train_score=lr.score(X_train,y_train)\n    lr_test_score=lr.score(X_test,y_test)\n    print \"LR training score:\", lr_train_score \n    print \"LR test score: \", lr_test_score\n    plt.figure (figsize= (12.8,9.6), dpi =100)\n    plt.subplot(1,2,1)\n    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; <span class=\"math-container\">$\\alpha = 1$</span>',zorder=7) # alpha here is for transparency\n    plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.01$</span>') # alpha here is for transparency\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.subplot(1,2,2)\n    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; <span class=\"math-container\">$\\alpha = 1$</span>',zorder=7) # alpha here is for transparency\n    plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.01$</span>') # alpha here is for transparency\n    plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.00001$</span>') # alpha here is for transparency\n    plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.tight_layout()\n    plt.show()\n</code></pre>\n\n<p>PS: Please ignore the indentation.</p>\n <machine-learning><regression><machine-learning-model><overfitting><p>You should probably post your code, since you have a negative score which is not possible if the score is R-squared.</p>\n\n<p>The code on the link you provided uses sklearn's .score() function which computes R-squared of the fit. The R-squared metric ranges from 0 to 1 and shows the percentage of the variation explained by the model. This means that an R-squared of 1 means your model perfectly fits the data. After you fix the code so that there is no negative values, taking a look at the R-square should give you enough to understand when it is over-fitting and under-fitting.</p>\n\n<p><strong>Hint:</strong> if you have significantly higher R-squared on your training data than test data, it means that the model is ovefitting. Good luck!</p>\n\n<p>Edit: turns out, R-squared can be negative, it means that it performs very poorly! If you just aim to find the best alpha, i suggest you use LassoCV which finds the alpha that optimises the model using cross validation, there is sk-learn implementation.</p>\n<p>I found this to be a good answer to my question. \n<a href=\"http://www.fairlynerdy.com/what-is-r-squared/\" rel=\"nofollow noreferrer\">http://www.fairlynerdy.com/what-is-r-squared/</a></p>\n",
                "codes": [
                    [],
                    []
                ],
                "question_id:": "43661",
                "question_votes:": "1",
                "question_text:": "<p>I used the code provided here:\n<a href=\"https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\" rel=\"nofollow noreferrer\">https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b</a></p>\n\n<p>The only difference is that i used StandardScalar on my data given below:</p>\n\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\nprint len(X_test), len(y_test)\n</code></pre>\n\n<p>Here are my ridge regression results:\n<code>linear regression train score: 1.0\nlinear regression test score: -0.07550729376673715\nridge regression train score low alpha: 0.9999999970240117\nridge regression test score low alpha: -0.07532716978805554\nridge regression train score high alpha: 0.8659167364307487\nridge regression test score high alpha: 0.013702748149851396</code></p>\n\n<p>My Lasso results:\n<code>training score: 0.48725444995774625\ntest score:  -0.3393210376146986\nnumber of features used:  4\ntraining score for alpha=0.01: 0.9998352085084429\ntest score for alpha =0.01:  -0.6995903332119675\nnumber of features used: for alpha =0.01: 24\ntraining score for alpha=0.0001: 0.9999999830932269\ntest score for alpha =0.0001:  -0.7189894474663594\nnumber of features used: for alpha =0.0001: 25\nLR training score: 1.0\nLR test score:  -0.7217224228737649</code></p>\n\n<p>I am not able to understand why am i getting such results!\nAny help is highly appreciated.</p>\n\n<p>Edit: The code is below</p>\n\n<pre><code>    #Importing modules\n\n        import sys\n        import math \n        import itertools\n        import numpy as np\n        import pandas as pd\n        from numpy import genfromtxt\n        from matplotlib import style\n        import matplotlib.pyplot as plt\n        from sklearn import linear_model\n        from matplotlib import style, figure\n        from sklearn.linear_model import Lasso\n        from sklearn.linear_model import Ridge\n        from sklearn.linear_model import LinearRegression\n        from sklearn.cross_validation import train_test_split\n\n    #Importing data\n    df = np.genfromtxt('/Users/pfc.csv', delimiter=',')\n\n    X = df[0:,1:298]\n    y = df[0:,0]\n    print (X).shape\n    print (y).shape\n    display (X)\n    display (y)\n    print (y)\n\n\n\n#print type(newY)# pandas core frame\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=4)\n\n  #Apply StandardScaler for feature scaling\n        from sklearn.preprocessing import StandardScaler\n        sc = StandardScaler()\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.transform (X_test)\n        print len(X_test), len(y_test)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha &gt; more generalization, coefficients are barely restricted and in this case linear and ridge regression resembles\n\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    rr.fit(X_train, y_train)\n    rr100 = Ridge(alpha=115.5) #  comparison with alpha value\n    rr100.fit(X_train, y_train)\n    train_score=lr.score(X_train, y_train)\n    test_score=lr.score(X_test, y_test)\n    Ridge_train_score = rr.score(X_train,y_train)\n    Ridge_test_score = rr.score(X_test, y_test)\n    Ridge_train_score100 = rr100.score(X_train,y_train)\n    Ridge_test_score100 = rr100.score(X_test, y_test)\n\n    print \"linear regression train score:\", train_score\n    print \"linear regression test score:\", test_score\n    print \"ridge regression train score low alpha:\", Ridge_train_score\n    print \"ridge regression test score low alpha:\", Ridge_test_score\n    print \"ridge regression train score high alpha:\", Ridge_train_score100\n    print \"ridge regression test score high alpha:\", Ridge_test_score100\n    plt.figure (figsize= (12.8,9.6), dpi =100)\n    plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; <span class=\"math-container\">$\\alpha = 0.01$</span>',zorder=7) # zorder for ordering the markers\n    plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; <span class=\"math-container\">$\\alpha = 100$</span>') # alpha here is for transparency\n    plt.plot(lr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.show()\n\n    # difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are \n    # completely neglected\n    lasso = Lasso()\n    lasso.fit(X_train,y_train)\n    train_score=lasso.score(X_train,y_train)\n    test_score=lasso.score(X_test,y_test)\n    coeff_used = np.sum(lasso.coef_!=0)\n    print \"training score:\", train_score \n    print \"test score: \", test_score\n    print \"number of features used: \", coeff_used\n    lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n    lasso001.fit(X_train,y_train)\n    train_score001=lasso001.score(X_train,y_train)\n    test_score001=lasso001.score(X_test,y_test)\n    coeff_used001 = np.sum(lasso001.coef_!=0)\n    print \"training score for alpha=0.01:\", train_score001 \n    print \"test score for alpha =0.01: \", test_score001\n    print \"number of features used: for alpha =0.01:\", coeff_used001\n    lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n    lasso00001.fit(X_train,y_train)\n    train_score00001=lasso00001.score(X_train,y_train)\n    test_score00001=lasso00001.score(X_test,y_test)\n    coeff_used00001 = np.sum(lasso00001.coef_!=0)\n    print \"training score for alpha=0.0001:\", train_score00001 \n    print \"test score for alpha =0.0001: \", test_score00001\n    print \"number of features used: for alpha =0.0001:\", coeff_used00001\n    lr = LinearRegression()\n    lr.fit(X_train,y_train)\n    lr_train_score=lr.score(X_train,y_train)\n    lr_test_score=lr.score(X_test,y_test)\n    print \"LR training score:\", lr_train_score \n    print \"LR test score: \", lr_test_score\n    plt.figure (figsize= (12.8,9.6), dpi =100)\n    plt.subplot(1,2,1)\n    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; <span class=\"math-container\">$\\alpha = 1$</span>',zorder=7) # alpha here is for transparency\n    plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.01$</span>') # alpha here is for transparency\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.subplot(1,2,2)\n    plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; <span class=\"math-container\">$\\alpha = 1$</span>',zorder=7) # alpha here is for transparency\n    plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.01$</span>') # alpha here is for transparency\n    plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; <span class=\"math-container\">$\\alpha = 0.00001$</span>') # alpha here is for transparency\n    plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n    plt.xlabel('Coefficient Index',fontsize=16)\n    plt.ylabel('Coefficient Magnitude',fontsize=16)\n    plt.legend(fontsize=13,loc=4)\n    plt.tight_layout()\n    plt.show()\n</code></pre>\n\n<p>PS: Please ignore the indentation.</p>\n",
                "tags": "<machine-learning><regression><machine-learning-model><overfitting>",
                "answers": [
                    [
                        "43716",
                        "2",
                        "43661",
                        "",
                        "",
                        "<p>You should probably post your code, since you have a negative score which is not possible if the score is R-squared.</p>\n\n<p>The code on the link you provided uses sklearn's .score() function which computes R-squared of the fit. The R-squared metric ranges from 0 to 1 and shows the percentage of the variation explained by the model. This means that an R-squared of 1 means your model perfectly fits the data. After you fix the code so that there is no negative values, taking a look at the R-square should give you enough to understand when it is over-fitting and under-fitting.</p>\n\n<p><strong>Hint:</strong> if you have significantly higher R-squared on your training data than test data, it means that the model is ovefitting. Good luck!</p>\n\n<p>Edit: turns out, R-squared can be negative, it means that it performs very poorly! If you just aim to find the best alpha, i suggest you use LassoCV which finds the alpha that optimises the model using cross validation, there is sk-learn implementation.</p>\n",
                        "",
                        "2"
                    ],
                    [
                        "43808",
                        "2",
                        "43661",
                        "",
                        "",
                        "<p>I found this to be a good answer to my question. \n<a href=\"http://www.fairlynerdy.com/what-is-r-squared/\" rel=\"nofollow noreferrer\">http://www.fairlynerdy.com/what-is-r-squared/</a></p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "12298",
            "_score": 13.565727,
            "_source": {
                "title": "Using the Python Keras multi_gpu_model with LSTM / GRU to predict Timeseries data",
                "content": "Using the Python Keras multi_gpu_model with LSTM / GRU to predict Timeseries data <p>I'm having an issue with python keras LSTM / GRU layers with <code>multi_gpu_model</code> for machine learning. </p>\n\n<p>When I use a single GPU, the predictions work correctly matching the sinusoidal data in the script below. See image labeled \"1 GPUs\".</p>\n\n<p><a href=\"https://i.stack.imgur.com/N4ANi.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/N4ANi.png\" alt=\"Using 1 GPU\"></a></p>\n\n<p>When I use multiple GPUs, the inverse transforms of both the training and test data return results that cluster around the lows of the original data  See image labeled \"4 GPUs\".</p>\n\n<p><a href=\"https://i.stack.imgur.com/WEbFn.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WEbFn.png\" alt=\"Using 4 GPUs\"></a></p>\n\n<p>Is this:</p>\n\n<ol>\n<li>a bug? </li>\n<li>a case where I'm missing a multiplier that should be used\nwhen <code>multi_gpu_model</code> is used? </li>\n<li>an example where the\n<code>multi_gpu_model</code> documentation isn't complete with a\ncaveat to cover this specific case.</li>\n<li>the result of flaw(s) in my code?</li>\n</ol>\n\n<hr>\n\n<p>Versions</p>\n\n<pre><code>Keras                   2.2.4  \nKeras-Applications      1.0.6  \nKeras-Preprocessing     1.0.5  \ntensorboard             1.12.0 \ntensorflow-gpu          1.12.0 \n</code></pre>\n\n<p>GPUs</p>\n\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 107...  Off  | 00000000:08:00.0 Off |                  N/A |\n| 30%   42C    P0    36W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 107...  Off  | 00000000:09:00.0 Off |                  N/A |\n| 36%   48C    P0    37W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 107...  Off  | 00000000:41:00.0 Off |                  N/A |\n| 34%   44C    P0    34W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX 107...  Off  | 00000000:42:00.0 Off |                  N/A |\n| 31%   42C    P0    32W / 180W |      0MiB /  8112MiB |      5%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre>\n\n<p>Script</p>\n\n<pre><code>#!/usr/bin/env python3\n\"\"\"LSTM for sinusoidal data problem with regression framing.\n\nBased on:\n\nhttps://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n\n\"\"\"\n\n# Standard imports\nimport argparse\nimport math\n\n# PIP3 imports\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.utils import multi_gpu_model\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\ndef main():\n    # fix random seed for reproducibility\n    numpy.random.seed(7)\n\n    # Get CLI arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--gpus',\n        help='Number of GPUs to use.',\n        type=int, default=1)\n    args = parser.parse_args()\n    gpus = args.gpus\n\n    # load the dataset\n    dataframe = DataFrame(\n        [0.00000, 5.99000, 11.92016, 17.73121, 23.36510, 28.76553, 33.87855,\n         38.65306, 43.04137, 46.99961, 50.48826, 53.47244, 55.92235, 57.81349,\n         59.12698, 59.84970, 59.97442, 59.49989, 58.43086, 56.77801, 54.55785,\n         51.79256, 48.50978, 44.74231, 40.52779, 35.90833, 30.93008, 25.64279,\n         20.09929, 14.35496, 8.46720, 2.49484, -3.50245, -9.46474, -15.33247,\n         -21.04699, -26.55123, -31.79017, -36.71147, -41.26597, -45.40815,\n         -49.09663, -52.29455, -54.96996, -57.09612, -58.65181, -59.62146,\n         -59.99540, -59.76988, -58.94716, -57.53546, -55.54888, -53.00728,\n         -49.93605, -46.36587, -42.33242, -37.87600, -33.04113, -27.87613,\n         -22.43260, -16.76493, -10.92975, -4.98536, 1.00883, 6.99295, 12.90720,\n         18.69248, 24.29100, 29.64680, 34.70639, 39.41920, 43.73814, 47.62007,\n         51.02620, 53.92249, 56.28000, 58.07518, 59.29009, 59.91260, 59.93648,\n         59.36149, 58.19339, 56.44383, 54.13031, 51.27593, 47.90923, 44.06383,\n         39.77815, 35.09503, 30.06125, 24.72711, 19.14590, 13.37339, 7.46727,\n         1.48653, -4.50907, -10.45961, -16.30564, -21.98875, -27.45215,\n         -32.64127, -37.50424, -41.99248, -46.06115, -49.66959, -52.78175,\n         -55.36653, -57.39810, -58.85617, -59.72618, -59.99941, -59.67316,\n         -58.75066, -57.24115, -55.15971, -52.52713, -49.36972, -45.71902,\n         -41.61151, -37.08823, -32.19438, -26.97885, -21.49376, -15.79391,\n         -9.93625, -3.97931, 2.01738, 7.99392, 13.89059, 19.64847, 25.21002,\n         30.51969, 35.52441, 40.17419, 44.42255, 48.22707, 51.54971, 54.35728,\n         56.62174, 58.32045, 59.43644, 59.95856, 59.88160, 59.20632, 57.93947,\n         56.09370, 53.68747, 50.74481, 47.29512, 43.37288, 39.01727, 34.27181,\n         29.18392, 23.80443, 18.18710, 12.38805, 6.46522, 0.47779, -5.51441,\n         -11.45151])\n    dataset = dataframe.values\n    dataset = dataset.astype('float32')\n\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    dataset = scaler.fit_transform(dataset)\n\n    # split into train and test sets\n    train_size = int(len(dataset) * 0.67)\n    test_size = len(dataset) - train_size\n    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n\n    # reshape into X=t and Y=t+1\n    look_back = 1\n    trainX, trainY = create_dataset(train, look_back)\n    testX, testY = create_dataset(test, look_back)\n\n    # reshape input to be [samples, time steps, features]\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n    # create and fit the LSTM network\n    with tf.device('/cpu:0'):\n        serial_model = Sequential()\n    serial_model.add(LSTM(4, input_shape=(1, look_back)))\n    serial_model.add(Dense(1))\n    if gpus == 1:\n        parallel_model = serial_model\n    else:\n        parallel_model = multi_gpu_model(\n            serial_model,\n            cpu_relocation=True,\n            gpus=gpus)\n    parallel_model.compile(\n        loss='mean_squared_error', optimizer='adam')\n    parallel_model.fit(\n        trainX, trainY,\n        epochs=100,\n        batch_size=int(dataset.size * gpus / 20),\n        verbose=2)\n\n    # make predictions\n    if gpus == 1:\n        trainPredict = parallel_model.predict(trainX)\n        testPredict = parallel_model.predict(testX)\n    else:\n        trainPredict = serial_model.predict(trainX)\n        testPredict = serial_model.predict(testX)\n\n    # invert predictions\n    trainPredict = scaler.inverse_transform(trainPredict)\n    trainY = scaler.inverse_transform([trainY])\n    testPredict = scaler.inverse_transform(testPredict)\n    testY = scaler.inverse_transform([testY])\n\n    # calculate root mean squared error\n    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:, 0]))\n    print('Train Score: %.2f RMSE' % (trainScore))\n    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:, 0]))\n    print('Test Score: %.2f RMSE' % (testScore))\n\n    # shift train predictions for plotting\n    trainPredictPlot = numpy.empty_like(dataset)\n    trainPredictPlot[:, :] = numpy.nan\n    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\n    # shift test predictions for plotting\n    testPredictPlot = numpy.empty_like(dataset)\n    testPredictPlot[:, :] = numpy.nan\n    testPredictPlot[\n        len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\n    # plot baseline and predictions\n    plt.plot(scaler.inverse_transform(dataset), label='Complete Data')\n    plt.plot(trainPredictPlot, label='Training Data')\n    plt.plot(testPredictPlot, label='Prediction Data')\n    plt.legend(loc='upper left')\n    plt.title('Using {} GPUs'.format(gpus))\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n\n<p>I thought it may have something to do with the Sequential model, but I get the same results when I replace:</p>\n\n<pre><code># create and fit the LSTM network\nwith tf.device('/cpu:0'):\n    serial_model = Sequential()\nserial_model.add(LSTM(4, input_shape=(1, look_back)))\nserial_model.add(Dense(1))\n</code></pre>\n\n<p>with:</p>\n\n<pre><code>from keras import Model, Input\n\n# Create layers for model\nx_tensor = Input(shape=(1, look_back))\nlayer_1 = LSTM(4)(x_tensor)\ny_tensor = Dense(1)(layer_1)\n\n# Create and fit the LSTM network\nwith tf.device('/cpu:0'):\n    serial_model = Model(inputs=x_tensor, outputs=y_tensor)\n</code></pre>\n\n<p>I now think it has something to do with the way <code>multi_gpu_model</code> splits the timeseries data across the GPUs. The RMSE error rates are noticeably different.</p>\n\n<p><strong>RMSE - I GPU</strong></p>\n\n<pre><code>Train Score: 4.49 RMSE\nTest Score: 4.79 RMSE\n</code></pre>\n\n<p><strong>RMSE - 4 GPUs</strong></p>\n\n<pre><code>Train Score: 76.54 RMSE\nTest Score: 77.55 RMSE\n</code></pre>\n <python><keras><tensorflow><lstm><gpu>",
                "codes": [],
                "question_id:": "43236",
                "question_votes:": "2",
                "question_text:": "<p>I'm having an issue with python keras LSTM / GRU layers with <code>multi_gpu_model</code> for machine learning. </p>\n\n<p>When I use a single GPU, the predictions work correctly matching the sinusoidal data in the script below. See image labeled \"1 GPUs\".</p>\n\n<p><a href=\"https://i.stack.imgur.com/N4ANi.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/N4ANi.png\" alt=\"Using 1 GPU\"></a></p>\n\n<p>When I use multiple GPUs, the inverse transforms of both the training and test data return results that cluster around the lows of the original data  See image labeled \"4 GPUs\".</p>\n\n<p><a href=\"https://i.stack.imgur.com/WEbFn.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WEbFn.png\" alt=\"Using 4 GPUs\"></a></p>\n\n<p>Is this:</p>\n\n<ol>\n<li>a bug? </li>\n<li>a case where I'm missing a multiplier that should be used\nwhen <code>multi_gpu_model</code> is used? </li>\n<li>an example where the\n<code>multi_gpu_model</code> documentation isn't complete with a\ncaveat to cover this specific case.</li>\n<li>the result of flaw(s) in my code?</li>\n</ol>\n\n<hr>\n\n<p>Versions</p>\n\n<pre><code>Keras                   2.2.4  \nKeras-Applications      1.0.6  \nKeras-Preprocessing     1.0.5  \ntensorboard             1.12.0 \ntensorflow-gpu          1.12.0 \n</code></pre>\n\n<p>GPUs</p>\n\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 107...  Off  | 00000000:08:00.0 Off |                  N/A |\n| 30%   42C    P0    36W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 107...  Off  | 00000000:09:00.0 Off |                  N/A |\n| 36%   48C    P0    37W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 107...  Off  | 00000000:41:00.0 Off |                  N/A |\n| 34%   44C    P0    34W / 180W |      0MiB /  8119MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX 107...  Off  | 00000000:42:00.0 Off |                  N/A |\n| 31%   42C    P0    32W / 180W |      0MiB /  8112MiB |      5%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre>\n\n<p>Script</p>\n\n<pre><code>#!/usr/bin/env python3\n\"\"\"LSTM for sinusoidal data problem with regression framing.\n\nBased on:\n\nhttps://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n\n\"\"\"\n\n# Standard imports\nimport argparse\nimport math\n\n# PIP3 imports\nimport numpy\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.utils import multi_gpu_model\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\ndef main():\n    # fix random seed for reproducibility\n    numpy.random.seed(7)\n\n    # Get CLI arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--gpus',\n        help='Number of GPUs to use.',\n        type=int, default=1)\n    args = parser.parse_args()\n    gpus = args.gpus\n\n    # load the dataset\n    dataframe = DataFrame(\n        [0.00000, 5.99000, 11.92016, 17.73121, 23.36510, 28.76553, 33.87855,\n         38.65306, 43.04137, 46.99961, 50.48826, 53.47244, 55.92235, 57.81349,\n         59.12698, 59.84970, 59.97442, 59.49989, 58.43086, 56.77801, 54.55785,\n         51.79256, 48.50978, 44.74231, 40.52779, 35.90833, 30.93008, 25.64279,\n         20.09929, 14.35496, 8.46720, 2.49484, -3.50245, -9.46474, -15.33247,\n         -21.04699, -26.55123, -31.79017, -36.71147, -41.26597, -45.40815,\n         -49.09663, -52.29455, -54.96996, -57.09612, -58.65181, -59.62146,\n         -59.99540, -59.76988, -58.94716, -57.53546, -55.54888, -53.00728,\n         -49.93605, -46.36587, -42.33242, -37.87600, -33.04113, -27.87613,\n         -22.43260, -16.76493, -10.92975, -4.98536, 1.00883, 6.99295, 12.90720,\n         18.69248, 24.29100, 29.64680, 34.70639, 39.41920, 43.73814, 47.62007,\n         51.02620, 53.92249, 56.28000, 58.07518, 59.29009, 59.91260, 59.93648,\n         59.36149, 58.19339, 56.44383, 54.13031, 51.27593, 47.90923, 44.06383,\n         39.77815, 35.09503, 30.06125, 24.72711, 19.14590, 13.37339, 7.46727,\n         1.48653, -4.50907, -10.45961, -16.30564, -21.98875, -27.45215,\n         -32.64127, -37.50424, -41.99248, -46.06115, -49.66959, -52.78175,\n         -55.36653, -57.39810, -58.85617, -59.72618, -59.99941, -59.67316,\n         -58.75066, -57.24115, -55.15971, -52.52713, -49.36972, -45.71902,\n         -41.61151, -37.08823, -32.19438, -26.97885, -21.49376, -15.79391,\n         -9.93625, -3.97931, 2.01738, 7.99392, 13.89059, 19.64847, 25.21002,\n         30.51969, 35.52441, 40.17419, 44.42255, 48.22707, 51.54971, 54.35728,\n         56.62174, 58.32045, 59.43644, 59.95856, 59.88160, 59.20632, 57.93947,\n         56.09370, 53.68747, 50.74481, 47.29512, 43.37288, 39.01727, 34.27181,\n         29.18392, 23.80443, 18.18710, 12.38805, 6.46522, 0.47779, -5.51441,\n         -11.45151])\n    dataset = dataframe.values\n    dataset = dataset.astype('float32')\n\n    # normalize the dataset\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    dataset = scaler.fit_transform(dataset)\n\n    # split into train and test sets\n    train_size = int(len(dataset) * 0.67)\n    test_size = len(dataset) - train_size\n    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n\n    # reshape into X=t and Y=t+1\n    look_back = 1\n    trainX, trainY = create_dataset(train, look_back)\n    testX, testY = create_dataset(test, look_back)\n\n    # reshape input to be [samples, time steps, features]\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n    # create and fit the LSTM network\n    with tf.device('/cpu:0'):\n        serial_model = Sequential()\n    serial_model.add(LSTM(4, input_shape=(1, look_back)))\n    serial_model.add(Dense(1))\n    if gpus == 1:\n        parallel_model = serial_model\n    else:\n        parallel_model = multi_gpu_model(\n            serial_model,\n            cpu_relocation=True,\n            gpus=gpus)\n    parallel_model.compile(\n        loss='mean_squared_error', optimizer='adam')\n    parallel_model.fit(\n        trainX, trainY,\n        epochs=100,\n        batch_size=int(dataset.size * gpus / 20),\n        verbose=2)\n\n    # make predictions\n    if gpus == 1:\n        trainPredict = parallel_model.predict(trainX)\n        testPredict = parallel_model.predict(testX)\n    else:\n        trainPredict = serial_model.predict(trainX)\n        testPredict = serial_model.predict(testX)\n\n    # invert predictions\n    trainPredict = scaler.inverse_transform(trainPredict)\n    trainY = scaler.inverse_transform([trainY])\n    testPredict = scaler.inverse_transform(testPredict)\n    testY = scaler.inverse_transform([testY])\n\n    # calculate root mean squared error\n    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:, 0]))\n    print('Train Score: %.2f RMSE' % (trainScore))\n    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:, 0]))\n    print('Test Score: %.2f RMSE' % (testScore))\n\n    # shift train predictions for plotting\n    trainPredictPlot = numpy.empty_like(dataset)\n    trainPredictPlot[:, :] = numpy.nan\n    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n\n    # shift test predictions for plotting\n    testPredictPlot = numpy.empty_like(dataset)\n    testPredictPlot[:, :] = numpy.nan\n    testPredictPlot[\n        len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n\n    # plot baseline and predictions\n    plt.plot(scaler.inverse_transform(dataset), label='Complete Data')\n    plt.plot(trainPredictPlot, label='Training Data')\n    plt.plot(testPredictPlot, label='Prediction Data')\n    plt.legend(loc='upper left')\n    plt.title('Using {} GPUs'.format(gpus))\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n\n<p>I thought it may have something to do with the Sequential model, but I get the same results when I replace:</p>\n\n<pre><code># create and fit the LSTM network\nwith tf.device('/cpu:0'):\n    serial_model = Sequential()\nserial_model.add(LSTM(4, input_shape=(1, look_back)))\nserial_model.add(Dense(1))\n</code></pre>\n\n<p>with:</p>\n\n<pre><code>from keras import Model, Input\n\n# Create layers for model\nx_tensor = Input(shape=(1, look_back))\nlayer_1 = LSTM(4)(x_tensor)\ny_tensor = Dense(1)(layer_1)\n\n# Create and fit the LSTM network\nwith tf.device('/cpu:0'):\n    serial_model = Model(inputs=x_tensor, outputs=y_tensor)\n</code></pre>\n\n<p>I now think it has something to do with the way <code>multi_gpu_model</code> splits the timeseries data across the GPUs. The RMSE error rates are noticeably different.</p>\n\n<p><strong>RMSE - I GPU</strong></p>\n\n<pre><code>Train Score: 4.49 RMSE\nTest Score: 4.79 RMSE\n</code></pre>\n\n<p><strong>RMSE - 4 GPUs</strong></p>\n\n<pre><code>Train Score: 76.54 RMSE\nTest Score: 77.55 RMSE\n</code></pre>\n",
                "tags": "<python><keras><tensorflow><lstm><gpu>",
                "answers": []
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "17487",
            "_score": 13.0907,
            "_source": {
                "title": "Interpretability of RMSE and R squared scores on cross validation",
                "content": "Interpretability of RMSE and R squared scores on cross validation <p>I'm working on a regression problem with 30k rows in my dataset, decided to use XGBoost mainly to avoid processing data for a quick primitive model. And i noticed upon doing cross-validation that there's a noticeable difference between R\u00b2 for train and R\u00b2 for CV => clear signs of overfitting. Here's my code for CV : </p>\n\n<pre><code>oof_train = np.zeros((len(train_maisons)))\nind = 0\ncv_scores = []\ntrain_scores=[]\nfor ind,(ind_train,ind_val) in (enumerate (kfolds.split(X,y))):\n    #ind=+1\n    X_train,X_val = X.iloc[ind_train],X.iloc[ind_val] \n    y_train,y_val = y.iloc[ind_train],y.iloc[ind_val]\n    xgb = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,objective='reg:squarederror',eval_metric='rmse')\n    xgb.fit(X_train,y_train)\n    val_pred = xgb.predict(X_val)\n    train_pred = xgb.predict(X_train)\n    oof_train[ind_val] += val_pred\n    score_fold_validation=np.sqrt(mean_squared_error(y_val, val_pred))\n    score_fold_train=np.sqrt(mean_squared_error(y_train, train_pred))\n    train_scores.append(score_fold_train)\n    cv_scores.append(score_fold_validation)\n    #r2_score(y_val, grid.best_estimator_.predict(X_val))\n    print('Iteration : {} - CV Score : {} - R\u00b2 Score CV : {} - Train Score : {} - R\u00b2 Score train : {}'.format(str(ind+1),score_fold_validation,r2_score(y_val, val_pred),score_fold_train,r2_score(y_train,train_pred)))\nend_train_score=np.mean(train_scores)\ntrain_scores.append(end_train_score)\nend_cv_score=np.mean(cv_scores)\n</code></pre>\n\n<p>Using SquaredError as objective ( loss function ) , evaluating with RMSE and R\u00b2, here are the metrics' outputs : </p>\n\n<pre><code>CV Score : 96416.84137549331 - R\u00b2 Score CV : 0.6545903695464426 - Train Score : 30605.655815355676 - R\u00b2 Score train : 0.9730563148067477\n</code></pre>\n\n<p>My <strong>question</strong>: is this considered an overwhelming overfitting problem? or is it mild? <em>and</em> should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters ). And one last thing, is my result on X_train indicative that my features are informative enough to learn the target? or is the <strong>R\u00b2 train score</strong> somehow <strong>biased</strong>?</p>\n\n<p><strong><em>Note</em></strong> : In this code i'm using <strong>10 folds</strong> for CV. Used <strong>3 folds</strong> gave me a <strong>better</strong> result on CV, if someone can also explain that , it would be great.</p>\n <regression><xgboost><cross-validation><overfitting><metric><blockquote>\n  <p>And one last thing, is my result on X_train indicative that my features are informative enough to learn the target? or is the R\u00b2 train score somehow biased?</p>\n</blockquote>\n\n<p>High scoring fits on training data does not necessarily indicate that your features are informative enough to learn the target <strong>in a general fashion</strong>. Only your cross validation scores can do so.</p>\n\n<blockquote>\n  <p>Note : In this code i'm using 10 folds for CV. Used 3 folds gave me a better result on CV, if someone can also explain that , it would be great.</p>\n</blockquote>\n\n<p>I wouldn't expect wildly significant differences so it depends on how much better they were, but remember that you're randomly selecting a subset of data for each fold. It's plausible that when you ran it for 3 folds the model was coincidentally better able to learn from the selected training sets to predict the validation sets. Generally speaking, the more folds you use the more confident you should be in the reliability of the score.</p>\n\n<blockquote>\n  <p>My question: is this considered an overwhelming overfitting problem? or is it mild? and should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters )</p>\n</blockquote>\n\n<p>I'm no professional, but I would consider that to be severely overfit. I would in the first instance return to hyperparameter tuning to try and bring them in line. I'll also  perform the obligatory plug of RandomizedSearchCV at this point: <a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" rel=\"nofollow noreferrer\">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a></p>\n\n<p>EDIT: </p>\n\n<p>Since you've asked for further details:</p>\n\n<p><strong>1) Does a high score against training data indicate that the features are informative enough to learn the target?</strong></p>\n\n<p>Consider the following example:</p>\n\n<pre><code>import numpy as np\nfrom xgboost import XGBRegressor\n\nx = np.random.rand(5000, 12)\ny = np.random.randint(0, 200, 5000)\n\nrfr = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,eval_metric='rmse')\nrfr.fit(x, y)\nrfr.score(x, y)\n</code></pre>\n\n<blockquote>\n  <p>Out[26]: 0.999918392694166</p>\n</blockquote>\n\n<p>Entirely random targets trained against entirely random inputs, and still scoring almost perfectly. That's a completely useless model; validating against your training data does not allow you to determine that your features are informative enough to allow you to predict the target against data that your model has not seen. <strong>Only</strong> cross-validation against unseen data can do this.</p>\n\n<p><strong>2) Used 3 folds gave me a better result on CV, if someone can also explain that , it would be great.</strong></p>\n\n<p>The RNG gods smiled upon you. Again, consider an example:</p>\n\n<pre><code>from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv', index_col=None)\n\nx = df.drop('species', axis=1)\ny = df.species\n\nxgb = XGBClassifier(max_depth=3)\n\nkf = KFold(n_splits=10, shuffle=True)\n\nfor train_idx, test_idx in kf.split(x):\n    xgb.fit(x.iloc[train_idx], y.iloc[train_idx])\n    print(xgb.score(x.iloc[test_idx], y.iloc[test_idx]))\n</code></pre>\n\n<blockquote>\n  <p>Out[17]: 0.9333333333333333<br>\n  0.9333333333333333<br>\n  0.9333333333333333<br>\n  1.0<br>\n  0.8666666666666667<br>\n  1.0<br>\n  1.0<br>\n  0.9333333333333333<br>\n  0.8666666666666667<br>\n  1.0  </p>\n</blockquote>\n\n<p>That code splits the data 10 times and then trains and scores the model on each selected slice. As you can see, there's a 13.4% difference between the highest and worst scoring slices.</p>\n\n<p>Your 3-Fold run coincidentally gave you 3 'good' folds. The more Folds you have, the more representative of the true result your cross-validation is.</p>\n\n<p><strong>3) is this considered an overwhelming overfitting problem? or is it mild? and should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters )</strong></p>\n\n<p>Well, whether or not it's <em>overwhelming</em> is something of a matter of opinion. In my mind however a loss against the validation set that is 300% the training score is very severely overfit indeed, but if your <em>validation</em> RMSE is still within the margin of error you're willing to accept then I suppose you could go ahead and use it anyway. It's really your call.</p>\n\n<p>To reduce the overfitting, you need to tune your hyperparameters better. Reducing <code>max_depth</code> and increasing <code>min_samples_split</code> is my usual go-to with trees. If your revised model (exhibiting either no overfitting or at least significantly reduced overfitting) then has a <strong>cross-validation</strong> score that is too low for you, you should return at that point to feature engineering. We can highlight the effect of hyperparameters on overfitting quite easily by plotting the effect of tweaking them:</p>\n\n<pre><code>from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv', index_col=None)\n\nohe = OneHotEncoder(sparse=False)\nx = pd.DataFrame(ohe.fit_transform(df[['cut', 'color', 'clarity']]))\n\ndf.drop(['cut', 'color', 'clarity'], axis=1, inplace=True)\n\ndf = df.merge(x, left_index=True, right_index=True)\n\nx = df.drop('price', axis=1)\ny = df.price\n\ncv = {}\nfor i in range(20):\n    xgb=XGBRegressor(max_depth=i+1)\n    x_train, x_test, y_train,  y_test = train_test_split(x, y)\n    xgb.fit(x_train, y_train)\n    cv[i+1] = (xgb.score(x_train, y_train), xgb.score(x_test, y_test))\n\ntrain = []\ntest = []\nfor i in cv.keys():\n    train.append(cv[i][0])\n    test.append(cv[i][1])\n\nfig = plt.figure()\nplt.plot(train)\nplt.plot(test)\nplt.legend(['train', 'test'])\nplt.xlabel('max_depth')\nplt.xlim(0, 20)\nplt.ylabel('R^2 Score')\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/r8RDP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/r8RDP.png\" alt=\"enter image description here\"></a></p>\n\n<p>Apologies for the stupid x-axis labels :P. We can see the affect of the <code>max_depth</code> hyperparameter on the model's tendency to overfit easily; anything over 3-ish and the train/test scores begin to diverge.</p>\n\n<p>Hope that those revisions are more helpful to you :)</p>\n",
                "codes": [
                    [
                        "import numpy as np\nfrom xgboost import XGBRegressor\n\nx = np.random.rand(5000, 12)\ny = np.random.randint(0, 200, 5000)\n\nrfr = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,eval_metric='rmse')\nrfr.fit(x, y)\nrfr.score(x, y)\n",
                        "from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv', index_col=None)\n\nx = df.drop('species', axis=1)\ny = df.species\n\nxgb = XGBClassifier(max_depth=3)\n\nkf = KFold(n_splits=10, shuffle=True)\n\nfor train_idx, test_idx in kf.split(x):\n    xgb.fit(x.iloc[train_idx], y.iloc[train_idx])\n    print(xgb.score(x.iloc[test_idx], y.iloc[test_idx]))\n",
                        "from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv', index_col=None)\n\nohe = OneHotEncoder(sparse=False)\nx = pd.DataFrame(ohe.fit_transform(df[['cut', 'color', 'clarity']]))\n\ndf.drop(['cut', 'color', 'clarity'], axis=1, inplace=True)\n\ndf = df.merge(x, left_index=True, right_index=True)\n\nx = df.drop('price', axis=1)\ny = df.price\n\ncv = {}\nfor i in range(20):\n    xgb=XGBRegressor(max_depth=i+1)\n    x_train, x_test, y_train,  y_test = train_test_split(x, y)\n    xgb.fit(x_train, y_train)\n    cv[i+1] = (xgb.score(x_train, y_train), xgb.score(x_test, y_test))\n\ntrain = []\ntest = []\nfor i in cv.keys():\n    train.append(cv[i][0])\n    test.append(cv[i][1])\n\nfig = plt.figure()\nplt.plot(train)\nplt.plot(test)\nplt.legend(['train', 'test'])\nplt.xlabel('max_depth')\nplt.xlim(0, 20)\nplt.ylabel('R^2 Score')\nplt.show()\n"
                    ]
                ],
                "question_id:": "56321",
                "question_votes:": "4",
                "question_text:": "<p>I'm working on a regression problem with 30k rows in my dataset, decided to use XGBoost mainly to avoid processing data for a quick primitive model. And i noticed upon doing cross-validation that there's a noticeable difference between R\u00b2 for train and R\u00b2 for CV => clear signs of overfitting. Here's my code for CV : </p>\n\n<pre><code>oof_train = np.zeros((len(train_maisons)))\nind = 0\ncv_scores = []\ntrain_scores=[]\nfor ind,(ind_train,ind_val) in (enumerate (kfolds.split(X,y))):\n    #ind=+1\n    X_train,X_val = X.iloc[ind_train],X.iloc[ind_val] \n    y_train,y_val = y.iloc[ind_train],y.iloc[ind_val]\n    xgb = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,objective='reg:squarederror',eval_metric='rmse')\n    xgb.fit(X_train,y_train)\n    val_pred = xgb.predict(X_val)\n    train_pred = xgb.predict(X_train)\n    oof_train[ind_val] += val_pred\n    score_fold_validation=np.sqrt(mean_squared_error(y_val, val_pred))\n    score_fold_train=np.sqrt(mean_squared_error(y_train, train_pred))\n    train_scores.append(score_fold_train)\n    cv_scores.append(score_fold_validation)\n    #r2_score(y_val, grid.best_estimator_.predict(X_val))\n    print('Iteration : {} - CV Score : {} - R\u00b2 Score CV : {} - Train Score : {} - R\u00b2 Score train : {}'.format(str(ind+1),score_fold_validation,r2_score(y_val, val_pred),score_fold_train,r2_score(y_train,train_pred)))\nend_train_score=np.mean(train_scores)\ntrain_scores.append(end_train_score)\nend_cv_score=np.mean(cv_scores)\n</code></pre>\n\n<p>Using SquaredError as objective ( loss function ) , evaluating with RMSE and R\u00b2, here are the metrics' outputs : </p>\n\n<pre><code>CV Score : 96416.84137549331 - R\u00b2 Score CV : 0.6545903695464426 - Train Score : 30605.655815355676 - R\u00b2 Score train : 0.9730563148067477\n</code></pre>\n\n<p>My <strong>question</strong>: is this considered an overwhelming overfitting problem? or is it mild? <em>and</em> should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters ). And one last thing, is my result on X_train indicative that my features are informative enough to learn the target? or is the <strong>R\u00b2 train score</strong> somehow <strong>biased</strong>?</p>\n\n<p><strong><em>Note</em></strong> : In this code i'm using <strong>10 folds</strong> for CV. Used <strong>3 folds</strong> gave me a <strong>better</strong> result on CV, if someone can also explain that , it would be great.</p>\n",
                "tags": "<regression><xgboost><cross-validation><overfitting><metric>",
                "answers": [
                    [
                        "56375",
                        "2",
                        "56321",
                        "",
                        "",
                        "<blockquote>\n  <p>And one last thing, is my result on X_train indicative that my features are informative enough to learn the target? or is the R\u00b2 train score somehow biased?</p>\n</blockquote>\n\n<p>High scoring fits on training data does not necessarily indicate that your features are informative enough to learn the target <strong>in a general fashion</strong>. Only your cross validation scores can do so.</p>\n\n<blockquote>\n  <p>Note : In this code i'm using 10 folds for CV. Used 3 folds gave me a better result on CV, if someone can also explain that , it would be great.</p>\n</blockquote>\n\n<p>I wouldn't expect wildly significant differences so it depends on how much better they were, but remember that you're randomly selecting a subset of data for each fold. It's plausible that when you ran it for 3 folds the model was coincidentally better able to learn from the selected training sets to predict the validation sets. Generally speaking, the more folds you use the more confident you should be in the reliability of the score.</p>\n\n<blockquote>\n  <p>My question: is this considered an overwhelming overfitting problem? or is it mild? and should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters )</p>\n</blockquote>\n\n<p>I'm no professional, but I would consider that to be severely overfit. I would in the first instance return to hyperparameter tuning to try and bring them in line. I'll also  perform the obligatory plug of RandomizedSearchCV at this point: <a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" rel=\"nofollow noreferrer\">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a></p>\n\n<p>EDIT: </p>\n\n<p>Since you've asked for further details:</p>\n\n<p><strong>1) Does a high score against training data indicate that the features are informative enough to learn the target?</strong></p>\n\n<p>Consider the following example:</p>\n\n<pre><code>import numpy as np\nfrom xgboost import XGBRegressor\n\nx = np.random.rand(5000, 12)\ny = np.random.randint(0, 200, 5000)\n\nrfr = XGBRegressor(colsample_bytree=0.6,gamma=0.3,learning_rate=0.1,max_depth=8,min_child_weight=3,subsample=0.9,n_estimators=1000,eval_metric='rmse')\nrfr.fit(x, y)\nrfr.score(x, y)\n</code></pre>\n\n<blockquote>\n  <p>Out[26]: 0.999918392694166</p>\n</blockquote>\n\n<p>Entirely random targets trained against entirely random inputs, and still scoring almost perfectly. That's a completely useless model; validating against your training data does not allow you to determine that your features are informative enough to allow you to predict the target against data that your model has not seen. <strong>Only</strong> cross-validation against unseen data can do this.</p>\n\n<p><strong>2) Used 3 folds gave me a better result on CV, if someone can also explain that , it would be great.</strong></p>\n\n<p>The RNG gods smiled upon you. Again, consider an example:</p>\n\n<pre><code>from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv', index_col=None)\n\nx = df.drop('species', axis=1)\ny = df.species\n\nxgb = XGBClassifier(max_depth=3)\n\nkf = KFold(n_splits=10, shuffle=True)\n\nfor train_idx, test_idx in kf.split(x):\n    xgb.fit(x.iloc[train_idx], y.iloc[train_idx])\n    print(xgb.score(x.iloc[test_idx], y.iloc[test_idx]))\n</code></pre>\n\n<blockquote>\n  <p>Out[17]: 0.9333333333333333<br>\n  0.9333333333333333<br>\n  0.9333333333333333<br>\n  1.0<br>\n  0.8666666666666667<br>\n  1.0<br>\n  1.0<br>\n  0.9333333333333333<br>\n  0.8666666666666667<br>\n  1.0  </p>\n</blockquote>\n\n<p>That code splits the data 10 times and then trains and scores the model on each selected slice. As you can see, there's a 13.4% difference between the highest and worst scoring slices.</p>\n\n<p>Your 3-Fold run coincidentally gave you 3 'good' folds. The more Folds you have, the more representative of the true result your cross-validation is.</p>\n\n<p><strong>3) is this considered an overwhelming overfitting problem? or is it mild? and should I do more feature engineering or tune hyperparameters more? ( used GridSearchCV for the current hyperparameters )</strong></p>\n\n<p>Well, whether or not it's <em>overwhelming</em> is something of a matter of opinion. In my mind however a loss against the validation set that is 300% the training score is very severely overfit indeed, but if your <em>validation</em> RMSE is still within the margin of error you're willing to accept then I suppose you could go ahead and use it anyway. It's really your call.</p>\n\n<p>To reduce the overfitting, you need to tune your hyperparameters better. Reducing <code>max_depth</code> and increasing <code>min_samples_split</code> is my usual go-to with trees. If your revised model (exhibiting either no overfitting or at least significantly reduced overfitting) then has a <strong>cross-validation</strong> score that is too low for you, you should return at that point to feature engineering. We can highlight the effect of hyperparameters on overfitting quite easily by plotting the effect of tweaking them:</p>\n\n<pre><code>from xgboost import XGBClassifier, XGBRegressor\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\n\ndf = pd.read_csv(r'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv', index_col=None)\n\nohe = OneHotEncoder(sparse=False)\nx = pd.DataFrame(ohe.fit_transform(df[['cut', 'color', 'clarity']]))\n\ndf.drop(['cut', 'color', 'clarity'], axis=1, inplace=True)\n\ndf = df.merge(x, left_index=True, right_index=True)\n\nx = df.drop('price', axis=1)\ny = df.price\n\ncv = {}\nfor i in range(20):\n    xgb=XGBRegressor(max_depth=i+1)\n    x_train, x_test, y_train,  y_test = train_test_split(x, y)\n    xgb.fit(x_train, y_train)\n    cv[i+1] = (xgb.score(x_train, y_train), xgb.score(x_test, y_test))\n\ntrain = []\ntest = []\nfor i in cv.keys():\n    train.append(cv[i][0])\n    test.append(cv[i][1])\n\nfig = plt.figure()\nplt.plot(train)\nplt.plot(test)\nplt.legend(['train', 'test'])\nplt.xlabel('max_depth')\nplt.xlim(0, 20)\nplt.ylabel('R^2 Score')\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/r8RDP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/r8RDP.png\" alt=\"enter image description here\"></a></p>\n\n<p>Apologies for the stupid x-axis labels :P. We can see the affect of the <code>max_depth</code> hyperparameter on the model's tendency to overfit easily; anything over 3-ish and the train/test scores begin to diverge.</p>\n\n<p>Hope that those revisions are more helpful to you :)</p>\n",
                        "",
                        "6"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "14915",
            "_score": 12.05065,
            "_source": {
                "title": "Random forest vs. XGBoost vs. MLP Regressor for estimating claims costs",
                "content": "Random forest vs. XGBoost vs. MLP Regressor for estimating claims costs <h1>Context</h1>\n\n<p>I'm building a (toy) machine learning model estimate the cost of an insurance claim (injury related). Aim is to teach myself machine learning by doing. I have settled on three algorithms to test: Random forest, XGBoost and a multi-layer perceptron.</p>\n\n<h1>Data set</h1>\n\n<p>The data set has the following columns: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>cols = [ 'AGE_RANGE', 'GENDER', 'TOTAL_PAID', 'INDUSTRY_DESCRIPTION', 'WORKER_AGE', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<p>'TOTAL_PAID' is the label ($s). The rest are features. The majority of features are categorical: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>categoricals = [ 'AGE_RANGE', 'GENDER', 'INDUSTRY_DESCRIPTION', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<h1>Code:</h1>\n\n<p>Import: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport numpy as np\n\ncols = [ 'AGE_RANGE', 'GENDER', 'TOTAL_PAID', 'INDUSTRY_DESCRIPTION', 'WORKER_AGE', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\nfeatures = pd.read_csv('gs://longtailclaims2/filename.csv', usecols = cols, header=0, encoding='ISO-8859-1')\ncategoricals = [ 'AGE_RANGE', 'GENDER', 'INDUSTRY_DESCRIPTION', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<p>First I turn categorical values into 0s and 1s: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>features2 = pd.get_dummies(features, columns = categoricals)\n</code></pre>\n\n<p>Then I isolate features from labels (TOTAL_PAID):</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>labels = np.array(features['TOTAL_PAID'])\nfeatures = features2.drop('TOTAL_PAID', axis = 1)\nfeature_list = list(features.columns)\nfeature_list_no_facts = list(features.columns)\n</code></pre>\n\n<p>Spit into SK Learn training and test set: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\ntest_features.head(5)\ntest_features.head(5)\n</code></pre>\n\n<p>Then I explore the data: </p>\n\n<pre><code>print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n</code></pre>\n\n<p>Output: </p>\n\n<pre><code>Training Features Shape: (128304, 337)\nTraining Labels Shape: (128304,)\nTesting Features Shape: (42768, 337)\nTesting Labels Shape: (42768,)\n</code></pre>\n\n<h1>1. XGBRegressor</h1>\n\n<p>First we try training XGBoost model. I needed so push # estimates above 10,000 to get a decent accuracy (R2 > 0.94)</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Import the model we are using\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import ensemble \nfrom sklearn.metrics import mean_squared_error\n\nx = XGBRegressor(random_state = 44, n_jobs = 8, n_estimators = 10000, max_depth=10, verbosity = 3)\nx.fit(train_features, train_labels)\nprint('xgboost train score: ', x.score(train_features, train_labels))\npredictions = x.predict(test_features)\nprint('xgboost test score: ', x.score(test_features, test_labels))\n</code></pre>\n\n<p>Here, train and test score: R2 ~0.94.</p>\n\n<h1>2. Random Forest Regressor</h1>\n\n<p>Then we try Random Forest model. After some fiddling it appears 100 estimators is enough to get a pretty good accuracy (R2 > 0.94)</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Instantiate model with 100 decision trees\nrf = RandomForestRegressor(n_estimators = 100, criterion='mse', verbose=1, random_state = np.random.RandomState(42), n_jobs = -1)\n# Train the model on training data\nrf.fit(train_features, train_labels);\nprint('random forest train score: ', rf.score(train_features, train_labels))\npredictions = rf.predict(test_features)\nprint('random forest test score: ', rf.score(test_features, test_labels))\n</code></pre>\n\n<p>In this case, train and test score R2 at ~0.94.</p>\n\n<h1>3. MLP Neural Network</h1>\n\n<p>Finally I wanted to compare performance to an MLP Regressor. According to the books I have been reading on deep learning, a neural network should be able to outperform any shallow learning algorithm given enough time and horse power. I am using a pretty beefy machine with 8 cores and 30 GB RAM on Google Cloud. </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.neural_network import MLPRegressor\nnn = MLPRegressor(hidden_layer_sizes=(338, 338, 50), \n                  activation='relu', solver='adam', max_iter = 100, random_state = 56, verbose = True)\nnn.fit(train_features, train_labels)\nnn_predictions = nn.predict(test_features)\nprint('nn train score: ', nn.score(train_features, train_labels))\nprint('nn test score: ', nn.score(test_features, test_labels))\n</code></pre>\n\n<p>R2 around 0.4.</p>\n\n<p>I've used 338 neurons in the input layer as this is the exact number of columns. The neural network stalls at 82 iterations and doesn't go any further. Running with 50 iterations I get accuracy at R2 &lt; 0.5, which is not impressive.</p>\n\n<h1>My questions:</h1>\n\n<ol>\n<li>Do I handle the management of categorical features correctly?\nWith the scores above, does it look like model #1 and #2 are\noverfitting? R2 > 0.94 is pretty good and both test and training\naccuracy look good and in same ballpark, so I don't think it is\noverfitting </li>\n<li><p>Why does the neural network not perform that well?</p></li>\n<li><p>Should I consider a different type of neural network for regression?</p></li>\n<li><p>Why do I have to add so many more estimators to XGBoost (10,000) to get the same performance as Random Forest (100)? </p></li>\n<li><p>What would be a fit for purpose neural network to solve this problem with deep learning? I am concerned the ensemble methods may not be appropriate or that I am doing something wrong.</p></li>\n</ol>\n <python><neural-network><scikit-learn><regression><xgboost><p>Regarding the <code>MLPRegressor</code>, you should use the <code>lbfgs</code> optimizer for better results with small datasets. Accuracy should be up to 0.99.  </p>\n<p>Some ideas:</p>\n\n<ol>\n<li><strong>Handling categorical features correctly:</strong> using one-hot encoding is one valid approach. Other approaches include target encoding (or mean encoding), and the hashing trick. There's no real hard and fast rule about when to choose which method.</li>\n<li><strong>Poor performance of neural network:</strong> I don't have much experience with neural networks, but I have read that inputs into neural networks should be scaled in some way - either standardised, or to lie within some narrow and consistent interval. You could also look at other layer structures - e.g. have you tried the default values from Scikit-Learn?</li>\n<li><strong>Considering different kind of network:</strong> Judge based on (2) above</li>\n<li><strong>More estimators in <code>xgboost</code>:</strong> <code>xgboost</code> has many parameters to fine tune. You should also consider that <code>xgboost</code> uses linear regression as a default regression task, which implies that your target insurance losses are normally distributed. This is not usually the case in the real world, where we see that insurance losses usually follow a Tweedie distribution. <code>xgboost</code> offers Tweedie regression capability.</li>\n<li><strong>Optimal neural network for this problem:</strong> Unsure as my experience with neural networks is limited.</li>\n</ol>\n",
                "codes": [
                    [],
                    []
                ],
                "question_id:": "49758",
                "question_votes:": "2",
                "question_text:": "<h1>Context</h1>\n\n<p>I'm building a (toy) machine learning model estimate the cost of an insurance claim (injury related). Aim is to teach myself machine learning by doing. I have settled on three algorithms to test: Random forest, XGBoost and a multi-layer perceptron.</p>\n\n<h1>Data set</h1>\n\n<p>The data set has the following columns: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>cols = [ 'AGE_RANGE', 'GENDER', 'TOTAL_PAID', 'INDUSTRY_DESCRIPTION', 'WORKER_AGE', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<p>'TOTAL_PAID' is the label ($s). The rest are features. The majority of features are categorical: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>categoricals = [ 'AGE_RANGE', 'GENDER', 'INDUSTRY_DESCRIPTION', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<h1>Code:</h1>\n\n<p>Import: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport numpy as np\n\ncols = [ 'AGE_RANGE', 'GENDER', 'TOTAL_PAID', 'INDUSTRY_DESCRIPTION', 'WORKER_AGE', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\nfeatures = pd.read_csv('gs://longtailclaims2/filename.csv', usecols = cols, header=0, encoding='ISO-8859-1')\ncategoricals = [ 'AGE_RANGE', 'GENDER', 'INDUSTRY_DESCRIPTION', 'NATURE_CODE', 'ACCIDENT_TYPE_CODE', 'INJURY_NATURE']\n</code></pre>\n\n<p>First I turn categorical values into 0s and 1s: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>features2 = pd.get_dummies(features, columns = categoricals)\n</code></pre>\n\n<p>Then I isolate features from labels (TOTAL_PAID):</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>labels = np.array(features['TOTAL_PAID'])\nfeatures = features2.drop('TOTAL_PAID', axis = 1)\nfeature_list = list(features.columns)\nfeature_list_no_facts = list(features.columns)\n</code></pre>\n\n<p>Spit into SK Learn training and test set: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\ntest_features.head(5)\ntest_features.head(5)\n</code></pre>\n\n<p>Then I explore the data: </p>\n\n<pre><code>print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n</code></pre>\n\n<p>Output: </p>\n\n<pre><code>Training Features Shape: (128304, 337)\nTraining Labels Shape: (128304,)\nTesting Features Shape: (42768, 337)\nTesting Labels Shape: (42768,)\n</code></pre>\n\n<h1>1. XGBRegressor</h1>\n\n<p>First we try training XGBoost model. I needed so push # estimates above 10,000 to get a decent accuracy (R2 > 0.94)</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Import the model we are using\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import ensemble \nfrom sklearn.metrics import mean_squared_error\n\nx = XGBRegressor(random_state = 44, n_jobs = 8, n_estimators = 10000, max_depth=10, verbosity = 3)\nx.fit(train_features, train_labels)\nprint('xgboost train score: ', x.score(train_features, train_labels))\npredictions = x.predict(test_features)\nprint('xgboost test score: ', x.score(test_features, test_labels))\n</code></pre>\n\n<p>Here, train and test score: R2 ~0.94.</p>\n\n<h1>2. Random Forest Regressor</h1>\n\n<p>Then we try Random Forest model. After some fiddling it appears 100 estimators is enough to get a pretty good accuracy (R2 > 0.94)</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Instantiate model with 100 decision trees\nrf = RandomForestRegressor(n_estimators = 100, criterion='mse', verbose=1, random_state = np.random.RandomState(42), n_jobs = -1)\n# Train the model on training data\nrf.fit(train_features, train_labels);\nprint('random forest train score: ', rf.score(train_features, train_labels))\npredictions = rf.predict(test_features)\nprint('random forest test score: ', rf.score(test_features, test_labels))\n</code></pre>\n\n<p>In this case, train and test score R2 at ~0.94.</p>\n\n<h1>3. MLP Neural Network</h1>\n\n<p>Finally I wanted to compare performance to an MLP Regressor. According to the books I have been reading on deep learning, a neural network should be able to outperform any shallow learning algorithm given enough time and horse power. I am using a pretty beefy machine with 8 cores and 30 GB RAM on Google Cloud. </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.neural_network import MLPRegressor\nnn = MLPRegressor(hidden_layer_sizes=(338, 338, 50), \n                  activation='relu', solver='adam', max_iter = 100, random_state = 56, verbose = True)\nnn.fit(train_features, train_labels)\nnn_predictions = nn.predict(test_features)\nprint('nn train score: ', nn.score(train_features, train_labels))\nprint('nn test score: ', nn.score(test_features, test_labels))\n</code></pre>\n\n<p>R2 around 0.4.</p>\n\n<p>I've used 338 neurons in the input layer as this is the exact number of columns. The neural network stalls at 82 iterations and doesn't go any further. Running with 50 iterations I get accuracy at R2 &lt; 0.5, which is not impressive.</p>\n\n<h1>My questions:</h1>\n\n<ol>\n<li>Do I handle the management of categorical features correctly?\nWith the scores above, does it look like model #1 and #2 are\noverfitting? R2 > 0.94 is pretty good and both test and training\naccuracy look good and in same ballpark, so I don't think it is\noverfitting </li>\n<li><p>Why does the neural network not perform that well?</p></li>\n<li><p>Should I consider a different type of neural network for regression?</p></li>\n<li><p>Why do I have to add so many more estimators to XGBoost (10,000) to get the same performance as Random Forest (100)? </p></li>\n<li><p>What would be a fit for purpose neural network to solve this problem with deep learning? I am concerned the ensemble methods may not be appropriate or that I am doing something wrong.</p></li>\n</ol>\n",
                "tags": "<python><neural-network><scikit-learn><regression><xgboost>",
                "answers": [
                    [
                        "56749",
                        "2",
                        "49758",
                        "",
                        "",
                        "<p>Regarding the <code>MLPRegressor</code>, you should use the <code>lbfgs</code> optimizer for better results with small datasets. Accuracy should be up to 0.99.  </p>\n",
                        "",
                        ""
                    ],
                    [
                        "49762",
                        "2",
                        "49758",
                        "",
                        "",
                        "<p>Some ideas:</p>\n\n<ol>\n<li><strong>Handling categorical features correctly:</strong> using one-hot encoding is one valid approach. Other approaches include target encoding (or mean encoding), and the hashing trick. There's no real hard and fast rule about when to choose which method.</li>\n<li><strong>Poor performance of neural network:</strong> I don't have much experience with neural networks, but I have read that inputs into neural networks should be scaled in some way - either standardised, or to lie within some narrow and consistent interval. You could also look at other layer structures - e.g. have you tried the default values from Scikit-Learn?</li>\n<li><strong>Considering different kind of network:</strong> Judge based on (2) above</li>\n<li><strong>More estimators in <code>xgboost</code>:</strong> <code>xgboost</code> has many parameters to fine tune. You should also consider that <code>xgboost</code> uses linear regression as a default regression task, which implies that your target insurance losses are normally distributed. This is not usually the case in the real world, where we see that insurance losses usually follow a Tweedie distribution. <code>xgboost</code> offers Tweedie regression capability.</li>\n<li><strong>Optimal neural network for this problem:</strong> Unsure as my experience with neural networks is limited.</li>\n</ol>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "14732",
            "_score": 7.960106,
            "_source": {
                "title": "Time Series - Models seem to not learn",
                "content": "Time Series - Models seem to not learn <p>I am doing my undergrad Dissertation on time series prediction, and use various models (linear /ridge regression,  AR(2), Random Forest, SVR, and 4 variations of Neural Networks) to try and 'predict' (for academic only reasons) daily return data, using as input lagged returns and SMA - RSI features (using TA - Lib) built based on those returns. However, I have noticed that my NNs do not learn anything, and upon inspecting the loss graph and the vector of predictions, I noticed it only predicts a single value, with the same applying for the Ridge and AR regressions.</p>\n\n<p>Also, when I try to calculate the correlation between the labels and the predictions (of the NNs) I get 'nan' as a result, no matter what I try, which I suspect has to do with the predictions. I also get wildly varying r2 scores on each re-run (even though I have set multiple seeds, both on Tensorflow backend as well as numpy) and always negative, which I cannot understand as even though my search on the internet and the sklearn's docs say it can be negative, my professor insists it cannot be, and I truly am bewildered.</p>\n\n<p>What can I do about it? Isn't it obviously wrong for an entire NN to predict only a single value? Below I include the code for the ridge / AR regressions as well as the 'Vanilla NN' and a couple of useful graphs. The data itself is quite large, so I don't know if there's much of a point to include it if not asked specifically, given there are no algorithmic errors below.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def vanillaNN(X_train, y_train,X_test,y_test):\n    n_cols = X_train.shape[1]\n    model = Sequential()\n    model.add(Dense(100,activation='relu', input_shape=(n_cols, )))\n    model.add(Dropout(0.3))\n    model.add(Dense(150, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1))\n\n    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n\n    history = model.fit(X_train,y_train,epochs=100,verbose=0,\n        shuffle=False, validation_split=0.1)  \n\n    # Use the last loss as the title\n    plt.plot(history.history['loss'])\n    plt.title('last loss:' + str(round(history.history['loss'][-1], 6)))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    # Calculate R^2 score and MSE\n    # .... Omitted Code ......\n\n    # it returns those for testing purposes in the IPython shell\n    return (train_scores, test_scores, y_pred_train, y_pred_test, y_train, y_test)\n    VNN_results = vanillaNN(\n        train_features,train_targets,\n        test_features,test_targets)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/tFp3c.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tFp3c.png\" alt=\"Neural Network Loss History - 100 epochs\"></a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def AR(X_train, order=2):\n    arma_train = np.array(X_train['returns'])\n    armodel = ARMA(arma_train, order=(order,0))\n    armodel_results = armodel.fit()\n    print(armodel_results.summary())\n    armodel_results.plot_predict(start=8670, end=8698)\n    plt.show()\n    ar_pred = armodel_results.predict(start=8699, end=9665)\n\n    # ...r2 and MSE scores omitted code...\n\n    return [mse_ar2, r2_ar2, ar_pred]\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/XULvD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/XULvD.png\" alt=\"AR(2) Model out-of-sample predictions\"></a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def Elastic(X_train, y_train, X_test, y_test):\n    elastic = ElasticNet()\n    param_grid_elastic = {'alpha': [0.001,  0.01, 0.1, 0.5],\n        'l1_ratio': [0.001, 0.01, 0.1, 0.5]\n                              }\n    grid_elastic = GridSearchCV(elastic, param_grid_elastic, \n        cv=tscv.split(X_train),scoring='neg_mean_squared_error')\n\n    grid_elastic.fit(X_train, y_train)\n\n    y_pred_train = grid_elastic.predict(X_train) \n    train_scores = scores(y_train, y_pred_train)\n\n    y_pred_test = grid_elastic.predict(X_test)\n\n    # ...Omitted Code...\n    return [train_scores, test_scores, y_pred_train, y_pred_test]\n</code></pre>\n\n<p>AR(2) sample test and train predictions:\n<a href=\"https://i.stack.imgur.com/RNtmw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/RNtmw.png\" alt=\"Test Set Predictions\"></a>\n<a href=\"https://i.stack.imgur.com/mmJDy.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/mmJDy.png\" alt=\"Train Set Predictions\"></a></p>\n\n<pre><code>SAMPLE DATA: Train features and Train Targets (future_returns) \n</code></pre>\n\n<p>It looks like a mess, but just copy paste into excel file and it should be good to go!)</p>\n\n<p>Date    returns ma14    rsi14   ma30    rsi30   ma50    rsi50   ma200   rsi200  future_returns\n10/14/1980  3.49E-05    42.76324407 49.21625218 66.6250545  49.69881565 49.45368438 49.93538688 37.78942977 50.51223405 0.013277481\n10/15/1980  0.013277481 0.239711734 53.45799196 0.16387242  51.78260494 0.140801819 51.19194274 0.10545251  50.79944024 -0.011855382\n10/16/1980  -0.011855382    -0.306338818    45.66265303 -0.159676722    47.88773425 -0.115851283    48.81901884 -0.107808537    50.24325364 -0.00414208\n10/17/1980  -0.00414208 -1.154286743    48.16105108 -0.451328445    49.1031414  -0.3083074  49.55134428 -0.299669528    50.4107189  0.007939494\n10/20/1980  0.007939494 0.548806765 51.89223141 0.338253188 50.95654204 0.15403304  50.679274   0.145277255 50.67207082 -0.0050544\n10/21/1980  -0.0050544  -0.580692978    47.89906352 -0.443621598    48.97241743 -0.250429072    49.46553614 -0.227539737    50.38503757 -2.93E-05\n10/22/1980  -2.93E-05   -85.38681662    49.51695915 -69.94007273    49.7551065  -46.09189634    49.93866124 -37.61970911    50.49403171 -0.018135363\n10/23/1980  -0.018135363    -0.020087358    44.19203763 -0.067897836    47.0643223  -0.037135977    48.27685366 -0.05615813 50.09551572 0.000415381\n10/24/1980  0.000415381 -2.422576075    50.11149401 3.125882141 49.93407273 1.480210269 50.01580668 2.418672772 50.49781053 -0.013535864\n10/27/1980  -0.013535864    0.12834969  46.14718747 -0.056014904    47.91325905 -0.053384853    48.75782925 -0.065375964    50.19198915 0.00337859\n10/28/1980  0.00337859  -0.566993349    51.18890074 0.168834456 50.4293269  0.275579337 50.30416736 0.265200747 50.55684672 -0.003396646\n10/29/1980  -0.003396646    0.522213275 49.20187924 0.045438303 49.43972414 -0.207144904    49.69125655 -0.266487054    50.40819543 -0.011421006\n10/30/1980  -0.011421006    0.185961701 46.88078737 0.029632441 48.27895767 -0.018832701    48.97017418 -0.073584097    50.23238873 0.013350935\n10/31/1980  0.013350935 -0.156079209    54.08231943 -0.003988301    51.88647852 0.031510014 51.2008709  0.064478565 50.76515097 0.01758565\n11/3/1980   0.01758565  -0.047207787    55.20045808 0.011243586 52.47271385 0.048876357 51.57016088 0.055301429 50.85553721 0.025052611\n11/5/1980   0.025052611 0.000435129 57.18044487 0.053812694 53.50605621 0.05421219  52.22072289 0.039977524 51.01490125 -0.017722306\n11/6/1980   -0.017722306    0.023031138 44.92992843 -0.03124573 47.39891281 -0.070442975    48.41875471 -0.050855544    50.07992286 0.000581639\n11/7/1980   0.000581639 -0.121650134    49.87840955 1.580058713 49.92880444 2.604518867 50.0080164  1.580277943 50.47031631 0.002508371\n11/10/1980  0.002508371 -0.182865195    50.38381628 0.640792927 50.18967587 0.608670831 50.17291603 0.34700777  50.51126003 0.012129939\n11/11/1980  0.012129939 0.063376989 52.93601236 0.221445976 51.49515928 0.145866445 50.99656886 0.079352102 50.71573089 0.024926655</p>\n <machine-learning><neural-network><deep-learning><time-series><linear-regression><p>Thanks for providing the sample data. I do not really see any severe problems to pin something down as the definite cause of your problem, but I can give you some advice for improvement that could help.</p>\n\n<h2>Standardizing and scaling</h2>\n\n<p>Some of your features have larger values and some have smaller values. If you don't standardize and scale your features and targets, it will result in \"unbalanced\" weights inside your NN which can lead to an unstable model. Therefore, use something like the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"nofollow noreferrer\">StandardScaler</a> to standardize and scale your data after splitting it up into a train and a test dataset.</p>\n\n<h2>Activation function</h2>\n\n<p>It is always worth a try to play around with different activation functions. ReLu is quite simple and computationally inexpensive compared to other activation functions, but in a bad setup, it can lead to many dead neurons. So I would suggest trying out <a href=\"https://stackoverflow.com/a/40775144/9291522\">other activation functions</a> like Tanh or Leaky Relu. <em>Note: That does not mean that ReLu is a bad activation function. For many reasons, it is actually a very popular one.</em></p>\n\n<h2>Learning rate</h2>\n\n<p>Especially if you stick with ReLu, check what difference it makes if you reduce the learning rate and/or set a learning rate decay.</p>\n\n<h2>Neural Network Architecture</h2>\n\n<p>Since you are working with a time-series, it would make sense to use a Recurrent Neural Network which was designed for time-series data like <a href=\"https://keras.io/layers/recurrent/#gru\" rel=\"nofollow noreferrer\">GRU</a> or <a href=\"https://keras.io/layers/recurrent/#lstm\" rel=\"nofollow noreferrer\">LSTM</a>.</p>\n\n<h3>Other</h3>\n\n<p>One side note to prevent you from falling into the same trap I did: If you work with TA-Lib, scale your values before you calculate any features. There is an <a href=\"https://github.com/mrjbq7/ta-lib/issues/151\" rel=\"nofollow noreferrer\">open issue</a> on Github of TA-Lib calculating wrong features if the input-values are too small. I see that your targets also have quite small values, so maybe keep an eye on that.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "49355",
                "question_votes:": "2",
                "question_text:": "<p>I am doing my undergrad Dissertation on time series prediction, and use various models (linear /ridge regression,  AR(2), Random Forest, SVR, and 4 variations of Neural Networks) to try and 'predict' (for academic only reasons) daily return data, using as input lagged returns and SMA - RSI features (using TA - Lib) built based on those returns. However, I have noticed that my NNs do not learn anything, and upon inspecting the loss graph and the vector of predictions, I noticed it only predicts a single value, with the same applying for the Ridge and AR regressions.</p>\n\n<p>Also, when I try to calculate the correlation between the labels and the predictions (of the NNs) I get 'nan' as a result, no matter what I try, which I suspect has to do with the predictions. I also get wildly varying r2 scores on each re-run (even though I have set multiple seeds, both on Tensorflow backend as well as numpy) and always negative, which I cannot understand as even though my search on the internet and the sklearn's docs say it can be negative, my professor insists it cannot be, and I truly am bewildered.</p>\n\n<p>What can I do about it? Isn't it obviously wrong for an entire NN to predict only a single value? Below I include the code for the ridge / AR regressions as well as the 'Vanilla NN' and a couple of useful graphs. The data itself is quite large, so I don't know if there's much of a point to include it if not asked specifically, given there are no algorithmic errors below.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def vanillaNN(X_train, y_train,X_test,y_test):\n    n_cols = X_train.shape[1]\n    model = Sequential()\n    model.add(Dense(100,activation='relu', input_shape=(n_cols, )))\n    model.add(Dropout(0.3))\n    model.add(Dense(150, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1))\n\n    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n\n    history = model.fit(X_train,y_train,epochs=100,verbose=0,\n        shuffle=False, validation_split=0.1)  \n\n    # Use the last loss as the title\n    plt.plot(history.history['loss'])\n    plt.title('last loss:' + str(round(history.history['loss'][-1], 6)))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    # Calculate R^2 score and MSE\n    # .... Omitted Code ......\n\n    # it returns those for testing purposes in the IPython shell\n    return (train_scores, test_scores, y_pred_train, y_pred_test, y_train, y_test)\n    VNN_results = vanillaNN(\n        train_features,train_targets,\n        test_features,test_targets)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/tFp3c.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tFp3c.png\" alt=\"Neural Network Loss History - 100 epochs\"></a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def AR(X_train, order=2):\n    arma_train = np.array(X_train['returns'])\n    armodel = ARMA(arma_train, order=(order,0))\n    armodel_results = armodel.fit()\n    print(armodel_results.summary())\n    armodel_results.plot_predict(start=8670, end=8698)\n    plt.show()\n    ar_pred = armodel_results.predict(start=8699, end=9665)\n\n    # ...r2 and MSE scores omitted code...\n\n    return [mse_ar2, r2_ar2, ar_pred]\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/XULvD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/XULvD.png\" alt=\"AR(2) Model out-of-sample predictions\"></a></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def Elastic(X_train, y_train, X_test, y_test):\n    elastic = ElasticNet()\n    param_grid_elastic = {'alpha': [0.001,  0.01, 0.1, 0.5],\n        'l1_ratio': [0.001, 0.01, 0.1, 0.5]\n                              }\n    grid_elastic = GridSearchCV(elastic, param_grid_elastic, \n        cv=tscv.split(X_train),scoring='neg_mean_squared_error')\n\n    grid_elastic.fit(X_train, y_train)\n\n    y_pred_train = grid_elastic.predict(X_train) \n    train_scores = scores(y_train, y_pred_train)\n\n    y_pred_test = grid_elastic.predict(X_test)\n\n    # ...Omitted Code...\n    return [train_scores, test_scores, y_pred_train, y_pred_test]\n</code></pre>\n\n<p>AR(2) sample test and train predictions:\n<a href=\"https://i.stack.imgur.com/RNtmw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/RNtmw.png\" alt=\"Test Set Predictions\"></a>\n<a href=\"https://i.stack.imgur.com/mmJDy.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/mmJDy.png\" alt=\"Train Set Predictions\"></a></p>\n\n<pre><code>SAMPLE DATA: Train features and Train Targets (future_returns) \n</code></pre>\n\n<p>It looks like a mess, but just copy paste into excel file and it should be good to go!)</p>\n\n<p>Date    returns ma14    rsi14   ma30    rsi30   ma50    rsi50   ma200   rsi200  future_returns\n10/14/1980  3.49E-05    42.76324407 49.21625218 66.6250545  49.69881565 49.45368438 49.93538688 37.78942977 50.51223405 0.013277481\n10/15/1980  0.013277481 0.239711734 53.45799196 0.16387242  51.78260494 0.140801819 51.19194274 0.10545251  50.79944024 -0.011855382\n10/16/1980  -0.011855382    -0.306338818    45.66265303 -0.159676722    47.88773425 -0.115851283    48.81901884 -0.107808537    50.24325364 -0.00414208\n10/17/1980  -0.00414208 -1.154286743    48.16105108 -0.451328445    49.1031414  -0.3083074  49.55134428 -0.299669528    50.4107189  0.007939494\n10/20/1980  0.007939494 0.548806765 51.89223141 0.338253188 50.95654204 0.15403304  50.679274   0.145277255 50.67207082 -0.0050544\n10/21/1980  -0.0050544  -0.580692978    47.89906352 -0.443621598    48.97241743 -0.250429072    49.46553614 -0.227539737    50.38503757 -2.93E-05\n10/22/1980  -2.93E-05   -85.38681662    49.51695915 -69.94007273    49.7551065  -46.09189634    49.93866124 -37.61970911    50.49403171 -0.018135363\n10/23/1980  -0.018135363    -0.020087358    44.19203763 -0.067897836    47.0643223  -0.037135977    48.27685366 -0.05615813 50.09551572 0.000415381\n10/24/1980  0.000415381 -2.422576075    50.11149401 3.125882141 49.93407273 1.480210269 50.01580668 2.418672772 50.49781053 -0.013535864\n10/27/1980  -0.013535864    0.12834969  46.14718747 -0.056014904    47.91325905 -0.053384853    48.75782925 -0.065375964    50.19198915 0.00337859\n10/28/1980  0.00337859  -0.566993349    51.18890074 0.168834456 50.4293269  0.275579337 50.30416736 0.265200747 50.55684672 -0.003396646\n10/29/1980  -0.003396646    0.522213275 49.20187924 0.045438303 49.43972414 -0.207144904    49.69125655 -0.266487054    50.40819543 -0.011421006\n10/30/1980  -0.011421006    0.185961701 46.88078737 0.029632441 48.27895767 -0.018832701    48.97017418 -0.073584097    50.23238873 0.013350935\n10/31/1980  0.013350935 -0.156079209    54.08231943 -0.003988301    51.88647852 0.031510014 51.2008709  0.064478565 50.76515097 0.01758565\n11/3/1980   0.01758565  -0.047207787    55.20045808 0.011243586 52.47271385 0.048876357 51.57016088 0.055301429 50.85553721 0.025052611\n11/5/1980   0.025052611 0.000435129 57.18044487 0.053812694 53.50605621 0.05421219  52.22072289 0.039977524 51.01490125 -0.017722306\n11/6/1980   -0.017722306    0.023031138 44.92992843 -0.03124573 47.39891281 -0.070442975    48.41875471 -0.050855544    50.07992286 0.000581639\n11/7/1980   0.000581639 -0.121650134    49.87840955 1.580058713 49.92880444 2.604518867 50.0080164  1.580277943 50.47031631 0.002508371\n11/10/1980  0.002508371 -0.182865195    50.38381628 0.640792927 50.18967587 0.608670831 50.17291603 0.34700777  50.51126003 0.012129939\n11/11/1980  0.012129939 0.063376989 52.93601236 0.221445976 51.49515928 0.145866445 50.99656886 0.079352102 50.71573089 0.024926655</p>\n",
                "tags": "<machine-learning><neural-network><deep-learning><time-series><linear-regression>",
                "answers": [
                    [
                        "49472",
                        "2",
                        "49355",
                        "",
                        "",
                        "<p>Thanks for providing the sample data. I do not really see any severe problems to pin something down as the definite cause of your problem, but I can give you some advice for improvement that could help.</p>\n\n<h2>Standardizing and scaling</h2>\n\n<p>Some of your features have larger values and some have smaller values. If you don't standardize and scale your features and targets, it will result in \"unbalanced\" weights inside your NN which can lead to an unstable model. Therefore, use something like the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" rel=\"nofollow noreferrer\">StandardScaler</a> to standardize and scale your data after splitting it up into a train and a test dataset.</p>\n\n<h2>Activation function</h2>\n\n<p>It is always worth a try to play around with different activation functions. ReLu is quite simple and computationally inexpensive compared to other activation functions, but in a bad setup, it can lead to many dead neurons. So I would suggest trying out <a href=\"https://stackoverflow.com/a/40775144/9291522\">other activation functions</a> like Tanh or Leaky Relu. <em>Note: That does not mean that ReLu is a bad activation function. For many reasons, it is actually a very popular one.</em></p>\n\n<h2>Learning rate</h2>\n\n<p>Especially if you stick with ReLu, check what difference it makes if you reduce the learning rate and/or set a learning rate decay.</p>\n\n<h2>Neural Network Architecture</h2>\n\n<p>Since you are working with a time-series, it would make sense to use a Recurrent Neural Network which was designed for time-series data like <a href=\"https://keras.io/layers/recurrent/#gru\" rel=\"nofollow noreferrer\">GRU</a> or <a href=\"https://keras.io/layers/recurrent/#lstm\" rel=\"nofollow noreferrer\">LSTM</a>.</p>\n\n<h3>Other</h3>\n\n<p>One side note to prevent you from falling into the same trap I did: If you work with TA-Lib, scale your values before you calculate any features. There is an <a href=\"https://github.com/mrjbq7/ta-lib/issues/151\" rel=\"nofollow noreferrer\">open issue</a> on Github of TA-Lib calculating wrong features if the input-values are too small. I see that your targets also have quite small values, so maybe keep an eye on that.</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "11571",
            "_score": 7.395008,
            "_source": {
                "title": "sales price prediction",
                "content": "sales price prediction <p>I have to find make a classifier for price prediction of a item. The question I have is which columns I should choose for price prediction.</p>\n\n<p>Also which machine learning classifier would be good to perform this, at present I choose random forest. </p>\n\n<p>Do I need to use time series concept in here?, I think No</p>\n <classification><regression><prediction><p>So firstly, what do you mean by \"classifier for price prediction\"? You can predict the price as a number, that would like be different for different cars, but if you want to predict a class of price (like, high, low and medium for instance), you would need a column for that (and you can ignore the column for price, as you are not predicting the price, you're predicting the price class).</p>\n\n<p><strong>Stage 1. Pre-processing the data</strong></p>\n\n<p>Assuming you have the column in the dataset which you want to predict for, you first want to do feature selection. That is, not all features in the data would be important or relevant for predicting the price. For example, in your dataset, the first column/feature (\"index\") is irrelevant for the price of the car. But how do we prove that? Or, how do we computationally select them (using some measure), especially when they're not as trivial as \"index\"?</p>\n\n<p>We generally check the statistical properties of the features for that. I copied the data you provided in the question, and here's some things for you to start with:</p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv('ex.csv')\ndata\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/llRbm.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/llRbm.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>data.describe() # to check the statistical properties of the features, like mean, std dev, etc\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/zln51.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zln51.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then, you could do a simple percentage count of the unique observations in each feature, and maybe you could get some insight about the features that way:</p>\n\n<pre><code>for column in data.select_dtypes(include=['object']).columns:\n    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/zJDqM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zJDqM.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then you could do a histogram analysis of the features and hopefully that gives you some more insight. For example, assuming you have sufficiently enough data, you'd normally expect the histogram of a feature to follow the normal or gaussian distribution. But if its doesn't, then you can further drill down into those features to understand why, and that might lead you to keep or discard those features from the model you're going to build.</p>\n\n<pre><code>hist = data.hist(figsize=(10, 10))\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/ko9D4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ko9D4.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then we can do correlation analysis of the features:</p>\n\n<pre><code>data.corr().style.background_gradient()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/sfY0P.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sfY0P.png\" alt=\"enter image description here\"></a></p>\n\n<p>Or, if you want a more fancy visualization:</p>\n\n<pre><code>import seaborn as sns\nsns.heatmap(data.corr(), annot=True)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/afk32.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/afk32.png\" alt=\"enter image description here\"></a></p>\n\n<p>After doing all these, hopefully you have figured out which features to discard and which to keep for your model. These are of course \"manual\" methods of feature selection; there are other more complex methods for feature selection like SHAPLEY values, etc, which you can explore.</p>\n\n<p><strong>Stage 2 - Building a model and training it</strong></p>\n\n<p>Firstly, you need to pick a technique/method using which you want to do the prediction. The simplest one, since you have only one target variable (i.e., only one feature you're predicting, which is the price or the price class), the simplest one would be linear regression, and the most complicated ones would be some deep learning model build with CNN or RNN. So, instead of showing you how to make predictions with the simplest one, i.e., linear regression, let me show you a middle-of-the-road algorithm in terms of complexity which is quite popular and a widely used method in many machine learning tasks, the accelerated gradient boost, or xgboost, algorithm.</p>\n\n<p>We need to import some libraries for this:</p>\n\n<pre><code>from sklearn.model_selection import train_test_split\nimport xgboost\nimport numpy as np\n\nX = data.drop(['price'], axis=1) # take all the features except the target variable\ny = data['price'] # the target variable\n</code></pre>\n\n<p>Then, we create a train/test split with 80-20 split randomly. That is, we randomly take 80% data for training and 20% for testing:</p>\n\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</code></pre>\n\n<p>You can of course do a 70-30 split if you want, and definitely try out different splits at both ends of the spectrum to see what happens - that way you'll learn more about why a 70-30 or 80-20 split is good and, say, a 50-50 split is not that good.</p>\n\n<p>Then, if there are missing values in your data, fill them with a high negative value so that it doesn't have any impact in the model. You can also choose to fill them with something else, depending on your goal.</p>\n\n<pre><code>X_train.fillna((-999), inplace=True)\nX_test.fillna((-999), inplace=True)\n</code></pre>\n\n<p>Some more preprocessing steps:</p>\n\n<pre><code># Some of values are float or integer and some object. This is why we need to cast them:\nfrom sklearn import preprocessing \nfor f in X_train.columns: \n    if X_train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_train[f].values)) \n        X_train[f] = lbl.transform(list(X_train[f].values))\n\nfor f in X_test.columns: \n    if X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_test[f].values)) \n        X_test[f] = lbl.transform(list(X_test[f].values))\n\n\nX_train=np.array(X_train) \nX_test=np.array(X_test) \nX_train = X_train.astype(float) \nX_test = X_test.astype(float)\n\nd_train = xgboost.DMatrix(X_train, label=y_train, feature_names=list(X))\nd_test = xgboost.DMatrix(X_test, label=y_test, feature_names=list(X))\n</code></pre>\n\n<p>Finally, we can make our model and train it:</p>\n\n<pre><code>params = {\n    \"eta\": 0.01, # something called the learning rate - read up about optimization and gradient descent to understand more about this\n    \"subsample\": 0.5,\n    \"base_score\": np.mean(y_train)\n}\n\n# these params are optional - if you don't feed the train function below with the params, it will take the default values\n\nmodel = xgboost.train(params, d_train, 5000, evals = [(d_test, \"test\")], verbose_eval=100, early_stopping_rounds=50)\n</code></pre>\n\n<p>You can check the root mean square error (RMSE) that this function returns at the end to see how good or bad the training has been (low RMSE is good, high RMSE is bad - but there's no max RMSE value, it can be arbitrarily high). There are other methods to check the error, and you can explore them (like MAE, etc), but this is probably the simplest one. Anyway, the above code will return something like this:</p>\n\n<pre><code>[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[0] test-rmse:2275\nWill train until test-rmse hasn't improved in 50 rounds.\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\nStopping. Best iteration:\n[0] test-rmse:1571.88\n</code></pre>\n\n<p>It ran the algo iteratively 5000 times, printing out the result every 100 lines (that's what those numbers are in the train method). To see what each of the parameters mean, you can read <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<p>You can also use linear regression, if you want, with xgboost, like so:</p>\n\n<pre><code>xg_reg = xgboost.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test) \nprint(preds) # these are the predicted prices for the test data\n&gt;&gt;&gt; array([2293.7073, 2891.9692, 3822.3757], dtype=float32)\n</code></pre>\n\n<p>And we can check the RMSE like so:</p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n&gt;&gt;&gt; RMSE: 1542.541395\n</code></pre>\n\n<p>Note that RMSE in the 2 methods is quite close (1571.88 vs 1542.54). This is like a sanity check for us that no matter which method we use, if we use it correctly, we should get similar results.</p>\n\n<p><strong>Stage 3 - testing and evaluation of the model - k-fold Cross Validation</strong></p>\n\n<p>Finally its time to see how our model performs on test data:</p>\n\n<pre><code>params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgboost.cv(dtrain=d_train, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True)\n</code></pre>\n\n<p>This will again give you quite a few lines of output like when training:</p>\n\n<pre><code>[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n</code></pre>\n\n<p>This is how it looks in each of the rounds of the boosting:</p>\n\n<pre><code>print(cv_results)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/WP6Zs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WP6Zs.png\" alt=\"enter image description here\"></a></p>\n\n<p>So, that's it. We have the predicted values.</p>\n\n<p><strong>P.S. Stage 2.5 - Visualizing the model (Optional)</strong></p>\n\n<p>Did you know that we can also visualize the model?</p>\n\n<pre><code>import matplotlib.pyplot as plt\n\nxgboost.plot_tree(xg_reg,num_trees=0)\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/5OwCw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/5OwCw.png\" alt=\"enter image description here\"></a></p>\n\n<p>It shows the tree structure following which the model you trained made its decisions.</p>\n\n<p>You can also see the importance of each feature in the dataset with respect to the model:</p>\n\n<pre><code>xgboost.plot_importance(xg_reg)\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/Rn7qS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Rn7qS.png\" alt=\"enter image description here\"></a></p>\n\n<p>These visualizations are of course not required for making the predictions, but they may sometimes give you useful insights about your predictions.</p>\n<p>We should not use time series concept here. You have to do a pair plot analysis to find your best predictor. I do not think index column is much useful here. I feel length, width or breadth are features if you classify the vehicles as sedan, suv or small car. Not sure if it be of much help in predicting the price. If you add more breadth to the dataset, make sure you have enough depth or training examples</p>\n\n<p>Do a heat map and find the co-relation between the feature and target variable as well. You can remove those features which does not have good co-relation with the target variable.</p>\n\n<p>Try to start with Linear regression model, since you have to predict the price. then move on to more complex models. Random forest regressor is also a good choice for smaller datasets</p>\n",
                "codes": [
                    [
                        "import pandas as pd\n\ndata = pd.read_csv('ex.csv')\ndata\n",
                        "data.describe() # to check the statistical properties of the features, like mean, std dev, etc\n",
                        "for column in data.select_dtypes(include=['object']).columns:\n    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
                        "hist = data.hist(figsize=(10, 10))\n",
                        "data.corr().style.background_gradient()\n",
                        "import seaborn as sns\nsns.heatmap(data.corr(), annot=True)\n",
                        "from sklearn.model_selection import train_test_split\nimport xgboost\nimport numpy as np\n\nX = data.drop(['price'], axis=1) # take all the features except the target variable\ny = data['price'] # the target variable\n",
                        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
                        "X_train.fillna((-999), inplace=True)\nX_test.fillna((-999), inplace=True)\n",
                        "# Some of values are float or integer and some object. This is why we need to cast them:\nfrom sklearn import preprocessing \nfor f in X_train.columns: \n    if X_train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_train[f].values)) \n        X_train[f] = lbl.transform(list(X_train[f].values))\n\nfor f in X_test.columns: \n    if X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_test[f].values)) \n        X_test[f] = lbl.transform(list(X_test[f].values))\n\n\nX_train=np.array(X_train) \nX_test=np.array(X_test) \nX_train = X_train.astype(float) \nX_test = X_test.astype(float)\n\nd_train = xgboost.DMatrix(X_train, label=y_train, feature_names=list(X))\nd_test = xgboost.DMatrix(X_test, label=y_test, feature_names=list(X))\n",
                        "params = {\n    \"eta\": 0.01, # something called the learning rate - read up about optimization and gradient descent to understand more about this\n    \"subsample\": 0.5,\n    \"base_score\": np.mean(y_train)\n}\n\n# these params are optional - if you don't feed the train function below with the params, it will take the default values\n\nmodel = xgboost.train(params, d_train, 5000, evals = [(d_test, \"test\")], verbose_eval=100, early_stopping_rounds=50)\n",
                        "[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[0] test-rmse:2275\nWill train until test-rmse hasn't improved in 50 rounds.\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\nStopping. Best iteration:\n[0] test-rmse:1571.88\n",
                        "xg_reg = xgboost.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test) \nprint(preds) # these are the predicted prices for the test data\n>>> array([2293.7073, 2891.9692, 3822.3757], dtype=float32)\n",
                        "from sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n>>> RMSE: 1542.541395\n",
                        "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgboost.cv(dtrain=d_train, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True)\n",
                        "[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
                        "print(cv_results)\n",
                        "import matplotlib.pyplot as plt\n\nxgboost.plot_tree(xg_reg,num_trees=0)\nplt.show()\n",
                        "xgboost.plot_importance(xg_reg)\nplt.show()\n"
                    ],
                    []
                ],
                "question_id:": "40840",
                "question_votes:": "1",
                "question_text:": "<p>I have to find make a classifier for price prediction of a item. The question I have is which columns I should choose for price prediction.</p>\n\n<p>Also which machine learning classifier would be good to perform this, at present I choose random forest. </p>\n\n<p>Do I need to use time series concept in here?, I think No</p>\n",
                "tags": "<classification><regression><prediction>",
                "answers": [
                    [
                        "40852",
                        "2",
                        "40840",
                        "",
                        "",
                        "<p>So firstly, what do you mean by \"classifier for price prediction\"? You can predict the price as a number, that would like be different for different cars, but if you want to predict a class of price (like, high, low and medium for instance), you would need a column for that (and you can ignore the column for price, as you are not predicting the price, you're predicting the price class).</p>\n\n<p><strong>Stage 1. Pre-processing the data</strong></p>\n\n<p>Assuming you have the column in the dataset which you want to predict for, you first want to do feature selection. That is, not all features in the data would be important or relevant for predicting the price. For example, in your dataset, the first column/feature (\"index\") is irrelevant for the price of the car. But how do we prove that? Or, how do we computationally select them (using some measure), especially when they're not as trivial as \"index\"?</p>\n\n<p>We generally check the statistical properties of the features for that. I copied the data you provided in the question, and here's some things for you to start with:</p>\n\n<pre><code>import pandas as pd\n\ndata = pd.read_csv('ex.csv')\ndata\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/llRbm.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/llRbm.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>data.describe() # to check the statistical properties of the features, like mean, std dev, etc\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/zln51.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zln51.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then, you could do a simple percentage count of the unique observations in each feature, and maybe you could get some insight about the features that way:</p>\n\n<pre><code>for column in data.select_dtypes(include=['object']).columns:\n    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/zJDqM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zJDqM.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then you could do a histogram analysis of the features and hopefully that gives you some more insight. For example, assuming you have sufficiently enough data, you'd normally expect the histogram of a feature to follow the normal or gaussian distribution. But if its doesn't, then you can further drill down into those features to understand why, and that might lead you to keep or discard those features from the model you're going to build.</p>\n\n<pre><code>hist = data.hist(figsize=(10, 10))\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/ko9D4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ko9D4.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then we can do correlation analysis of the features:</p>\n\n<pre><code>data.corr().style.background_gradient()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/sfY0P.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sfY0P.png\" alt=\"enter image description here\"></a></p>\n\n<p>Or, if you want a more fancy visualization:</p>\n\n<pre><code>import seaborn as sns\nsns.heatmap(data.corr(), annot=True)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/afk32.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/afk32.png\" alt=\"enter image description here\"></a></p>\n\n<p>After doing all these, hopefully you have figured out which features to discard and which to keep for your model. These are of course \"manual\" methods of feature selection; there are other more complex methods for feature selection like SHAPLEY values, etc, which you can explore.</p>\n\n<p><strong>Stage 2 - Building a model and training it</strong></p>\n\n<p>Firstly, you need to pick a technique/method using which you want to do the prediction. The simplest one, since you have only one target variable (i.e., only one feature you're predicting, which is the price or the price class), the simplest one would be linear regression, and the most complicated ones would be some deep learning model build with CNN or RNN. So, instead of showing you how to make predictions with the simplest one, i.e., linear regression, let me show you a middle-of-the-road algorithm in terms of complexity which is quite popular and a widely used method in many machine learning tasks, the accelerated gradient boost, or xgboost, algorithm.</p>\n\n<p>We need to import some libraries for this:</p>\n\n<pre><code>from sklearn.model_selection import train_test_split\nimport xgboost\nimport numpy as np\n\nX = data.drop(['price'], axis=1) # take all the features except the target variable\ny = data['price'] # the target variable\n</code></pre>\n\n<p>Then, we create a train/test split with 80-20 split randomly. That is, we randomly take 80% data for training and 20% for testing:</p>\n\n<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</code></pre>\n\n<p>You can of course do a 70-30 split if you want, and definitely try out different splits at both ends of the spectrum to see what happens - that way you'll learn more about why a 70-30 or 80-20 split is good and, say, a 50-50 split is not that good.</p>\n\n<p>Then, if there are missing values in your data, fill them with a high negative value so that it doesn't have any impact in the model. You can also choose to fill them with something else, depending on your goal.</p>\n\n<pre><code>X_train.fillna((-999), inplace=True)\nX_test.fillna((-999), inplace=True)\n</code></pre>\n\n<p>Some more preprocessing steps:</p>\n\n<pre><code># Some of values are float or integer and some object. This is why we need to cast them:\nfrom sklearn import preprocessing \nfor f in X_train.columns: \n    if X_train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_train[f].values)) \n        X_train[f] = lbl.transform(list(X_train[f].values))\n\nfor f in X_test.columns: \n    if X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(X_test[f].values)) \n        X_test[f] = lbl.transform(list(X_test[f].values))\n\n\nX_train=np.array(X_train) \nX_test=np.array(X_test) \nX_train = X_train.astype(float) \nX_test = X_test.astype(float)\n\nd_train = xgboost.DMatrix(X_train, label=y_train, feature_names=list(X))\nd_test = xgboost.DMatrix(X_test, label=y_test, feature_names=list(X))\n</code></pre>\n\n<p>Finally, we can make our model and train it:</p>\n\n<pre><code>params = {\n    \"eta\": 0.01, # something called the learning rate - read up about optimization and gradient descent to understand more about this\n    \"subsample\": 0.5,\n    \"base_score\": np.mean(y_train)\n}\n\n# these params are optional - if you don't feed the train function below with the params, it will take the default values\n\nmodel = xgboost.train(params, d_train, 5000, evals = [(d_test, \"test\")], verbose_eval=100, early_stopping_rounds=50)\n</code></pre>\n\n<p>You can check the root mean square error (RMSE) that this function returns at the end to see how good or bad the training has been (low RMSE is good, high RMSE is bad - but there's no max RMSE value, it can be arbitrarily high). There are other methods to check the error, and you can explore them (like MAE, etc), but this is probably the simplest one. Anyway, the above code will return something like this:</p>\n\n<pre><code>[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[0] test-rmse:2275\nWill train until test-rmse hasn't improved in 50 rounds.\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[16:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\nStopping. Best iteration:\n[0] test-rmse:1571.88\n</code></pre>\n\n<p>It ran the algo iteratively 5000 times, printing out the result every 100 lines (that's what those numbers are in the train method). To see what each of the parameters mean, you can read <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<p>You can also use linear regression, if you want, with xgboost, like so:</p>\n\n<pre><code>xg_reg = xgboost.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test) \nprint(preds) # these are the predicted prices for the test data\n&gt;&gt;&gt; array([2293.7073, 2891.9692, 3822.3757], dtype=float32)\n</code></pre>\n\n<p>And we can check the RMSE like so:</p>\n\n<pre><code>from sklearn.metrics import mean_squared_error\n\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n&gt;&gt;&gt; RMSE: 1542.541395\n</code></pre>\n\n<p>Note that RMSE in the 2 methods is quite close (1571.88 vs 1542.54). This is like a sanity check for us that no matter which method we use, if we use it correctly, we should get similar results.</p>\n\n<p><strong>Stage 3 - testing and evaluation of the model - k-fold Cross Validation</strong></p>\n\n<p>Finally its time to see how our model performs on test data:</p>\n\n<pre><code>params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgboost.cv(dtrain=d_train, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True)\n</code></pre>\n\n<p>This will again give you quite a few lines of output like when training:</p>\n\n<pre><code>[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 0 pruned nodes, max_depth=0\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n[17:38:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n</code></pre>\n\n<p>This is how it looks in each of the rounds of the boosting:</p>\n\n<pre><code>print(cv_results)\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/WP6Zs.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WP6Zs.png\" alt=\"enter image description here\"></a></p>\n\n<p>So, that's it. We have the predicted values.</p>\n\n<p><strong>P.S. Stage 2.5 - Visualizing the model (Optional)</strong></p>\n\n<p>Did you know that we can also visualize the model?</p>\n\n<pre><code>import matplotlib.pyplot as plt\n\nxgboost.plot_tree(xg_reg,num_trees=0)\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/5OwCw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/5OwCw.png\" alt=\"enter image description here\"></a></p>\n\n<p>It shows the tree structure following which the model you trained made its decisions.</p>\n\n<p>You can also see the importance of each feature in the dataset with respect to the model:</p>\n\n<pre><code>xgboost.plot_importance(xg_reg)\nplt.show()\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/Rn7qS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Rn7qS.png\" alt=\"enter image description here\"></a></p>\n\n<p>These visualizations are of course not required for making the predictions, but they may sometimes give you useful insights about your predictions.</p>\n",
                        "",
                        "1"
                    ],
                    [
                        "40850",
                        "2",
                        "40840",
                        "",
                        "",
                        "<p>We should not use time series concept here. You have to do a pair plot analysis to find your best predictor. I do not think index column is much useful here. I feel length, width or breadth are features if you classify the vehicles as sedan, suv or small car. Not sure if it be of much help in predicting the price. If you add more breadth to the dataset, make sure you have enough depth or training examples</p>\n\n<p>Do a heat map and find the co-relation between the feature and target variable as well. You can remove those features which does not have good co-relation with the target variable.</p>\n\n<p>Try to start with Linear regression model, since you have to predict the price. then move on to more complex models. Random forest regressor is also a good choice for smaller datasets</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        }
    ]
}