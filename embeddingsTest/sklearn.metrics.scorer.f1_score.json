{
    "module": "sklearn",
    "function": "sklearn.metrics.scorer.f1_score",
    "stackoverflow": [
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "1284",
            "_score": 28.045128,
            "_source": {
                "title": "How to use Cohen's Kappa as the evaluation metric in GridSearchCV in Scikit Learn?",
                "content": "How to use Cohen's Kappa as the evaluation metric in GridSearchCV in Scikit Learn? <p>I have class imbalance in the ratio 1:15 i.e. very low event rate. So to select tuning parameters of GBM in scikit learn I want to use Kappa instead of F1 score. My understanding is Kappa is a better metric than F1 score for class imbalance.</p>\n\n<p>But I couldn't find kappa as an evaluation_metric in scikit learn here \n<a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\" rel=\"noreferrer\">sklearn.metrics</a>.</p>\n\n<p><strong>Questions</strong></p>\n\n<ol>\n<li>Is there any workaround for including kappa in gridsearchcv?</li>\n<li>Is there any other better metric i can use instead of kappa in scikit learn?</li>\n</ol>\n <machine-learning><classification><python><predictive-modeling><scikit-learn><p><a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa\" rel=\"noreferrer\">Cohen's kappa</a> was introduced in scikit-learn 0.17.</p>\n\n<p>You can wrap it in <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#scoring\" rel=\"noreferrer\">make_scorer</a> for use in GridSearchCV.</p>\n\n<pre><code>from sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import LinearSVC\n\nkappa_scorer = make_scorer(cohen_kappa_score)\ngrid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=kappa_scorer)\n</code></pre>\n<p>In addition to the link in the existing answer, there is also a Scikit-Learn laboratory, where methods and algorithms are being experimented.</p>\n\n<p>In case you are okay with working with bleeding edge code, <a href=\"http://skll.readthedocs.org/en/latest/index.html\" rel=\"nofollow\">this library</a> would be a nice reference.</p>\n\n<p>The Cohen's Kappa is <a href=\"http://skll.readthedocs.org/en/latest/api/metrics.html#skll.metrics.kappa\" rel=\"nofollow\">also one of the metrics in the library</a>, which takes in <code>true labels</code>, <code>predicted labels</code>, <code>weights</code> and <code>allowing one off?</code> as the input parameters. Obviously, the metric would range from <code>[-1, 1]</code>.</p>\n\n<p>You can also have a look at the <a href=\"http://skll.readthedocs.org/en/latest/_modules/skll/metrics.html\" rel=\"nofollow\">implementation code</a>, just in case you would like to contribute.</p>\n\n<p>Note: Cohen's Kappa is also <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py#L260-L308\" rel=\"nofollow\">implemented in Scikit-Learn</a>.</p>\n\n<p>Yes, <a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\" rel=\"nofollow\">there</a> <a href=\"http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics\" rel=\"nofollow\">are</a> <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" rel=\"nofollow\">alternatives</a> to the Cohen Kappa metric.</p>\n",
                "codes": [
                    [
                        "from sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import LinearSVC\n\nkappa_scorer = make_scorer(cohen_kappa_score)\ngrid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=kappa_scorer)\n"
                    ],
                    []
                ],
                "question_id:": "8064",
                "question_votes:": "7",
                "question_text:": "<p>I have class imbalance in the ratio 1:15 i.e. very low event rate. So to select tuning parameters of GBM in scikit learn I want to use Kappa instead of F1 score. My understanding is Kappa is a better metric than F1 score for class imbalance.</p>\n\n<p>But I couldn't find kappa as an evaluation_metric in scikit learn here \n<a href=\"http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\" rel=\"noreferrer\">sklearn.metrics</a>.</p>\n\n<p><strong>Questions</strong></p>\n\n<ol>\n<li>Is there any workaround for including kappa in gridsearchcv?</li>\n<li>Is there any other better metric i can use instead of kappa in scikit learn?</li>\n</ol>\n",
                "tags": "<machine-learning><classification><python><predictive-modeling><scikit-learn>",
                "answers": [
                    [
                        "9034",
                        "2",
                        "8064",
                        "",
                        "",
                        "<p><a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa\" rel=\"noreferrer\">Cohen's kappa</a> was introduced in scikit-learn 0.17.</p>\n\n<p>You can wrap it in <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#scoring\" rel=\"noreferrer\">make_scorer</a> for use in GridSearchCV.</p>\n\n<pre><code>from sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import LinearSVC\n\nkappa_scorer = make_scorer(cohen_kappa_score)\ngrid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=kappa_scorer)\n</code></pre>\n",
                        "",
                        "11"
                    ],
                    [
                        "9040",
                        "2",
                        "8064",
                        "",
                        "",
                        "<p>In addition to the link in the existing answer, there is also a Scikit-Learn laboratory, where methods and algorithms are being experimented.</p>\n\n<p>In case you are okay with working with bleeding edge code, <a href=\"http://skll.readthedocs.org/en/latest/index.html\" rel=\"nofollow\">this library</a> would be a nice reference.</p>\n\n<p>The Cohen's Kappa is <a href=\"http://skll.readthedocs.org/en/latest/api/metrics.html#skll.metrics.kappa\" rel=\"nofollow\">also one of the metrics in the library</a>, which takes in <code>true labels</code>, <code>predicted labels</code>, <code>weights</code> and <code>allowing one off?</code> as the input parameters. Obviously, the metric would range from <code>[-1, 1]</code>.</p>\n\n<p>You can also have a look at the <a href=\"http://skll.readthedocs.org/en/latest/_modules/skll/metrics.html\" rel=\"nofollow\">implementation code</a>, just in case you would like to contribute.</p>\n\n<p>Note: Cohen's Kappa is also <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py#L260-L308\" rel=\"nofollow\">implemented in Scikit-Learn</a>.</p>\n\n<p>Yes, <a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\" rel=\"nofollow\">there</a> <a href=\"http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics\" rel=\"nofollow\">are</a> <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" rel=\"nofollow\">alternatives</a> to the Cohen Kappa metric.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "9599",
            "_score": 23.741161,
            "_source": {
                "title": "cross_val_score meaning",
                "content": "cross_val_score meaning <p>I'm studying the following code, which <strong>cross_val_score_</strong> was used as well as <strong>.mean()</strong> and <strong>.std()</strong>. I read many documentation of the meanings, but didn't get what each of the above does.</p>\n\n<pre><code>import pandas as pd \nimport numpy as np\nfrom sklearn import tree \nimport graphviz\nfrom sklearn.model_selection import cross_val_score\n\n#importing the dataset\nd = pd.read_csv('student-por.csv', sep= ';')\n\nd['pass'] = d.apply(lambda row: 1 if (row['G1']+ row['G2']+ row ['G3']) &gt;= 35 else 0 , axis=1)\nd = d.drop(['G1', 'G2','G3'], axis=1 )\n\n#Doing one-hot encoding\nd=pd.get_dummies(d, columns =['sex','activities','school', 'address', 'famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','nursery','higher','internet','romantic'])\n\n#shuffle rows\nd = d.sample(frac=1)\n\n#split traning and test\nd_train = d[:500]\nd_test = d[500:]\n\nd_train_att = d_train.drop(['pass'], axis=1)\nd_train_pass= d_train['pass']\n\nd_test_att = d_test.drop(['pass'], axis=1)\nd_test_pass= d_test['pass']\n\nd_att = d.drop(['pass'], axis=1)\nd_pass = d['pass']\n\nt = tree.DecisionTreeClassifier(criterion ='entropy', max_depth = 5)\nt= t.fit (d_train_att, d_train_pass)\n\n#to export the tree\ndot_data = tree.export_graphviz(t,out_file = 'students-tree.png', label ='all', impurity=False, proportion= True, feature_names=list(d_train_att), class_names=['fail', 'pass'], filled = True, rounded=True)\n\nt.score (d_test_att, d_test_pass)\nscores = cross_val_score(t, d_att,d_pass, cv=5)\n\nprint ('Acuracy %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() *2))\n</code></pre>\n\n<p>in short this is what I need to know:</p>\n\n<pre><code>scores = cross_val_score(t, d_att,d_pass, cv=5)\n\nprint ('Acuracy %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() *2))\n</code></pre>\n\n<p>one more thing, am I suppose to get the same score as in the original code publisher? because I didn't.</p>\n <machine-learning><scikit-learn><cross-validation><p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/eec7649236d5216380d05916bb7f6aa3b2fc5508/sklearn/metrics/scorer.py\" rel=\"nofollow noreferrer\">The source</a>, around line 274 is where the default scoring for <code>cross_validation_score</code> gets set, if you pass in <code>None</code> for the <code>scorer</code> argument.  For classifiers, the usual default score is accuracy.  For regression, it's rmse, IIRC.  So, since you're applying a decision tree classifier, <code>cross_val_score</code> splits the data into 5 equalish sized pieces, trains on each combination of 4 and gives back the accuracy of the estimator on the 5th.  The mean and std of these accuracies presumably tells one something about the performance of the family of decision tree classifiers on your dataset, but I would take it with a grain of salt.</p>\n",
                "codes": [
                    []
                ],
                "question_id:": "34118",
                "question_votes:": "2",
                "question_text:": "<p>I'm studying the following code, which <strong>cross_val_score_</strong> was used as well as <strong>.mean()</strong> and <strong>.std()</strong>. I read many documentation of the meanings, but didn't get what each of the above does.</p>\n\n<pre><code>import pandas as pd \nimport numpy as np\nfrom sklearn import tree \nimport graphviz\nfrom sklearn.model_selection import cross_val_score\n\n#importing the dataset\nd = pd.read_csv('student-por.csv', sep= ';')\n\nd['pass'] = d.apply(lambda row: 1 if (row['G1']+ row['G2']+ row ['G3']) &gt;= 35 else 0 , axis=1)\nd = d.drop(['G1', 'G2','G3'], axis=1 )\n\n#Doing one-hot encoding\nd=pd.get_dummies(d, columns =['sex','activities','school', 'address', 'famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','nursery','higher','internet','romantic'])\n\n#shuffle rows\nd = d.sample(frac=1)\n\n#split traning and test\nd_train = d[:500]\nd_test = d[500:]\n\nd_train_att = d_train.drop(['pass'], axis=1)\nd_train_pass= d_train['pass']\n\nd_test_att = d_test.drop(['pass'], axis=1)\nd_test_pass= d_test['pass']\n\nd_att = d.drop(['pass'], axis=1)\nd_pass = d['pass']\n\nt = tree.DecisionTreeClassifier(criterion ='entropy', max_depth = 5)\nt= t.fit (d_train_att, d_train_pass)\n\n#to export the tree\ndot_data = tree.export_graphviz(t,out_file = 'students-tree.png', label ='all', impurity=False, proportion= True, feature_names=list(d_train_att), class_names=['fail', 'pass'], filled = True, rounded=True)\n\nt.score (d_test_att, d_test_pass)\nscores = cross_val_score(t, d_att,d_pass, cv=5)\n\nprint ('Acuracy %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() *2))\n</code></pre>\n\n<p>in short this is what I need to know:</p>\n\n<pre><code>scores = cross_val_score(t, d_att,d_pass, cv=5)\n\nprint ('Acuracy %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() *2))\n</code></pre>\n\n<p>one more thing, am I suppose to get the same score as in the original code publisher? because I didn't.</p>\n",
                "tags": "<machine-learning><scikit-learn><cross-validation>",
                "answers": [
                    [
                        "34119",
                        "2",
                        "34118",
                        "",
                        "",
                        "<p><a href=\"https://github.com/scikit-learn/scikit-learn/blob/eec7649236d5216380d05916bb7f6aa3b2fc5508/sklearn/metrics/scorer.py\" rel=\"nofollow noreferrer\">The source</a>, around line 274 is where the default scoring for <code>cross_validation_score</code> gets set, if you pass in <code>None</code> for the <code>scorer</code> argument.  For classifiers, the usual default score is accuracy.  For regression, it's rmse, IIRC.  So, since you're applying a decision tree classifier, <code>cross_val_score</code> splits the data into 5 equalish sized pieces, trains on each combination of 4 and gives back the accuracy of the estimator on the 5th.  The mean and std of these accuracies presumably tells one something about the performance of the family of decision tree classifiers on your dataset, but I would take it with a grain of salt.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "17683",
            "_score": 22.750202,
            "_source": {
                "title": "Unable to understand the usage of labels argument in sklearn.metrics.f1_score",
                "content": "Unable to understand the usage of labels argument in sklearn.metrics.f1_score <p>I am trying to model a dataset with RandomForest Classifier.\nMy dataset has 3 classes viz. <code>A, B, C</code>. <code>'A'</code> is the negative class and <code>'B'</code> and <code>'C'</code> are positive classes.  </p>\n\n<p>In GridSearch I wanted to optimize on <code>F1-score</code> since the number of samples in all the classes are not evenly distributed and class <code>'A'</code> has the highest number of samples.  </p>\n\n<p>That is where I wanted to understand the usage of labels argument.\nThe <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" rel=\"nofollow noreferrer\">doc</a> says:</p>\n\n<blockquote>\n  <p><strong>labels : list, optional</strong> The set of labels to include when average !=\n  'binary', and their order if average is None. Labels present in the\n  data can be excluded, for example to calculate a multiclass average\n  ignoring a majority negative class, while labels not present in the\n  data will result in 0 components in a macro average.</p>\n</blockquote>\n\n<p>I could not understand it properly. Does it mean, In my screnario I should have labels as <code>labels = ['B', 'C']</code>, just the positive class?<br>\nKindly Help</p>\n\n<pre><code>custom_scoring = make_scorer(f1_score, labels=[???],average='weighted')\nclf = RandomForestClassifier(class_weight='balanced', random_state=args.random_state)\ngrid_search = GridSearchCV(clf, param_grid=param_grid, n_jobs=20, scoring=custom_scoring)\n</code></pre>\n <machine-learning><scikit-learn><machine-learning-model><accuracy><grid-search><p>In case of imbalanced dataset, accuracy score of sampling algorithm yields an accuracy of 99% which seems impressive, but minority class could be totally ignored in case of imbalanced datasets. </p>\n\n<p>If data set is imbalanced, pre-processed data set with sampling algorithm (for e.g SMOTE) and re-sample it. It will create equal sets of examples for class based on neighbors.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/57205718/how-can-we-be-sure-of-the-efficiency-of-a-neural-network/57211888#57211888\">https://stackoverflow.com/questions/57205718/how-can-we-be-sure-of-the-efficiency-of-a-neural-network/57211888#57211888</a></p>\n",
                "codes": [
                    []
                ],
                "question_id:": "56731",
                "question_votes:": "",
                "question_text:": "<p>I am trying to model a dataset with RandomForest Classifier.\nMy dataset has 3 classes viz. <code>A, B, C</code>. <code>'A'</code> is the negative class and <code>'B'</code> and <code>'C'</code> are positive classes.  </p>\n\n<p>In GridSearch I wanted to optimize on <code>F1-score</code> since the number of samples in all the classes are not evenly distributed and class <code>'A'</code> has the highest number of samples.  </p>\n\n<p>That is where I wanted to understand the usage of labels argument.\nThe <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" rel=\"nofollow noreferrer\">doc</a> says:</p>\n\n<blockquote>\n  <p><strong>labels : list, optional</strong> The set of labels to include when average !=\n  'binary', and their order if average is None. Labels present in the\n  data can be excluded, for example to calculate a multiclass average\n  ignoring a majority negative class, while labels not present in the\n  data will result in 0 components in a macro average.</p>\n</blockquote>\n\n<p>I could not understand it properly. Does it mean, In my screnario I should have labels as <code>labels = ['B', 'C']</code>, just the positive class?<br>\nKindly Help</p>\n\n<pre><code>custom_scoring = make_scorer(f1_score, labels=[???],average='weighted')\nclf = RandomForestClassifier(class_weight='balanced', random_state=args.random_state)\ngrid_search = GridSearchCV(clf, param_grid=param_grid, n_jobs=20, scoring=custom_scoring)\n</code></pre>\n",
                "tags": "<machine-learning><scikit-learn><machine-learning-model><accuracy><grid-search>",
                "answers": [
                    [
                        "56742",
                        "2",
                        "56731",
                        "",
                        "",
                        "<p>In case of imbalanced dataset, accuracy score of sampling algorithm yields an accuracy of 99% which seems impressive, but minority class could be totally ignored in case of imbalanced datasets. </p>\n\n<p>If data set is imbalanced, pre-processed data set with sampling algorithm (for e.g SMOTE) and re-sample it. It will create equal sets of examples for class based on neighbors.</p>\n\n<p><a href=\"https://stackoverflow.com/questions/57205718/how-can-we-be-sure-of-the-efficiency-of-a-neural-network/57211888#57211888\">https://stackoverflow.com/questions/57205718/how-can-we-be-sure-of-the-efficiency-of-a-neural-network/57211888#57211888</a></p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "3183",
            "_score": 20.599554,
            "_source": {
                "title": "Feature importance with scikit-learn Random Forest shows very high Standard Deviation",
                "content": "Feature importance with scikit-learn Random Forest shows very high Standard Deviation <p>I am using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">scikit-learn Random Forest Classifier</a> and I want to plot the feature importance such as in <a href=\"http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\" rel=\"noreferrer\">this example</a>.</p>\n\n<p>However my result is completely different, in the sense that feature importance standard deviation is almost always bigger than feature importance itself (see attached image).</p>\n\n<p><a href=\"https://i.stack.imgur.com/xjhXa.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/xjhXa.png\" alt=\"feature importance\"></a></p>\n\n<p>Is it possible to have such kind of behaviour, or am I doing some mistakes when plotting it?</p>\n\n<p>My code is the following:</p>\n\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(predictors.values, outcome.values.ravel())\n\nimportance = clf.feature_importances_\nimportance = pd.DataFrame(importance, index=predictors.columns, \n                          columns=[\"Importance\"])\n\nimportance[\"Std\"] = np.std([tree.feature_importances_\n                            for tree in clf.estimators_], axis=0)\n\nx = range(importance.shape[0])\ny = importance.ix[:, 0]\nyerr = importance.ix[:, 1]\n\nplt.bar(x, y, yerr=yerr, align=\"center\")\n\nplt.show()\n</code></pre>\n <python><random-forest><p>Your result is not that weird. As <a href=\"https://datascience.stackexchange.com/users/9085/lanenok\">lanenok</a> states, you should in a first step increase the number of trees in order to make sure that you get a 'statistical' result concerning the feature importances. </p>\n\n<p>However, as this <a href=\"http://ac.els-cdn.com/S0167865510000954/1-s2.0-S0167865510000954-main.pdf?_tid=76378d52-ed1a-11e6-98c1-00000aab0f27&amp;acdnat=1486461039_56773ffa03e91d2b763381344e93d09f\" rel=\"nofollow noreferrer\">paper</a> by Genuer et al. (2010) shows, you can actually use the standard deviations in order to eliminate features. To quote: \"<em>We can see that true variables standard deviation is large compared to the noisy variables one, which is close to zero.</em> \"</p>\n<p>There could be multiple reasons. The number of trees and the depth can change your results.  If your model doesn't perform well after selecting the parameters (cross-validation etc.), it's probably because your features are not very predictive, so they get picked almost \"randomly\" which leads to high standard deviations from tree to tree. \nBut there are other possibilities, e.g. it could also be that your features are highly correlated. A little more information would be helpful.</p>\n<p>You are using RandomForest with the default number of trees, which is 10.\nFor around 30 features this is too few. Therefore standard deviation is large. Try at least 100 or even 1000 trees, like</p>\n\n<pre><code>clf = RandomForestClassifier(n_estimators=1000)\n</code></pre>\n\n<p>For a more refined analysis you can also check how large the correlation between your features is.</p>\n<p>A common reason for this is that the parameters you supplied (or defaulted) to <code>RandomForestClassifier</code> are not suited for your dataset.</p>\n\n<p>A common way to address this problem is to search the hyperparameter space using e.g. <code>GridSearchCV</code>:</p>\n\n<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nparam_grid = {'n_estimators': [10, 100, 1000], 'max_features': [5, 10, 20, 30]}\nclf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=make_scorer(accuracy_score))\n</code></pre>\n\n<p><code>param_grid</code> here is the permutations of parameters that you want to search in, and the <code>make_scorer(accuracy_score)</code> is the measure you want to optimize.</p>\n\n<p>Note that <code>accuracy_score</code> is suitable for balanced sets, but not for unbalanced sets. Choose a suitable metric to on your particular objective.</p>\n<p>Try <code>clf = RandomForestClassifier(max_features=None)</code>. The <code>max_features</code> param defaults to <code>'auto'</code> which is equivalent to <code>sqrt(n_features)</code>. <code>max_features</code> is described as \"The number of features to consider when looking for the best split.\" Only looking at a small number of features at any point in the decision tree means the importance of a single feature may vary widely across many tree. So, don't look at a random subset, just look at all features at every level of the tree.</p>\n",
                "codes": [
                    [],
                    [],
                    [
                        "clf = RandomForestClassifier(n_estimators=1000)\n"
                    ],
                    [
                        "from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nparam_grid = {'n_estimators': [10, 100, 1000], 'max_features': [5, 10, 20, 30]}\nclf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=make_scorer(accuracy_score))\n"
                    ],
                    []
                ],
                "question_id:": "13754",
                "question_votes:": "13",
                "question_text:": "<p>I am using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">scikit-learn Random Forest Classifier</a> and I want to plot the feature importance such as in <a href=\"http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\" rel=\"noreferrer\">this example</a>.</p>\n\n<p>However my result is completely different, in the sense that feature importance standard deviation is almost always bigger than feature importance itself (see attached image).</p>\n\n<p><a href=\"https://i.stack.imgur.com/xjhXa.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/xjhXa.png\" alt=\"feature importance\"></a></p>\n\n<p>Is it possible to have such kind of behaviour, or am I doing some mistakes when plotting it?</p>\n\n<p>My code is the following:</p>\n\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(predictors.values, outcome.values.ravel())\n\nimportance = clf.feature_importances_\nimportance = pd.DataFrame(importance, index=predictors.columns, \n                          columns=[\"Importance\"])\n\nimportance[\"Std\"] = np.std([tree.feature_importances_\n                            for tree in clf.estimators_], axis=0)\n\nx = range(importance.shape[0])\ny = importance.ix[:, 0]\nyerr = importance.ix[:, 1]\n\nplt.bar(x, y, yerr=yerr, align=\"center\")\n\nplt.show()\n</code></pre>\n",
                "tags": "<python><random-forest>",
                "answers": [
                    [
                        "16799",
                        "2",
                        "13754",
                        "",
                        "",
                        "<p>Your result is not that weird. As <a href=\"https://datascience.stackexchange.com/users/9085/lanenok\">lanenok</a> states, you should in a first step increase the number of trees in order to make sure that you get a 'statistical' result concerning the feature importances. </p>\n\n<p>However, as this <a href=\"http://ac.els-cdn.com/S0167865510000954/1-s2.0-S0167865510000954-main.pdf?_tid=76378d52-ed1a-11e6-98c1-00000aab0f27&amp;acdnat=1486461039_56773ffa03e91d2b763381344e93d09f\" rel=\"nofollow noreferrer\">paper</a> by Genuer et al. (2010) shows, you can actually use the standard deviations in order to eliminate features. To quote: \"<em>We can see that true variables standard deviation is large compared to the noisy variables one, which is close to zero.</em> \"</p>\n",
                        "",
                        "2"
                    ],
                    [
                        "14302",
                        "2",
                        "13754",
                        "",
                        "",
                        "<p>There could be multiple reasons. The number of trees and the depth can change your results.  If your model doesn't perform well after selecting the parameters (cross-validation etc.), it's probably because your features are not very predictive, so they get picked almost \"randomly\" which leads to high standard deviations from tree to tree. \nBut there are other possibilities, e.g. it could also be that your features are highly correlated. A little more information would be helpful.</p>\n",
                        "",
                        ""
                    ],
                    [
                        "13755",
                        "2",
                        "13754",
                        "",
                        "",
                        "<p>You are using RandomForest with the default number of trees, which is 10.\nFor around 30 features this is too few. Therefore standard deviation is large. Try at least 100 or even 1000 trees, like</p>\n\n<pre><code>clf = RandomForestClassifier(n_estimators=1000)\n</code></pre>\n\n<p>For a more refined analysis you can also check how large the correlation between your features is.</p>\n",
                        "",
                        "3"
                    ],
                    [
                        "16798",
                        "2",
                        "13754",
                        "",
                        "",
                        "<p>A common reason for this is that the parameters you supplied (or defaulted) to <code>RandomForestClassifier</code> are not suited for your dataset.</p>\n\n<p>A common way to address this problem is to search the hyperparameter space using e.g. <code>GridSearchCV</code>:</p>\n\n<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nparam_grid = {'n_estimators': [10, 100, 1000], 'max_features': [5, 10, 20, 30]}\nclf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=make_scorer(accuracy_score))\n</code></pre>\n\n<p><code>param_grid</code> here is the permutations of parameters that you want to search in, and the <code>make_scorer(accuracy_score)</code> is the measure you want to optimize.</p>\n\n<p>Note that <code>accuracy_score</code> is suitable for balanced sets, but not for unbalanced sets. Choose a suitable metric to on your particular objective.</p>\n",
                        "",
                        "1"
                    ],
                    [
                        "14463",
                        "2",
                        "13754",
                        "",
                        "",
                        "<p>Try <code>clf = RandomForestClassifier(max_features=None)</code>. The <code>max_features</code> param defaults to <code>'auto'</code> which is equivalent to <code>sqrt(n_features)</code>. <code>max_features</code> is described as \"The number of features to consider when looking for the best split.\" Only looking at a small number of features at any point in the decision tree means the importance of a single feature may vary widely across many tree. So, don't look at a random subset, just look at all features at every level of the tree.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "5479",
            "_score": 19.393467,
            "_source": {
                "title": "Custom metrics for unbalanced classes problem in RandomForest or SVM",
                "content": "Custom metrics for unbalanced classes problem in RandomForest or SVM <p>My dataset has highly unbalanced classes \u2012 foreground of 30 classes with tens of samples against background set of >100k samples. Classifying foreground class as background is quite OK, while classifying background as foreground should be penalised.</p>\n\n<p>I am using Scikit-learn's RandomForests, and I was experimenting with SVM and OneVsRest classifiers as well. I would like to specify the scoring metrics used for the method <code>fit()</code> of the model, so it will correspond to my goal (I imagine something like fitness function with evolution algorithms). However, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit\" rel=\"noreferrer\">API</a> does not allow something like that.</p>\n\n<p>So far I tried:</p>\n\n<ul>\n<li>Use <code>class_weight</code> parameter of the model. If I set it so it represents the real world, then the classifier learns to classify everything as background having accuracy >99 %. If I set <code>class_weight = 'balanced'</code>, then it seems better, but it has high false positive rate.</li>\n<li>Use <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" rel=\"noreferrer\">scoring method</a> for <code>GridSearchCV</code>, which outputs values specified by me (even F1-score makes more sense than simple accuracy), but it is used only for the parameter selection and the final model is learnt by <code>fit()</code> method, which again ignores my scoring.</li>\n</ul>\n\n<p>Is there a way to provide my own custom scoring function for the <code>fit()</code> method?</p>\n <python><scikit-learn><random-forest><svm><p>I think I understand what you are trying to do. First, let me attempt to obtain clarity on two concepts that may be getting conflated (if not for you, then for other users).</p>\n\n<p>When a random forest is being <code>fit</code>, it typically uses entropy or gini impurity for categorical outcome variables, and mean squared error or mean absolute error for numerical outcome variables -- <strong>this is the model building step</strong>. However, when the fit model is being evaluated, the scoring method is used to understand the models performance -- <strong>this is the step where we learn how accurate the model might be</strong>.</p>\n\n<p>I've seen that sklearn allows specifying a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\" rel=\"nofollow noreferrer\">custom scoring method</a>, but as far as I know, I don't believe you can specify your own <code>criterion</code> parameter in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">RandomForestClassifier</a> or <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" rel=\"nofollow noreferrer\">RandomForestRegressor</a> methods/APIs. </p>\n",
                "codes": [
                    []
                ],
                "question_id:": "21961",
                "question_votes:": "6",
                "question_text:": "<p>My dataset has highly unbalanced classes \u2012 foreground of 30 classes with tens of samples against background set of >100k samples. Classifying foreground class as background is quite OK, while classifying background as foreground should be penalised.</p>\n\n<p>I am using Scikit-learn's RandomForests, and I was experimenting with SVM and OneVsRest classifiers as well. I would like to specify the scoring metrics used for the method <code>fit()</code> of the model, so it will correspond to my goal (I imagine something like fitness function with evolution algorithms). However, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit\" rel=\"noreferrer\">API</a> does not allow something like that.</p>\n\n<p>So far I tried:</p>\n\n<ul>\n<li>Use <code>class_weight</code> parameter of the model. If I set it so it represents the real world, then the classifier learns to classify everything as background having accuracy >99 %. If I set <code>class_weight = 'balanced'</code>, then it seems better, but it has high false positive rate.</li>\n<li>Use <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" rel=\"noreferrer\">scoring method</a> for <code>GridSearchCV</code>, which outputs values specified by me (even F1-score makes more sense than simple accuracy), but it is used only for the parameter selection and the final model is learnt by <code>fit()</code> method, which again ignores my scoring.</li>\n</ul>\n\n<p>Is there a way to provide my own custom scoring function for the <code>fit()</code> method?</p>\n",
                "tags": "<python><scikit-learn><random-forest><svm>",
                "answers": [
                    [
                        "35685",
                        "2",
                        "21961",
                        "",
                        "",
                        "<p>I think I understand what you are trying to do. First, let me attempt to obtain clarity on two concepts that may be getting conflated (if not for you, then for other users).</p>\n\n<p>When a random forest is being <code>fit</code>, it typically uses entropy or gini impurity for categorical outcome variables, and mean squared error or mean absolute error for numerical outcome variables -- <strong>this is the model building step</strong>. However, when the fit model is being evaluated, the scoring method is used to understand the models performance -- <strong>this is the step where we learn how accurate the model might be</strong>.</p>\n\n<p>I've seen that sklearn allows specifying a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\" rel=\"nofollow noreferrer\">custom scoring method</a>, but as far as I know, I don't believe you can specify your own <code>criterion</code> parameter in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">RandomForestClassifier</a> or <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" rel=\"nofollow noreferrer\">RandomForestRegressor</a> methods/APIs. </p>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "False"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "3174",
            "_score": 15.161978,
            "_source": {
                "title": "Fitting error with Neural Network Grid Search in Keras",
                "content": "Fitting error with Neural Network Grid Search in Keras <p>I try to build a NN classifier on the well-known MNIST image database with Sklearn's Grid Search according the following:</p>\n\n<pre><code>model = KerasClassifier(build_fn=create_model, verbose=1)\nparam_grid = dict(batch_size=[10, 50, 100, 250], nb_epoch=[10, 50, 100])\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n</code></pre>\n\n<p>create_model is a function that builds the Neural Network Model. The fitting (last row) gives a long error message:</p>\n\n<pre><code>JoblibTypeError                           Traceback (most recent call last)\n&lt;ipython-input-109-bcb5d85cabad&gt; in &lt;module&gt;()\n      2 param_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\n      3 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10)\n----&gt; 4 grid_result = grid.fit(X_train, Y_train)\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y)\n    802 \n    803         \"\"\"\n--&gt; 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n    805 \n    806 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in _fit(self, X, y, parameter_iterable)\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--&gt; 553                 for parameters in parameter_iterable\n    554                 for train, test in cv)\n    555 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\n    808                 # consumption.\n    809                 self._iterating = False\n--&gt; 810             self.retrieve()\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in retrieve(self)\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--&gt; 757                 raise exception\n    758 \n    759     def __call__(self, iterable):\n</code></pre>\n\n<p>[very many rows here]</p>\n\n<pre><code>...........................................................................\n    /home/buda/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py in fit(self=&lt;keras.wrappers.scikit_learn.KerasClassifier object&gt;, X=memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ....,  0.,  0., ...,  0.,  1.,  0.]], dtype=float32), **kwargs={})\n        132             self.model = self.__call__(**self.filter_sk_params(self.__call__))\n        133         elif not isinstance(self.build_fn, types.FunctionType):\n        134             self.model = self.build_fn(\n        135                 **self.filter_sk_params(self.build_fn.__call__))\n        136         else:\n    --&gt; 137             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n            self.model = undefined\n            self.build_fn = &lt;function create_model&gt;\n            self.filter_sk_params = &lt;bound method KerasClassifier.filter_sk_params o...as.wrappers.scikit_learn.KerasClassifier object&gt;&gt;\n        138 \n        139         loss_name = self.model.loss\n        140         if hasattr(loss_name, '__name__'):\n        141             loss_name = loss_name.__name__\n\n    ...........................................................................\n    /home/buda/Projects/Kaggle/Nerve/&lt;ipython-input-62-14daa2be96d1&gt; in create_model(optimizer='SGD')\n          1 \n          2 np.random.seed(100)\n          3 \n          4 def create_model(optimizer=\"SGD\"):\n    ----&gt; 5     model=Sequential()\n          6     model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n          7     model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n          8 \n          9     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n         10     return model\n\n    TypeError: __init__() takes at least 2 arguments (4 given)\n</code></pre>\n\n<p>What can be the error here?</p>\n\n<p>The full code is the following:</p>\n\n<pre><code>(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train=X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]).astype(\"float32\")\nX_test=X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]).astype(\"float32\")\n\nY_train=np_utils.to_categorical(y_train,10).astype(\"float32\")\nY_test=np_utils.to_categorical(y_test,10).astype(\"float32\")\n\nnp.random.seed(100)\n\ndef create_model(optimizer=\"SGD\"):\n    model=Sequential()\n    model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n    model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model, nb_epoch=50, batch_size=10, verbose=0)\nparam_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10)\ngrid_result = grid.fit(X_train, Y_train, param_grid)\n</code></pre>\n\n<p>With n_jobs=1 in GridSearchCV:</p>\n\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-110-538ac4724f14&gt; in &lt;module&gt;()\n      2 param_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\n      3 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=10)\n----&gt; 4 grid_result = grid.fit(X_train, Y_train)\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y)\n    802 \n    803         \"\"\"\n--&gt; 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n    805 \n    806 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in _fit(self, X, y, parameter_iterable)\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--&gt; 553                 for parameters in parameter_iterable\n    554                 for train, test in cv)\n    555 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\n    798             # was dispatched. In particular this covers the edge\n    799             # case of Parallel used with an exhausted iterator.\n--&gt; 800             while self.dispatch_one_batch(iterator):\n    801                 self._iterating = True\n    802             else:\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in dispatch_one_batch(self, iterator)\n    656                 return False\n    657             else:\n--&gt; 658                 self._dispatch(tasks)\n    659                 return True\n    660 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in _dispatch(self, batch)\n    564 \n    565         if self._pool is None:\n--&gt; 566             job = ImmediateComputeBatch(batch)\n    567             self._jobs.append(job)\n    568             self.n_dispatched_batches += 1\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __init__(self, batch)\n    178         # Don't delay the application, to avoid keeping the input\n    179         # arguments in memory\n--&gt; 180         self.results = batch()\n    181 \n    182     def get(self):\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self)\n     70 \n     71     def __call__(self):\n---&gt; 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-&gt; 1531             estimator.fit(X_train, y_train, **fit_params)\n   1532 \n   1533     except Exception as e:\n\n/home/buda/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.pyc in fit(self, X, y, **kwargs)\n    135                 **self.filter_sk_params(self.build_fn.__call__))\n    136         else:\n--&gt; 137             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n    138 \n    139         loss_name = self.model.loss\n\n&lt;ipython-input-62-14daa2be96d1&gt; in create_model(optimizer)\n      3 def create_model(optimizer=\"SGD\"):\n      4     model=Sequential()\n----&gt; 5     model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n      6     model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n      7 \n\nTypeError: __init__() takes at least 2 arguments (4 given)\n</code></pre>\n <scikit-learn><keras><p><code>model.fit()</code> accepts only two parameters. Remove the third one in the last line of your code.</p>\n<p>The last line in your code is the result of above error. This is because the 'fit' method takes only two arguments i.e the data and the labels. In order to get rid of the above error, modify your code as following:</p>\n\n<pre><code>grid_result = grid.fit(X_train,Y_train)\n</code></pre>\n\n<p>After that you can perform various operations on your classifier such as :</p>\n\n<pre><code>best_model = grid_result.best_estimator_.model\nbest_param = grid_result.best_params_\n</code></pre>\n",
                "codes": [
                    [],
                    [
                        "grid_result = grid.fit(X_train,Y_train)\n",
                        "best_model = grid_result.best_estimator_.model\nbest_param = grid_result.best_params_\n"
                    ]
                ],
                "question_id:": "13727",
                "question_votes:": "",
                "question_text:": "<p>I try to build a NN classifier on the well-known MNIST image database with Sklearn's Grid Search according the following:</p>\n\n<pre><code>model = KerasClassifier(build_fn=create_model, verbose=1)\nparam_grid = dict(batch_size=[10, 50, 100, 250], nb_epoch=[10, 50, 100])\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\ngrid_result = grid.fit(X_train, y_train)\n</code></pre>\n\n<p>create_model is a function that builds the Neural Network Model. The fitting (last row) gives a long error message:</p>\n\n<pre><code>JoblibTypeError                           Traceback (most recent call last)\n&lt;ipython-input-109-bcb5d85cabad&gt; in &lt;module&gt;()\n      2 param_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\n      3 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10)\n----&gt; 4 grid_result = grid.fit(X_train, Y_train)\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y)\n    802 \n    803         \"\"\"\n--&gt; 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n    805 \n    806 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in _fit(self, X, y, parameter_iterable)\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--&gt; 553                 for parameters in parameter_iterable\n    554                 for train, test in cv)\n    555 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\n    808                 # consumption.\n    809                 self._iterating = False\n--&gt; 810             self.retrieve()\n    811             # Make sure that we get a last message telling us we are done\n    812             elapsed_time = time.time() - self._start_time\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in retrieve(self)\n    755                     # a working pool as they expect.\n    756                     self._initialize_pool()\n--&gt; 757                 raise exception\n    758 \n    759     def __call__(self, iterable):\n</code></pre>\n\n<p>[very many rows here]</p>\n\n<pre><code>...........................................................................\n    /home/buda/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.py in fit(self=&lt;keras.wrappers.scikit_learn.KerasClassifier object&gt;, X=memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=memmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ....,  0.,  0., ...,  0.,  1.,  0.]], dtype=float32), **kwargs={})\n        132             self.model = self.__call__(**self.filter_sk_params(self.__call__))\n        133         elif not isinstance(self.build_fn, types.FunctionType):\n        134             self.model = self.build_fn(\n        135                 **self.filter_sk_params(self.build_fn.__call__))\n        136         else:\n    --&gt; 137             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n            self.model = undefined\n            self.build_fn = &lt;function create_model&gt;\n            self.filter_sk_params = &lt;bound method KerasClassifier.filter_sk_params o...as.wrappers.scikit_learn.KerasClassifier object&gt;&gt;\n        138 \n        139         loss_name = self.model.loss\n        140         if hasattr(loss_name, '__name__'):\n        141             loss_name = loss_name.__name__\n\n    ...........................................................................\n    /home/buda/Projects/Kaggle/Nerve/&lt;ipython-input-62-14daa2be96d1&gt; in create_model(optimizer='SGD')\n          1 \n          2 np.random.seed(100)\n          3 \n          4 def create_model(optimizer=\"SGD\"):\n    ----&gt; 5     model=Sequential()\n          6     model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n          7     model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n          8 \n          9     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n         10     return model\n\n    TypeError: __init__() takes at least 2 arguments (4 given)\n</code></pre>\n\n<p>What can be the error here?</p>\n\n<p>The full code is the following:</p>\n\n<pre><code>(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train=X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]).astype(\"float32\")\nX_test=X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]).astype(\"float32\")\n\nY_train=np_utils.to_categorical(y_train,10).astype(\"float32\")\nY_test=np_utils.to_categorical(y_test,10).astype(\"float32\")\n\nnp.random.seed(100)\n\ndef create_model(optimizer=\"SGD\"):\n    model=Sequential()\n    model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n    model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model, nb_epoch=50, batch_size=10, verbose=0)\nparam_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10)\ngrid_result = grid.fit(X_train, Y_train, param_grid)\n</code></pre>\n\n<p>With n_jobs=1 in GridSearchCV:</p>\n\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-110-538ac4724f14&gt; in &lt;module&gt;()\n      2 param_grid = dict(optimizer=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'])\n      3 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=10)\n----&gt; 4 grid_result = grid.fit(X_train, Y_train)\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in fit(self, X, y)\n    802 \n    803         \"\"\"\n--&gt; 804         return self._fit(X, y, ParameterGrid(self.param_grid))\n    805 \n    806 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc in _fit(self, X, y, parameter_iterable)\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--&gt; 553                 for parameters in parameter_iterable\n    554                 for train, test in cv)\n    555 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\n    798             # was dispatched. In particular this covers the edge\n    799             # case of Parallel used with an exhausted iterator.\n--&gt; 800             while self.dispatch_one_batch(iterator):\n    801                 self._iterating = True\n    802             else:\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in dispatch_one_batch(self, iterator)\n    656                 return False\n    657             else:\n--&gt; 658                 self._dispatch(tasks)\n    659                 return True\n    660 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in _dispatch(self, batch)\n    564 \n    565         if self._pool is None:\n--&gt; 566             job = ImmediateComputeBatch(batch)\n    567             self._jobs.append(job)\n    568             self.n_dispatched_batches += 1\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __init__(self, batch)\n    178         # Don't delay the application, to avoid keeping the input\n    179         # arguments in memory\n--&gt; 180         self.results = batch()\n    181 \n    182     def get(self):\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self)\n     70 \n     71     def __call__(self):\n---&gt; 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n\n/home/buda/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\n   1529             estimator.fit(X_train, **fit_params)\n   1530         else:\n-&gt; 1531             estimator.fit(X_train, y_train, **fit_params)\n   1532 \n   1533     except Exception as e:\n\n/home/buda/anaconda2/lib/python2.7/site-packages/keras/wrappers/scikit_learn.pyc in fit(self, X, y, **kwargs)\n    135                 **self.filter_sk_params(self.build_fn.__call__))\n    136         else:\n--&gt; 137             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n    138 \n    139         loss_name = self.model.loss\n\n&lt;ipython-input-62-14daa2be96d1&gt; in create_model(optimizer)\n      3 def create_model(optimizer=\"SGD\"):\n      4     model=Sequential()\n----&gt; 5     model.add(Dense(input_dim=784, init=\"uniform\", activation=\"relu\"))\n      6     model.add(Dense(output_dim=10, activation=\"sigmoid\"))\n      7 \n\nTypeError: __init__() takes at least 2 arguments (4 given)\n</code></pre>\n",
                "tags": "<scikit-learn><keras>",
                "answers": [
                    [
                        "20411",
                        "2",
                        "13727",
                        "",
                        "",
                        "<p><code>model.fit()</code> accepts only two parameters. Remove the third one in the last line of your code.</p>\n",
                        "",
                        ""
                    ],
                    [
                        "13739",
                        "2",
                        "13727",
                        "",
                        "",
                        "<p>The last line in your code is the result of above error. This is because the 'fit' method takes only two arguments i.e the data and the labels. In order to get rid of the above error, modify your code as following:</p>\n\n<pre><code>grid_result = grid.fit(X_train,Y_train)\n</code></pre>\n\n<p>After that you can perform various operations on your classifier such as :</p>\n\n<pre><code>best_model = grid_result.best_estimator_.model\nbest_param = grid_result.best_params_\n</code></pre>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "False"
        }
    ]
}