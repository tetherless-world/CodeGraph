{
    "module": "numpy",
    "function": "numpy.linalg.linalg.inv",
    "stackoverflow": [
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "14962",
            "_score": 22.780369,
            "_source": {
                "title": "Issue in computing matrix inverse",
                "content": "Issue in computing matrix inverse <p>I need to compute inverse of a matrix that has very small values of the range of 10^-9. When I use numpy.linalg.inv it gives all entries of the inverse matrix as 0. I checked in the original matrix there are no rows/columns that are completely zero. Why is it happening and how can it be solved.</p>\n <numpy><matrix>",
                "codes": [],
                "question_id:": "50860",
                "question_votes:": "",
                "question_text:": "<p>I need to compute inverse of a matrix that has very small values of the range of 10^-9. When I use numpy.linalg.inv it gives all entries of the inverse matrix as 0. I checked in the original matrix there are no rows/columns that are completely zero. Why is it happening and how can it be solved.</p>\n",
                "tags": "<numpy><matrix>",
                "answers": []
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "18017",
            "_score": 20.18132,
            "_source": {
                "title": "Implementing Mahalanobis Distance in Python",
                "content": "Implementing Mahalanobis Distance in Python <p>I am trying to implementing Mahalanobis Distance from scratch\nbut I am getting an error-</p>\n\n<p>The formula for Mahalanobis Distance is-\n<a href=\"https://i.stack.imgur.com/1ZUVb.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1ZUVb.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Now my code is-</p>\n\n<pre><code>import numpy as np\n\ndef mahalanobis(x, y, cov=None):\n    x_mean = np.mean(x)\n    x_minus_mn_with_transpose =np.transpose(x- x_mean)\n    Covariance = covar(x, np.transpose(y))\n    inv_covmat = np.linalg.inv(Covariance)\n    x_minus_mn = x - x_mean\n    left_term = (x_minus_mn, inv_covmat)\n    D_square = np.dot(left_term, x_minus_mn_with_transpose)\n    return D_square\n\ndef covar(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    Cov_numerator = sum(((a - x_mean)*(b - y_mean)) for a, b in zip(x, y))\n    Cov_denomerator = len(x) - 1\n    Covariance = (Cov_numerator / Cov_denomerator)\n    return  Covariance\n\nimport pandas as pd\n\nfilepath = 'https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv'\ndf = pd.read_csv(filepath).iloc[:, [0,4,6]]\ndf.head()\n\nX = np.asarray(df[['carat', 'depth', 'price']].head(500).values)\nY =np.asarray(df[['carat', 'depth', 'price']].values)\n\nmahalanobis(X, Y)\n</code></pre>\n\n<p>Error-<a href=\"https://i.stack.imgur.com/6d8jK.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6d8jK.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Now I don't want to compute covariance using numpy by using <code>np.cov</code>. I want to implement covariance from scratch, whats the problem plz help. </p>\n <machine-learning><python><distance><jupyter><p>I haven't tried the rest of the code, but for the covariance, if you don't have a problem to use <code>np.dot</code>, something like that might work</p>\n\n<pre><code>def covar(x, y):\n    X = np.column_stack([x, y])\n    X -= X.mean(axis=0) \n    denomerator= len(X) - 1 \n    return np.dot(X.T, X.conj()) / denomerator\n</code></pre>\n",
                "codes": [
                    [
                        "def covar(x, y):\n    X = np.column_stack([x, y])\n    X -= X.mean(axis=0) \n    denomerator= len(X) - 1 \n    return np.dot(X.T, X.conj()) / denomerator\n"
                    ]
                ],
                "question_id:": "57491",
                "question_votes:": "",
                "question_text:": "<p>I am trying to implementing Mahalanobis Distance from scratch\nbut I am getting an error-</p>\n\n<p>The formula for Mahalanobis Distance is-\n<a href=\"https://i.stack.imgur.com/1ZUVb.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1ZUVb.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Now my code is-</p>\n\n<pre><code>import numpy as np\n\ndef mahalanobis(x, y, cov=None):\n    x_mean = np.mean(x)\n    x_minus_mn_with_transpose =np.transpose(x- x_mean)\n    Covariance = covar(x, np.transpose(y))\n    inv_covmat = np.linalg.inv(Covariance)\n    x_minus_mn = x - x_mean\n    left_term = (x_minus_mn, inv_covmat)\n    D_square = np.dot(left_term, x_minus_mn_with_transpose)\n    return D_square\n\ndef covar(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    Cov_numerator = sum(((a - x_mean)*(b - y_mean)) for a, b in zip(x, y))\n    Cov_denomerator = len(x) - 1\n    Covariance = (Cov_numerator / Cov_denomerator)\n    return  Covariance\n\nimport pandas as pd\n\nfilepath = 'https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv'\ndf = pd.read_csv(filepath).iloc[:, [0,4,6]]\ndf.head()\n\nX = np.asarray(df[['carat', 'depth', 'price']].head(500).values)\nY =np.asarray(df[['carat', 'depth', 'price']].values)\n\nmahalanobis(X, Y)\n</code></pre>\n\n<p>Error-<a href=\"https://i.stack.imgur.com/6d8jK.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6d8jK.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Now I don't want to compute covariance using numpy by using <code>np.cov</code>. I want to implement covariance from scratch, whats the problem plz help. </p>\n",
                "tags": "<machine-learning><python><distance><jupyter>",
                "answers": [
                    [
                        "57496",
                        "2",
                        "57491",
                        "",
                        "",
                        "<p>I haven't tried the rest of the code, but for the covariance, if you don't have a problem to use <code>np.dot</code>, something like that might work</p>\n\n<pre><code>def covar(x, y):\n    X = np.column_stack([x, y])\n    X -= X.mean(axis=0) \n    denomerator= len(X) - 1 \n    return np.dot(X.T, X.conj()) / denomerator\n</code></pre>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "15860",
            "_score": 19.65797,
            "_source": {
                "title": "Why does least squares linear regression perform so bad when switching from 2D to 3D line?",
                "content": "Why does least squares linear regression perform so bad when switching from 2D to 3D line? <p>The vector of coefficients that minimize least squares can be found like so:</p>\n\n<pre><code>beta = ((X'X)^-1)*X'y\n</code></pre>\n\n<p>Theoretically this result is true for any number of variables (columns of X).</p>\n\n<p>I have made two python notebooks: one for <a href=\"https://github.com/Pign4/ScratchML/blob/master/Linear%20Regression.ipynb\" rel=\"nofollow noreferrer\">univariate linear regression</a> and another one for <a href=\"https://github.com/Pign4/ScratchML/blob/master/Multivariate%20Linear%20Regression.ipynb\" rel=\"nofollow noreferrer\">multivariate linear regression</a>.</p>\n\n<p>The code is analogous for both notebooks:</p>\n\n<ul>\n<li>create a random line</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code># multivariate snippet. A part from the plotting part,\n# this is the only part of the code that differs between the two notebooks.\nbeta = np.random.randint(1,6,(3,))\nX = np.linspace([1, 3, 7],[1, 14, 23],100)\ny = X @ beta\n</code></pre>\n\n<ul>\n<li>create noise</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>mu, sigma = 0, 1\nnoise = np.random.normal(mu, sigma, 100)\n</code></pre>\n\n<ul>\n<li>add noise to y</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>y_noise = y + noise\n</code></pre>\n\n<ul>\n<li>try to fit a line trough y_noise</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>beta_ = np.linalg.inv(X.T @ X) @ X.T @ y_noise\ny_ = X @ beta_\n</code></pre>\n\n<ul>\n<li>compare y_ with y_noise</li>\n</ul>\n\n<p><img src=\"https://i.stack.imgur.com/7lneZ.png\" width=\"250\">\n<img src=\"https://i.stack.imgur.com/aEdeZ.png\" width=\"250\"></p>\n\n<p>Why do I get such a different result when fitting a 2D line and a 3D one?</p>\n <python><linear-regression><p>The matrix <span class=\"math-container\">$X^TX$</span> in your example is close to a singular:</p>\n\n<pre><code>from numpy.linalg import inv, det\n\nprint(det(X.T @ X))\n\n1.479949989718308e-06\n</code></pre>\n\n<p>which makes its inverse very large</p>\n\n<pre><code>inv(X.T @ X)\n\n[[ 4.83118746e+11  2.66548273e+11 -1.83251938e+11]\n [ 2.66548273e+11  1.47061116e+11 -1.01104517e+11]\n [-1.83251938e+11 -1.01104517e+11  6.95093558e+10]]\n</code></pre>\n\n<p>This is the reason of the error in your question. The following expression</p>\n\n<pre><code>inv(X.T @ X) @ X.T @ X\n</code></pre>\n\n<p>should produce an identity matrix. But in your example it is</p>\n\n<pre><code>[[ 1.2265625   1.10289171  4.8378709 ]\n [ 0.02282715  0.61713325  0.95782902]\n [-0.01519775  0.25675456  0.33339437]] \n</code></pre>\n\n<p>So, when you multiply this matrix by <code>beta</code> the result is far from <code>beta</code>.</p>\n\n<p>Another way to look at this singularity is that your 3D problem is not actually 3D. It is still 2D. That's why you have a line, not a 2D surface in your plot. It can be converted to a 2D representation by a transformation of coordinates. If you make a matrix for a proper 3D problem, say</p>\n\n<pre><code>np.random.seed(3)\nX = np.hstack((np.ones((100,1)),np.random.rand(100,2)*100))\n</code></pre>\n\n<p>then <code>inv(X.T @ X) @ X.T @ X</code> gives the correct result:</p>\n\n<pre><code>[[ 1.00000000e+00 -1.24233956e-13 -6.21724894e-14]\n [-1.90819582e-17  1.00000000e+00  6.38378239e-16]\n [-8.67361738e-18  9.28077060e-16  1.00000000e+00]]\n</code></pre>\n\n<p>and the formula <span class=\"math-container\">$\\beta=(X^TX)^{-1}X^Ty$</span> works well.</p>\n",
                "codes": [
                    [
                        "from numpy.linalg import inv, det\n\nprint(det(X.T @ X))\n\n1.479949989718308e-06\n",
                        "inv(X.T @ X)\n\n[[ 4.83118746e+11  2.66548273e+11 -1.83251938e+11]\n [ 2.66548273e+11  1.47061116e+11 -1.01104517e+11]\n [-1.83251938e+11 -1.01104517e+11  6.95093558e+10]]\n",
                        "inv(X.T @ X) @ X.T @ X\n",
                        "[[ 1.2265625   1.10289171  4.8378709 ]\n [ 0.02282715  0.61713325  0.95782902]\n [-0.01519775  0.25675456  0.33339437]] \n",
                        "np.random.seed(3)\nX = np.hstack((np.ones((100,1)),np.random.rand(100,2)*100))\n",
                        "[[ 1.00000000e+00 -1.24233956e-13 -6.21724894e-14]\n [-1.90819582e-17  1.00000000e+00  6.38378239e-16]\n [-8.67361738e-18  9.28077060e-16  1.00000000e+00]]\n"
                    ]
                ],
                "question_id:": "52843",
                "question_votes:": "4",
                "question_text:": "<p>The vector of coefficients that minimize least squares can be found like so:</p>\n\n<pre><code>beta = ((X'X)^-1)*X'y\n</code></pre>\n\n<p>Theoretically this result is true for any number of variables (columns of X).</p>\n\n<p>I have made two python notebooks: one for <a href=\"https://github.com/Pign4/ScratchML/blob/master/Linear%20Regression.ipynb\" rel=\"nofollow noreferrer\">univariate linear regression</a> and another one for <a href=\"https://github.com/Pign4/ScratchML/blob/master/Multivariate%20Linear%20Regression.ipynb\" rel=\"nofollow noreferrer\">multivariate linear regression</a>.</p>\n\n<p>The code is analogous for both notebooks:</p>\n\n<ul>\n<li>create a random line</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code># multivariate snippet. A part from the plotting part,\n# this is the only part of the code that differs between the two notebooks.\nbeta = np.random.randint(1,6,(3,))\nX = np.linspace([1, 3, 7],[1, 14, 23],100)\ny = X @ beta\n</code></pre>\n\n<ul>\n<li>create noise</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>mu, sigma = 0, 1\nnoise = np.random.normal(mu, sigma, 100)\n</code></pre>\n\n<ul>\n<li>add noise to y</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>y_noise = y + noise\n</code></pre>\n\n<ul>\n<li>try to fit a line trough y_noise</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>beta_ = np.linalg.inv(X.T @ X) @ X.T @ y_noise\ny_ = X @ beta_\n</code></pre>\n\n<ul>\n<li>compare y_ with y_noise</li>\n</ul>\n\n<p><img src=\"https://i.stack.imgur.com/7lneZ.png\" width=\"250\">\n<img src=\"https://i.stack.imgur.com/aEdeZ.png\" width=\"250\"></p>\n\n<p>Why do I get such a different result when fitting a 2D line and a 3D one?</p>\n",
                "tags": "<python><linear-regression>",
                "answers": [
                    [
                        "53620",
                        "2",
                        "52843",
                        "",
                        "",
                        "<p>The matrix <span class=\"math-container\">$X^TX$</span> in your example is close to a singular:</p>\n\n<pre><code>from numpy.linalg import inv, det\n\nprint(det(X.T @ X))\n\n1.479949989718308e-06\n</code></pre>\n\n<p>which makes its inverse very large</p>\n\n<pre><code>inv(X.T @ X)\n\n[[ 4.83118746e+11  2.66548273e+11 -1.83251938e+11]\n [ 2.66548273e+11  1.47061116e+11 -1.01104517e+11]\n [-1.83251938e+11 -1.01104517e+11  6.95093558e+10]]\n</code></pre>\n\n<p>This is the reason of the error in your question. The following expression</p>\n\n<pre><code>inv(X.T @ X) @ X.T @ X\n</code></pre>\n\n<p>should produce an identity matrix. But in your example it is</p>\n\n<pre><code>[[ 1.2265625   1.10289171  4.8378709 ]\n [ 0.02282715  0.61713325  0.95782902]\n [-0.01519775  0.25675456  0.33339437]] \n</code></pre>\n\n<p>So, when you multiply this matrix by <code>beta</code> the result is far from <code>beta</code>.</p>\n\n<p>Another way to look at this singularity is that your 3D problem is not actually 3D. It is still 2D. That's why you have a line, not a 2D surface in your plot. It can be converted to a 2D representation by a transformation of coordinates. If you make a matrix for a proper 3D problem, say</p>\n\n<pre><code>np.random.seed(3)\nX = np.hstack((np.ones((100,1)),np.random.rand(100,2)*100))\n</code></pre>\n\n<p>then <code>inv(X.T @ X) @ X.T @ X</code> gives the correct result:</p>\n\n<pre><code>[[ 1.00000000e+00 -1.24233956e-13 -6.21724894e-14]\n [-1.90819582e-17  1.00000000e+00  6.38378239e-16]\n [-8.67361738e-18  9.28077060e-16  1.00000000e+00]]\n</code></pre>\n\n<p>and the formula <span class=\"math-container\">$\\beta=(X^TX)^{-1}X^Ty$</span> works well.</p>\n",
                        "",
                        "2"
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "6280",
            "_score": 16.92517,
            "_source": {
                "title": "How to force weights to be non-negative in Linear regression only with Numpy",
                "content": "How to force weights to be non-negative in Linear regression only with Numpy <p>My question is the same as here:\n<a href=\"https://datascience.stackexchange.com/questions/18258/how-to-force-weights-to-be-non-negative-in-linear-regression\">How to force weights to be non-negative in Linear regression</a></p>\n\n<p>Except that I can only use Numpy (I cannot use Scipy or Scikit Learn).\nIndeed, I am running my Python script on a server which doesn't include these modules.</p>\n\n<p>Is there any solution ?</p>\n\n<p>Thank you very much!</p>\n <python><regression><numpy><p>The sklearn implementation of Lasso that can force non-negative weights (as in <a href=\"https://datascience.stackexchange.com/a/19791/24162\">this answer</a>) is based on the <a href=\"https://en.wikipedia.org/wiki/Coordinate_descent\" rel=\"nofollow noreferrer\">coordinate descent</a> algorithm. You can reimplement it, using for example coordinate-wise <a href=\"https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\" rel=\"nofollow noreferrer\">Newton method</a>. For simplicity, I did not inclide intercept into the model:</p>\n\n<pre><code>import numpy as np\n# generate the data\nnp.random.seed(1)\nn = 1000\nX = np.random.normal(size=(n, 5))\ny = np.dot(X, [0,0,1,2,3]) + np.random.normal(size=n, scale=3)\n# initial solution (with some negative weights):\nbeta = np.dot(np.linalg.inv(np.dot(X.transpose(),X)), np.dot(X.transpose(), y))\nprint(beta)\n# clip the solution from below with zero\nprev_beta = beta.copy()\nbeta = np.maximum(beta, 1)\n# improve the solution by restricted coordinate-wise Newton descent\nhessian = np.dot(X.transpose(), X)\nwhile not (prev_beta == beta).all():\n    prev_beta = beta.copy()\n    for i in range(len(beta)):\n        grad = np.dot(np.dot(X,beta)-y, X)\n        beta[i] = np.maximum(0, beta[i] - grad[i] / hessian[i,i])\nprint(beta)\n</code></pre>\n\n<p>This code will output initial and final beta's:</p>\n\n<pre><code>[-0.01404546 -0.02633036  1.06028543  1.99696564  2.93511618]\n[ 0.          0.          1.05919989  1.99673774  2.93442334]\n</code></pre>\n\n<p>You can see that OLS beta differ from your optimal beta not only in the first two coefficients (that have been negative), but the rest of coefficients were also adjusted.</p>\n",
                "codes": [
                    [
                        "import numpy as np\n# generate the data\nnp.random.seed(1)\nn = 1000\nX = np.random.normal(size=(n, 5))\ny = np.dot(X, [0,0,1,2,3]) + np.random.normal(size=n, scale=3)\n# initial solution (with some negative weights):\nbeta = np.dot(np.linalg.inv(np.dot(X.transpose(),X)), np.dot(X.transpose(), y))\nprint(beta)\n# clip the solution from below with zero\nprev_beta = beta.copy()\nbeta = np.maximum(beta, 1)\n# improve the solution by restricted coordinate-wise Newton descent\nhessian = np.dot(X.transpose(), X)\nwhile not (prev_beta == beta).all():\n    prev_beta = beta.copy()\n    for i in range(len(beta)):\n        grad = np.dot(np.dot(X,beta)-y, X)\n        beta[i] = np.maximum(0, beta[i] - grad[i] / hessian[i,i])\nprint(beta)\n",
                        "[-0.01404546 -0.02633036  1.06028543  1.99696564  2.93511618]\n[ 0.          0.          1.05919989  1.99673774  2.93442334]\n"
                    ]
                ],
                "question_id:": "24595",
                "question_votes:": "",
                "question_text:": "<p>My question is the same as here:\n<a href=\"https://datascience.stackexchange.com/questions/18258/how-to-force-weights-to-be-non-negative-in-linear-regression\">How to force weights to be non-negative in Linear regression</a></p>\n\n<p>Except that I can only use Numpy (I cannot use Scipy or Scikit Learn).\nIndeed, I am running my Python script on a server which doesn't include these modules.</p>\n\n<p>Is there any solution ?</p>\n\n<p>Thank you very much!</p>\n",
                "tags": "<python><regression><numpy>",
                "answers": [
                    [
                        "24630",
                        "2",
                        "24595",
                        "",
                        "",
                        "<p>The sklearn implementation of Lasso that can force non-negative weights (as in <a href=\"https://datascience.stackexchange.com/a/19791/24162\">this answer</a>) is based on the <a href=\"https://en.wikipedia.org/wiki/Coordinate_descent\" rel=\"nofollow noreferrer\">coordinate descent</a> algorithm. You can reimplement it, using for example coordinate-wise <a href=\"https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\" rel=\"nofollow noreferrer\">Newton method</a>. For simplicity, I did not inclide intercept into the model:</p>\n\n<pre><code>import numpy as np\n# generate the data\nnp.random.seed(1)\nn = 1000\nX = np.random.normal(size=(n, 5))\ny = np.dot(X, [0,0,1,2,3]) + np.random.normal(size=n, scale=3)\n# initial solution (with some negative weights):\nbeta = np.dot(np.linalg.inv(np.dot(X.transpose(),X)), np.dot(X.transpose(), y))\nprint(beta)\n# clip the solution from below with zero\nprev_beta = beta.copy()\nbeta = np.maximum(beta, 1)\n# improve the solution by restricted coordinate-wise Newton descent\nhessian = np.dot(X.transpose(), X)\nwhile not (prev_beta == beta).all():\n    prev_beta = beta.copy()\n    for i in range(len(beta)):\n        grad = np.dot(np.dot(X,beta)-y, X)\n        beta[i] = np.maximum(0, beta[i] - grad[i] / hessian[i,i])\nprint(beta)\n</code></pre>\n\n<p>This code will output initial and final beta's:</p>\n\n<pre><code>[-0.01404546 -0.02633036  1.06028543  1.99696564  2.93511618]\n[ 0.          0.          1.05919989  1.99673774  2.93442334]\n</code></pre>\n\n<p>You can see that OLS beta differ from your optimal beta not only in the first two coefficients (that have been negative), but the rest of coefficients were also adjusted.</p>\n",
                        "",
                        ""
                    ]
                ]
            },
            "good_match": "True"
        },
        {
            "_index": "datascience_stackexchange",
            "_type": "_doc",
            "_id": "14164",
            "_score": 10.654844,
            "_source": {
                "title": "Python memory error: compute inverse of large sized matrix",
                "content": "Python memory error: compute inverse of large sized matrix <p>I have a big matrix of size 200000 x 200000. I need to compute its inverse. But it gives out of memory error on using numpy.linalg.inv. Is there any way to compute the inverse of a large sized matrix.</p>\n <python><matrix><p>Well it does not matter the language you will use, I am surprised you can even store that matrice in memory... I had a similar problem while researching on kernel methods.</p>\n\n<ul>\n<li>First, to put in perspective how huge that matrix is, I will assume each element is 32 bit word: <span class=\"math-container\">$ 200,000 \\times 200,000 \\times 32 = 1.28 \\times 10^{12} $</span> bits or 149 GB</li>\n</ul>\n\n<p>The bitlenght is variable but even for a boolean matrix that is about 4,65 GB and you said your data is a bit more complex than that.</p>\n\n<h2>Solution 1</h2>\n\n<p>If you trying to solve a linear system there are many iterative solutions that might help you computing an 200,000 x 1 array aproximation of your system answer without having to store that absurdly large matrix in memory. But you should be aware that this might take a bit long because you might have to load information from the disk.</p>\n\n<h2>Solution 2</h2>\n\n<p>Another possible solution, if you really need that inverse is if that matrix can be expressed as sumation of outer products of vectors. If so, you can do this:</p>\n\n<p><span class=\"math-container\">$$\nM = \\sum v_i.u_i^T\n$$</span></p>\n\n<p>Get the first set element of the sum and invert it so you have <span class=\"math-container\">$M^{-1}_{bad aproximation}$</span> matrix by using pseudoinverse, instead of doing the outerproduct you can invert <span class=\"math-container\">$u_1$</span> and <span class=\"math-container\">$v_1$</span> and use <a href=\"https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse\" rel=\"nofollow noreferrer\">Moore-Penrose Inverse</a> property:</p>\n\n<p><span class=\"math-container\">$$\nM^{-1}_{bad aproximation} = (v_1.u_1)^\\dagger  = v_1^\\dagger . u_1^\\dagger\n$$</span></p>\n\n<p>Combine this with <a href=\"https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\" rel=\"nofollow noreferrer\">Sherman-Morrison formula</a> to add the other <span class=\"math-container\">$v_i$</span> and <span class=\"math-container\">$u_i$</span> vectors.</p>\n\n<p>Sherman-Morrison formula gives:\n<span class=\"math-container\">$$\n(A + uv^T)^{-1} = A^{-1} - \\dfrac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}\n$$</span></p>\n\n<h2>Solution 3</h2>\n\n<p>You might try using <a href=\"https://en.wikipedia.org/wiki/Schur_complement\" rel=\"nofollow noreferrer\">Schur Complement</a> which allows you to do the following:</p>\n\n<p>Given a matrix <span class=\"math-container\">$M$</span>, it can be expressed as \n<span class=\"math-container\">$$\nM =\n    \\begin{bmatrix}\n    A &amp; B  \\\\\n    C &amp; D  \\\\\n    \\end{bmatrix}\n$$</span>\nwhere <span class=\"math-container\">$A$</span> and <span class=\"math-container\">$B$</span> must be square you can solve the inverse of <span class=\"math-container\">$M$</span> blockwise as:</p>\n\n<p><span class=\"math-container\">$$\nM^{-1} =\n    \\begin{bmatrix}\n    (M/D)^{-1} &amp; (M/D)^{-1}BD  \\\\\n    -D^{-1}C(M/D)^{-1} &amp; (M/A)^{-1}  \\\\\n    \\end{bmatrix}\n$$</span>\nwhere <span class=\"math-container\">$(M/D)$</span> denotes the schur complement of block D of the Matrix M and is defines as</p>\n\n<p><span class=\"math-container\">$$\n(M/D) = A - BD^{-1}C\n$$</span></p>\n\n<p>also\n<span class=\"math-container\">$$\n(M/A) = D - BA^{-1}C\n$$</span></p>\n\n<p>If <span class=\"math-container\">$A$</span> or <span class=\"math-container\">$D$</span> are singular, the matrice <span class=\"math-container\">$M$</span> is singular and a pseudo inverse might be used.</p>\n\n<p>This can give a reduction from inverting a <span class=\"math-container\">$200,000 \\times 200,000$</span> matrice to inverting 4 <span class=\"math-container\">$50,000 \\times 50,000$</span> matrices and some matrice multiplication, also this can be implement in parallel applications and there is way to do this recursively given certain conditions (enabling a reduction greater than 4 in size)</p>\n\n<p>The <span class=\"math-container\">$50,000 \\times 50,000$</span> matrices store is only 9.3 GB each and will be easier to invert</p>\n\n<p>Hope it helps. Also you might want to search answers in <a href=\"https://math.stackexchange.com/\">Math Stack</a> or in Stack Overflow and even ask there.</p>\n\n<blockquote>\n  <p>Edit: Added another solution with Schur Complement</p>\n</blockquote>\n",
                "codes": [
                    []
                ],
                "question_id:": "47830",
                "question_votes:": "1",
                "question_text:": "<p>I have a big matrix of size 200000 x 200000. I need to compute its inverse. But it gives out of memory error on using numpy.linalg.inv. Is there any way to compute the inverse of a large sized matrix.</p>\n",
                "tags": "<python><matrix>",
                "answers": [
                    [
                        "47877",
                        "2",
                        "47830",
                        "",
                        "",
                        "<p>Well it does not matter the language you will use, I am surprised you can even store that matrice in memory... I had a similar problem while researching on kernel methods.</p>\n\n<ul>\n<li>First, to put in perspective how huge that matrix is, I will assume each element is 32 bit word: <span class=\"math-container\">$ 200,000 \\times 200,000 \\times 32 = 1.28 \\times 10^{12} $</span> bits or 149 GB</li>\n</ul>\n\n<p>The bitlenght is variable but even for a boolean matrix that is about 4,65 GB and you said your data is a bit more complex than that.</p>\n\n<h2>Solution 1</h2>\n\n<p>If you trying to solve a linear system there are many iterative solutions that might help you computing an 200,000 x 1 array aproximation of your system answer without having to store that absurdly large matrix in memory. But you should be aware that this might take a bit long because you might have to load information from the disk.</p>\n\n<h2>Solution 2</h2>\n\n<p>Another possible solution, if you really need that inverse is if that matrix can be expressed as sumation of outer products of vectors. If so, you can do this:</p>\n\n<p><span class=\"math-container\">$$\nM = \\sum v_i.u_i^T\n$$</span></p>\n\n<p>Get the first set element of the sum and invert it so you have <span class=\"math-container\">$M^{-1}_{bad aproximation}$</span> matrix by using pseudoinverse, instead of doing the outerproduct you can invert <span class=\"math-container\">$u_1$</span> and <span class=\"math-container\">$v_1$</span> and use <a href=\"https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse\" rel=\"nofollow noreferrer\">Moore-Penrose Inverse</a> property:</p>\n\n<p><span class=\"math-container\">$$\nM^{-1}_{bad aproximation} = (v_1.u_1)^\\dagger  = v_1^\\dagger . u_1^\\dagger\n$$</span></p>\n\n<p>Combine this with <a href=\"https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula\" rel=\"nofollow noreferrer\">Sherman-Morrison formula</a> to add the other <span class=\"math-container\">$v_i$</span> and <span class=\"math-container\">$u_i$</span> vectors.</p>\n\n<p>Sherman-Morrison formula gives:\n<span class=\"math-container\">$$\n(A + uv^T)^{-1} = A^{-1} - \\dfrac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}\n$$</span></p>\n\n<h2>Solution 3</h2>\n\n<p>You might try using <a href=\"https://en.wikipedia.org/wiki/Schur_complement\" rel=\"nofollow noreferrer\">Schur Complement</a> which allows you to do the following:</p>\n\n<p>Given a matrix <span class=\"math-container\">$M$</span>, it can be expressed as \n<span class=\"math-container\">$$\nM =\n    \\begin{bmatrix}\n    A &amp; B  \\\\\n    C &amp; D  \\\\\n    \\end{bmatrix}\n$$</span>\nwhere <span class=\"math-container\">$A$</span> and <span class=\"math-container\">$B$</span> must be square you can solve the inverse of <span class=\"math-container\">$M$</span> blockwise as:</p>\n\n<p><span class=\"math-container\">$$\nM^{-1} =\n    \\begin{bmatrix}\n    (M/D)^{-1} &amp; (M/D)^{-1}BD  \\\\\n    -D^{-1}C(M/D)^{-1} &amp; (M/A)^{-1}  \\\\\n    \\end{bmatrix}\n$$</span>\nwhere <span class=\"math-container\">$(M/D)$</span> denotes the schur complement of block D of the Matrix M and is defines as</p>\n\n<p><span class=\"math-container\">$$\n(M/D) = A - BD^{-1}C\n$$</span></p>\n\n<p>also\n<span class=\"math-container\">$$\n(M/A) = D - BA^{-1}C\n$$</span></p>\n\n<p>If <span class=\"math-container\">$A$</span> or <span class=\"math-container\">$D$</span> are singular, the matrice <span class=\"math-container\">$M$</span> is singular and a pseudo inverse might be used.</p>\n\n<p>This can give a reduction from inverting a <span class=\"math-container\">$200,000 \\times 200,000$</span> matrice to inverting 4 <span class=\"math-container\">$50,000 \\times 50,000$</span> matrices and some matrice multiplication, also this can be implement in parallel applications and there is way to do this recursively given certain conditions (enabling a reduction greater than 4 in size)</p>\n\n<p>The <span class=\"math-container\">$50,000 \\times 50,000$</span> matrices store is only 9.3 GB each and will be easier to invert</p>\n\n<p>Hope it helps. Also you might want to search answers in <a href=\"https://math.stackexchange.com/\">Math Stack</a> or in Stack Overflow and even ask there.</p>\n\n<blockquote>\n  <p>Edit: Added another solution with Schur Complement</p>\n</blockquote>\n",
                        "",
                        "1"
                    ]
                ]
            },
            "good_match": "True"
        }
    ]
}